{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14465,"status":"ok","timestamp":1756853352134,"user":{"displayName":"Jacob Wynne","userId":"04420788155919199366"},"user_tz":420},"id":"gGAU83p5R-7S","outputId":"4d061f2c-d688-48b0-8b79-b000df7b1918"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18662,"status":"ok","timestamp":1756853373831,"user":{"displayName":"Jacob Wynne","userId":"04420788155919199366"},"user_tz":420},"id":"PYcI4CmISmLa","outputId":"719a1c9c-1a35-4b27-be45-9c383b86a638"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sympy==1.12\n","  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.12/dist-packages (from sympy==1.12) (1.3.0)\n","Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sympy\n","  Attempting uninstall: sympy\n","    Found existing installation: sympy 1.13.3\n","    Uninstalling sympy-1.13.3:\n","      Successfully uninstalled sympy-1.13.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torch 2.8.0+cu126 requires sympy>=1.13.3, but you have sympy 1.12 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed sympy-1.12\n"]}],"source":["pip install sympy==1.12"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"elcqeS08ur99","outputId":"f4fa2c0c-ea34-463b-b2a5-7ebae34db41f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saving results to: /content/drive/MyDrive/cycformer_run/cycformer_sep2/cycformer/temperature_scaling/cyc_90/lambda_0\n","Collecting logits and labels...\n"]},{"output_type":"stream","name":"stderr","text":["Processing validation set: 100%|██████████| 20477/20477 [27:57<00:00, 12.20batch/s]\n"]},{"output_type":"stream","name":"stdout","text":["Before temperature - NLL: 5.511, ECE: 0.477\n","Optimizing temperature with asymmetric penalty (lambda=0)...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/optim/lbfgs.py:457: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n","Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n","  loss = float(closure())\n"]},{"output_type":"stream","name":"stdout","text":["Optimal temperature: 1.781\n","After temperature - NLL: 3.756, ECE: 0.263\n","Saved optimal temperature to: /content/drive/MyDrive/cycformer_run/cycformer_sep2/cycformer/temperature_scaling/cyc_90/lambda_0/optimal_temperature.pt\n","Saving results to: /content/drive/MyDrive/cycformer_run/cycformer_sep2/cycformer/temperature_scaling/cyc_90/lambda_10\n","Collecting logits and labels...\n"]},{"output_type":"stream","name":"stderr","text":["Processing validation set: 100%|██████████| 20477/20477 [27:57<00:00, 12.21batch/s]\n"]},{"output_type":"stream","name":"stdout","text":["Before temperature - NLL: 5.511, ECE: 0.477\n","Optimizing temperature with asymmetric penalty (lambda=10)...\n","Optimal temperature: 2.110\n","After temperature - NLL: 3.508, ECE: 0.189\n","Saved optimal temperature to: /content/drive/MyDrive/cycformer_run/cycformer_sep2/cycformer/temperature_scaling/cyc_90/lambda_10/optimal_temperature.pt\n","Saving results to: /content/drive/MyDrive/cycformer_run/cycformer_sep2/cycformer/temperature_scaling/cyc_90/lambda_20\n","Collecting logits and labels...\n"]},{"output_type":"stream","name":"stderr","text":["Processing validation set:  44%|████▍     | 9112/20477 [12:26<15:31, 12.20batch/s]"]}],"source":["import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, EsmForSequenceClassification\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pickle\n","\n","class TokenizedDataset(Dataset):\n","    def __init__(self, csv_file, tokenizer, label_mapper, max_length=1024):\n","        self.data = pd.read_csv(csv_file)\n","        self.tokenizer = tokenizer\n","        self.label_mapper = label_mapper\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        sequence = self.data.iloc[idx]['sequence']\n","        label = self.label_mapper[self.data.iloc[idx]['cycle']]\n","        inputs = self.tokenizer(sequence, return_tensors=\"pt\", padding='max_length',\n","                                truncation=True, max_length=self.max_length)\n","        return {\n","            'input_ids': inputs['input_ids'].squeeze(0),\n","            'label': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","class ModelWithTemperature(nn.Module):\n","    def __init__(self, model):\n","        super(ModelWithTemperature, self).__init__()\n","        self.model = model\n","        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n","\n","    def forward(self, input_ids):\n","        logits = self.model(input_ids).logits\n","        return self.temperature_scale(logits)\n","\n","    def temperature_scale(self, logits):\n","        return logits / self.temperature\n","\n","    def set_temperature(self, valid_loader, save_path, device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\", asym_lambda=50, output_dir=None):\n","        self.to(device)\n","        self.model.eval()\n","        nll_criterion = nn.CrossEntropyLoss().to(device)\n","\n","        logits_list, labels_list = [], []\n","\n","        print(\"Collecting logits and labels...\")\n","        with torch.no_grad():\n","            for batch in tqdm(valid_loader, desc=\"Processing validation set\", unit=\"batch\"):\n","                inputs = batch['input_ids'].to(device)\n","                labels = batch['label'].to(device)\n","                logits = self.model(inputs).logits\n","                logits_list.append(logits)\n","                labels_list.append(labels)\n","\n","        logits = torch.cat(logits_list, dim=0)\n","        labels = torch.cat(labels_list, dim=0)\n","\n","        before_nll = nll_criterion(logits, labels).item()\n","        before_ece = calculate_ece(logits, labels)\n","        print(f\"Before temperature - NLL: {before_nll:.3f}, ECE: {before_ece:.3f}\")\n","\n","        if output_dir:\n","            make_reliability_diagram(logits, labels, title=f\"Before Temperature Scaling\\nECE: {before_ece:.3f}\",\n","                                     save_path=os.path.join(output_dir, \"before_scaling.png\"))\n","        else:\n","            make_reliability_diagram(logits, labels, title=f\"Before Temperature Scaling\\nECE: {before_ece:.3f}\")\n","\n","        optimizer = optim.LBFGS([self.temperature], lr=0.01, max_iter=50)\n","\n","        def closure():\n","            optimizer.zero_grad()\n","            nll_loss = nll_criterion(self.temperature_scale(logits), labels)\n","            asym_loss = asymmetric_confidence_penalty(self.temperature_scale(logits), labels)\n","            loss = nll_loss + asym_lambda * asym_loss\n","            loss.backward()\n","            return loss\n","\n","        print(f\"Optimizing temperature with asymmetric penalty (lambda={asym_lambda})...\")\n","        optimizer.step(closure)\n","\n","        after_logits = self.temperature_scale(logits)\n","        after_nll = nll_criterion(after_logits, labels).item()\n","        after_ece = calculate_ece(after_logits, labels)\n","\n","        print(f\"Optimal temperature: {self.temperature.item():.3f}\")\n","        print(f\"After temperature - NLL: {after_nll:.3f}, ECE: {after_ece:.3f}\")\n","\n","        if output_dir:\n","            make_reliability_diagram(after_logits, labels, title=f\"After Temperature Scaling\\nECE: {after_ece:.3f}\",\n","                                     save_path=os.path.join(output_dir, \"after_scaling.png\"))\n","            with open(os.path.join(output_dir, \"metrics.txt\"), \"w\") as f:\n","                f.write(f\"Lambda: {asym_lambda}\\n\")\n","                f.write(f\"Before Temp - NLL: {before_nll:.3f}, ECE: {before_ece:.3f}\\n\")\n","                f.write(f\"After Temp - NLL: {after_nll:.3f}, ECE: {after_ece:.3f}\\n\")\n","\n","        torch.save(self.temperature.item(), save_path)\n","        print(f\"Saved optimal temperature to: {save_path}\")\n","\n","        return self\n","\n","def asymmetric_confidence_penalty(logits, labels, threshold=0.7, weight=2.0):\n","    softmaxes = F.softmax(logits, dim=1)\n","    confidences, predictions = torch.max(softmaxes, 1)\n","    is_correct = predictions.eq(labels)\n","    overconfident = confidences > threshold\n","    penalty = torch.where(\n","        is_correct,\n","        torch.zeros_like(confidences),\n","        (confidences - threshold) * overconfident.float() * weight\n","    )\n","    return penalty.mean()\n","\n","def calculate_ece(logits, labels, n_bins=15):\n","    softmaxes = F.softmax(logits, dim=1)\n","    confidences, predictions = torch.max(softmaxes, 1)\n","    accuracies = predictions.eq(labels)\n","\n","    bins = torch.linspace(0, 1, n_bins + 1)\n","    bin_lowers = bins[:-1]\n","    bin_uppers = bins[1:]\n","\n","    ece = 0.0\n","    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n","        in_bin = confidences.ge(bin_lower) * confidences.lt(bin_upper)\n","        prop_in_bin = in_bin.float().mean()\n","        if prop_in_bin.item() > 0:\n","            accuracy_in_bin = accuracies[in_bin].float().mean()\n","            avg_confidence_in_bin = confidences[in_bin].mean()\n","            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n","\n","    return ece.item()\n","\n","def make_reliability_diagram(logits, labels, n_bins=15, title=\"Reliability Diagram\", save_path=None):\n","    softmaxes = F.softmax(logits, dim=1)\n","    confidences, predictions = softmaxes.max(1)\n","    accuracies = predictions.eq(labels)\n","\n","    bins = torch.linspace(0, 1, n_bins + 1)\n","    width = 1.0 / n_bins\n","    bin_centers = np.linspace(0, 1.0 - width, n_bins) + width / 2\n","    bin_indices = [confidences.ge(bin_lower) * confidences.lt(bin_upper) for bin_lower, bin_upper in zip(bins[:-1], bins[1:])]\n","\n","    bin_corrects = np.array([torch.mean(accuracies[bin_idx].float()).item() if bin_idx.any() else 0 for bin_idx in bin_indices])\n","    bin_scores = np.array([torch.mean(confidences[bin_idx].float()).item() if bin_idx.any() else 0 for bin_idx in bin_indices])\n","\n","    plt.figure(figsize=(8, 8))\n","    gap = (bin_scores - bin_corrects)\n","    plt.bar(bin_centers, bin_corrects, width=width, alpha=0.5, ec='black', label='Outputs')\n","    plt.bar(bin_centers, gap, bottom=bin_corrects, color='red', alpha=0.5, width=width, hatch='//', edgecolor='r', label='Gap')\n","    plt.plot([0, 1], [0, 1], '--', color='gray')\n","    plt.legend(loc='best', fontsize='small')\n","\n","    ece = calculate_ece(logits, labels, n_bins)\n","    bbox_props = dict(boxstyle=\"round\", fc=\"lightgrey\", ec=\"brown\", lw=2)\n","    plt.text(0.2, 0.85, f\"ECE: {ece:.2f}\", ha=\"center\", va=\"center\", size=20, weight='bold', bbox=bbox_props)\n","\n","    plt.title(title, size=20)\n","    plt.ylabel(\"Accuracy (P[y])\", size=18)\n","    plt.xlabel(\"Confidence\", size=18)\n","    plt.xlim(0, 1)\n","    plt.ylim(0, 1)\n","\n","    if save_path:\n","        plt.savefig(save_path, dpi=600, bbox_inches=\"tight\")\n","    else:\n","        plt.show()\n","    plt.close()\n","\n","if __name__ == \"__main__\":\n","    model_path = \"/content/drive/MyDrive/cycformer_run/cycformer_sep2/cycformer/models/cyc_90\"\n","    validation_csv = \"/content/drive/MyDrive/cycformer_run/cycformer_sep2/cycformer/data/validation_files/final_selected_val_90.csv\"\n","    pickle_path = \"/content/drive/MyDrive/cycformer_run/cycformer_sep2/cycformer/models/cyc_id_maps/cyc_label_id_map_90.pickle\"\n","    tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8m_UR50D\")\n","    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","    with open(pickle_path, 'rb') as f:\n","        label_mapper = pickle.load(f)\n","    label_mapper = {v: k for k, v in label_mapper.items()}\n","\n","    model = EsmForSequenceClassification.from_pretrained(model_path, num_labels=len(label_mapper))\n","    model_with_temp = ModelWithTemperature(model)\n","\n","    val_dataset = TokenizedDataset(validation_csv, tokenizer, label_mapper)\n","    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n","\n","\n","    base_temp_dir = \"/content/drive/MyDrive/cycformer_run/cycformer_sep2/cycformer/temperature_scaling/cyc_90\"\n","    lambda_list = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n","\n","    for asym_lambda in lambda_list:\n","      output_dir = os.path.join(base_temp_dir, f\"lambda_{asym_lambda}\")\n","      os.makedirs(output_dir, exist_ok=True)\n","      temp_save_path = os.path.join(output_dir, \"optimal_temperature.pt\")\n","\n","      print(f\"Saving results to: {output_dir}\")\n","\n","      model_with_temp.set_temperature(\n","          val_loader,\n","          save_path=temp_save_path,\n","          device=device,\n","          asym_lambda=asym_lambda,\n","          output_dir=output_dir\n","      )\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1J_XfwOrwSHRT5uWuQkoLq76e9NQiznce","timestamp":1756852912238},{"file_id":"1YaVjgEn5hQl8fpJgpUMadby1EqkVdbx4","timestamp":1747763126926},{"file_id":"1BgLPAigcgs4f5Xqd1zM1St7wgEjMjzU7","timestamp":1747685035246},{"file_id":"103_0XRq1YLPKwVLpX49vCYq8m4eTWdVz","timestamp":1743651842921}],"authorship_tag":"ABX9TyMdsx9tPPj3SzxPrwom1im1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}