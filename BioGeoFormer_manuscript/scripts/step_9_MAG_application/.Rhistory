# Apply the F1 score calculation function to each dataframe in the merged data list
f1_scores <- sapply(merged_data_list, compute_f1_score)
# View F1 scores for each similarity level
f1_scores
library(dplyr)
# Function to calculate true positives for each dataframe
calculate_true_positives <- function(df) {
if ("predicted_cycle" %in% colnames(df) & "cycle" %in% colnames(df)) {
# Count the rows where predicted_cycle matches cycle
true_positives <- sum(df$predicted_cycle == df$cycle, na.rm = TRUE)
} else {
true_positives <- NA  # Assign NA if necessary columns are missing
}
return(true_positives)
}
# Apply the true positive calculation function to each dataframe in the merged data list
true_positives <- sapply(merged_data_list, calculate_true_positives)
# View true positives count for each similarity level
true_positives
library(dplyr)
# Function to calculate TP, TN, FP, FN for each dataframe
calculate_confusion_matrix <- function(df) {
if ("predicted_cycle" %in% colnames(df) & "cycle" %in% colnames(df)) {
# Get the unique classes present in both actual and predicted cycles
classes <- unique(c(df$cycle, df$predicted_cycle))
# Initialize counters for TP, TN, FP, FN
tp <- 0
tn <- 0
fp <- 0
fn <- 0
# Calculate TP, FP, FN for each class
for (class in classes) {
tp <- tp + sum(df$predicted_cycle == class & df$cycle == class, na.rm = TRUE)
fp <- fp + sum(df$predicted_cycle == class & df$cycle != class, na.rm = TRUE)
fn <- fn + sum(df$predicted_cycle != class & df$cycle == class, na.rm = TRUE)
}
# Calculate TN by counting non-matching class instances
tn <- nrow(df) - (tp + fp + fn)
# Return a list with the counts for each metric
return(list(True_Positive = tp, True_Negative = tn, False_Positive = fp, False_Negative = fn))
} else {
# Return NA if necessary columns are missing
return(list(True_Positive = NA, True_Negative = NA, False_Positive = NA, False_Negative = NA))
}
}
# Apply the confusion matrix calculation function to each dataframe in the merged data list
confusion_matrices <- lapply(merged_data_list, calculate_confusion_matrix)
# View the confusion matrices for each similarity level
confusion_matrices
library(dplyr)
# Merge each pair of dataframes from selected_hmm_data_split and csv_data_list
merged_data_list <- lapply(names(selected_hmm_data_split), function(similarity) {
hmm_df <- selected_hmm_data_split[[similarity]]
csv_df <- csv_data_list[[similarity]]
# Perform a full join on target_name and id
full_join(hmm_df, csv_df, by = c("target_name" = "id"))
})
# Name the merged data list by similarity levels
names(merged_data_list) <- names(selected_hmm_data_split)
# View the first few rows of the merged dataframe for a specific similarity level (e.g., "90")
head(merged_data_list[["90"]])
library(dplyr)
# Function to calculate true positives for each dataframe
calculate_true_positives <- function(df) {
if ("predicted_cycle" %in% colnames(df) & "cycle" %in% colnames(df)) {
# Count the rows where predicted_cycle matches cycle
true_positives <- sum(df$predicted_cycle == df$cycle, na.rm = TRUE)
} else {
true_positives <- NA  # Assign NA if necessary columns are missing
}
return(true_positives)
}
# Apply the true positive calculation function to each dataframe in the merged data list
true_positives <- sapply(merged_data_list, calculate_true_positives)
# View true positives count for each similarity level
true_positives
library(dplyr)
library(dplyr)
# Function to calculate confusion matrix metrics
calculate_metrics <- function(df) {
if ("predicted_cycle" %in% colnames(df) & "cycle" %in% colnames(df)) {
# Get unique classes in both actual and predicted cycles
classes <- unique(c(df$cycle, df$predicted_cycle))
# Initialize counters for TP, TN, FP, FN
tp <- 0
tn <- 0
fp <- 0
fn <- 0
# Calculate TP, FP, FN for each class
for (class in classes) {
tp <- tp + sum(df$predicted_cycle == class & df$cycle == class, na.rm = TRUE)
fp <- fp + sum(df$predicted_cycle == class & df$cycle != class, na.rm = TRUE)
fn <- fn + sum(df$predicted_cycle != class & df$cycle == class, na.rm = TRUE)
}
# Calculate TN by counting remaining rows
tn <- nrow(df) - (tp + fp + fn)
# Calculate metrics
accuracy <- (tp + tn) / (tp + tn + fp + fn)
precision <- ifelse((tp + fp) > 0, tp / (tp + fp), NA)
recall <- ifelse((tp + fn) > 0, tp / (tp + fn), NA)
f1_score <- ifelse(!is.na(precision) & !is.na(recall), 2 * (precision * recall) / (precision + recall), NA)
mcc <- ifelse((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) > 0,
(tp * tn - fp * fn) / sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)),
NA)
# Return a list of calculated metrics
return(list(Accuracy = accuracy, F1_Score = f1_score, MCC = mcc))
} else {
return(list(Accuracy = NA, F1_Score = NA, MCC = NA))  # Return NA if columns are missing
}
}
# Apply the metrics calculation function to each dataframe in the merged data list
metrics <- lapply(merged_data_list, calculate_metrics)
# View metrics for each similarity level
metrics
library(ggplot2)
library(dplyr)
library(tidyr)
metrics_long <- metrics %>%
pivot_longer(cols = -Similarity, names_to = "Metric", values_to = "Value")
metrics_df <- bind_rows(metrics, .id = "Similarity") %>%
mutate(Similarity = as.numeric(Similarity))
# Reshape the data to long format for plotting
metrics_long <- metrics_df %>%
pivot_longer(cols = -Similarity, names_to = "Metric", values_to = "Value")
# Plot each metric across similarity scores
ggplot(metrics_long, aes(x = Similarity, y = Value, color = Metric)) +
geom_line(size = 1) +
geom_point(size = 2) +
labs(title = "Metrics Across Similarity Scores",
x = "Similarity Score",
y = "Metric Value") +
theme_minimal() +
theme(legend.title = element_blank())
output_dir <- "Dropbox/cycle_split_check/ml_nima/hmm_val_output"
# Write each dataframe in merged_data_list to a .csv file
lapply(names(merged_data_list), function(similarity) {
# Define the filename for each similarity level
file_name <- paste0(output_dir, "/merged_data_", similarity, ".csv")
# Write the dataframe to a .csv file
write.csv(merged_data_list[[similarity]], file_name, row.names = FALSE)
})
merged_data_list <- lapply(merged_data_list, function(df) {
df$predicted_cycle[is.na(df$predicted_cycle)] <- "unassigned"
return(df)
})
library(dplyr)
# Function to calculate true positives for each dataframe
calculate_true_positives <- function(df) {
if ("predicted_cycle" %in% colnames(df) & "cycle" %in% colnames(df)) {
# Count the rows where predicted_cycle matches cycle
true_positives <- sum(df$predicted_cycle == df$cycle, na.rm = TRUE)
} else {
true_positives <- NA  # Assign NA if necessary columns are missing
}
return(true_positives)
}
# Apply the true positive calculation function to each dataframe in the merged data list
true_positives <- sapply(merged_data_list, calculate_true_positives)
# View true positives count for each similarity level
true_positives
merged_data_list
View(merged_data_list[["40"]])
# View the first few rows of the merged dataframe for a specific similarity level (e.g., "90")
head(merged_data_list[["90"]])
merged_data_list <- lapply(merged_data_list, function(df) {
df$predicted_cycle[is.na(df$predicted_cycle)] <- "unassigned"
return(df)
})
output_dir <- "Dropbox/cycle_split_check/ml_nima/hmm_val_output"
# Write each dataframe in merged_data_list to a .csv file
lapply(names(merged_data_list), function(similarity) {
# Define the filename for each similarity level
file_name <- paste0(output_dir, "/merged_data_", similarity, ".csv")
# Write the dataframe to a .csv file
write.csv(merged_data_list[[similarity]], file_name, row.names = FALSE)
})
library(dplyr)
# Function to calculate true positives for each dataframe
calculate_true_positives <- function(df) {
if ("predicted_cycle" %in% colnames(df) & "cycle" %in% colnames(df)) {
# Count the rows where predicted_cycle matches cycle
true_positives <- sum(df$predicted_cycle == df$cycle, na.rm = TRUE)
} else {
true_positives <- NA  # Assign NA if necessary columns are missing
}
return(true_positives)
}
# Apply the true positive calculation function to each dataframe in the merged data list
true_positives <- sapply(merged_data_list, calculate_true_positives)
# View true positives count for each similarity level
true_positives
library(dplyr)
library(dplyr)
# Function to calculate confusion matrix metrics
calculate_metrics <- function(df) {
if ("predicted_cycle" %in% colnames(df) & "cycle" %in% colnames(df)) {
# Get unique classes in both actual and predicted cycles
classes <- unique(c(df$cycle, df$predicted_cycle))
# Initialize counters for TP, TN, FP, FN
tp <- 0
tn <- 0
fp <- 0
fn <- 0
# Calculate TP, FP, FN for each class
for (class in classes) {
tp <- tp + sum(df$predicted_cycle == class & df$cycle == class, na.rm = TRUE)
fp <- fp + sum(df$predicted_cycle == class & df$cycle != class, na.rm = TRUE)
fn <- fn + sum(df$predicted_cycle != class & df$cycle == class, na.rm = TRUE)
}
# Calculate TN by counting remaining rows
tn <- nrow(df) - (tp + fp + fn)
# Calculate metrics
accuracy <- (tp + tn) / (tp + tn + fp + fn)
precision <- ifelse((tp + fp) > 0, tp / (tp + fp), NA)
recall <- ifelse((tp + fn) > 0, tp / (tp + fn), NA)
f1_score <- ifelse(!is.na(precision) & !is.na(recall), 2 * (precision * recall) / (precision + recall), NA)
mcc <- ifelse((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) > 0,
(tp * tn - fp * fn) / sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)),
NA)
# Return a list of calculated metrics
return(list(Accuracy = accuracy, F1_Score = f1_score, MCC = mcc))
} else {
return(list(Accuracy = NA, F1_Score = NA, MCC = NA))  # Return NA if columns are missing
}
}
# Apply the metrics calculation function to each dataframe in the merged data list
metrics <- lapply(merged_data_list, calculate_metrics)
# View metrics for each similarity level
metrics
library(dplyr)
# Function to calculate accuracy, F1 score, and MCC for each dataframe
calculate_metrics <- function(df) {
if ("predicted_cycle" %in% colnames(df) & "cycle" %in% colnames(df)) {
# Get unique classes in both actual and predicted cycles
classes <- unique(c(df$cycle, df$predicted_cycle))
# Initialize counters for TP, TN, FP, FN
tp <- 0
tn <- 0
fp <- 0
fn <- 0
# Calculate TP, FP, FN for each class
for (class in classes) {
tp <- tp + sum(df$predicted_cycle == class & df$cycle == class, na.rm = TRUE)
fp <- fp + sum(df$predicted_cycle == class & df$cycle != class, na.rm = TRUE)
fn <- fn + sum(df$predicted_cycle != class & df$cycle == class, na.rm = TRUE)
}
# Calculate TN by counting non-matching class instances
tn <- nrow(df) - (tp + fp + fn)
# Calculate metrics
accuracy <- (tp + tn) / (tp + tn + fp + fn)
precision <- ifelse((tp + fp) > 0, tp / (tp + fp), NA)
recall <- ifelse((tp + fn) > 0, tp / (tp + fn), NA)
f1_score <- ifelse(!is.na(precision) & !is.na(recall), 2 * (precision * recall) / (precision + recall), NA)
mcc <- ifelse((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) > 0,
(tp * tn - fp * fn) / sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)),
NA)
# Return a list of calculated metrics
return(list(Accuracy = accuracy, F1_Score = f1_score, MCC = mcc))
} else {
return(list(Accuracy = NA, F1_Score = NA, MCC = NA))  # Return NA if columns are missing
}
}
# Apply the metrics calculation function to each dataframe in the merged data list
metrics <- lapply(merged_data_list, calculate_metrics)
# View metrics for each similarity level
metrics
install.packages("yardstick")
library(dplyr)
library(yardstick)
# Define a function to calculate metrics using yardstick
calculate_metrics_yardstick <- function(df) {
if ("predicted_cycle" %in% colnames(df) & "cycle" %in% colnames(df)) {
# Convert to a tibble and specify `predicted_cycle` and `cycle` as factors
df <- df %>%
mutate(predicted_cycle = as.factor(predicted_cycle),
cycle = as.factor(cycle))
# Calculate metrics using yardstick functions
accuracy <- accuracy(df, truth = cycle, estimate = predicted_cycle)$.estimate
f1_score <- f_meas(df, truth = cycle, estimate = predicted_cycle, estimator = "macro")$.estimate
mcc <- mcc(df, truth = cycle, estimate = predicted_cycle)$.estimate
# Return a list with the metrics
return(list(Accuracy = accuracy, F1_Score = f1_score, MCC = mcc))
} else {
return(list(Accuracy = NA, F1_Score = NA, MCC = NA))  # Return NA if columns are missing
}
}
# Apply the metrics calculation function to each dataframe in merged_data_list
metrics <- lapply(merged_data_list, calculate_metrics_yardstick)
library(dplyr)
library(stringr)
# Specify the path to your folder of HMM output files
folder_path <- "Dropbox/cycle_split_check/val_output/"
# Get a list of all HMM output files in the folder
files <- list.files(folder_path, pattern = "*.txt", full.names = TRUE)
# Function to read and clean a single HMM output file
read_hmm_file <- function(file_path) {
hmm_data <- read.table(file_path, header = FALSE, sep = "", fill = TRUE, skip = 3, comment.char = "")
# Specify column names
colnames(hmm_data) <- c("target_name", "accession", "query_name", "query_accession",
"E_value_full_seq", "score_full_seq", "bias_full_seq",
"E_value_best_domain", "score_best_domain", "bias_best_domain",
"exp", "reg", "clu", "ov", "env", "dom", "rep", "inc",
"description")
# Remove footer lines by filtering out rows that start with '#'
hmm_data <- filter(hmm_data, target_name != "#")
# Extract sequence similarity from the query name
hmm_data$similarity <- str_extract(hmm_data$query_name, "\\d+")
# Add a column with the file name for traceability
hmm_data$file <- basename(file_path)
return(hmm_data)
}
# Apply the function to all files and combine into one dataframe
all_hmm_data <- files %>%
lapply(read_hmm_file) %>%
bind_rows()
# Split the combined dataframe into a list of dataframes based on similarity
hmm_data_split <- split(all_hmm_data, all_hmm_data$similarity)
# View one of the split dataframes (e.g., for similarity level "40")
head(hmm_data_split[["40"]])
hmm_data_split <- lapply(hmm_data_split, function(df) {
df %>%
mutate(predicted_cycle = str_extract(query_name, "(?<=_\\d{2}_).+"))
})
filtered_unique_hmm_data_split <- lapply(hmm_data_split, function(df) {
df %>%
filter(E_value_best_domain <= 1e-5 & score_best_domain >= 25) %>% # Apply E-value and score filters
group_by(target_name) %>%                                         # Group by target_name
slice_max(score_best_domain, n = 1, with_ties = FALSE) %>%        # Keep only the row with the highest domain score
ungroup()                                                         # Ungroup to finalize the dataframe
})
filtered_unique_hmm_data_split <- lapply(hmm_data_split, function(df) {
df %>%
filter(E_value_best_domain <= 1e-5 & score_best_domain >= 25) %>% # Apply E-value and score filters
group_by(target_name) %>%                                         # Group by target_name
slice_max(score_best_domain, n = 1, with_ties = FALSE) %>%        # Keep only the row with the highest domain score
ungroup()                                                         # Ungroup to finalize the dataframe
})
library(ggplot2)
ggplot(filtered_unique_hmm_data_split[["90"]], aes(x = score_best_domain)) +
geom_histogram(binwidth = 50, color = "black", fill = "skyblue") +
labs(title = "Histogram of score_best_domain",
x = "score_best_domain",
y = "Frequency") +
theme_minimal()
selected_hmm_data_split <- lapply(filtered_unique_hmm_data_split, function(df) {
df %>% select(target_name, predicted_cycle)
})
library(stringr)
# Specify the path to your folder containing the .csv files
folder_path <- "/Users/wynne/Dropbox/cycle_split_check/ml_nima/ml_nima/val"
# List all .csv files in the directory
csv_files <- list.files(folder_path, pattern = "\\.csv$", full.names = TRUE)
# Read each .csv file into a dataframe and store them in a list
csv_data_list <- lapply(csv_files, read.csv)
# Extract similarity score from filenames and set it as the name for each dataframe
similarity_labels <- str_extract(basename(csv_files), "\\d+")
names(csv_data_list) <- similarity_labels
# View the first few rows of one of the dataframes (e.g., for similarity level "90")
head(csv_data_list[["90"]])
library(dplyr)
# Merge each pair of dataframes from selected_hmm_data_split and csv_data_list
merged_data_list <- lapply(names(selected_hmm_data_split), function(similarity) {
hmm_df <- selected_hmm_data_split[[similarity]]
csv_df <- csv_data_list[[similarity]]
# Perform a full join on target_name and id
full_join(hmm_df, csv_df, by = c("target_name" = "id"))
})
# Name the merged data list by similarity levels
names(merged_data_list) <- names(selected_hmm_data_split)
# View the first few rows of the merged dataframe for a specific similarity level (e.g., "90")
head(merged_data_list[["90"]])
merged_data_list <- lapply(merged_data_list, function(df) {
df$predicted_cycle[is.na(df$predicted_cycle)] <- "unassigned"
return(df)
})
output_dir <- "Dropbox/cycle_split_check/ml_nima/hmm_val_output"
# Write each dataframe in merged_data_list to a .csv file
lapply(names(merged_data_list), function(similarity) {
# Define the filename for each similarity level
file_name <- paste0(output_dir, "/merged_data_", similarity, ".csv")
# Write the dataframe to a .csv file
write.csv(merged_data_list[[similarity]], file_name, row.names = FALSE)
})
# Parameters for the simulation
initial_cells <- 1  # Starting number of cells
division_time <- 15  # Time it takes for one cell division (in hours)
simulation_time <- 7200  # Total simulation time (in hours)
time_step <- 24  # Time step for recording (in hours)
# Initialize variables
time <- seq(0, simulation_time, by = time_step)
cell_count <- initial_cells * 2^(time / division_time)
# Create a data frame for the results
simulation_results <- data.frame(
Time_Hours = time,
Cell_Count = cell_count
)
# Print the first few rows of the simulation
print(head(simulation_results))
# Plot the simulation results
plot(
simulation_results$Time_Hours,
simulation_results$Cell_Count,
type = "o",
col = "blue",
xlab = "Time (hours)",
ylab = "Number of Cells",
main = "Cell Division Simulation (Hours)"
)
# Parameters for the simulation
initial_cells <- 1  # Starting number of cells
division_time <- 15  # Time it takes for one cell division (in hours)
simulation_time <- 500  # Total simulation time (in hours)
time_step <- 24  # Time step for recording (in hours)
# Initialize variables
time <- seq(0, simulation_time, by = time_step)
cell_count <- initial_cells * 2^(time / division_time)
# Create a data frame for the results
simulation_results <- data.frame(
Time_Hours = time,
Cell_Count = cell_count
)
# Print the first few rows of the simulation
print(head(simulation_results))
# Plot the simulation results
plot(
simulation_results$Time_Hours,
simulation_results$Cell_Count,
type = "o",
col = "blue",
xlab = "Time (hours)",
ylab = "Number of Cells",
main = "Cell Division Simulation (Hours)"
)
# Parameters for the simulation
initial_cells <- 1  # Starting number of cells
division_time <- 15  # Time it takes for one cell division (in hours)
simulation_time <- 7200  # Total simulation time (in hours) (set to 300 days)
time_step <- 24  # Time step for recording (in hours)
# Initialize variables
time <- seq(0, simulation_time, by = time_step)
cell_count <- initial_cells * 2^(time / division_time)
# Create a data frame for the results
simulation_results <- data.frame(
Time_Hours = time,
Cell_Count = cell_count
)
# Print the first few rows of the simulation
print(head(simulation_results))
# Plot the simulation results
plot(
simulation_results$Time_Hours,
simulation_results$Cell_Count,
type = "o",
col = "blue",
xlab = "Time (hours)",
ylab = "Number of Cells",
main = "Cell Division Simulation (Hours)"
)
read.csv("~/Dropbox/deep_sea_MAGs_public/mags_annotated/MAGS_annotated_70_aa_fna.csv")
mags <- read.csv("~/Dropbox/deep_sea_MAGs_public/mags_annotated/MAGS_annotated_70_aa_fna.csv")
View(mags)
mod_id <- "M00001"  # Example module: Glycolysis, core module
mod_info <- keggGet(mod_id)[[1]]
library(KEGGREST)
mod_id <- "M00001"  # Example module: Glycolysis, core module
mod_info <- keggGet(mod_id)[[1]]
cat(mod_info$DEFINITION)
cat(mod_info$CLASS)
cat(mod_info$GENE)
cat(mod_info$ORTHOLOGY)
View(mod_info)
mod_info[["REACTION"]]
mod_info[["DEFINITION"]]
mod_info[["ORTHOLOGY"]]
mod_info[["DEFINITION"]]
mod_info[["ORTHOLOGY"]]
read_csv("~/Dropbox/pcr_run_jun25/ER OSMO Project Sample Sheet - Sheet1.csv")
library(dplyr)
read_csv("~/Dropbox/pcr_run_jun25/ER OSMO Project Sample Sheet - Sheet1.csv")
library(tidyverse)
library(dplyr)
read_csv("~/Dropbox/pcr_run_jun25/ER OSMO Project Sample Sheet - Sheet1.csv")
er_sheet <- read_csv("~/Dropbox/pcr_run_jun25/ER OSMO Project Sample Sheet - Sheet1.csv")
View(er_sheet)
colnames(er_sheet) <- c("sample", "forward_primer", "reverse_primer", "concentration")
whole_sheet <- read_csv("~/Dropbox/pcr_run_jun25/PCR_map_SamplesMay2025.xlsx - Sheet1.csv")
View(whole_sheet)
er_sheet <- select("sample", "forward_primer", "reverse_primer", "concentration")
er_sheet <- select(er_sheet, "sample", "forward_primer", "reverse_primer", "concentration")
whole_sheet <- read_csv("~/Dropbox/pcr_run_jun25/PCR_map_SamplesMay2025.xlsx - Sheet1.csv")
View(whole_sheet)
colnames(whole_sheet) <- c("sample_id", "forward_primer", "forward_barcode", "reverse_primer", "reverse_barcode")
whole_sheet <- select(whole_sheet, "sample_id", "forward_primer", "forward_barcode", "reverse_primer", "reverse_barcode")
View(whole_sheet)
View(er_sheet)
View(whole_sheet)
# Merge the two dataframes on forward and reverse primers
merged_df <- merge(er_sheet, whole_sheet, by = c("forward_primer", "reverse_primer"), all.x = TRUE)
View(merged_df)
View(merged_df)
# Merge the two dataframes on forward and reverse primers
merged_df <- merge(whole_sheet, er_sheet, by = c("forward_primer", "reverse_primer"), all.x = TRUE)
View(merged_df)
whole_sheet$original_order <- seq_len(nrow(whole_sheet))
View(whole_sheet)
# Merge the two dataframes on forward and reverse primers
merged_df <- merge(whole_sheet, er_sheet, by = c("forward_primer", "reverse_primer"), all.x = TRUE)
View(merged_df)
save.csv(merged_df, "~/Dropbox/pcr_run_jun25/finaldf.csv")
write.csv(merged_df, "~/Dropbox/pcr_run_jun25/finaldf.csv")
source("~/Dropbox/BioGeoFormer/scripts/step_9_MAG_application/9.15_comparing_methods_bubble_bar.R", echo=TRUE)
source("~/Dropbox/BioGeoFormer/scripts/step_9_MAG_application/9.14_comparing_methods_upsetplot.R", echo=TRUE)
