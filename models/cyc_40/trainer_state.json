{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 289081,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0003459238068223093,
      "grad_norm": 3.2363388538360596,
      "learning_rate": 0.0002998962228579533,
      "loss": 3.6187,
      "step": 100
    },
    {
      "epoch": 0.0006918476136446186,
      "grad_norm": 4.239456653594971,
      "learning_rate": 0.00029979244571590657,
      "loss": 3.6495,
      "step": 200
    },
    {
      "epoch": 0.001037771420466928,
      "grad_norm": 3.717846155166626,
      "learning_rate": 0.0002996886685738599,
      "loss": 3.6112,
      "step": 300
    },
    {
      "epoch": 0.0013836952272892372,
      "grad_norm": 4.163496494293213,
      "learning_rate": 0.0002995848914318132,
      "loss": 3.6043,
      "step": 400
    },
    {
      "epoch": 0.0017296190341115467,
      "grad_norm": 7.6904754638671875,
      "learning_rate": 0.0002994811142897665,
      "loss": 3.573,
      "step": 500
    },
    {
      "epoch": 0.002075542840933856,
      "grad_norm": 5.790747165679932,
      "learning_rate": 0.0002993773371477198,
      "loss": 3.4084,
      "step": 600
    },
    {
      "epoch": 0.0024214666477561653,
      "grad_norm": 19.65361976623535,
      "learning_rate": 0.00029927356000567315,
      "loss": 3.1905,
      "step": 700
    },
    {
      "epoch": 0.0027673904545784745,
      "grad_norm": 5.932183265686035,
      "learning_rate": 0.0002991697828636264,
      "loss": 3.2276,
      "step": 800
    },
    {
      "epoch": 0.0031133142614007837,
      "grad_norm": 6.615226745605469,
      "learning_rate": 0.00029906600572157974,
      "loss": 3.1336,
      "step": 900
    },
    {
      "epoch": 0.0034592380682230933,
      "grad_norm": 6.696294784545898,
      "learning_rate": 0.000298962228579533,
      "loss": 3.0192,
      "step": 1000
    },
    {
      "epoch": 0.0038051618750454025,
      "grad_norm": 5.416477203369141,
      "learning_rate": 0.00029885845143748634,
      "loss": 2.9057,
      "step": 1100
    },
    {
      "epoch": 0.004151085681867712,
      "grad_norm": 8.084598541259766,
      "learning_rate": 0.00029875467429543967,
      "loss": 2.8217,
      "step": 1200
    },
    {
      "epoch": 0.004497009488690021,
      "grad_norm": 7.369443893432617,
      "learning_rate": 0.00029865089715339294,
      "loss": 2.7428,
      "step": 1300
    },
    {
      "epoch": 0.0048429332955123305,
      "grad_norm": 10.772486686706543,
      "learning_rate": 0.00029854712001134627,
      "loss": 2.7943,
      "step": 1400
    },
    {
      "epoch": 0.00518885710233464,
      "grad_norm": 6.878811836242676,
      "learning_rate": 0.0002984433428692996,
      "loss": 2.652,
      "step": 1500
    },
    {
      "epoch": 0.005534780909156949,
      "grad_norm": 6.936098098754883,
      "learning_rate": 0.0002983395657272529,
      "loss": 2.6401,
      "step": 1600
    },
    {
      "epoch": 0.005880704715979258,
      "grad_norm": 7.094151496887207,
      "learning_rate": 0.0002982357885852062,
      "loss": 2.658,
      "step": 1700
    },
    {
      "epoch": 0.006226628522801567,
      "grad_norm": 12.389655113220215,
      "learning_rate": 0.0002981320114431595,
      "loss": 2.5625,
      "step": 1800
    },
    {
      "epoch": 0.006572552329623877,
      "grad_norm": 7.440746307373047,
      "learning_rate": 0.00029802823430111284,
      "loss": 2.4872,
      "step": 1900
    },
    {
      "epoch": 0.006918476136446187,
      "grad_norm": 11.143177032470703,
      "learning_rate": 0.0002979244571590661,
      "loss": 2.5166,
      "step": 2000
    },
    {
      "epoch": 0.007264399943268496,
      "grad_norm": 11.448108673095703,
      "learning_rate": 0.00029782068001701944,
      "loss": 2.3953,
      "step": 2100
    },
    {
      "epoch": 0.007610323750090805,
      "grad_norm": 11.709714889526367,
      "learning_rate": 0.0002977169028749727,
      "loss": 2.3899,
      "step": 2200
    },
    {
      "epoch": 0.007956247556913114,
      "grad_norm": 13.832782745361328,
      "learning_rate": 0.00029761312573292604,
      "loss": 2.4307,
      "step": 2300
    },
    {
      "epoch": 0.008302171363735424,
      "grad_norm": 10.265323638916016,
      "learning_rate": 0.00029750934859087936,
      "loss": 2.3769,
      "step": 2400
    },
    {
      "epoch": 0.008648095170557733,
      "grad_norm": 12.647282600402832,
      "learning_rate": 0.00029740557144883263,
      "loss": 2.2624,
      "step": 2500
    },
    {
      "epoch": 0.008994018977380043,
      "grad_norm": 20.055917739868164,
      "learning_rate": 0.00029730179430678596,
      "loss": 2.3754,
      "step": 2600
    },
    {
      "epoch": 0.009339942784202351,
      "grad_norm": 17.430932998657227,
      "learning_rate": 0.0002971980171647393,
      "loss": 2.1621,
      "step": 2700
    },
    {
      "epoch": 0.009685866591024661,
      "grad_norm": 10.268449783325195,
      "learning_rate": 0.00029709424002269256,
      "loss": 2.3642,
      "step": 2800
    },
    {
      "epoch": 0.01003179039784697,
      "grad_norm": 6.076950550079346,
      "learning_rate": 0.0002969904628806459,
      "loss": 2.1979,
      "step": 2900
    },
    {
      "epoch": 0.01037771420466928,
      "grad_norm": 10.113947868347168,
      "learning_rate": 0.0002968866857385992,
      "loss": 2.1765,
      "step": 3000
    },
    {
      "epoch": 0.01072363801149159,
      "grad_norm": 19.931928634643555,
      "learning_rate": 0.0002967829085965525,
      "loss": 2.1387,
      "step": 3100
    },
    {
      "epoch": 0.011069561818313898,
      "grad_norm": 13.335663795471191,
      "learning_rate": 0.0002966791314545058,
      "loss": 1.8995,
      "step": 3200
    },
    {
      "epoch": 0.011415485625136208,
      "grad_norm": 8.607416152954102,
      "learning_rate": 0.00029657535431245913,
      "loss": 1.9788,
      "step": 3300
    },
    {
      "epoch": 0.011761409431958516,
      "grad_norm": 3.414219617843628,
      "learning_rate": 0.00029647157717041246,
      "loss": 2.1213,
      "step": 3400
    },
    {
      "epoch": 0.012107333238780826,
      "grad_norm": 21.24483871459961,
      "learning_rate": 0.00029636780002836573,
      "loss": 1.9914,
      "step": 3500
    },
    {
      "epoch": 0.012453257045603135,
      "grad_norm": 19.157197952270508,
      "learning_rate": 0.000296264022886319,
      "loss": 1.9932,
      "step": 3600
    },
    {
      "epoch": 0.012799180852425445,
      "grad_norm": 45.6085319519043,
      "learning_rate": 0.0002961602457442723,
      "loss": 1.9487,
      "step": 3700
    },
    {
      "epoch": 0.013145104659247755,
      "grad_norm": 20.080530166625977,
      "learning_rate": 0.00029605646860222565,
      "loss": 2.0069,
      "step": 3800
    },
    {
      "epoch": 0.013491028466070063,
      "grad_norm": 34.87556838989258,
      "learning_rate": 0.0002959526914601789,
      "loss": 1.9081,
      "step": 3900
    },
    {
      "epoch": 0.013836952272892373,
      "grad_norm": 26.319072723388672,
      "learning_rate": 0.00029584891431813225,
      "loss": 1.8524,
      "step": 4000
    },
    {
      "epoch": 0.014182876079714682,
      "grad_norm": 4.397453308105469,
      "learning_rate": 0.0002957451371760856,
      "loss": 1.9179,
      "step": 4100
    },
    {
      "epoch": 0.014528799886536992,
      "grad_norm": 13.738934516906738,
      "learning_rate": 0.0002956413600340389,
      "loss": 1.8004,
      "step": 4200
    },
    {
      "epoch": 0.0148747236933593,
      "grad_norm": 17.24891471862793,
      "learning_rate": 0.0002955375828919922,
      "loss": 1.7442,
      "step": 4300
    },
    {
      "epoch": 0.01522064750018161,
      "grad_norm": 15.591594696044922,
      "learning_rate": 0.0002954338057499455,
      "loss": 2.004,
      "step": 4400
    },
    {
      "epoch": 0.01556657130700392,
      "grad_norm": 6.502081394195557,
      "learning_rate": 0.0002953300286078988,
      "loss": 1.7136,
      "step": 4500
    },
    {
      "epoch": 0.01591249511382623,
      "grad_norm": 21.543058395385742,
      "learning_rate": 0.0002952262514658521,
      "loss": 2.019,
      "step": 4600
    },
    {
      "epoch": 0.016258418920648537,
      "grad_norm": 0.9100191593170166,
      "learning_rate": 0.0002951224743238054,
      "loss": 1.6999,
      "step": 4700
    },
    {
      "epoch": 0.01660434272747085,
      "grad_norm": 8.684651374816895,
      "learning_rate": 0.00029501869718175875,
      "loss": 1.7703,
      "step": 4800
    },
    {
      "epoch": 0.016950266534293157,
      "grad_norm": 20.14565658569336,
      "learning_rate": 0.000294914920039712,
      "loss": 1.8164,
      "step": 4900
    },
    {
      "epoch": 0.017296190341115465,
      "grad_norm": 19.93289566040039,
      "learning_rate": 0.00029481114289766535,
      "loss": 1.5224,
      "step": 5000
    },
    {
      "epoch": 0.017642114147937774,
      "grad_norm": 3.875558614730835,
      "learning_rate": 0.0002947073657556186,
      "loss": 1.4647,
      "step": 5100
    },
    {
      "epoch": 0.017988037954760085,
      "grad_norm": 0.20713846385478973,
      "learning_rate": 0.00029460358861357194,
      "loss": 1.5846,
      "step": 5200
    },
    {
      "epoch": 0.018333961761582394,
      "grad_norm": 7.597685813903809,
      "learning_rate": 0.00029449981147152527,
      "loss": 1.237,
      "step": 5300
    },
    {
      "epoch": 0.018679885568404702,
      "grad_norm": 6.12367057800293,
      "learning_rate": 0.00029439603432947854,
      "loss": 1.5603,
      "step": 5400
    },
    {
      "epoch": 0.019025809375227014,
      "grad_norm": 27.274810791015625,
      "learning_rate": 0.00029429225718743187,
      "loss": 1.5255,
      "step": 5500
    },
    {
      "epoch": 0.019371733182049322,
      "grad_norm": 22.365060806274414,
      "learning_rate": 0.0002941884800453852,
      "loss": 1.736,
      "step": 5600
    },
    {
      "epoch": 0.01971765698887163,
      "grad_norm": 26.6815185546875,
      "learning_rate": 0.00029408470290333847,
      "loss": 1.3587,
      "step": 5700
    },
    {
      "epoch": 0.02006358079569394,
      "grad_norm": 14.127948760986328,
      "learning_rate": 0.0002939809257612918,
      "loss": 1.4832,
      "step": 5800
    },
    {
      "epoch": 0.02040950460251625,
      "grad_norm": 0.11359845846891403,
      "learning_rate": 0.0002938771486192451,
      "loss": 1.6432,
      "step": 5900
    },
    {
      "epoch": 0.02075542840933856,
      "grad_norm": 17.556425094604492,
      "learning_rate": 0.00029377337147719844,
      "loss": 1.4408,
      "step": 6000
    },
    {
      "epoch": 0.021101352216160867,
      "grad_norm": 16.15484046936035,
      "learning_rate": 0.0002936695943351517,
      "loss": 1.7189,
      "step": 6100
    },
    {
      "epoch": 0.02144727602298318,
      "grad_norm": 11.892928123474121,
      "learning_rate": 0.00029356581719310504,
      "loss": 1.6016,
      "step": 6200
    },
    {
      "epoch": 0.021793199829805487,
      "grad_norm": 11.489931106567383,
      "learning_rate": 0.00029346204005105837,
      "loss": 1.389,
      "step": 6300
    },
    {
      "epoch": 0.022139123636627796,
      "grad_norm": 29.542606353759766,
      "learning_rate": 0.00029335826290901164,
      "loss": 1.3225,
      "step": 6400
    },
    {
      "epoch": 0.022485047443450104,
      "grad_norm": 22.99835968017578,
      "learning_rate": 0.0002932544857669649,
      "loss": 1.54,
      "step": 6500
    },
    {
      "epoch": 0.022830971250272416,
      "grad_norm": 0.25122252106666565,
      "learning_rate": 0.00029315070862491824,
      "loss": 1.3885,
      "step": 6600
    },
    {
      "epoch": 0.023176895057094724,
      "grad_norm": 22.22683334350586,
      "learning_rate": 0.00029304693148287156,
      "loss": 1.3706,
      "step": 6700
    },
    {
      "epoch": 0.023522818863917033,
      "grad_norm": 0.7177174091339111,
      "learning_rate": 0.00029294315434082483,
      "loss": 1.2742,
      "step": 6800
    },
    {
      "epoch": 0.023868742670739344,
      "grad_norm": 3.175211191177368,
      "learning_rate": 0.00029283937719877816,
      "loss": 1.3408,
      "step": 6900
    },
    {
      "epoch": 0.024214666477561653,
      "grad_norm": 1.918046474456787,
      "learning_rate": 0.0002927356000567315,
      "loss": 1.4219,
      "step": 7000
    },
    {
      "epoch": 0.02456059028438396,
      "grad_norm": 38.01722717285156,
      "learning_rate": 0.0002926318229146848,
      "loss": 1.2027,
      "step": 7100
    },
    {
      "epoch": 0.02490651409120627,
      "grad_norm": 9.99024772644043,
      "learning_rate": 0.0002925280457726381,
      "loss": 1.3517,
      "step": 7200
    },
    {
      "epoch": 0.02525243789802858,
      "grad_norm": 18.97922134399414,
      "learning_rate": 0.0002924242686305914,
      "loss": 1.3376,
      "step": 7300
    },
    {
      "epoch": 0.02559836170485089,
      "grad_norm": 3.6104576587677,
      "learning_rate": 0.00029232049148854473,
      "loss": 1.1344,
      "step": 7400
    },
    {
      "epoch": 0.025944285511673198,
      "grad_norm": 33.106712341308594,
      "learning_rate": 0.000292216714346498,
      "loss": 1.2511,
      "step": 7500
    },
    {
      "epoch": 0.02629020931849551,
      "grad_norm": 52.06671142578125,
      "learning_rate": 0.00029211293720445133,
      "loss": 1.1789,
      "step": 7600
    },
    {
      "epoch": 0.026636133125317818,
      "grad_norm": 18.536094665527344,
      "learning_rate": 0.00029200916006240466,
      "loss": 1.1469,
      "step": 7700
    },
    {
      "epoch": 0.026982056932140126,
      "grad_norm": 10.584085464477539,
      "learning_rate": 0.00029190538292035793,
      "loss": 1.4037,
      "step": 7800
    },
    {
      "epoch": 0.027327980738962435,
      "grad_norm": 2.4679195880889893,
      "learning_rate": 0.00029180160577831126,
      "loss": 1.0954,
      "step": 7900
    },
    {
      "epoch": 0.027673904545784746,
      "grad_norm": 0.2397138774394989,
      "learning_rate": 0.0002916978286362645,
      "loss": 1.1652,
      "step": 8000
    },
    {
      "epoch": 0.028019828352607055,
      "grad_norm": 9.843579292297363,
      "learning_rate": 0.00029159405149421785,
      "loss": 1.2914,
      "step": 8100
    },
    {
      "epoch": 0.028365752159429363,
      "grad_norm": 13.521444320678711,
      "learning_rate": 0.0002914902743521712,
      "loss": 1.3802,
      "step": 8200
    },
    {
      "epoch": 0.028711675966251675,
      "grad_norm": 25.34923553466797,
      "learning_rate": 0.00029138649721012445,
      "loss": 1.5029,
      "step": 8300
    },
    {
      "epoch": 0.029057599773073983,
      "grad_norm": 26.7293758392334,
      "learning_rate": 0.0002912827200680778,
      "loss": 1.5374,
      "step": 8400
    },
    {
      "epoch": 0.02940352357989629,
      "grad_norm": 34.689517974853516,
      "learning_rate": 0.0002911789429260311,
      "loss": 1.2299,
      "step": 8500
    },
    {
      "epoch": 0.0297494473867186,
      "grad_norm": 0.17902758717536926,
      "learning_rate": 0.0002910751657839844,
      "loss": 1.1665,
      "step": 8600
    },
    {
      "epoch": 0.03009537119354091,
      "grad_norm": 0.43895766139030457,
      "learning_rate": 0.0002909713886419377,
      "loss": 1.2222,
      "step": 8700
    },
    {
      "epoch": 0.03044129500036322,
      "grad_norm": 39.29387664794922,
      "learning_rate": 0.000290867611499891,
      "loss": 1.3872,
      "step": 8800
    },
    {
      "epoch": 0.03078721880718553,
      "grad_norm": 4.985807418823242,
      "learning_rate": 0.00029076383435784435,
      "loss": 1.4157,
      "step": 8900
    },
    {
      "epoch": 0.03113314261400784,
      "grad_norm": 32.13833999633789,
      "learning_rate": 0.0002906600572157976,
      "loss": 1.3165,
      "step": 9000
    },
    {
      "epoch": 0.03147906642083015,
      "grad_norm": 0.1891213059425354,
      "learning_rate": 0.00029055628007375095,
      "loss": 1.3271,
      "step": 9100
    },
    {
      "epoch": 0.03182499022765246,
      "grad_norm": 35.723182678222656,
      "learning_rate": 0.0002904525029317042,
      "loss": 1.5884,
      "step": 9200
    },
    {
      "epoch": 0.032170914034474765,
      "grad_norm": 0.6688001751899719,
      "learning_rate": 0.00029034872578965755,
      "loss": 1.2848,
      "step": 9300
    },
    {
      "epoch": 0.032516837841297073,
      "grad_norm": 0.13540582358837128,
      "learning_rate": 0.0002902449486476108,
      "loss": 0.951,
      "step": 9400
    },
    {
      "epoch": 0.03286276164811938,
      "grad_norm": 44.17378234863281,
      "learning_rate": 0.00029014117150556414,
      "loss": 1.0379,
      "step": 9500
    },
    {
      "epoch": 0.0332086854549417,
      "grad_norm": 0.09296164661645889,
      "learning_rate": 0.00029003739436351747,
      "loss": 1.0767,
      "step": 9600
    },
    {
      "epoch": 0.033554609261764005,
      "grad_norm": 32.08467483520508,
      "learning_rate": 0.0002899336172214708,
      "loss": 1.2733,
      "step": 9700
    },
    {
      "epoch": 0.033900533068586314,
      "grad_norm": 0.1348792463541031,
      "learning_rate": 0.00028982984007942407,
      "loss": 1.1318,
      "step": 9800
    },
    {
      "epoch": 0.03424645687540862,
      "grad_norm": 13.669504165649414,
      "learning_rate": 0.0002897260629373774,
      "loss": 1.1911,
      "step": 9900
    },
    {
      "epoch": 0.03459238068223093,
      "grad_norm": 3.717595338821411,
      "learning_rate": 0.0002896222857953307,
      "loss": 1.0528,
      "step": 10000
    },
    {
      "epoch": 0.03493830448905324,
      "grad_norm": 1.8654392957687378,
      "learning_rate": 0.000289518508653284,
      "loss": 1.435,
      "step": 10100
    },
    {
      "epoch": 0.03528422829587555,
      "grad_norm": 39.10133743286133,
      "learning_rate": 0.0002894147315112373,
      "loss": 1.3838,
      "step": 10200
    },
    {
      "epoch": 0.03563015210269786,
      "grad_norm": 1.7518813610076904,
      "learning_rate": 0.00028931095436919064,
      "loss": 1.3754,
      "step": 10300
    },
    {
      "epoch": 0.03597607590952017,
      "grad_norm": 30.18690299987793,
      "learning_rate": 0.0002892071772271439,
      "loss": 1.3408,
      "step": 10400
    },
    {
      "epoch": 0.03632199971634248,
      "grad_norm": 108.66680908203125,
      "learning_rate": 0.00028910340008509724,
      "loss": 1.1315,
      "step": 10500
    },
    {
      "epoch": 0.03666792352316479,
      "grad_norm": 34.57599639892578,
      "learning_rate": 0.0002889996229430505,
      "loss": 1.1684,
      "step": 10600
    },
    {
      "epoch": 0.037013847329987096,
      "grad_norm": 9.84961223602295,
      "learning_rate": 0.00028889584580100384,
      "loss": 1.1525,
      "step": 10700
    },
    {
      "epoch": 0.037359771136809404,
      "grad_norm": 35.54201126098633,
      "learning_rate": 0.00028879206865895716,
      "loss": 1.2108,
      "step": 10800
    },
    {
      "epoch": 0.03770569494363171,
      "grad_norm": 1.1628706455230713,
      "learning_rate": 0.00028868829151691044,
      "loss": 1.1542,
      "step": 10900
    },
    {
      "epoch": 0.03805161875045403,
      "grad_norm": 20.17703628540039,
      "learning_rate": 0.00028858451437486376,
      "loss": 1.4841,
      "step": 11000
    },
    {
      "epoch": 0.038397542557276336,
      "grad_norm": 14.596845626831055,
      "learning_rate": 0.0002884807372328171,
      "loss": 1.1341,
      "step": 11100
    },
    {
      "epoch": 0.038743466364098644,
      "grad_norm": 1.7422442436218262,
      "learning_rate": 0.00028837696009077036,
      "loss": 1.0188,
      "step": 11200
    },
    {
      "epoch": 0.03908939017092095,
      "grad_norm": 14.851640701293945,
      "learning_rate": 0.0002882731829487237,
      "loss": 1.0788,
      "step": 11300
    },
    {
      "epoch": 0.03943531397774326,
      "grad_norm": 31.126018524169922,
      "learning_rate": 0.000288169405806677,
      "loss": 1.5532,
      "step": 11400
    },
    {
      "epoch": 0.03978123778456557,
      "grad_norm": 21.85923957824707,
      "learning_rate": 0.00028806562866463034,
      "loss": 0.9811,
      "step": 11500
    },
    {
      "epoch": 0.04012716159138788,
      "grad_norm": 0.013984736986458302,
      "learning_rate": 0.0002879618515225836,
      "loss": 1.1642,
      "step": 11600
    },
    {
      "epoch": 0.04047308539821019,
      "grad_norm": 8.54637336730957,
      "learning_rate": 0.00028785807438053693,
      "loss": 1.5314,
      "step": 11700
    },
    {
      "epoch": 0.0408190092050325,
      "grad_norm": 0.5168002247810364,
      "learning_rate": 0.00028775429723849026,
      "loss": 1.0464,
      "step": 11800
    },
    {
      "epoch": 0.04116493301185481,
      "grad_norm": 22.318632125854492,
      "learning_rate": 0.00028765052009644353,
      "loss": 1.1885,
      "step": 11900
    },
    {
      "epoch": 0.04151085681867712,
      "grad_norm": 40.29890060424805,
      "learning_rate": 0.0002875467429543968,
      "loss": 1.0308,
      "step": 12000
    },
    {
      "epoch": 0.041856780625499426,
      "grad_norm": 46.875396728515625,
      "learning_rate": 0.00028744296581235013,
      "loss": 1.1613,
      "step": 12100
    },
    {
      "epoch": 0.042202704432321735,
      "grad_norm": 0.01721416786313057,
      "learning_rate": 0.00028733918867030346,
      "loss": 1.0916,
      "step": 12200
    },
    {
      "epoch": 0.04254862823914404,
      "grad_norm": 28.924535751342773,
      "learning_rate": 0.0002872354115282567,
      "loss": 1.0376,
      "step": 12300
    },
    {
      "epoch": 0.04289455204596636,
      "grad_norm": 0.21433667838573456,
      "learning_rate": 0.00028713163438621005,
      "loss": 0.8871,
      "step": 12400
    },
    {
      "epoch": 0.043240475852788667,
      "grad_norm": 0.007189616560935974,
      "learning_rate": 0.0002870278572441634,
      "loss": 1.0263,
      "step": 12500
    },
    {
      "epoch": 0.043586399659610975,
      "grad_norm": 26.759153366088867,
      "learning_rate": 0.0002869240801021167,
      "loss": 0.8731,
      "step": 12600
    },
    {
      "epoch": 0.04393232346643328,
      "grad_norm": 17.66629981994629,
      "learning_rate": 0.00028682030296007,
      "loss": 1.348,
      "step": 12700
    },
    {
      "epoch": 0.04427824727325559,
      "grad_norm": 0.07709188759326935,
      "learning_rate": 0.0002867165258180233,
      "loss": 1.2398,
      "step": 12800
    },
    {
      "epoch": 0.0446241710800779,
      "grad_norm": 5.556016445159912,
      "learning_rate": 0.00028661274867597663,
      "loss": 1.0688,
      "step": 12900
    },
    {
      "epoch": 0.04497009488690021,
      "grad_norm": 34.07424545288086,
      "learning_rate": 0.0002865089715339299,
      "loss": 1.1092,
      "step": 13000
    },
    {
      "epoch": 0.04531601869372252,
      "grad_norm": 82.37522888183594,
      "learning_rate": 0.0002864051943918832,
      "loss": 0.9398,
      "step": 13100
    },
    {
      "epoch": 0.04566194250054483,
      "grad_norm": 29.923152923583984,
      "learning_rate": 0.00028630141724983655,
      "loss": 1.3891,
      "step": 13200
    },
    {
      "epoch": 0.04600786630736714,
      "grad_norm": 0.06628228724002838,
      "learning_rate": 0.0002861976401077898,
      "loss": 1.0512,
      "step": 13300
    },
    {
      "epoch": 0.04635379011418945,
      "grad_norm": 22.453433990478516,
      "learning_rate": 0.00028609386296574315,
      "loss": 0.7641,
      "step": 13400
    },
    {
      "epoch": 0.04669971392101176,
      "grad_norm": 17.259891510009766,
      "learning_rate": 0.0002859900858236964,
      "loss": 1.0797,
      "step": 13500
    },
    {
      "epoch": 0.047045637727834065,
      "grad_norm": 0.19873961806297302,
      "learning_rate": 0.00028588630868164975,
      "loss": 1.0886,
      "step": 13600
    },
    {
      "epoch": 0.04739156153465637,
      "grad_norm": 0.06096380203962326,
      "learning_rate": 0.00028578253153960307,
      "loss": 0.8773,
      "step": 13700
    },
    {
      "epoch": 0.04773748534147869,
      "grad_norm": 0.13880255818367004,
      "learning_rate": 0.00028567875439755634,
      "loss": 0.9993,
      "step": 13800
    },
    {
      "epoch": 0.048083409148301,
      "grad_norm": 0.053049296140670776,
      "learning_rate": 0.00028557497725550967,
      "loss": 1.1187,
      "step": 13900
    },
    {
      "epoch": 0.048429332955123305,
      "grad_norm": 38.713008880615234,
      "learning_rate": 0.000285471200113463,
      "loss": 1.044,
      "step": 14000
    },
    {
      "epoch": 0.048775256761945614,
      "grad_norm": 0.8370997309684753,
      "learning_rate": 0.00028536742297141627,
      "loss": 1.1379,
      "step": 14100
    },
    {
      "epoch": 0.04912118056876792,
      "grad_norm": 68.49484252929688,
      "learning_rate": 0.0002852636458293696,
      "loss": 1.034,
      "step": 14200
    },
    {
      "epoch": 0.04946710437559023,
      "grad_norm": 33.68391418457031,
      "learning_rate": 0.0002851598686873229,
      "loss": 0.961,
      "step": 14300
    },
    {
      "epoch": 0.04981302818241254,
      "grad_norm": 0.5843760371208191,
      "learning_rate": 0.00028505609154527625,
      "loss": 0.9826,
      "step": 14400
    },
    {
      "epoch": 0.050158951989234854,
      "grad_norm": 0.19618448615074158,
      "learning_rate": 0.0002849523144032295,
      "loss": 1.084,
      "step": 14500
    },
    {
      "epoch": 0.05050487579605716,
      "grad_norm": 13.745158195495605,
      "learning_rate": 0.00028484853726118284,
      "loss": 0.9254,
      "step": 14600
    },
    {
      "epoch": 0.05085079960287947,
      "grad_norm": 9.316824913024902,
      "learning_rate": 0.00028474476011913617,
      "loss": 1.0001,
      "step": 14700
    },
    {
      "epoch": 0.05119672340970178,
      "grad_norm": 24.31148910522461,
      "learning_rate": 0.00028464098297708944,
      "loss": 1.0247,
      "step": 14800
    },
    {
      "epoch": 0.05154264721652409,
      "grad_norm": 37.41509246826172,
      "learning_rate": 0.0002845372058350427,
      "loss": 1.2367,
      "step": 14900
    },
    {
      "epoch": 0.051888571023346396,
      "grad_norm": 1.9712918996810913,
      "learning_rate": 0.00028443342869299604,
      "loss": 1.2696,
      "step": 15000
    },
    {
      "epoch": 0.052234494830168704,
      "grad_norm": 17.504945755004883,
      "learning_rate": 0.00028432965155094936,
      "loss": 1.0173,
      "step": 15100
    },
    {
      "epoch": 0.05258041863699102,
      "grad_norm": 58.27314376831055,
      "learning_rate": 0.0002842258744089027,
      "loss": 1.2573,
      "step": 15200
    },
    {
      "epoch": 0.05292634244381333,
      "grad_norm": 22.758159637451172,
      "learning_rate": 0.00028412209726685596,
      "loss": 1.2751,
      "step": 15300
    },
    {
      "epoch": 0.053272266250635636,
      "grad_norm": 26.539188385009766,
      "learning_rate": 0.0002840183201248093,
      "loss": 1.0576,
      "step": 15400
    },
    {
      "epoch": 0.053618190057457944,
      "grad_norm": 37.52001953125,
      "learning_rate": 0.0002839145429827626,
      "loss": 1.3299,
      "step": 15500
    },
    {
      "epoch": 0.05396411386428025,
      "grad_norm": 0.04908539354801178,
      "learning_rate": 0.0002838107658407159,
      "loss": 1.0354,
      "step": 15600
    },
    {
      "epoch": 0.05431003767110256,
      "grad_norm": 60.814048767089844,
      "learning_rate": 0.0002837069886986692,
      "loss": 0.9542,
      "step": 15700
    },
    {
      "epoch": 0.05465596147792487,
      "grad_norm": 28.678247451782227,
      "learning_rate": 0.00028360321155662254,
      "loss": 0.9479,
      "step": 15800
    },
    {
      "epoch": 0.055001885284747185,
      "grad_norm": 18.822139739990234,
      "learning_rate": 0.0002834994344145758,
      "loss": 1.3164,
      "step": 15900
    },
    {
      "epoch": 0.05534780909156949,
      "grad_norm": 10.875166893005371,
      "learning_rate": 0.00028339565727252913,
      "loss": 1.3307,
      "step": 16000
    },
    {
      "epoch": 0.0556937328983918,
      "grad_norm": 0.011217968538403511,
      "learning_rate": 0.00028329188013048246,
      "loss": 0.8266,
      "step": 16100
    },
    {
      "epoch": 0.05603965670521411,
      "grad_norm": 1.8794060945510864,
      "learning_rate": 0.00028318810298843573,
      "loss": 0.8848,
      "step": 16200
    },
    {
      "epoch": 0.05638558051203642,
      "grad_norm": 0.13480834662914276,
      "learning_rate": 0.00028308432584638906,
      "loss": 0.9613,
      "step": 16300
    },
    {
      "epoch": 0.056731504318858726,
      "grad_norm": 12.450859069824219,
      "learning_rate": 0.00028298054870434233,
      "loss": 0.7662,
      "step": 16400
    },
    {
      "epoch": 0.057077428125681035,
      "grad_norm": 0.02006376162171364,
      "learning_rate": 0.00028287677156229566,
      "loss": 1.0055,
      "step": 16500
    },
    {
      "epoch": 0.05742335193250335,
      "grad_norm": 0.023753250017762184,
      "learning_rate": 0.000282772994420249,
      "loss": 0.9398,
      "step": 16600
    },
    {
      "epoch": 0.05776927573932566,
      "grad_norm": 34.98540115356445,
      "learning_rate": 0.00028266921727820225,
      "loss": 1.1524,
      "step": 16700
    },
    {
      "epoch": 0.058115199546147966,
      "grad_norm": 32.15651321411133,
      "learning_rate": 0.0002825654401361556,
      "loss": 1.1439,
      "step": 16800
    },
    {
      "epoch": 0.058461123352970275,
      "grad_norm": 1.6485605239868164,
      "learning_rate": 0.0002824616629941089,
      "loss": 1.3041,
      "step": 16900
    },
    {
      "epoch": 0.05880704715979258,
      "grad_norm": 8.141571044921875,
      "learning_rate": 0.00028235788585206223,
      "loss": 1.1728,
      "step": 17000
    },
    {
      "epoch": 0.05915297096661489,
      "grad_norm": 3.806985855102539,
      "learning_rate": 0.0002822541087100155,
      "loss": 0.7641,
      "step": 17100
    },
    {
      "epoch": 0.0594988947734372,
      "grad_norm": 41.69156265258789,
      "learning_rate": 0.00028215033156796883,
      "loss": 0.9672,
      "step": 17200
    },
    {
      "epoch": 0.059844818580259515,
      "grad_norm": 72.48908996582031,
      "learning_rate": 0.00028204655442592215,
      "loss": 1.1813,
      "step": 17300
    },
    {
      "epoch": 0.06019074238708182,
      "grad_norm": 0.016926489770412445,
      "learning_rate": 0.0002819427772838754,
      "loss": 1.036,
      "step": 17400
    },
    {
      "epoch": 0.06053666619390413,
      "grad_norm": 19.951967239379883,
      "learning_rate": 0.00028183900014182875,
      "loss": 1.2293,
      "step": 17500
    },
    {
      "epoch": 0.06088259000072644,
      "grad_norm": 10.30691146850586,
      "learning_rate": 0.000281735222999782,
      "loss": 0.739,
      "step": 17600
    },
    {
      "epoch": 0.06122851380754875,
      "grad_norm": 1.744829535484314,
      "learning_rate": 0.00028163144585773535,
      "loss": 0.8017,
      "step": 17700
    },
    {
      "epoch": 0.06157443761437106,
      "grad_norm": 33.97617721557617,
      "learning_rate": 0.0002815276687156887,
      "loss": 0.9363,
      "step": 17800
    },
    {
      "epoch": 0.061920361421193365,
      "grad_norm": 33.83882522583008,
      "learning_rate": 0.00028142389157364195,
      "loss": 0.9735,
      "step": 17900
    },
    {
      "epoch": 0.06226628522801568,
      "grad_norm": 0.872090756893158,
      "learning_rate": 0.00028132011443159527,
      "loss": 1.3358,
      "step": 18000
    },
    {
      "epoch": 0.06261220903483798,
      "grad_norm": 0.01828472688794136,
      "learning_rate": 0.0002812163372895486,
      "loss": 1.0637,
      "step": 18100
    },
    {
      "epoch": 0.0629581328416603,
      "grad_norm": 0.4782370328903198,
      "learning_rate": 0.00028111256014750187,
      "loss": 1.2449,
      "step": 18200
    },
    {
      "epoch": 0.0633040566484826,
      "grad_norm": 3.189732789993286,
      "learning_rate": 0.0002810087830054552,
      "loss": 1.0404,
      "step": 18300
    },
    {
      "epoch": 0.06364998045530491,
      "grad_norm": 52.19766616821289,
      "learning_rate": 0.0002809050058634085,
      "loss": 0.8302,
      "step": 18400
    },
    {
      "epoch": 0.06399590426212723,
      "grad_norm": 0.0665881484746933,
      "learning_rate": 0.0002808012287213618,
      "loss": 0.7324,
      "step": 18500
    },
    {
      "epoch": 0.06434182806894953,
      "grad_norm": 25.72271156311035,
      "learning_rate": 0.0002806974515793151,
      "loss": 0.9384,
      "step": 18600
    },
    {
      "epoch": 0.06468775187577185,
      "grad_norm": 20.242876052856445,
      "learning_rate": 0.00028059367443726845,
      "loss": 0.7797,
      "step": 18700
    },
    {
      "epoch": 0.06503367568259415,
      "grad_norm": 7.621591091156006,
      "learning_rate": 0.0002804898972952217,
      "loss": 0.9339,
      "step": 18800
    },
    {
      "epoch": 0.06537959948941646,
      "grad_norm": 0.28822004795074463,
      "learning_rate": 0.00028038612015317504,
      "loss": 0.8043,
      "step": 18900
    },
    {
      "epoch": 0.06572552329623876,
      "grad_norm": 0.01867011748254299,
      "learning_rate": 0.0002802823430111283,
      "loss": 1.273,
      "step": 19000
    },
    {
      "epoch": 0.06607144710306108,
      "grad_norm": 0.18649722635746002,
      "learning_rate": 0.00028017856586908164,
      "loss": 0.9249,
      "step": 19100
    },
    {
      "epoch": 0.0664173709098834,
      "grad_norm": 58.618648529052734,
      "learning_rate": 0.00028007478872703497,
      "loss": 0.9647,
      "step": 19200
    },
    {
      "epoch": 0.0667632947167057,
      "grad_norm": 3.2176239490509033,
      "learning_rate": 0.00027997101158498824,
      "loss": 1.0686,
      "step": 19300
    },
    {
      "epoch": 0.06710921852352801,
      "grad_norm": 65.09715270996094,
      "learning_rate": 0.00027986723444294156,
      "loss": 0.6952,
      "step": 19400
    },
    {
      "epoch": 0.06745514233035031,
      "grad_norm": 0.13284561038017273,
      "learning_rate": 0.0002797634573008949,
      "loss": 0.8463,
      "step": 19500
    },
    {
      "epoch": 0.06780106613717263,
      "grad_norm": 0.07415883243083954,
      "learning_rate": 0.00027965968015884816,
      "loss": 0.9203,
      "step": 19600
    },
    {
      "epoch": 0.06814698994399493,
      "grad_norm": 2.8948347568511963,
      "learning_rate": 0.0002795559030168015,
      "loss": 0.9365,
      "step": 19700
    },
    {
      "epoch": 0.06849291375081724,
      "grad_norm": 0.035569872707128525,
      "learning_rate": 0.0002794521258747548,
      "loss": 1.0337,
      "step": 19800
    },
    {
      "epoch": 0.06883883755763956,
      "grad_norm": 0.1256285011768341,
      "learning_rate": 0.00027934834873270814,
      "loss": 1.0547,
      "step": 19900
    },
    {
      "epoch": 0.06918476136446186,
      "grad_norm": 72.818115234375,
      "learning_rate": 0.0002792445715906614,
      "loss": 0.9817,
      "step": 20000
    },
    {
      "epoch": 0.06953068517128418,
      "grad_norm": 0.01756134256720543,
      "learning_rate": 0.00027914079444861474,
      "loss": 1.1668,
      "step": 20100
    },
    {
      "epoch": 0.06987660897810648,
      "grad_norm": 6.691540241241455,
      "learning_rate": 0.00027903701730656806,
      "loss": 0.5589,
      "step": 20200
    },
    {
      "epoch": 0.07022253278492879,
      "grad_norm": 15.572288513183594,
      "learning_rate": 0.00027893324016452133,
      "loss": 0.8338,
      "step": 20300
    },
    {
      "epoch": 0.0705684565917511,
      "grad_norm": 0.004233324434608221,
      "learning_rate": 0.0002788294630224746,
      "loss": 0.7665,
      "step": 20400
    },
    {
      "epoch": 0.07091438039857341,
      "grad_norm": 0.26289495825767517,
      "learning_rate": 0.00027872568588042793,
      "loss": 0.9019,
      "step": 20500
    },
    {
      "epoch": 0.07126030420539572,
      "grad_norm": 0.12709487974643707,
      "learning_rate": 0.00027862190873838126,
      "loss": 1.1414,
      "step": 20600
    },
    {
      "epoch": 0.07160622801221803,
      "grad_norm": 59.60597610473633,
      "learning_rate": 0.0002785181315963346,
      "loss": 0.867,
      "step": 20700
    },
    {
      "epoch": 0.07195215181904034,
      "grad_norm": 0.40308690071105957,
      "learning_rate": 0.00027841435445428786,
      "loss": 0.7428,
      "step": 20800
    },
    {
      "epoch": 0.07229807562586264,
      "grad_norm": 26.831558227539062,
      "learning_rate": 0.0002783105773122412,
      "loss": 0.8756,
      "step": 20900
    },
    {
      "epoch": 0.07264399943268496,
      "grad_norm": 40.08370590209961,
      "learning_rate": 0.0002782068001701945,
      "loss": 0.9958,
      "step": 21000
    },
    {
      "epoch": 0.07298992323950726,
      "grad_norm": 26.740564346313477,
      "learning_rate": 0.0002781030230281478,
      "loss": 0.809,
      "step": 21100
    },
    {
      "epoch": 0.07333584704632957,
      "grad_norm": 0.01483603660017252,
      "learning_rate": 0.0002779992458861011,
      "loss": 0.9326,
      "step": 21200
    },
    {
      "epoch": 0.07368177085315189,
      "grad_norm": 3.099724531173706,
      "learning_rate": 0.00027789546874405443,
      "loss": 1.1592,
      "step": 21300
    },
    {
      "epoch": 0.07402769465997419,
      "grad_norm": 14.24448013305664,
      "learning_rate": 0.0002777916916020077,
      "loss": 0.8488,
      "step": 21400
    },
    {
      "epoch": 0.0743736184667965,
      "grad_norm": 2.3506715297698975,
      "learning_rate": 0.00027768791445996103,
      "loss": 0.7767,
      "step": 21500
    },
    {
      "epoch": 0.07471954227361881,
      "grad_norm": 29.806486129760742,
      "learning_rate": 0.00027758413731791435,
      "loss": 0.9748,
      "step": 21600
    },
    {
      "epoch": 0.07506546608044112,
      "grad_norm": 90.11419677734375,
      "learning_rate": 0.0002774803601758677,
      "loss": 1.0301,
      "step": 21700
    },
    {
      "epoch": 0.07541138988726342,
      "grad_norm": 0.15609107911586761,
      "learning_rate": 0.00027737658303382095,
      "loss": 0.6934,
      "step": 21800
    },
    {
      "epoch": 0.07575731369408574,
      "grad_norm": 50.93667984008789,
      "learning_rate": 0.0002772728058917742,
      "loss": 0.8232,
      "step": 21900
    },
    {
      "epoch": 0.07610323750090806,
      "grad_norm": 49.360145568847656,
      "learning_rate": 0.00027716902874972755,
      "loss": 0.6496,
      "step": 22000
    },
    {
      "epoch": 0.07644916130773036,
      "grad_norm": 31.15629005432129,
      "learning_rate": 0.0002770652516076809,
      "loss": 0.8469,
      "step": 22100
    },
    {
      "epoch": 0.07679508511455267,
      "grad_norm": 69.76289367675781,
      "learning_rate": 0.00027696147446563415,
      "loss": 0.8909,
      "step": 22200
    },
    {
      "epoch": 0.07714100892137497,
      "grad_norm": 34.92308807373047,
      "learning_rate": 0.00027685769732358747,
      "loss": 1.1018,
      "step": 22300
    },
    {
      "epoch": 0.07748693272819729,
      "grad_norm": 0.006994518451392651,
      "learning_rate": 0.0002767539201815408,
      "loss": 0.7154,
      "step": 22400
    },
    {
      "epoch": 0.07783285653501959,
      "grad_norm": 36.40975570678711,
      "learning_rate": 0.0002766501430394941,
      "loss": 0.6901,
      "step": 22500
    },
    {
      "epoch": 0.0781787803418419,
      "grad_norm": 80.17790222167969,
      "learning_rate": 0.0002765463658974474,
      "loss": 1.166,
      "step": 22600
    },
    {
      "epoch": 0.07852470414866422,
      "grad_norm": 0.006499050185084343,
      "learning_rate": 0.0002764425887554007,
      "loss": 0.9195,
      "step": 22700
    },
    {
      "epoch": 0.07887062795548652,
      "grad_norm": 0.012725182808935642,
      "learning_rate": 0.00027633881161335405,
      "loss": 0.786,
      "step": 22800
    },
    {
      "epoch": 0.07921655176230884,
      "grad_norm": 22.043800354003906,
      "learning_rate": 0.0002762350344713073,
      "loss": 1.1037,
      "step": 22900
    },
    {
      "epoch": 0.07956247556913114,
      "grad_norm": 0.005602159537374973,
      "learning_rate": 0.00027613125732926065,
      "loss": 0.7752,
      "step": 23000
    },
    {
      "epoch": 0.07990839937595345,
      "grad_norm": 9.887605667114258,
      "learning_rate": 0.00027602748018721397,
      "loss": 1.0716,
      "step": 23100
    },
    {
      "epoch": 0.08025432318277576,
      "grad_norm": 0.17287983000278473,
      "learning_rate": 0.00027592370304516724,
      "loss": 0.6897,
      "step": 23200
    },
    {
      "epoch": 0.08060024698959807,
      "grad_norm": 19.431245803833008,
      "learning_rate": 0.00027581992590312057,
      "loss": 1.0807,
      "step": 23300
    },
    {
      "epoch": 0.08094617079642039,
      "grad_norm": 0.30737367272377014,
      "learning_rate": 0.00027571614876107384,
      "loss": 0.9777,
      "step": 23400
    },
    {
      "epoch": 0.08129209460324269,
      "grad_norm": 24.50035285949707,
      "learning_rate": 0.00027561237161902717,
      "loss": 1.1233,
      "step": 23500
    },
    {
      "epoch": 0.081638018410065,
      "grad_norm": 27.092575073242188,
      "learning_rate": 0.0002755085944769805,
      "loss": 0.8173,
      "step": 23600
    },
    {
      "epoch": 0.0819839422168873,
      "grad_norm": 30.669782638549805,
      "learning_rate": 0.00027540481733493376,
      "loss": 1.0173,
      "step": 23700
    },
    {
      "epoch": 0.08232986602370962,
      "grad_norm": 11.8908109664917,
      "learning_rate": 0.0002753010401928871,
      "loss": 1.0856,
      "step": 23800
    },
    {
      "epoch": 0.08267578983053192,
      "grad_norm": 0.03209935128688812,
      "learning_rate": 0.0002751972630508404,
      "loss": 1.0647,
      "step": 23900
    },
    {
      "epoch": 0.08302171363735424,
      "grad_norm": 23.582286834716797,
      "learning_rate": 0.0002750934859087937,
      "loss": 0.751,
      "step": 24000
    },
    {
      "epoch": 0.08336763744417655,
      "grad_norm": 69.18562316894531,
      "learning_rate": 0.000274989708766747,
      "loss": 1.1025,
      "step": 24100
    },
    {
      "epoch": 0.08371356125099885,
      "grad_norm": 0.867703378200531,
      "learning_rate": 0.00027488593162470034,
      "loss": 0.7511,
      "step": 24200
    },
    {
      "epoch": 0.08405948505782117,
      "grad_norm": 15.36386489868164,
      "learning_rate": 0.00027478215448265367,
      "loss": 0.8997,
      "step": 24300
    },
    {
      "epoch": 0.08440540886464347,
      "grad_norm": 39.88374710083008,
      "learning_rate": 0.00027467837734060694,
      "loss": 0.6984,
      "step": 24400
    },
    {
      "epoch": 0.08475133267146578,
      "grad_norm": 38.36083221435547,
      "learning_rate": 0.00027457460019856026,
      "loss": 0.7966,
      "step": 24500
    },
    {
      "epoch": 0.08509725647828809,
      "grad_norm": 0.12500996887683868,
      "learning_rate": 0.00027447082305651353,
      "loss": 0.7344,
      "step": 24600
    },
    {
      "epoch": 0.0854431802851104,
      "grad_norm": 21.871421813964844,
      "learning_rate": 0.00027436704591446686,
      "loss": 1.0309,
      "step": 24700
    },
    {
      "epoch": 0.08578910409193272,
      "grad_norm": 0.08251050114631653,
      "learning_rate": 0.00027426326877242013,
      "loss": 0.7797,
      "step": 24800
    },
    {
      "epoch": 0.08613502789875502,
      "grad_norm": 0.1386294960975647,
      "learning_rate": 0.00027415949163037346,
      "loss": 0.9512,
      "step": 24900
    },
    {
      "epoch": 0.08648095170557733,
      "grad_norm": 43.0820426940918,
      "learning_rate": 0.0002740557144883268,
      "loss": 1.0277,
      "step": 25000
    },
    {
      "epoch": 0.08682687551239963,
      "grad_norm": 27.893861770629883,
      "learning_rate": 0.00027395193734628006,
      "loss": 0.9875,
      "step": 25100
    },
    {
      "epoch": 0.08717279931922195,
      "grad_norm": 1.906010389328003,
      "learning_rate": 0.0002738481602042334,
      "loss": 0.8856,
      "step": 25200
    },
    {
      "epoch": 0.08751872312604425,
      "grad_norm": 6.489293098449707,
      "learning_rate": 0.0002737443830621867,
      "loss": 0.9586,
      "step": 25300
    },
    {
      "epoch": 0.08786464693286657,
      "grad_norm": 0.014287804253399372,
      "learning_rate": 0.00027364060592014003,
      "loss": 0.872,
      "step": 25400
    },
    {
      "epoch": 0.08821057073968888,
      "grad_norm": 22.512619018554688,
      "learning_rate": 0.0002735368287780933,
      "loss": 0.6025,
      "step": 25500
    },
    {
      "epoch": 0.08855649454651118,
      "grad_norm": 9.195573806762695,
      "learning_rate": 0.00027343305163604663,
      "loss": 0.7283,
      "step": 25600
    },
    {
      "epoch": 0.0889024183533335,
      "grad_norm": 53.39454650878906,
      "learning_rate": 0.00027332927449399996,
      "loss": 0.7764,
      "step": 25700
    },
    {
      "epoch": 0.0892483421601558,
      "grad_norm": 5.661221981048584,
      "learning_rate": 0.00027322549735195323,
      "loss": 0.9204,
      "step": 25800
    },
    {
      "epoch": 0.08959426596697811,
      "grad_norm": 0.024227820336818695,
      "learning_rate": 0.00027312172020990655,
      "loss": 0.5991,
      "step": 25900
    },
    {
      "epoch": 0.08994018977380042,
      "grad_norm": 1.2937324047088623,
      "learning_rate": 0.0002730179430678598,
      "loss": 0.8736,
      "step": 26000
    },
    {
      "epoch": 0.09028611358062273,
      "grad_norm": 0.1168808788061142,
      "learning_rate": 0.00027291416592581315,
      "loss": 0.697,
      "step": 26100
    },
    {
      "epoch": 0.09063203738744505,
      "grad_norm": 30.202219009399414,
      "learning_rate": 0.0002728103887837665,
      "loss": 0.7636,
      "step": 26200
    },
    {
      "epoch": 0.09097796119426735,
      "grad_norm": 0.943638801574707,
      "learning_rate": 0.00027270661164171975,
      "loss": 0.7661,
      "step": 26300
    },
    {
      "epoch": 0.09132388500108966,
      "grad_norm": 10.064764022827148,
      "learning_rate": 0.0002726028344996731,
      "loss": 0.7369,
      "step": 26400
    },
    {
      "epoch": 0.09166980880791196,
      "grad_norm": 62.77017593383789,
      "learning_rate": 0.0002724990573576264,
      "loss": 0.7246,
      "step": 26500
    },
    {
      "epoch": 0.09201573261473428,
      "grad_norm": 0.4119310975074768,
      "learning_rate": 0.00027239528021557967,
      "loss": 0.9244,
      "step": 26600
    },
    {
      "epoch": 0.09236165642155658,
      "grad_norm": 2.3914098739624023,
      "learning_rate": 0.000272291503073533,
      "loss": 0.9078,
      "step": 26700
    },
    {
      "epoch": 0.0927075802283789,
      "grad_norm": 7.003692150115967,
      "learning_rate": 0.0002721877259314863,
      "loss": 0.8659,
      "step": 26800
    },
    {
      "epoch": 0.09305350403520121,
      "grad_norm": 33.57501220703125,
      "learning_rate": 0.0002720839487894396,
      "loss": 0.7706,
      "step": 26900
    },
    {
      "epoch": 0.09339942784202351,
      "grad_norm": 0.4765031933784485,
      "learning_rate": 0.0002719801716473929,
      "loss": 0.9199,
      "step": 27000
    },
    {
      "epoch": 0.09374535164884583,
      "grad_norm": 33.75341033935547,
      "learning_rate": 0.00027187639450534625,
      "loss": 0.9709,
      "step": 27100
    },
    {
      "epoch": 0.09409127545566813,
      "grad_norm": 9.60455322265625,
      "learning_rate": 0.0002717726173632996,
      "loss": 0.7718,
      "step": 27200
    },
    {
      "epoch": 0.09443719926249045,
      "grad_norm": 48.93857192993164,
      "learning_rate": 0.00027166884022125285,
      "loss": 0.7401,
      "step": 27300
    },
    {
      "epoch": 0.09478312306931275,
      "grad_norm": 0.11474484950304031,
      "learning_rate": 0.0002715650630792061,
      "loss": 0.6839,
      "step": 27400
    },
    {
      "epoch": 0.09512904687613506,
      "grad_norm": 63.01510238647461,
      "learning_rate": 0.00027146128593715944,
      "loss": 0.6778,
      "step": 27500
    },
    {
      "epoch": 0.09547497068295738,
      "grad_norm": 0.7262968420982361,
      "learning_rate": 0.00027135750879511277,
      "loss": 0.7048,
      "step": 27600
    },
    {
      "epoch": 0.09582089448977968,
      "grad_norm": 0.4285304844379425,
      "learning_rate": 0.00027125373165306604,
      "loss": 1.1925,
      "step": 27700
    },
    {
      "epoch": 0.096166818296602,
      "grad_norm": 71.14854431152344,
      "learning_rate": 0.00027114995451101937,
      "loss": 0.6942,
      "step": 27800
    },
    {
      "epoch": 0.0965127421034243,
      "grad_norm": 26.070632934570312,
      "learning_rate": 0.0002710461773689727,
      "loss": 0.8469,
      "step": 27900
    },
    {
      "epoch": 0.09685866591024661,
      "grad_norm": 0.004775227978825569,
      "learning_rate": 0.000270942400226926,
      "loss": 0.8203,
      "step": 28000
    },
    {
      "epoch": 0.09720458971706891,
      "grad_norm": 0.020252911373972893,
      "learning_rate": 0.0002708386230848793,
      "loss": 0.801,
      "step": 28100
    },
    {
      "epoch": 0.09755051352389123,
      "grad_norm": 5.033877372741699,
      "learning_rate": 0.0002707348459428326,
      "loss": 1.1486,
      "step": 28200
    },
    {
      "epoch": 0.09789643733071354,
      "grad_norm": 0.15113267302513123,
      "learning_rate": 0.00027063106880078594,
      "loss": 1.1616,
      "step": 28300
    },
    {
      "epoch": 0.09824236113753584,
      "grad_norm": 59.056427001953125,
      "learning_rate": 0.0002705272916587392,
      "loss": 1.3532,
      "step": 28400
    },
    {
      "epoch": 0.09858828494435816,
      "grad_norm": 16.869014739990234,
      "learning_rate": 0.00027042351451669254,
      "loss": 0.923,
      "step": 28500
    },
    {
      "epoch": 0.09893420875118046,
      "grad_norm": 0.30444493889808655,
      "learning_rate": 0.00027031973737464587,
      "loss": 0.6065,
      "step": 28600
    },
    {
      "epoch": 0.09928013255800278,
      "grad_norm": 39.621253967285156,
      "learning_rate": 0.00027021596023259914,
      "loss": 0.7827,
      "step": 28700
    },
    {
      "epoch": 0.09962605636482508,
      "grad_norm": 36.627540588378906,
      "learning_rate": 0.00027011218309055246,
      "loss": 0.673,
      "step": 28800
    },
    {
      "epoch": 0.09997198017164739,
      "grad_norm": 3.002368927001953,
      "learning_rate": 0.00027000840594850573,
      "loss": 0.5127,
      "step": 28900
    },
    {
      "epoch": 0.10031790397846971,
      "grad_norm": 48.44084548950195,
      "learning_rate": 0.00026990462880645906,
      "loss": 0.6804,
      "step": 29000
    },
    {
      "epoch": 0.10066382778529201,
      "grad_norm": 31.04706382751465,
      "learning_rate": 0.0002698008516644124,
      "loss": 0.7783,
      "step": 29100
    },
    {
      "epoch": 0.10100975159211432,
      "grad_norm": 27.92231559753418,
      "learning_rate": 0.00026969707452236566,
      "loss": 0.7027,
      "step": 29200
    },
    {
      "epoch": 0.10135567539893663,
      "grad_norm": 0.5463458299636841,
      "learning_rate": 0.000269593297380319,
      "loss": 0.7839,
      "step": 29300
    },
    {
      "epoch": 0.10170159920575894,
      "grad_norm": 0.004078391939401627,
      "learning_rate": 0.0002694895202382723,
      "loss": 0.7893,
      "step": 29400
    },
    {
      "epoch": 0.10204752301258124,
      "grad_norm": 61.73844909667969,
      "learning_rate": 0.0002693857430962256,
      "loss": 0.9376,
      "step": 29500
    },
    {
      "epoch": 0.10239344681940356,
      "grad_norm": 93.8118896484375,
      "learning_rate": 0.0002692819659541789,
      "loss": 0.7931,
      "step": 29600
    },
    {
      "epoch": 0.10273937062622587,
      "grad_norm": 29.57907485961914,
      "learning_rate": 0.00026917818881213223,
      "loss": 0.5563,
      "step": 29700
    },
    {
      "epoch": 0.10308529443304817,
      "grad_norm": 0.1390123963356018,
      "learning_rate": 0.00026907441167008556,
      "loss": 0.6272,
      "step": 29800
    },
    {
      "epoch": 0.10343121823987049,
      "grad_norm": 25.575069427490234,
      "learning_rate": 0.00026897063452803883,
      "loss": 0.8581,
      "step": 29900
    },
    {
      "epoch": 0.10377714204669279,
      "grad_norm": 2.092301368713379,
      "learning_rate": 0.00026886685738599216,
      "loss": 0.6681,
      "step": 30000
    },
    {
      "epoch": 0.1041230658535151,
      "grad_norm": 0.0036356502678245306,
      "learning_rate": 0.0002687630802439455,
      "loss": 0.803,
      "step": 30100
    },
    {
      "epoch": 0.10446898966033741,
      "grad_norm": 0.23175400495529175,
      "learning_rate": 0.00026865930310189875,
      "loss": 0.723,
      "step": 30200
    },
    {
      "epoch": 0.10481491346715972,
      "grad_norm": 21.897218704223633,
      "learning_rate": 0.000268555525959852,
      "loss": 0.6237,
      "step": 30300
    },
    {
      "epoch": 0.10516083727398204,
      "grad_norm": 0.5052214860916138,
      "learning_rate": 0.00026845174881780535,
      "loss": 0.8921,
      "step": 30400
    },
    {
      "epoch": 0.10550676108080434,
      "grad_norm": 12.71338939666748,
      "learning_rate": 0.0002683479716757587,
      "loss": 0.5703,
      "step": 30500
    },
    {
      "epoch": 0.10585268488762666,
      "grad_norm": 41.472408294677734,
      "learning_rate": 0.00026824419453371195,
      "loss": 0.8408,
      "step": 30600
    },
    {
      "epoch": 0.10619860869444896,
      "grad_norm": 0.01693153753876686,
      "learning_rate": 0.0002681404173916653,
      "loss": 0.6759,
      "step": 30700
    },
    {
      "epoch": 0.10654453250127127,
      "grad_norm": 50.42009353637695,
      "learning_rate": 0.0002680366402496186,
      "loss": 0.7022,
      "step": 30800
    },
    {
      "epoch": 0.10689045630809357,
      "grad_norm": 34.55710983276367,
      "learning_rate": 0.0002679328631075719,
      "loss": 0.9314,
      "step": 30900
    },
    {
      "epoch": 0.10723638011491589,
      "grad_norm": 1.7255438566207886,
      "learning_rate": 0.0002678290859655252,
      "loss": 0.77,
      "step": 31000
    },
    {
      "epoch": 0.1075823039217382,
      "grad_norm": 73.54975891113281,
      "learning_rate": 0.0002677253088234785,
      "loss": 0.9436,
      "step": 31100
    },
    {
      "epoch": 0.1079282277285605,
      "grad_norm": 0.011119499802589417,
      "learning_rate": 0.00026762153168143185,
      "loss": 0.6906,
      "step": 31200
    },
    {
      "epoch": 0.10827415153538282,
      "grad_norm": 1.8602495193481445,
      "learning_rate": 0.0002675177545393851,
      "loss": 0.907,
      "step": 31300
    },
    {
      "epoch": 0.10862007534220512,
      "grad_norm": 5.065736293792725,
      "learning_rate": 0.00026741397739733845,
      "loss": 1.0681,
      "step": 31400
    },
    {
      "epoch": 0.10896599914902744,
      "grad_norm": 12.521893501281738,
      "learning_rate": 0.0002673102002552918,
      "loss": 0.7471,
      "step": 31500
    },
    {
      "epoch": 0.10931192295584974,
      "grad_norm": 0.8981766104698181,
      "learning_rate": 0.00026720642311324505,
      "loss": 0.7083,
      "step": 31600
    },
    {
      "epoch": 0.10965784676267205,
      "grad_norm": 0.9378552436828613,
      "learning_rate": 0.00026710264597119837,
      "loss": 1.4127,
      "step": 31700
    },
    {
      "epoch": 0.11000377056949437,
      "grad_norm": 0.0843615010380745,
      "learning_rate": 0.00026699886882915164,
      "loss": 1.0021,
      "step": 31800
    },
    {
      "epoch": 0.11034969437631667,
      "grad_norm": 28.86037254333496,
      "learning_rate": 0.00026689509168710497,
      "loss": 0.8108,
      "step": 31900
    },
    {
      "epoch": 0.11069561818313899,
      "grad_norm": 51.84280014038086,
      "learning_rate": 0.0002667913145450583,
      "loss": 0.8698,
      "step": 32000
    },
    {
      "epoch": 0.11104154198996129,
      "grad_norm": 5.089604377746582,
      "learning_rate": 0.00026668753740301157,
      "loss": 0.7946,
      "step": 32100
    },
    {
      "epoch": 0.1113874657967836,
      "grad_norm": 0.3307335376739502,
      "learning_rate": 0.0002665837602609649,
      "loss": 0.7367,
      "step": 32200
    },
    {
      "epoch": 0.1117333896036059,
      "grad_norm": 15.191527366638184,
      "learning_rate": 0.0002664799831189182,
      "loss": 0.9132,
      "step": 32300
    },
    {
      "epoch": 0.11207931341042822,
      "grad_norm": 0.4689025282859802,
      "learning_rate": 0.0002663762059768715,
      "loss": 0.683,
      "step": 32400
    },
    {
      "epoch": 0.11242523721725053,
      "grad_norm": 19.12290382385254,
      "learning_rate": 0.0002662724288348248,
      "loss": 0.7776,
      "step": 32500
    },
    {
      "epoch": 0.11277116102407284,
      "grad_norm": 0.003379718866199255,
      "learning_rate": 0.00026616865169277814,
      "loss": 0.8217,
      "step": 32600
    },
    {
      "epoch": 0.11311708483089515,
      "grad_norm": 8.844401359558105,
      "learning_rate": 0.00026606487455073147,
      "loss": 0.7601,
      "step": 32700
    },
    {
      "epoch": 0.11346300863771745,
      "grad_norm": 4.675299644470215,
      "learning_rate": 0.00026596109740868474,
      "loss": 0.4982,
      "step": 32800
    },
    {
      "epoch": 0.11380893244453977,
      "grad_norm": 65.31459045410156,
      "learning_rate": 0.00026585732026663807,
      "loss": 0.8693,
      "step": 32900
    },
    {
      "epoch": 0.11415485625136207,
      "grad_norm": 4.074530124664307,
      "learning_rate": 0.00026575354312459134,
      "loss": 0.7247,
      "step": 33000
    },
    {
      "epoch": 0.11450078005818438,
      "grad_norm": 17.19121742248535,
      "learning_rate": 0.00026564976598254466,
      "loss": 0.7009,
      "step": 33100
    },
    {
      "epoch": 0.1148467038650067,
      "grad_norm": 0.08856948465108871,
      "learning_rate": 0.00026554598884049793,
      "loss": 0.78,
      "step": 33200
    },
    {
      "epoch": 0.115192627671829,
      "grad_norm": 65.01275634765625,
      "learning_rate": 0.00026544221169845126,
      "loss": 0.7635,
      "step": 33300
    },
    {
      "epoch": 0.11553855147865132,
      "grad_norm": 0.05998363345861435,
      "learning_rate": 0.0002653384345564046,
      "loss": 0.5599,
      "step": 33400
    },
    {
      "epoch": 0.11588447528547362,
      "grad_norm": 0.7426332235336304,
      "learning_rate": 0.0002652346574143579,
      "loss": 0.7585,
      "step": 33500
    },
    {
      "epoch": 0.11623039909229593,
      "grad_norm": 0.12173561751842499,
      "learning_rate": 0.0002651308802723112,
      "loss": 0.7864,
      "step": 33600
    },
    {
      "epoch": 0.11657632289911823,
      "grad_norm": 96.049072265625,
      "learning_rate": 0.0002650271031302645,
      "loss": 0.8062,
      "step": 33700
    },
    {
      "epoch": 0.11692224670594055,
      "grad_norm": 0.07900673896074295,
      "learning_rate": 0.00026492332598821784,
      "loss": 0.5629,
      "step": 33800
    },
    {
      "epoch": 0.11726817051276286,
      "grad_norm": 0.04264984652400017,
      "learning_rate": 0.0002648195488461711,
      "loss": 0.9508,
      "step": 33900
    },
    {
      "epoch": 0.11761409431958517,
      "grad_norm": 0.20858576893806458,
      "learning_rate": 0.00026471577170412443,
      "loss": 0.7293,
      "step": 34000
    },
    {
      "epoch": 0.11796001812640748,
      "grad_norm": 10.208868026733398,
      "learning_rate": 0.00026461199456207776,
      "loss": 0.6787,
      "step": 34100
    },
    {
      "epoch": 0.11830594193322978,
      "grad_norm": 13.528360366821289,
      "learning_rate": 0.00026450821742003103,
      "loss": 0.6854,
      "step": 34200
    },
    {
      "epoch": 0.1186518657400521,
      "grad_norm": 52.12978744506836,
      "learning_rate": 0.00026440444027798436,
      "loss": 0.8152,
      "step": 34300
    },
    {
      "epoch": 0.1189977895468744,
      "grad_norm": 0.010471667163074017,
      "learning_rate": 0.00026430066313593763,
      "loss": 0.705,
      "step": 34400
    },
    {
      "epoch": 0.11934371335369671,
      "grad_norm": 3.1140363216400146,
      "learning_rate": 0.00026419688599389095,
      "loss": 0.9893,
      "step": 34500
    },
    {
      "epoch": 0.11968963716051903,
      "grad_norm": 0.009006294421851635,
      "learning_rate": 0.0002640931088518443,
      "loss": 0.6925,
      "step": 34600
    },
    {
      "epoch": 0.12003556096734133,
      "grad_norm": 0.3707354664802551,
      "learning_rate": 0.00026398933170979755,
      "loss": 0.4506,
      "step": 34700
    },
    {
      "epoch": 0.12038148477416365,
      "grad_norm": 37.739013671875,
      "learning_rate": 0.0002638855545677509,
      "loss": 0.5178,
      "step": 34800
    },
    {
      "epoch": 0.12072740858098595,
      "grad_norm": 0.007386815268546343,
      "learning_rate": 0.0002637817774257042,
      "loss": 0.7737,
      "step": 34900
    },
    {
      "epoch": 0.12107333238780826,
      "grad_norm": 0.08617031574249268,
      "learning_rate": 0.0002636780002836575,
      "loss": 0.5638,
      "step": 35000
    },
    {
      "epoch": 0.12141925619463056,
      "grad_norm": 2.850306510925293,
      "learning_rate": 0.0002635742231416108,
      "loss": 0.6619,
      "step": 35100
    },
    {
      "epoch": 0.12176518000145288,
      "grad_norm": 0.9081066846847534,
      "learning_rate": 0.0002634704459995641,
      "loss": 0.7886,
      "step": 35200
    },
    {
      "epoch": 0.1221111038082752,
      "grad_norm": 45.83928298950195,
      "learning_rate": 0.00026336666885751745,
      "loss": 0.6704,
      "step": 35300
    },
    {
      "epoch": 0.1224570276150975,
      "grad_norm": 103.85492706298828,
      "learning_rate": 0.0002632628917154707,
      "loss": 0.7241,
      "step": 35400
    },
    {
      "epoch": 0.12280295142191981,
      "grad_norm": 1.5023765563964844,
      "learning_rate": 0.00026315911457342405,
      "loss": 0.7202,
      "step": 35500
    },
    {
      "epoch": 0.12314887522874211,
      "grad_norm": 0.002660330617800355,
      "learning_rate": 0.0002630553374313774,
      "loss": 0.6839,
      "step": 35600
    },
    {
      "epoch": 0.12349479903556443,
      "grad_norm": 0.004767289850860834,
      "learning_rate": 0.00026295156028933065,
      "loss": 0.6774,
      "step": 35700
    },
    {
      "epoch": 0.12384072284238673,
      "grad_norm": 87.4713363647461,
      "learning_rate": 0.0002628477831472839,
      "loss": 0.7072,
      "step": 35800
    },
    {
      "epoch": 0.12418664664920905,
      "grad_norm": 6.6865763664245605,
      "learning_rate": 0.00026274400600523725,
      "loss": 0.7641,
      "step": 35900
    },
    {
      "epoch": 0.12453257045603136,
      "grad_norm": 0.06156729534268379,
      "learning_rate": 0.00026264022886319057,
      "loss": 0.5388,
      "step": 36000
    },
    {
      "epoch": 0.12487849426285366,
      "grad_norm": 1.1268056631088257,
      "learning_rate": 0.00026253645172114384,
      "loss": 0.9548,
      "step": 36100
    },
    {
      "epoch": 0.12522441806967596,
      "grad_norm": 2.000131130218506,
      "learning_rate": 0.00026243267457909717,
      "loss": 0.8065,
      "step": 36200
    },
    {
      "epoch": 0.1255703418764983,
      "grad_norm": 0.9717380404472351,
      "learning_rate": 0.0002623288974370505,
      "loss": 0.6268,
      "step": 36300
    },
    {
      "epoch": 0.1259162656833206,
      "grad_norm": 0.003485446097329259,
      "learning_rate": 0.0002622251202950038,
      "loss": 0.8236,
      "step": 36400
    },
    {
      "epoch": 0.1262621894901429,
      "grad_norm": 0.5149649381637573,
      "learning_rate": 0.0002621213431529571,
      "loss": 0.8707,
      "step": 36500
    },
    {
      "epoch": 0.1266081132969652,
      "grad_norm": 0.11759306490421295,
      "learning_rate": 0.0002620175660109104,
      "loss": 0.7633,
      "step": 36600
    },
    {
      "epoch": 0.12695403710378753,
      "grad_norm": 82.27706146240234,
      "learning_rate": 0.00026191378886886374,
      "loss": 0.6285,
      "step": 36700
    },
    {
      "epoch": 0.12729996091060983,
      "grad_norm": 0.010891885496675968,
      "learning_rate": 0.000261810011726817,
      "loss": 0.8134,
      "step": 36800
    },
    {
      "epoch": 0.12764588471743213,
      "grad_norm": 22.781845092773438,
      "learning_rate": 0.00026170623458477034,
      "loss": 0.684,
      "step": 36900
    },
    {
      "epoch": 0.12799180852425446,
      "grad_norm": 0.0009496148559264839,
      "learning_rate": 0.00026160245744272367,
      "loss": 0.6415,
      "step": 37000
    },
    {
      "epoch": 0.12833773233107676,
      "grad_norm": 0.0031553292647004128,
      "learning_rate": 0.00026149868030067694,
      "loss": 0.7219,
      "step": 37100
    },
    {
      "epoch": 0.12868365613789906,
      "grad_norm": 0.3109859526157379,
      "learning_rate": 0.00026139490315863027,
      "loss": 0.7261,
      "step": 37200
    },
    {
      "epoch": 0.12902957994472136,
      "grad_norm": 0.006597843486815691,
      "learning_rate": 0.00026129112601658354,
      "loss": 0.6027,
      "step": 37300
    },
    {
      "epoch": 0.1293755037515437,
      "grad_norm": 47.20368957519531,
      "learning_rate": 0.00026118734887453686,
      "loss": 0.6821,
      "step": 37400
    },
    {
      "epoch": 0.129721427558366,
      "grad_norm": 28.742307662963867,
      "learning_rate": 0.0002610835717324902,
      "loss": 0.531,
      "step": 37500
    },
    {
      "epoch": 0.1300673513651883,
      "grad_norm": 0.012127486057579517,
      "learning_rate": 0.00026097979459044346,
      "loss": 1.0673,
      "step": 37600
    },
    {
      "epoch": 0.13041327517201062,
      "grad_norm": 18.680944442749023,
      "learning_rate": 0.0002608760174483968,
      "loss": 0.8901,
      "step": 37700
    },
    {
      "epoch": 0.13075919897883292,
      "grad_norm": 28.250856399536133,
      "learning_rate": 0.0002607722403063501,
      "loss": 0.7348,
      "step": 37800
    },
    {
      "epoch": 0.13110512278565523,
      "grad_norm": 0.036035045981407166,
      "learning_rate": 0.0002606684631643034,
      "loss": 0.906,
      "step": 37900
    },
    {
      "epoch": 0.13145104659247753,
      "grad_norm": 52.573158264160156,
      "learning_rate": 0.0002605646860222567,
      "loss": 0.6246,
      "step": 38000
    },
    {
      "epoch": 0.13179697039929986,
      "grad_norm": 24.587541580200195,
      "learning_rate": 0.00026046090888021004,
      "loss": 0.5503,
      "step": 38100
    },
    {
      "epoch": 0.13214289420612216,
      "grad_norm": 0.5687814950942993,
      "learning_rate": 0.00026035713173816336,
      "loss": 0.7118,
      "step": 38200
    },
    {
      "epoch": 0.13248881801294446,
      "grad_norm": 1.9664520025253296,
      "learning_rate": 0.00026025335459611663,
      "loss": 0.7982,
      "step": 38300
    },
    {
      "epoch": 0.1328347418197668,
      "grad_norm": 56.48698806762695,
      "learning_rate": 0.00026014957745406996,
      "loss": 0.6167,
      "step": 38400
    },
    {
      "epoch": 0.1331806656265891,
      "grad_norm": 0.0030478439293801785,
      "learning_rate": 0.0002600458003120233,
      "loss": 0.7867,
      "step": 38500
    },
    {
      "epoch": 0.1335265894334114,
      "grad_norm": 0.004400971811264753,
      "learning_rate": 0.00025994202316997656,
      "loss": 0.6058,
      "step": 38600
    },
    {
      "epoch": 0.1338725132402337,
      "grad_norm": 67.7569351196289,
      "learning_rate": 0.00025983824602792983,
      "loss": 0.4806,
      "step": 38700
    },
    {
      "epoch": 0.13421843704705602,
      "grad_norm": 0.3944048583507538,
      "learning_rate": 0.00025973446888588315,
      "loss": 0.6073,
      "step": 38800
    },
    {
      "epoch": 0.13456436085387832,
      "grad_norm": 0.0045761968940496445,
      "learning_rate": 0.0002596306917438365,
      "loss": 0.7541,
      "step": 38900
    },
    {
      "epoch": 0.13491028466070062,
      "grad_norm": 44.724151611328125,
      "learning_rate": 0.0002595269146017898,
      "loss": 0.9443,
      "step": 39000
    },
    {
      "epoch": 0.13525620846752295,
      "grad_norm": 0.04867250844836235,
      "learning_rate": 0.0002594231374597431,
      "loss": 1.001,
      "step": 39100
    },
    {
      "epoch": 0.13560213227434526,
      "grad_norm": 0.01695965975522995,
      "learning_rate": 0.0002593193603176964,
      "loss": 0.414,
      "step": 39200
    },
    {
      "epoch": 0.13594805608116756,
      "grad_norm": 14.907327651977539,
      "learning_rate": 0.00025921558317564973,
      "loss": 0.5635,
      "step": 39300
    },
    {
      "epoch": 0.13629397988798986,
      "grad_norm": 0.029858388006687164,
      "learning_rate": 0.000259111806033603,
      "loss": 0.7004,
      "step": 39400
    },
    {
      "epoch": 0.1366399036948122,
      "grad_norm": 11.56954288482666,
      "learning_rate": 0.0002590080288915563,
      "loss": 0.7601,
      "step": 39500
    },
    {
      "epoch": 0.1369858275016345,
      "grad_norm": 0.06237567216157913,
      "learning_rate": 0.00025890425174950965,
      "loss": 0.5402,
      "step": 39600
    },
    {
      "epoch": 0.1373317513084568,
      "grad_norm": 0.10405508428812027,
      "learning_rate": 0.0002588004746074629,
      "loss": 0.9311,
      "step": 39700
    },
    {
      "epoch": 0.13767767511527912,
      "grad_norm": 56.156524658203125,
      "learning_rate": 0.00025869669746541625,
      "loss": 0.5891,
      "step": 39800
    },
    {
      "epoch": 0.13802359892210142,
      "grad_norm": 9.494271278381348,
      "learning_rate": 0.0002585929203233696,
      "loss": 0.7597,
      "step": 39900
    },
    {
      "epoch": 0.13836952272892372,
      "grad_norm": 2.6317434310913086,
      "learning_rate": 0.00025848914318132285,
      "loss": 0.7132,
      "step": 40000
    },
    {
      "epoch": 0.13871544653574602,
      "grad_norm": 0.005374206230044365,
      "learning_rate": 0.0002583853660392762,
      "loss": 0.8303,
      "step": 40100
    },
    {
      "epoch": 0.13906137034256835,
      "grad_norm": 0.45178353786468506,
      "learning_rate": 0.00025828158889722945,
      "loss": 0.8025,
      "step": 40200
    },
    {
      "epoch": 0.13940729414939065,
      "grad_norm": 1.2795859575271606,
      "learning_rate": 0.00025817781175518277,
      "loss": 0.7745,
      "step": 40300
    },
    {
      "epoch": 0.13975321795621296,
      "grad_norm": 11.942971229553223,
      "learning_rate": 0.0002580740346131361,
      "loss": 0.5967,
      "step": 40400
    },
    {
      "epoch": 0.14009914176303528,
      "grad_norm": 5.200428485870361,
      "learning_rate": 0.00025797025747108937,
      "loss": 0.8126,
      "step": 40500
    },
    {
      "epoch": 0.14044506556985759,
      "grad_norm": 0.0016287300968542695,
      "learning_rate": 0.0002578664803290427,
      "loss": 0.6629,
      "step": 40600
    },
    {
      "epoch": 0.1407909893766799,
      "grad_norm": 75.3824234008789,
      "learning_rate": 0.000257762703186996,
      "loss": 0.7879,
      "step": 40700
    },
    {
      "epoch": 0.1411369131835022,
      "grad_norm": 2.8236947059631348,
      "learning_rate": 0.00025765892604494935,
      "loss": 0.7776,
      "step": 40800
    },
    {
      "epoch": 0.14148283699032452,
      "grad_norm": 0.056702952831983566,
      "learning_rate": 0.0002575551489029026,
      "loss": 0.7891,
      "step": 40900
    },
    {
      "epoch": 0.14182876079714682,
      "grad_norm": 0.10374967753887177,
      "learning_rate": 0.00025745137176085594,
      "loss": 0.7311,
      "step": 41000
    },
    {
      "epoch": 0.14217468460396912,
      "grad_norm": 28.299179077148438,
      "learning_rate": 0.00025734759461880927,
      "loss": 0.7422,
      "step": 41100
    },
    {
      "epoch": 0.14252060841079145,
      "grad_norm": 0.05797557160258293,
      "learning_rate": 0.00025724381747676254,
      "loss": 0.6328,
      "step": 41200
    },
    {
      "epoch": 0.14286653221761375,
      "grad_norm": 39.767757415771484,
      "learning_rate": 0.00025714004033471587,
      "loss": 0.6661,
      "step": 41300
    },
    {
      "epoch": 0.14321245602443605,
      "grad_norm": 55.5600700378418,
      "learning_rate": 0.00025703626319266914,
      "loss": 0.6152,
      "step": 41400
    },
    {
      "epoch": 0.14355837983125835,
      "grad_norm": 0.018507007509469986,
      "learning_rate": 0.00025693248605062247,
      "loss": 0.7057,
      "step": 41500
    },
    {
      "epoch": 0.14390430363808068,
      "grad_norm": 0.0036437148228287697,
      "learning_rate": 0.0002568287089085758,
      "loss": 0.6626,
      "step": 41600
    },
    {
      "epoch": 0.14425022744490298,
      "grad_norm": 0.09694688767194748,
      "learning_rate": 0.00025672493176652906,
      "loss": 0.7859,
      "step": 41700
    },
    {
      "epoch": 0.14459615125172529,
      "grad_norm": 0.28352075815200806,
      "learning_rate": 0.0002566211546244824,
      "loss": 0.6015,
      "step": 41800
    },
    {
      "epoch": 0.14494207505854761,
      "grad_norm": 0.10044676065444946,
      "learning_rate": 0.0002565173774824357,
      "loss": 0.8197,
      "step": 41900
    },
    {
      "epoch": 0.14528799886536992,
      "grad_norm": 0.6443066596984863,
      "learning_rate": 0.000256413600340389,
      "loss": 0.5408,
      "step": 42000
    },
    {
      "epoch": 0.14563392267219222,
      "grad_norm": 0.020836202427744865,
      "learning_rate": 0.0002563098231983423,
      "loss": 1.0067,
      "step": 42100
    },
    {
      "epoch": 0.14597984647901452,
      "grad_norm": 0.002259710803627968,
      "learning_rate": 0.00025620604605629564,
      "loss": 0.6867,
      "step": 42200
    },
    {
      "epoch": 0.14632577028583685,
      "grad_norm": 0.002030960749834776,
      "learning_rate": 0.0002561022689142489,
      "loss": 0.716,
      "step": 42300
    },
    {
      "epoch": 0.14667169409265915,
      "grad_norm": 0.019050583243370056,
      "learning_rate": 0.00025599849177220224,
      "loss": 0.489,
      "step": 42400
    },
    {
      "epoch": 0.14701761789948145,
      "grad_norm": 0.10145772248506546,
      "learning_rate": 0.00025589471463015556,
      "loss": 0.3994,
      "step": 42500
    },
    {
      "epoch": 0.14736354170630378,
      "grad_norm": 0.5101712942123413,
      "learning_rate": 0.00025579093748810883,
      "loss": 0.7299,
      "step": 42600
    },
    {
      "epoch": 0.14770946551312608,
      "grad_norm": 28.45737075805664,
      "learning_rate": 0.00025568716034606216,
      "loss": 0.7251,
      "step": 42700
    },
    {
      "epoch": 0.14805538931994838,
      "grad_norm": 0.003951540216803551,
      "learning_rate": 0.00025558338320401543,
      "loss": 0.7512,
      "step": 42800
    },
    {
      "epoch": 0.14840131312677068,
      "grad_norm": 0.019009966403245926,
      "learning_rate": 0.00025547960606196876,
      "loss": 0.5704,
      "step": 42900
    },
    {
      "epoch": 0.148747236933593,
      "grad_norm": 0.9919891953468323,
      "learning_rate": 0.0002553758289199221,
      "loss": 0.5845,
      "step": 43000
    },
    {
      "epoch": 0.14909316074041531,
      "grad_norm": 0.05571897327899933,
      "learning_rate": 0.00025527205177787535,
      "loss": 0.8375,
      "step": 43100
    },
    {
      "epoch": 0.14943908454723762,
      "grad_norm": 77.39750671386719,
      "learning_rate": 0.0002551682746358287,
      "loss": 0.7141,
      "step": 43200
    },
    {
      "epoch": 0.14978500835405995,
      "grad_norm": 0.01636575348675251,
      "learning_rate": 0.000255064497493782,
      "loss": 0.722,
      "step": 43300
    },
    {
      "epoch": 0.15013093216088225,
      "grad_norm": 0.003891963278874755,
      "learning_rate": 0.0002549607203517353,
      "loss": 0.6364,
      "step": 43400
    },
    {
      "epoch": 0.15047685596770455,
      "grad_norm": 0.907105565071106,
      "learning_rate": 0.0002548569432096886,
      "loss": 0.8882,
      "step": 43500
    },
    {
      "epoch": 0.15082277977452685,
      "grad_norm": 0.0588458888232708,
      "learning_rate": 0.00025475316606764193,
      "loss": 0.6999,
      "step": 43600
    },
    {
      "epoch": 0.15116870358134918,
      "grad_norm": 101.43437957763672,
      "learning_rate": 0.00025464938892559526,
      "loss": 0.6996,
      "step": 43700
    },
    {
      "epoch": 0.15151462738817148,
      "grad_norm": 0.00903837289661169,
      "learning_rate": 0.0002545456117835485,
      "loss": 0.8904,
      "step": 43800
    },
    {
      "epoch": 0.15186055119499378,
      "grad_norm": 0.1306694746017456,
      "learning_rate": 0.00025444183464150185,
      "loss": 0.6175,
      "step": 43900
    },
    {
      "epoch": 0.1522064750018161,
      "grad_norm": 0.08643608540296555,
      "learning_rate": 0.0002543380574994552,
      "loss": 0.7182,
      "step": 44000
    },
    {
      "epoch": 0.1525523988086384,
      "grad_norm": 18.30243492126465,
      "learning_rate": 0.00025423428035740845,
      "loss": 0.5323,
      "step": 44100
    },
    {
      "epoch": 0.1528983226154607,
      "grad_norm": 19.928043365478516,
      "learning_rate": 0.0002541305032153617,
      "loss": 0.6474,
      "step": 44200
    },
    {
      "epoch": 0.15324424642228301,
      "grad_norm": 29.54545021057129,
      "learning_rate": 0.00025402672607331505,
      "loss": 0.6284,
      "step": 44300
    },
    {
      "epoch": 0.15359017022910534,
      "grad_norm": 0.0017415358452126384,
      "learning_rate": 0.0002539229489312684,
      "loss": 0.3984,
      "step": 44400
    },
    {
      "epoch": 0.15393609403592765,
      "grad_norm": 2.63386607170105,
      "learning_rate": 0.0002538191717892217,
      "loss": 0.6451,
      "step": 44500
    },
    {
      "epoch": 0.15428201784274995,
      "grad_norm": 98.4328842163086,
      "learning_rate": 0.00025371539464717497,
      "loss": 0.7224,
      "step": 44600
    },
    {
      "epoch": 0.15462794164957228,
      "grad_norm": 32.83982849121094,
      "learning_rate": 0.0002536116175051283,
      "loss": 0.5721,
      "step": 44700
    },
    {
      "epoch": 0.15497386545639458,
      "grad_norm": 6.805930137634277,
      "learning_rate": 0.0002535078403630816,
      "loss": 0.7116,
      "step": 44800
    },
    {
      "epoch": 0.15531978926321688,
      "grad_norm": 0.004987741354852915,
      "learning_rate": 0.0002534040632210349,
      "loss": 0.7608,
      "step": 44900
    },
    {
      "epoch": 0.15566571307003918,
      "grad_norm": 0.031199900433421135,
      "learning_rate": 0.0002533002860789882,
      "loss": 0.9139,
      "step": 45000
    },
    {
      "epoch": 0.1560116368768615,
      "grad_norm": 1.0506904125213623,
      "learning_rate": 0.00025319650893694155,
      "loss": 0.6372,
      "step": 45100
    },
    {
      "epoch": 0.1563575606836838,
      "grad_norm": 15.075906753540039,
      "learning_rate": 0.0002530927317948948,
      "loss": 0.5758,
      "step": 45200
    },
    {
      "epoch": 0.1567034844905061,
      "grad_norm": 0.0167478546500206,
      "learning_rate": 0.00025298895465284814,
      "loss": 0.6449,
      "step": 45300
    },
    {
      "epoch": 0.15704940829732844,
      "grad_norm": 0.08245480805635452,
      "learning_rate": 0.00025288517751080147,
      "loss": 0.7598,
      "step": 45400
    },
    {
      "epoch": 0.15739533210415074,
      "grad_norm": 0.006243739277124405,
      "learning_rate": 0.0002527814003687548,
      "loss": 0.4366,
      "step": 45500
    },
    {
      "epoch": 0.15774125591097304,
      "grad_norm": 0.006302604917436838,
      "learning_rate": 0.00025267762322670807,
      "loss": 0.634,
      "step": 45600
    },
    {
      "epoch": 0.15808717971779535,
      "grad_norm": 0.001213563489727676,
      "learning_rate": 0.00025257384608466134,
      "loss": 0.6728,
      "step": 45700
    },
    {
      "epoch": 0.15843310352461767,
      "grad_norm": 0.0024464938323944807,
      "learning_rate": 0.00025247006894261467,
      "loss": 0.662,
      "step": 45800
    },
    {
      "epoch": 0.15877902733143998,
      "grad_norm": 66.13030242919922,
      "learning_rate": 0.000252366291800568,
      "loss": 0.6568,
      "step": 45900
    },
    {
      "epoch": 0.15912495113826228,
      "grad_norm": 35.59573745727539,
      "learning_rate": 0.00025226251465852126,
      "loss": 0.8388,
      "step": 46000
    },
    {
      "epoch": 0.1594708749450846,
      "grad_norm": 0.0010591420577839017,
      "learning_rate": 0.0002521587375164746,
      "loss": 0.5384,
      "step": 46100
    },
    {
      "epoch": 0.1598167987519069,
      "grad_norm": 53.17021942138672,
      "learning_rate": 0.0002520549603744279,
      "loss": 0.4748,
      "step": 46200
    },
    {
      "epoch": 0.1601627225587292,
      "grad_norm": 27.039772033691406,
      "learning_rate": 0.00025195118323238124,
      "loss": 0.551,
      "step": 46300
    },
    {
      "epoch": 0.1605086463655515,
      "grad_norm": 1.9142788648605347,
      "learning_rate": 0.0002518474060903345,
      "loss": 0.6152,
      "step": 46400
    },
    {
      "epoch": 0.16085457017237384,
      "grad_norm": 0.0037474636919796467,
      "learning_rate": 0.00025174362894828784,
      "loss": 0.732,
      "step": 46500
    },
    {
      "epoch": 0.16120049397919614,
      "grad_norm": 0.043653227388858795,
      "learning_rate": 0.00025163985180624116,
      "loss": 0.5458,
      "step": 46600
    },
    {
      "epoch": 0.16154641778601844,
      "grad_norm": 2.7554843425750732,
      "learning_rate": 0.00025153607466419444,
      "loss": 0.7344,
      "step": 46700
    },
    {
      "epoch": 0.16189234159284077,
      "grad_norm": 0.15896357595920563,
      "learning_rate": 0.00025143229752214776,
      "loss": 0.4681,
      "step": 46800
    },
    {
      "epoch": 0.16223826539966307,
      "grad_norm": 0.10297688096761703,
      "learning_rate": 0.0002513285203801011,
      "loss": 0.6117,
      "step": 46900
    },
    {
      "epoch": 0.16258418920648537,
      "grad_norm": 1.3008304834365845,
      "learning_rate": 0.00025122474323805436,
      "loss": 0.514,
      "step": 47000
    },
    {
      "epoch": 0.16293011301330768,
      "grad_norm": 0.024940507486462593,
      "learning_rate": 0.0002511209660960077,
      "loss": 0.5054,
      "step": 47100
    },
    {
      "epoch": 0.16327603682013,
      "grad_norm": 0.3913843631744385,
      "learning_rate": 0.00025101718895396096,
      "loss": 0.5601,
      "step": 47200
    },
    {
      "epoch": 0.1636219606269523,
      "grad_norm": 0.14994412660598755,
      "learning_rate": 0.0002509134118119143,
      "loss": 0.7861,
      "step": 47300
    },
    {
      "epoch": 0.1639678844337746,
      "grad_norm": 0.0011808366980403662,
      "learning_rate": 0.0002508096346698676,
      "loss": 0.8703,
      "step": 47400
    },
    {
      "epoch": 0.16431380824059694,
      "grad_norm": 0.015865763649344444,
      "learning_rate": 0.0002507058575278209,
      "loss": 0.5621,
      "step": 47500
    },
    {
      "epoch": 0.16465973204741924,
      "grad_norm": 59.59840393066406,
      "learning_rate": 0.0002506020803857742,
      "loss": 0.8149,
      "step": 47600
    },
    {
      "epoch": 0.16500565585424154,
      "grad_norm": 2.08787202835083,
      "learning_rate": 0.00025049830324372753,
      "loss": 0.8926,
      "step": 47700
    },
    {
      "epoch": 0.16535157966106384,
      "grad_norm": 0.02175288461148739,
      "learning_rate": 0.0002503945261016808,
      "loss": 0.5911,
      "step": 47800
    },
    {
      "epoch": 0.16569750346788617,
      "grad_norm": 0.5118091106414795,
      "learning_rate": 0.00025029074895963413,
      "loss": 0.6801,
      "step": 47900
    },
    {
      "epoch": 0.16604342727470847,
      "grad_norm": 0.01756940595805645,
      "learning_rate": 0.00025018697181758746,
      "loss": 0.4612,
      "step": 48000
    },
    {
      "epoch": 0.16638935108153077,
      "grad_norm": 25.814594268798828,
      "learning_rate": 0.0002500831946755408,
      "loss": 0.5016,
      "step": 48100
    },
    {
      "epoch": 0.1667352748883531,
      "grad_norm": 0.07942269742488861,
      "learning_rate": 0.00024997941753349405,
      "loss": 0.4474,
      "step": 48200
    },
    {
      "epoch": 0.1670811986951754,
      "grad_norm": 3.803884506225586,
      "learning_rate": 0.0002498756403914474,
      "loss": 0.6034,
      "step": 48300
    },
    {
      "epoch": 0.1674271225019977,
      "grad_norm": 51.07011032104492,
      "learning_rate": 0.00024977186324940065,
      "loss": 0.6811,
      "step": 48400
    },
    {
      "epoch": 0.16777304630882,
      "grad_norm": 0.004737697541713715,
      "learning_rate": 0.000249668086107354,
      "loss": 0.7864,
      "step": 48500
    },
    {
      "epoch": 0.16811897011564234,
      "grad_norm": 0.38467663526535034,
      "learning_rate": 0.00024956430896530725,
      "loss": 0.6492,
      "step": 48600
    },
    {
      "epoch": 0.16846489392246464,
      "grad_norm": 1.22112238407135,
      "learning_rate": 0.0002494605318232606,
      "loss": 0.785,
      "step": 48700
    },
    {
      "epoch": 0.16881081772928694,
      "grad_norm": 12.822296142578125,
      "learning_rate": 0.0002493567546812139,
      "loss": 0.6347,
      "step": 48800
    },
    {
      "epoch": 0.16915674153610927,
      "grad_norm": 0.060878999531269073,
      "learning_rate": 0.00024925297753916717,
      "loss": 0.6152,
      "step": 48900
    },
    {
      "epoch": 0.16950266534293157,
      "grad_norm": 43.8944091796875,
      "learning_rate": 0.0002491492003971205,
      "loss": 0.5867,
      "step": 49000
    },
    {
      "epoch": 0.16984858914975387,
      "grad_norm": 0.016391919925808907,
      "learning_rate": 0.0002490454232550738,
      "loss": 1.0005,
      "step": 49100
    },
    {
      "epoch": 0.17019451295657617,
      "grad_norm": 0.0018040116410702467,
      "learning_rate": 0.00024894164611302715,
      "loss": 0.3295,
      "step": 49200
    },
    {
      "epoch": 0.1705404367633985,
      "grad_norm": 0.13991083204746246,
      "learning_rate": 0.0002488378689709804,
      "loss": 0.4883,
      "step": 49300
    },
    {
      "epoch": 0.1708863605702208,
      "grad_norm": 0.05847681686282158,
      "learning_rate": 0.00024873409182893375,
      "loss": 0.8876,
      "step": 49400
    },
    {
      "epoch": 0.1712322843770431,
      "grad_norm": 7.4166669845581055,
      "learning_rate": 0.00024863031468688707,
      "loss": 0.6308,
      "step": 49500
    },
    {
      "epoch": 0.17157820818386543,
      "grad_norm": 0.026077650487422943,
      "learning_rate": 0.00024852653754484034,
      "loss": 0.6219,
      "step": 49600
    },
    {
      "epoch": 0.17192413199068773,
      "grad_norm": 47.41862487792969,
      "learning_rate": 0.00024842276040279367,
      "loss": 0.6974,
      "step": 49700
    },
    {
      "epoch": 0.17227005579751004,
      "grad_norm": 1.0840524435043335,
      "learning_rate": 0.00024831898326074694,
      "loss": 0.5265,
      "step": 49800
    },
    {
      "epoch": 0.17261597960433234,
      "grad_norm": 28.907358169555664,
      "learning_rate": 0.00024821520611870027,
      "loss": 0.4116,
      "step": 49900
    },
    {
      "epoch": 0.17296190341115467,
      "grad_norm": 0.0024855234660208225,
      "learning_rate": 0.0002481114289766536,
      "loss": 0.7371,
      "step": 50000
    },
    {
      "epoch": 0.17330782721797697,
      "grad_norm": 0.11991404742002487,
      "learning_rate": 0.00024800765183460687,
      "loss": 0.4457,
      "step": 50100
    },
    {
      "epoch": 0.17365375102479927,
      "grad_norm": 12.33198070526123,
      "learning_rate": 0.0002479038746925602,
      "loss": 0.4422,
      "step": 50200
    },
    {
      "epoch": 0.1739996748316216,
      "grad_norm": 11.643524169921875,
      "learning_rate": 0.0002478000975505135,
      "loss": 0.8286,
      "step": 50300
    },
    {
      "epoch": 0.1743455986384439,
      "grad_norm": 36.76458740234375,
      "learning_rate": 0.0002476963204084668,
      "loss": 0.5729,
      "step": 50400
    },
    {
      "epoch": 0.1746915224452662,
      "grad_norm": 0.8070856332778931,
      "learning_rate": 0.0002475925432664201,
      "loss": 0.5052,
      "step": 50500
    },
    {
      "epoch": 0.1750374462520885,
      "grad_norm": 0.8073468208312988,
      "learning_rate": 0.00024748876612437344,
      "loss": 0.8513,
      "step": 50600
    },
    {
      "epoch": 0.17538337005891083,
      "grad_norm": 21.093820571899414,
      "learning_rate": 0.0002473849889823267,
      "loss": 0.7028,
      "step": 50700
    },
    {
      "epoch": 0.17572929386573313,
      "grad_norm": 11.972187042236328,
      "learning_rate": 0.00024728121184028004,
      "loss": 0.7439,
      "step": 50800
    },
    {
      "epoch": 0.17607521767255543,
      "grad_norm": 10.332941055297852,
      "learning_rate": 0.00024717743469823336,
      "loss": 0.7219,
      "step": 50900
    },
    {
      "epoch": 0.17642114147937776,
      "grad_norm": 0.0014391440199688077,
      "learning_rate": 0.0002470736575561867,
      "loss": 0.6429,
      "step": 51000
    },
    {
      "epoch": 0.17676706528620006,
      "grad_norm": 21.55113410949707,
      "learning_rate": 0.00024696988041413996,
      "loss": 0.6946,
      "step": 51100
    },
    {
      "epoch": 0.17711298909302237,
      "grad_norm": 0.01746668480336666,
      "learning_rate": 0.00024686610327209323,
      "loss": 0.5915,
      "step": 51200
    },
    {
      "epoch": 0.17745891289984467,
      "grad_norm": 0.03883163630962372,
      "learning_rate": 0.00024676232613004656,
      "loss": 0.7179,
      "step": 51300
    },
    {
      "epoch": 0.177804836706667,
      "grad_norm": 0.058344513177871704,
      "learning_rate": 0.0002466585489879999,
      "loss": 0.7318,
      "step": 51400
    },
    {
      "epoch": 0.1781507605134893,
      "grad_norm": 11.904901504516602,
      "learning_rate": 0.00024655477184595316,
      "loss": 0.4718,
      "step": 51500
    },
    {
      "epoch": 0.1784966843203116,
      "grad_norm": 0.43689507246017456,
      "learning_rate": 0.0002464509947039065,
      "loss": 0.4312,
      "step": 51600
    },
    {
      "epoch": 0.17884260812713393,
      "grad_norm": 48.760223388671875,
      "learning_rate": 0.0002463472175618598,
      "loss": 0.8924,
      "step": 51700
    },
    {
      "epoch": 0.17918853193395623,
      "grad_norm": 0.003887410741299391,
      "learning_rate": 0.00024624344041981313,
      "loss": 0.3829,
      "step": 51800
    },
    {
      "epoch": 0.17953445574077853,
      "grad_norm": 0.11192022264003754,
      "learning_rate": 0.0002461396632777664,
      "loss": 0.5698,
      "step": 51900
    },
    {
      "epoch": 0.17988037954760083,
      "grad_norm": 0.4563397169113159,
      "learning_rate": 0.00024603588613571973,
      "loss": 0.7153,
      "step": 52000
    },
    {
      "epoch": 0.18022630335442316,
      "grad_norm": 0.1755942851305008,
      "learning_rate": 0.00024593210899367306,
      "loss": 0.9469,
      "step": 52100
    },
    {
      "epoch": 0.18057222716124546,
      "grad_norm": 0.0025626239366829395,
      "learning_rate": 0.00024582833185162633,
      "loss": 0.6283,
      "step": 52200
    },
    {
      "epoch": 0.18091815096806776,
      "grad_norm": 11.123466491699219,
      "learning_rate": 0.00024572455470957966,
      "loss": 0.7766,
      "step": 52300
    },
    {
      "epoch": 0.1812640747748901,
      "grad_norm": 0.09528250247240067,
      "learning_rate": 0.000245620777567533,
      "loss": 0.7823,
      "step": 52400
    },
    {
      "epoch": 0.1816099985817124,
      "grad_norm": 0.05451948568224907,
      "learning_rate": 0.00024551700042548625,
      "loss": 0.7213,
      "step": 52500
    },
    {
      "epoch": 0.1819559223885347,
      "grad_norm": 12.916853904724121,
      "learning_rate": 0.0002454132232834396,
      "loss": 0.6357,
      "step": 52600
    },
    {
      "epoch": 0.182301846195357,
      "grad_norm": 0.0006530140526592731,
      "learning_rate": 0.00024530944614139285,
      "loss": 0.6061,
      "step": 52700
    },
    {
      "epoch": 0.18264777000217933,
      "grad_norm": 0.019719937816262245,
      "learning_rate": 0.0002452056689993462,
      "loss": 0.5288,
      "step": 52800
    },
    {
      "epoch": 0.18299369380900163,
      "grad_norm": 0.1810104101896286,
      "learning_rate": 0.0002451018918572995,
      "loss": 0.4887,
      "step": 52900
    },
    {
      "epoch": 0.18333961761582393,
      "grad_norm": 37.527584075927734,
      "learning_rate": 0.0002449981147152528,
      "loss": 0.559,
      "step": 53000
    },
    {
      "epoch": 0.18368554142264626,
      "grad_norm": 14.57819652557373,
      "learning_rate": 0.0002448943375732061,
      "loss": 0.505,
      "step": 53100
    },
    {
      "epoch": 0.18403146522946856,
      "grad_norm": 0.002666672458872199,
      "learning_rate": 0.0002447905604311594,
      "loss": 0.307,
      "step": 53200
    },
    {
      "epoch": 0.18437738903629086,
      "grad_norm": 0.01861351728439331,
      "learning_rate": 0.0002446867832891127,
      "loss": 0.3793,
      "step": 53300
    },
    {
      "epoch": 0.18472331284311316,
      "grad_norm": 0.0002592272066976875,
      "learning_rate": 0.000244583006147066,
      "loss": 0.6225,
      "step": 53400
    },
    {
      "epoch": 0.1850692366499355,
      "grad_norm": 24.957103729248047,
      "learning_rate": 0.00024447922900501935,
      "loss": 0.6836,
      "step": 53500
    },
    {
      "epoch": 0.1854151604567578,
      "grad_norm": 8.593294143676758,
      "learning_rate": 0.0002443754518629727,
      "loss": 0.6122,
      "step": 53600
    },
    {
      "epoch": 0.1857610842635801,
      "grad_norm": 14.771017074584961,
      "learning_rate": 0.00024427167472092595,
      "loss": 0.7592,
      "step": 53700
    },
    {
      "epoch": 0.18610700807040242,
      "grad_norm": 4.02374267578125,
      "learning_rate": 0.00024416789757887927,
      "loss": 0.5794,
      "step": 53800
    },
    {
      "epoch": 0.18645293187722473,
      "grad_norm": 2.623853921890259,
      "learning_rate": 0.00024406412043683257,
      "loss": 0.5081,
      "step": 53900
    },
    {
      "epoch": 0.18679885568404703,
      "grad_norm": 0.0008269323734566569,
      "learning_rate": 0.00024396034329478584,
      "loss": 0.64,
      "step": 54000
    },
    {
      "epoch": 0.18714477949086933,
      "grad_norm": 23.05223274230957,
      "learning_rate": 0.00024385656615273917,
      "loss": 0.822,
      "step": 54100
    },
    {
      "epoch": 0.18749070329769166,
      "grad_norm": 0.08721721917390823,
      "learning_rate": 0.00024375278901069247,
      "loss": 0.7036,
      "step": 54200
    },
    {
      "epoch": 0.18783662710451396,
      "grad_norm": 0.0060638682916760445,
      "learning_rate": 0.0002436490118686458,
      "loss": 0.6715,
      "step": 54300
    },
    {
      "epoch": 0.18818255091133626,
      "grad_norm": 10.704259872436523,
      "learning_rate": 0.0002435452347265991,
      "loss": 0.8856,
      "step": 54400
    },
    {
      "epoch": 0.1885284747181586,
      "grad_norm": 0.03614269569516182,
      "learning_rate": 0.0002434414575845524,
      "loss": 0.5134,
      "step": 54500
    },
    {
      "epoch": 0.1888743985249809,
      "grad_norm": 0.00572844035923481,
      "learning_rate": 0.00024333768044250572,
      "loss": 0.7885,
      "step": 54600
    },
    {
      "epoch": 0.1892203223318032,
      "grad_norm": 0.006967676337808371,
      "learning_rate": 0.00024323390330045902,
      "loss": 0.2868,
      "step": 54700
    },
    {
      "epoch": 0.1895662461386255,
      "grad_norm": 7.788168907165527,
      "learning_rate": 0.00024313012615841234,
      "loss": 0.3609,
      "step": 54800
    },
    {
      "epoch": 0.18991216994544782,
      "grad_norm": 64.81468963623047,
      "learning_rate": 0.00024302634901636564,
      "loss": 0.845,
      "step": 54900
    },
    {
      "epoch": 0.19025809375227012,
      "grad_norm": 0.4740806221961975,
      "learning_rate": 0.00024292257187431894,
      "loss": 0.5003,
      "step": 55000
    },
    {
      "epoch": 0.19060401755909243,
      "grad_norm": 10.667905807495117,
      "learning_rate": 0.00024281879473227227,
      "loss": 0.6773,
      "step": 55100
    },
    {
      "epoch": 0.19094994136591475,
      "grad_norm": 0.4931919574737549,
      "learning_rate": 0.00024271501759022556,
      "loss": 0.748,
      "step": 55200
    },
    {
      "epoch": 0.19129586517273706,
      "grad_norm": 18.081871032714844,
      "learning_rate": 0.0002426112404481789,
      "loss": 0.512,
      "step": 55300
    },
    {
      "epoch": 0.19164178897955936,
      "grad_norm": 45.390193939208984,
      "learning_rate": 0.00024250746330613216,
      "loss": 0.5344,
      "step": 55400
    },
    {
      "epoch": 0.19198771278638166,
      "grad_norm": 35.14976501464844,
      "learning_rate": 0.00024240368616408546,
      "loss": 0.6103,
      "step": 55500
    },
    {
      "epoch": 0.192333636593204,
      "grad_norm": 0.9525635242462158,
      "learning_rate": 0.00024229990902203879,
      "loss": 0.5521,
      "step": 55600
    },
    {
      "epoch": 0.1926795604000263,
      "grad_norm": 0.0008172673406079412,
      "learning_rate": 0.00024219613187999209,
      "loss": 0.4161,
      "step": 55700
    },
    {
      "epoch": 0.1930254842068486,
      "grad_norm": 0.38703233003616333,
      "learning_rate": 0.00024209235473794538,
      "loss": 0.7391,
      "step": 55800
    },
    {
      "epoch": 0.19337140801367092,
      "grad_norm": 56.64665985107422,
      "learning_rate": 0.0002419885775958987,
      "loss": 0.4818,
      "step": 55900
    },
    {
      "epoch": 0.19371733182049322,
      "grad_norm": 0.010038621723651886,
      "learning_rate": 0.000241884800453852,
      "loss": 0.6968,
      "step": 56000
    },
    {
      "epoch": 0.19406325562731552,
      "grad_norm": 0.5423555970191956,
      "learning_rate": 0.00024178102331180533,
      "loss": 0.4412,
      "step": 56100
    },
    {
      "epoch": 0.19440917943413782,
      "grad_norm": 0.0015962272882461548,
      "learning_rate": 0.00024167724616975863,
      "loss": 0.5534,
      "step": 56200
    },
    {
      "epoch": 0.19475510324096015,
      "grad_norm": 0.0006714225746691227,
      "learning_rate": 0.00024157346902771193,
      "loss": 0.5859,
      "step": 56300
    },
    {
      "epoch": 0.19510102704778245,
      "grad_norm": 54.886592864990234,
      "learning_rate": 0.00024146969188566526,
      "loss": 0.6104,
      "step": 56400
    },
    {
      "epoch": 0.19544695085460476,
      "grad_norm": 0.2875933051109314,
      "learning_rate": 0.00024136591474361856,
      "loss": 0.6597,
      "step": 56500
    },
    {
      "epoch": 0.19579287466142709,
      "grad_norm": 1.6694773435592651,
      "learning_rate": 0.00024126213760157188,
      "loss": 0.7006,
      "step": 56600
    },
    {
      "epoch": 0.1961387984682494,
      "grad_norm": 0.12700943648815155,
      "learning_rate": 0.00024115836045952518,
      "loss": 0.5663,
      "step": 56700
    },
    {
      "epoch": 0.1964847222750717,
      "grad_norm": 0.03536854311823845,
      "learning_rate": 0.00024105458331747845,
      "loss": 0.7215,
      "step": 56800
    },
    {
      "epoch": 0.196830646081894,
      "grad_norm": 0.013085488229990005,
      "learning_rate": 0.00024095080617543175,
      "loss": 0.4859,
      "step": 56900
    },
    {
      "epoch": 0.19717656988871632,
      "grad_norm": 8.783520698547363,
      "learning_rate": 0.00024084702903338508,
      "loss": 0.8135,
      "step": 57000
    },
    {
      "epoch": 0.19752249369553862,
      "grad_norm": 0.003931689541786909,
      "learning_rate": 0.00024074325189133838,
      "loss": 0.5497,
      "step": 57100
    },
    {
      "epoch": 0.19786841750236092,
      "grad_norm": 0.01868565008044243,
      "learning_rate": 0.0002406394747492917,
      "loss": 0.5595,
      "step": 57200
    },
    {
      "epoch": 0.19821434130918325,
      "grad_norm": 114.29637908935547,
      "learning_rate": 0.000240535697607245,
      "loss": 0.7998,
      "step": 57300
    },
    {
      "epoch": 0.19856026511600555,
      "grad_norm": 0.02619025856256485,
      "learning_rate": 0.0002404319204651983,
      "loss": 0.5206,
      "step": 57400
    },
    {
      "epoch": 0.19890618892282785,
      "grad_norm": 22.24169921875,
      "learning_rate": 0.00024032814332315163,
      "loss": 0.6286,
      "step": 57500
    },
    {
      "epoch": 0.19925211272965015,
      "grad_norm": 0.003645445453003049,
      "learning_rate": 0.00024022436618110492,
      "loss": 0.6202,
      "step": 57600
    },
    {
      "epoch": 0.19959803653647248,
      "grad_norm": 18.110519409179688,
      "learning_rate": 0.00024012058903905825,
      "loss": 0.6743,
      "step": 57700
    },
    {
      "epoch": 0.19994396034329479,
      "grad_norm": 0.8915045857429504,
      "learning_rate": 0.00024001681189701155,
      "loss": 0.7659,
      "step": 57800
    },
    {
      "epoch": 0.2002898841501171,
      "grad_norm": 119.44865417480469,
      "learning_rate": 0.00023991303475496485,
      "loss": 0.6132,
      "step": 57900
    },
    {
      "epoch": 0.20063580795693942,
      "grad_norm": 0.006138347089290619,
      "learning_rate": 0.00023980925761291817,
      "loss": 0.5837,
      "step": 58000
    },
    {
      "epoch": 0.20098173176376172,
      "grad_norm": 0.005660656839609146,
      "learning_rate": 0.00023970548047087147,
      "loss": 0.4539,
      "step": 58100
    },
    {
      "epoch": 0.20132765557058402,
      "grad_norm": 0.00974536594003439,
      "learning_rate": 0.00023960170332882474,
      "loss": 0.4826,
      "step": 58200
    },
    {
      "epoch": 0.20167357937740632,
      "grad_norm": 2.0289459228515625,
      "learning_rate": 0.00023949792618677807,
      "loss": 0.7059,
      "step": 58300
    },
    {
      "epoch": 0.20201950318422865,
      "grad_norm": 0.003967947326600552,
      "learning_rate": 0.00023939414904473137,
      "loss": 0.6873,
      "step": 58400
    },
    {
      "epoch": 0.20236542699105095,
      "grad_norm": 2.0890328884124756,
      "learning_rate": 0.0002392903719026847,
      "loss": 0.5142,
      "step": 58500
    },
    {
      "epoch": 0.20271135079787325,
      "grad_norm": 0.0013649289030581713,
      "learning_rate": 0.000239186594760638,
      "loss": 0.5915,
      "step": 58600
    },
    {
      "epoch": 0.20305727460469558,
      "grad_norm": 0.004587590228766203,
      "learning_rate": 0.0002390828176185913,
      "loss": 0.6224,
      "step": 58700
    },
    {
      "epoch": 0.20340319841151788,
      "grad_norm": 0.006895347498357296,
      "learning_rate": 0.00023897904047654462,
      "loss": 0.4826,
      "step": 58800
    },
    {
      "epoch": 0.20374912221834018,
      "grad_norm": 0.0027299816720187664,
      "learning_rate": 0.00023887526333449792,
      "loss": 0.5352,
      "step": 58900
    },
    {
      "epoch": 0.20409504602516249,
      "grad_norm": 4.115712642669678,
      "learning_rate": 0.00023877148619245124,
      "loss": 0.7467,
      "step": 59000
    },
    {
      "epoch": 0.20444096983198481,
      "grad_norm": 3.6442346572875977,
      "learning_rate": 0.00023866770905040454,
      "loss": 0.7417,
      "step": 59100
    },
    {
      "epoch": 0.20478689363880712,
      "grad_norm": 0.00025650390307419,
      "learning_rate": 0.00023856393190835784,
      "loss": 0.497,
      "step": 59200
    },
    {
      "epoch": 0.20513281744562942,
      "grad_norm": 0.0006126594380475581,
      "learning_rate": 0.00023846015476631117,
      "loss": 0.5487,
      "step": 59300
    },
    {
      "epoch": 0.20547874125245175,
      "grad_norm": 0.08829184621572495,
      "learning_rate": 0.00023835637762426447,
      "loss": 0.5139,
      "step": 59400
    },
    {
      "epoch": 0.20582466505927405,
      "grad_norm": 23.35518455505371,
      "learning_rate": 0.0002382526004822178,
      "loss": 0.5719,
      "step": 59500
    },
    {
      "epoch": 0.20617058886609635,
      "grad_norm": 0.383443683385849,
      "learning_rate": 0.00023814882334017106,
      "loss": 0.5535,
      "step": 59600
    },
    {
      "epoch": 0.20651651267291865,
      "grad_norm": 73.76378631591797,
      "learning_rate": 0.00023804504619812436,
      "loss": 0.4124,
      "step": 59700
    },
    {
      "epoch": 0.20686243647974098,
      "grad_norm": 5.435461521148682,
      "learning_rate": 0.0002379412690560777,
      "loss": 0.4491,
      "step": 59800
    },
    {
      "epoch": 0.20720836028656328,
      "grad_norm": 0.08425473421812057,
      "learning_rate": 0.00023783749191403099,
      "loss": 0.5208,
      "step": 59900
    },
    {
      "epoch": 0.20755428409338558,
      "grad_norm": 0.03251434117555618,
      "learning_rate": 0.00023773371477198429,
      "loss": 0.5603,
      "step": 60000
    },
    {
      "epoch": 0.2079002079002079,
      "grad_norm": 0.04353400692343712,
      "learning_rate": 0.0002376299376299376,
      "loss": 0.5156,
      "step": 60100
    },
    {
      "epoch": 0.2082461317070302,
      "grad_norm": 0.007395694963634014,
      "learning_rate": 0.0002375261604878909,
      "loss": 0.6345,
      "step": 60200
    },
    {
      "epoch": 0.20859205551385251,
      "grad_norm": 13.086221694946289,
      "learning_rate": 0.00023742238334584424,
      "loss": 0.6151,
      "step": 60300
    },
    {
      "epoch": 0.20893797932067482,
      "grad_norm": 0.03658069297671318,
      "learning_rate": 0.00023731860620379753,
      "loss": 0.4641,
      "step": 60400
    },
    {
      "epoch": 0.20928390312749715,
      "grad_norm": 0.005366142373532057,
      "learning_rate": 0.00023721482906175083,
      "loss": 0.6734,
      "step": 60500
    },
    {
      "epoch": 0.20962982693431945,
      "grad_norm": 56.76243591308594,
      "learning_rate": 0.00023711105191970416,
      "loss": 0.7348,
      "step": 60600
    },
    {
      "epoch": 0.20997575074114175,
      "grad_norm": 0.9807271361351013,
      "learning_rate": 0.00023700727477765746,
      "loss": 0.6583,
      "step": 60700
    },
    {
      "epoch": 0.21032167454796408,
      "grad_norm": 31.249862670898438,
      "learning_rate": 0.00023690349763561078,
      "loss": 0.8896,
      "step": 60800
    },
    {
      "epoch": 0.21066759835478638,
      "grad_norm": 2.806527614593506,
      "learning_rate": 0.00023679972049356408,
      "loss": 0.359,
      "step": 60900
    },
    {
      "epoch": 0.21101352216160868,
      "grad_norm": 0.0335770919919014,
      "learning_rate": 0.00023669594335151735,
      "loss": 0.54,
      "step": 61000
    },
    {
      "epoch": 0.21135944596843098,
      "grad_norm": 0.0024304306134581566,
      "learning_rate": 0.00023659216620947068,
      "loss": 0.732,
      "step": 61100
    },
    {
      "epoch": 0.2117053697752533,
      "grad_norm": 0.036130428314208984,
      "learning_rate": 0.00023648838906742398,
      "loss": 0.4711,
      "step": 61200
    },
    {
      "epoch": 0.2120512935820756,
      "grad_norm": 55.855995178222656,
      "learning_rate": 0.00023638461192537728,
      "loss": 0.389,
      "step": 61300
    },
    {
      "epoch": 0.2123972173888979,
      "grad_norm": 52.94243621826172,
      "learning_rate": 0.0002362808347833306,
      "loss": 0.5693,
      "step": 61400
    },
    {
      "epoch": 0.21274314119572024,
      "grad_norm": 52.47568893432617,
      "learning_rate": 0.0002361770576412839,
      "loss": 0.5962,
      "step": 61500
    },
    {
      "epoch": 0.21308906500254254,
      "grad_norm": 0.0006772588822059333,
      "learning_rate": 0.00023607328049923723,
      "loss": 0.3134,
      "step": 61600
    },
    {
      "epoch": 0.21343498880936485,
      "grad_norm": 62.4485969543457,
      "learning_rate": 0.00023596950335719053,
      "loss": 0.6146,
      "step": 61700
    },
    {
      "epoch": 0.21378091261618715,
      "grad_norm": 0.2410792112350464,
      "learning_rate": 0.00023586572621514383,
      "loss": 0.7163,
      "step": 61800
    },
    {
      "epoch": 0.21412683642300948,
      "grad_norm": 86.51529693603516,
      "learning_rate": 0.00023576194907309715,
      "loss": 0.7276,
      "step": 61900
    },
    {
      "epoch": 0.21447276022983178,
      "grad_norm": 0.012129202485084534,
      "learning_rate": 0.00023565817193105045,
      "loss": 0.5138,
      "step": 62000
    },
    {
      "epoch": 0.21481868403665408,
      "grad_norm": 0.027931947261095047,
      "learning_rate": 0.00023555439478900378,
      "loss": 0.5013,
      "step": 62100
    },
    {
      "epoch": 0.2151646078434764,
      "grad_norm": 38.459022521972656,
      "learning_rate": 0.00023545061764695708,
      "loss": 0.5946,
      "step": 62200
    },
    {
      "epoch": 0.2155105316502987,
      "grad_norm": 0.04514158517122269,
      "learning_rate": 0.00023534684050491037,
      "loss": 0.5324,
      "step": 62300
    },
    {
      "epoch": 0.215856455457121,
      "grad_norm": 33.01972961425781,
      "learning_rate": 0.00023524306336286367,
      "loss": 0.765,
      "step": 62400
    },
    {
      "epoch": 0.2162023792639433,
      "grad_norm": 0.01844627782702446,
      "learning_rate": 0.00023513928622081697,
      "loss": 0.5197,
      "step": 62500
    },
    {
      "epoch": 0.21654830307076564,
      "grad_norm": 1.241931676864624,
      "learning_rate": 0.00023503550907877027,
      "loss": 0.5793,
      "step": 62600
    },
    {
      "epoch": 0.21689422687758794,
      "grad_norm": 0.20162053406238556,
      "learning_rate": 0.0002349317319367236,
      "loss": 0.5893,
      "step": 62700
    },
    {
      "epoch": 0.21724015068441024,
      "grad_norm": 0.2532392144203186,
      "learning_rate": 0.0002348279547946769,
      "loss": 0.5537,
      "step": 62800
    },
    {
      "epoch": 0.21758607449123257,
      "grad_norm": 0.11481273174285889,
      "learning_rate": 0.00023472417765263022,
      "loss": 0.6998,
      "step": 62900
    },
    {
      "epoch": 0.21793199829805487,
      "grad_norm": 1.660927414894104,
      "learning_rate": 0.00023462040051058352,
      "loss": 0.5427,
      "step": 63000
    },
    {
      "epoch": 0.21827792210487718,
      "grad_norm": 5.844988822937012,
      "learning_rate": 0.00023451662336853682,
      "loss": 0.5628,
      "step": 63100
    },
    {
      "epoch": 0.21862384591169948,
      "grad_norm": 0.0021101790480315685,
      "learning_rate": 0.00023441284622649014,
      "loss": 0.4338,
      "step": 63200
    },
    {
      "epoch": 0.2189697697185218,
      "grad_norm": 0.00016571514424867928,
      "learning_rate": 0.00023430906908444344,
      "loss": 0.4514,
      "step": 63300
    },
    {
      "epoch": 0.2193156935253441,
      "grad_norm": 0.0009159299661405385,
      "learning_rate": 0.00023420529194239677,
      "loss": 0.3202,
      "step": 63400
    },
    {
      "epoch": 0.2196616173321664,
      "grad_norm": 0.00042679242324084044,
      "learning_rate": 0.00023410151480035007,
      "loss": 0.7985,
      "step": 63500
    },
    {
      "epoch": 0.22000754113898874,
      "grad_norm": 0.047667913138866425,
      "learning_rate": 0.00023399773765830337,
      "loss": 0.4985,
      "step": 63600
    },
    {
      "epoch": 0.22035346494581104,
      "grad_norm": 25.460651397705078,
      "learning_rate": 0.0002338939605162567,
      "loss": 0.6065,
      "step": 63700
    },
    {
      "epoch": 0.22069938875263334,
      "grad_norm": 0.004254149738699198,
      "learning_rate": 0.00023379018337420996,
      "loss": 0.7456,
      "step": 63800
    },
    {
      "epoch": 0.22104531255945564,
      "grad_norm": 0.03095031902194023,
      "learning_rate": 0.00023368640623216326,
      "loss": 0.6927,
      "step": 63900
    },
    {
      "epoch": 0.22139123636627797,
      "grad_norm": 0.0035567006561905146,
      "learning_rate": 0.0002335826290901166,
      "loss": 0.5665,
      "step": 64000
    },
    {
      "epoch": 0.22173716017310027,
      "grad_norm": 39.32142639160156,
      "learning_rate": 0.0002334788519480699,
      "loss": 0.6242,
      "step": 64100
    },
    {
      "epoch": 0.22208308397992257,
      "grad_norm": 3.3560564517974854,
      "learning_rate": 0.00023337507480602319,
      "loss": 0.3641,
      "step": 64200
    },
    {
      "epoch": 0.2224290077867449,
      "grad_norm": 43.23099136352539,
      "learning_rate": 0.0002332712976639765,
      "loss": 0.4481,
      "step": 64300
    },
    {
      "epoch": 0.2227749315935672,
      "grad_norm": 0.013023339211940765,
      "learning_rate": 0.0002331675205219298,
      "loss": 0.6499,
      "step": 64400
    },
    {
      "epoch": 0.2231208554003895,
      "grad_norm": 0.08659455925226212,
      "learning_rate": 0.00023306374337988314,
      "loss": 0.474,
      "step": 64500
    },
    {
      "epoch": 0.2234667792072118,
      "grad_norm": 31.68217658996582,
      "learning_rate": 0.00023295996623783644,
      "loss": 0.5084,
      "step": 64600
    },
    {
      "epoch": 0.22381270301403414,
      "grad_norm": 0.022486960515379906,
      "learning_rate": 0.00023285618909578973,
      "loss": 0.5261,
      "step": 64700
    },
    {
      "epoch": 0.22415862682085644,
      "grad_norm": 0.0007789632072672248,
      "learning_rate": 0.00023275241195374306,
      "loss": 0.4427,
      "step": 64800
    },
    {
      "epoch": 0.22450455062767874,
      "grad_norm": 0.016282962635159492,
      "learning_rate": 0.00023264863481169636,
      "loss": 0.7715,
      "step": 64900
    },
    {
      "epoch": 0.22485047443450107,
      "grad_norm": 0.9983776807785034,
      "learning_rate": 0.00023254485766964969,
      "loss": 0.501,
      "step": 65000
    },
    {
      "epoch": 0.22519639824132337,
      "grad_norm": 26.631479263305664,
      "learning_rate": 0.00023244108052760298,
      "loss": 0.664,
      "step": 65100
    },
    {
      "epoch": 0.22554232204814567,
      "grad_norm": 98.18363952636719,
      "learning_rate": 0.00023233730338555626,
      "loss": 0.4375,
      "step": 65200
    },
    {
      "epoch": 0.22588824585496797,
      "grad_norm": 6.733233451843262,
      "learning_rate": 0.00023223352624350958,
      "loss": 0.4433,
      "step": 65300
    },
    {
      "epoch": 0.2262341696617903,
      "grad_norm": 20.7401180267334,
      "learning_rate": 0.00023212974910146288,
      "loss": 0.9267,
      "step": 65400
    },
    {
      "epoch": 0.2265800934686126,
      "grad_norm": 105.81749725341797,
      "learning_rate": 0.00023202597195941618,
      "loss": 0.5258,
      "step": 65500
    },
    {
      "epoch": 0.2269260172754349,
      "grad_norm": 0.16750119626522064,
      "learning_rate": 0.0002319221948173695,
      "loss": 0.4714,
      "step": 65600
    },
    {
      "epoch": 0.22727194108225723,
      "grad_norm": 21.711151123046875,
      "learning_rate": 0.0002318184176753228,
      "loss": 0.3863,
      "step": 65700
    },
    {
      "epoch": 0.22761786488907954,
      "grad_norm": 0.002209804719313979,
      "learning_rate": 0.00023171464053327613,
      "loss": 0.5026,
      "step": 65800
    },
    {
      "epoch": 0.22796378869590184,
      "grad_norm": 18.929153442382812,
      "learning_rate": 0.00023161086339122943,
      "loss": 0.565,
      "step": 65900
    },
    {
      "epoch": 0.22830971250272414,
      "grad_norm": 43.92854690551758,
      "learning_rate": 0.00023150708624918273,
      "loss": 0.7758,
      "step": 66000
    },
    {
      "epoch": 0.22865563630954647,
      "grad_norm": 0.009139001369476318,
      "learning_rate": 0.00023140330910713605,
      "loss": 0.5076,
      "step": 66100
    },
    {
      "epoch": 0.22900156011636877,
      "grad_norm": 3.7166171073913574,
      "learning_rate": 0.00023129953196508935,
      "loss": 0.498,
      "step": 66200
    },
    {
      "epoch": 0.22934748392319107,
      "grad_norm": 0.07415026426315308,
      "learning_rate": 0.00023119575482304268,
      "loss": 0.6326,
      "step": 66300
    },
    {
      "epoch": 0.2296934077300134,
      "grad_norm": 69.5672607421875,
      "learning_rate": 0.00023109197768099598,
      "loss": 0.6594,
      "step": 66400
    },
    {
      "epoch": 0.2300393315368357,
      "grad_norm": 52.77131271362305,
      "learning_rate": 0.00023098820053894928,
      "loss": 0.7158,
      "step": 66500
    },
    {
      "epoch": 0.230385255343658,
      "grad_norm": 28.40241813659668,
      "learning_rate": 0.00023088442339690257,
      "loss": 0.5365,
      "step": 66600
    },
    {
      "epoch": 0.2307311791504803,
      "grad_norm": 0.2863311171531677,
      "learning_rate": 0.00023078064625485587,
      "loss": 0.5216,
      "step": 66700
    },
    {
      "epoch": 0.23107710295730263,
      "grad_norm": 0.18482322990894318,
      "learning_rate": 0.00023067686911280917,
      "loss": 0.6036,
      "step": 66800
    },
    {
      "epoch": 0.23142302676412493,
      "grad_norm": 27.415924072265625,
      "learning_rate": 0.0002305730919707625,
      "loss": 0.5581,
      "step": 66900
    },
    {
      "epoch": 0.23176895057094724,
      "grad_norm": 0.1070929542183876,
      "learning_rate": 0.0002304693148287158,
      "loss": 0.5714,
      "step": 67000
    },
    {
      "epoch": 0.23211487437776956,
      "grad_norm": 0.0019106457475572824,
      "learning_rate": 0.00023036553768666912,
      "loss": 0.832,
      "step": 67100
    },
    {
      "epoch": 0.23246079818459187,
      "grad_norm": 0.03783779963850975,
      "learning_rate": 0.00023026176054462242,
      "loss": 0.7083,
      "step": 67200
    },
    {
      "epoch": 0.23280672199141417,
      "grad_norm": 0.010052070952951908,
      "learning_rate": 0.00023015798340257572,
      "loss": 0.5125,
      "step": 67300
    },
    {
      "epoch": 0.23315264579823647,
      "grad_norm": 42.07722473144531,
      "learning_rate": 0.00023005420626052905,
      "loss": 0.7201,
      "step": 67400
    },
    {
      "epoch": 0.2334985696050588,
      "grad_norm": 72.9076919555664,
      "learning_rate": 0.00022995042911848234,
      "loss": 0.4499,
      "step": 67500
    },
    {
      "epoch": 0.2338444934118811,
      "grad_norm": 14.55495548248291,
      "learning_rate": 0.00022984665197643567,
      "loss": 0.4972,
      "step": 67600
    },
    {
      "epoch": 0.2341904172187034,
      "grad_norm": 11.779839515686035,
      "learning_rate": 0.00022974287483438897,
      "loss": 0.5378,
      "step": 67700
    },
    {
      "epoch": 0.23453634102552573,
      "grad_norm": 70.2092056274414,
      "learning_rate": 0.00022963909769234227,
      "loss": 0.3879,
      "step": 67800
    },
    {
      "epoch": 0.23488226483234803,
      "grad_norm": 0.002127970103174448,
      "learning_rate": 0.0002295353205502956,
      "loss": 0.6066,
      "step": 67900
    },
    {
      "epoch": 0.23522818863917033,
      "grad_norm": 0.002082447288557887,
      "learning_rate": 0.00022943154340824887,
      "loss": 0.6258,
      "step": 68000
    },
    {
      "epoch": 0.23557411244599263,
      "grad_norm": 1.66973876953125,
      "learning_rate": 0.00022932776626620216,
      "loss": 0.649,
      "step": 68100
    },
    {
      "epoch": 0.23592003625281496,
      "grad_norm": 0.0010283613810315728,
      "learning_rate": 0.0002292239891241555,
      "loss": 0.4851,
      "step": 68200
    },
    {
      "epoch": 0.23626596005963726,
      "grad_norm": 0.00216593942604959,
      "learning_rate": 0.0002291202119821088,
      "loss": 0.3928,
      "step": 68300
    },
    {
      "epoch": 0.23661188386645957,
      "grad_norm": 0.0012712787138298154,
      "learning_rate": 0.00022901643484006211,
      "loss": 0.5776,
      "step": 68400
    },
    {
      "epoch": 0.2369578076732819,
      "grad_norm": 0.024761758744716644,
      "learning_rate": 0.0002289126576980154,
      "loss": 0.3421,
      "step": 68500
    },
    {
      "epoch": 0.2373037314801042,
      "grad_norm": 38.135032653808594,
      "learning_rate": 0.0002288088805559687,
      "loss": 0.4957,
      "step": 68600
    },
    {
      "epoch": 0.2376496552869265,
      "grad_norm": 0.07469280064105988,
      "learning_rate": 0.00022870510341392204,
      "loss": 0.3669,
      "step": 68700
    },
    {
      "epoch": 0.2379955790937488,
      "grad_norm": 0.07893290370702744,
      "learning_rate": 0.00022860132627187534,
      "loss": 0.4276,
      "step": 68800
    },
    {
      "epoch": 0.23834150290057113,
      "grad_norm": 60.53995895385742,
      "learning_rate": 0.00022849754912982866,
      "loss": 0.5112,
      "step": 68900
    },
    {
      "epoch": 0.23868742670739343,
      "grad_norm": 3.434239625930786,
      "learning_rate": 0.00022839377198778196,
      "loss": 0.6192,
      "step": 69000
    },
    {
      "epoch": 0.23903335051421573,
      "grad_norm": 31.86847496032715,
      "learning_rate": 0.00022828999484573526,
      "loss": 0.5932,
      "step": 69100
    },
    {
      "epoch": 0.23937927432103806,
      "grad_norm": 39.753662109375,
      "learning_rate": 0.00022818621770368859,
      "loss": 0.6472,
      "step": 69200
    },
    {
      "epoch": 0.23972519812786036,
      "grad_norm": 0.030515214428305626,
      "learning_rate": 0.00022808244056164189,
      "loss": 0.51,
      "step": 69300
    },
    {
      "epoch": 0.24007112193468266,
      "grad_norm": 94.57492065429688,
      "learning_rate": 0.00022797866341959516,
      "loss": 0.4443,
      "step": 69400
    },
    {
      "epoch": 0.24041704574150496,
      "grad_norm": 0.0896575003862381,
      "learning_rate": 0.00022787488627754848,
      "loss": 0.6044,
      "step": 69500
    },
    {
      "epoch": 0.2407629695483273,
      "grad_norm": 17.437435150146484,
      "learning_rate": 0.00022777110913550178,
      "loss": 0.4327,
      "step": 69600
    },
    {
      "epoch": 0.2411088933551496,
      "grad_norm": 47.52354049682617,
      "learning_rate": 0.00022766733199345508,
      "loss": 0.4837,
      "step": 69700
    },
    {
      "epoch": 0.2414548171619719,
      "grad_norm": 3.264887571334839,
      "learning_rate": 0.0002275635548514084,
      "loss": 0.3884,
      "step": 69800
    },
    {
      "epoch": 0.24180074096879423,
      "grad_norm": 0.4509597718715668,
      "learning_rate": 0.0002274597777093617,
      "loss": 0.6585,
      "step": 69900
    },
    {
      "epoch": 0.24214666477561653,
      "grad_norm": 0.18765632808208466,
      "learning_rate": 0.00022735600056731503,
      "loss": 0.6776,
      "step": 70000
    },
    {
      "epoch": 0.24249258858243883,
      "grad_norm": 0.004320170730352402,
      "learning_rate": 0.00022725222342526833,
      "loss": 0.6238,
      "step": 70100
    },
    {
      "epoch": 0.24283851238926113,
      "grad_norm": 7.149511814117432,
      "learning_rate": 0.00022714844628322163,
      "loss": 0.4482,
      "step": 70200
    },
    {
      "epoch": 0.24318443619608346,
      "grad_norm": 68.22732543945312,
      "learning_rate": 0.00022704466914117495,
      "loss": 0.6284,
      "step": 70300
    },
    {
      "epoch": 0.24353036000290576,
      "grad_norm": 29.211788177490234,
      "learning_rate": 0.00022694089199912825,
      "loss": 0.5999,
      "step": 70400
    },
    {
      "epoch": 0.24387628380972806,
      "grad_norm": 0.005749278236180544,
      "learning_rate": 0.00022683711485708158,
      "loss": 0.4705,
      "step": 70500
    },
    {
      "epoch": 0.2442222076165504,
      "grad_norm": 0.007509247865527868,
      "learning_rate": 0.00022673333771503488,
      "loss": 0.3275,
      "step": 70600
    },
    {
      "epoch": 0.2445681314233727,
      "grad_norm": 45.00638961791992,
      "learning_rate": 0.00022662956057298818,
      "loss": 0.2473,
      "step": 70700
    },
    {
      "epoch": 0.244914055230195,
      "grad_norm": 0.002974900184199214,
      "learning_rate": 0.00022652578343094148,
      "loss": 0.3957,
      "step": 70800
    },
    {
      "epoch": 0.2452599790370173,
      "grad_norm": 0.016206586733460426,
      "learning_rate": 0.00022642200628889477,
      "loss": 0.5823,
      "step": 70900
    },
    {
      "epoch": 0.24560590284383962,
      "grad_norm": 0.0037677029613405466,
      "learning_rate": 0.00022631822914684807,
      "loss": 0.5375,
      "step": 71000
    },
    {
      "epoch": 0.24595182665066193,
      "grad_norm": 0.004874897189438343,
      "learning_rate": 0.0002262144520048014,
      "loss": 0.5453,
      "step": 71100
    },
    {
      "epoch": 0.24629775045748423,
      "grad_norm": 19.6954288482666,
      "learning_rate": 0.0002261106748627547,
      "loss": 0.5726,
      "step": 71200
    },
    {
      "epoch": 0.24664367426430656,
      "grad_norm": 18.343339920043945,
      "learning_rate": 0.00022600689772070802,
      "loss": 0.2846,
      "step": 71300
    },
    {
      "epoch": 0.24698959807112886,
      "grad_norm": 0.3629262149333954,
      "learning_rate": 0.00022590312057866132,
      "loss": 0.4762,
      "step": 71400
    },
    {
      "epoch": 0.24733552187795116,
      "grad_norm": 0.007144853472709656,
      "learning_rate": 0.00022579934343661462,
      "loss": 0.3762,
      "step": 71500
    },
    {
      "epoch": 0.24768144568477346,
      "grad_norm": 0.0006473817629739642,
      "learning_rate": 0.00022569556629456795,
      "loss": 0.4004,
      "step": 71600
    },
    {
      "epoch": 0.2480273694915958,
      "grad_norm": 0.4451776146888733,
      "learning_rate": 0.00022559178915252125,
      "loss": 0.5543,
      "step": 71700
    },
    {
      "epoch": 0.2483732932984181,
      "grad_norm": 19.990121841430664,
      "learning_rate": 0.00022548801201047457,
      "loss": 0.2959,
      "step": 71800
    },
    {
      "epoch": 0.2487192171052404,
      "grad_norm": 0.01532837375998497,
      "learning_rate": 0.00022538423486842787,
      "loss": 0.4226,
      "step": 71900
    },
    {
      "epoch": 0.24906514091206272,
      "grad_norm": 0.0005993971135467291,
      "learning_rate": 0.00022528045772638117,
      "loss": 0.4167,
      "step": 72000
    },
    {
      "epoch": 0.24941106471888502,
      "grad_norm": 0.00044128316221758723,
      "learning_rate": 0.0002251766805843345,
      "loss": 0.5303,
      "step": 72100
    },
    {
      "epoch": 0.24975698852570732,
      "grad_norm": 0.006824145559221506,
      "learning_rate": 0.00022507290344228777,
      "loss": 0.795,
      "step": 72200
    },
    {
      "epoch": 0.2501029123325296,
      "grad_norm": 69.94476318359375,
      "learning_rate": 0.00022496912630024107,
      "loss": 0.5361,
      "step": 72300
    },
    {
      "epoch": 0.2504488361393519,
      "grad_norm": 0.004337566439062357,
      "learning_rate": 0.0002248653491581944,
      "loss": 0.5213,
      "step": 72400
    },
    {
      "epoch": 0.25079475994617423,
      "grad_norm": 0.06312613934278488,
      "learning_rate": 0.0002247615720161477,
      "loss": 0.488,
      "step": 72500
    },
    {
      "epoch": 0.2511406837529966,
      "grad_norm": 0.002032649703323841,
      "learning_rate": 0.00022465779487410102,
      "loss": 0.5483,
      "step": 72600
    },
    {
      "epoch": 0.2514866075598189,
      "grad_norm": 0.03220398351550102,
      "learning_rate": 0.00022455401773205431,
      "loss": 0.642,
      "step": 72700
    },
    {
      "epoch": 0.2518325313666412,
      "grad_norm": 0.001858710078522563,
      "learning_rate": 0.00022445024059000761,
      "loss": 0.6708,
      "step": 72800
    },
    {
      "epoch": 0.2521784551734635,
      "grad_norm": 40.65060806274414,
      "learning_rate": 0.00022434646344796094,
      "loss": 0.5498,
      "step": 72900
    },
    {
      "epoch": 0.2525243789802858,
      "grad_norm": 0.7494527697563171,
      "learning_rate": 0.00022424268630591424,
      "loss": 0.689,
      "step": 73000
    },
    {
      "epoch": 0.2528703027871081,
      "grad_norm": 0.0011318490142002702,
      "learning_rate": 0.00022413890916386756,
      "loss": 0.4423,
      "step": 73100
    },
    {
      "epoch": 0.2532162265939304,
      "grad_norm": 174.0220947265625,
      "learning_rate": 0.00022403513202182086,
      "loss": 0.8753,
      "step": 73200
    },
    {
      "epoch": 0.25356215040075275,
      "grad_norm": 37.058868408203125,
      "learning_rate": 0.00022393135487977416,
      "loss": 0.3964,
      "step": 73300
    },
    {
      "epoch": 0.25390807420757505,
      "grad_norm": 0.04288489744067192,
      "learning_rate": 0.0002238275777377275,
      "loss": 0.8051,
      "step": 73400
    },
    {
      "epoch": 0.25425399801439735,
      "grad_norm": 50.2910270690918,
      "learning_rate": 0.00022372380059568079,
      "loss": 0.486,
      "step": 73500
    },
    {
      "epoch": 0.25459992182121965,
      "grad_norm": 0.02880709245800972,
      "learning_rate": 0.00022362002345363406,
      "loss": 0.5063,
      "step": 73600
    },
    {
      "epoch": 0.25494584562804196,
      "grad_norm": 1.1218492984771729,
      "learning_rate": 0.00022351624631158738,
      "loss": 0.2913,
      "step": 73700
    },
    {
      "epoch": 0.25529176943486426,
      "grad_norm": 33.97835159301758,
      "learning_rate": 0.00022341246916954068,
      "loss": 0.6595,
      "step": 73800
    },
    {
      "epoch": 0.25563769324168656,
      "grad_norm": 36.727783203125,
      "learning_rate": 0.000223308692027494,
      "loss": 0.3331,
      "step": 73900
    },
    {
      "epoch": 0.2559836170485089,
      "grad_norm": 45.512168884277344,
      "learning_rate": 0.0002232049148854473,
      "loss": 0.5154,
      "step": 74000
    },
    {
      "epoch": 0.2563295408553312,
      "grad_norm": 0.009020447731018066,
      "learning_rate": 0.0002231011377434006,
      "loss": 0.4772,
      "step": 74100
    },
    {
      "epoch": 0.2566754646621535,
      "grad_norm": 14.13952350616455,
      "learning_rate": 0.00022299736060135393,
      "loss": 0.5153,
      "step": 74200
    },
    {
      "epoch": 0.2570213884689758,
      "grad_norm": 0.0003124140785075724,
      "learning_rate": 0.00022289358345930723,
      "loss": 0.5587,
      "step": 74300
    },
    {
      "epoch": 0.2573673122757981,
      "grad_norm": 0.005228283349424601,
      "learning_rate": 0.00022278980631726056,
      "loss": 0.4092,
      "step": 74400
    },
    {
      "epoch": 0.2577132360826204,
      "grad_norm": 0.0003243852115701884,
      "learning_rate": 0.00022268602917521386,
      "loss": 0.4836,
      "step": 74500
    },
    {
      "epoch": 0.2580591598894427,
      "grad_norm": 0.006912593729794025,
      "learning_rate": 0.00022258225203316715,
      "loss": 0.6626,
      "step": 74600
    },
    {
      "epoch": 0.2584050836962651,
      "grad_norm": 1.3928498029708862,
      "learning_rate": 0.00022247847489112048,
      "loss": 0.8281,
      "step": 74700
    },
    {
      "epoch": 0.2587510075030874,
      "grad_norm": 0.007475649937987328,
      "learning_rate": 0.00022237469774907378,
      "loss": 0.3095,
      "step": 74800
    },
    {
      "epoch": 0.2590969313099097,
      "grad_norm": 0.19335341453552246,
      "learning_rate": 0.0002222709206070271,
      "loss": 0.6461,
      "step": 74900
    },
    {
      "epoch": 0.259442855116732,
      "grad_norm": 0.024875063449144363,
      "learning_rate": 0.00022216714346498038,
      "loss": 0.5851,
      "step": 75000
    },
    {
      "epoch": 0.2597887789235543,
      "grad_norm": 20.214920043945312,
      "learning_rate": 0.00022206336632293368,
      "loss": 0.8338,
      "step": 75100
    },
    {
      "epoch": 0.2601347027303766,
      "grad_norm": 0.00045301063801161945,
      "learning_rate": 0.00022195958918088697,
      "loss": 0.439,
      "step": 75200
    },
    {
      "epoch": 0.2604806265371989,
      "grad_norm": 0.02206069603562355,
      "learning_rate": 0.0002218558120388403,
      "loss": 0.5273,
      "step": 75300
    },
    {
      "epoch": 0.26082655034402125,
      "grad_norm": 0.0637717917561531,
      "learning_rate": 0.0002217520348967936,
      "loss": 0.489,
      "step": 75400
    },
    {
      "epoch": 0.26117247415084355,
      "grad_norm": 0.009222008287906647,
      "learning_rate": 0.00022164825775474692,
      "loss": 0.621,
      "step": 75500
    },
    {
      "epoch": 0.26151839795766585,
      "grad_norm": 2.647873878479004,
      "learning_rate": 0.00022154448061270022,
      "loss": 0.6224,
      "step": 75600
    },
    {
      "epoch": 0.26186432176448815,
      "grad_norm": 9.235819816589355,
      "learning_rate": 0.00022144070347065352,
      "loss": 0.301,
      "step": 75700
    },
    {
      "epoch": 0.26221024557131045,
      "grad_norm": 0.004985516890883446,
      "learning_rate": 0.00022133692632860685,
      "loss": 0.485,
      "step": 75800
    },
    {
      "epoch": 0.26255616937813275,
      "grad_norm": 0.18412913382053375,
      "learning_rate": 0.00022123314918656015,
      "loss": 0.4777,
      "step": 75900
    },
    {
      "epoch": 0.26290209318495505,
      "grad_norm": 0.005824988707900047,
      "learning_rate": 0.00022112937204451347,
      "loss": 0.462,
      "step": 76000
    },
    {
      "epoch": 0.2632480169917774,
      "grad_norm": 0.027720553800463676,
      "learning_rate": 0.00022102559490246677,
      "loss": 0.4845,
      "step": 76100
    },
    {
      "epoch": 0.2635939407985997,
      "grad_norm": 0.005006400868296623,
      "learning_rate": 0.00022092181776042007,
      "loss": 0.1944,
      "step": 76200
    },
    {
      "epoch": 0.263939864605422,
      "grad_norm": 8.456136703491211,
      "learning_rate": 0.0002208180406183734,
      "loss": 0.7171,
      "step": 76300
    },
    {
      "epoch": 0.2642857884122443,
      "grad_norm": 0.0019206528086215258,
      "learning_rate": 0.00022071426347632667,
      "loss": 0.474,
      "step": 76400
    },
    {
      "epoch": 0.2646317122190666,
      "grad_norm": 2.643465280532837,
      "learning_rate": 0.00022061048633427997,
      "loss": 0.5727,
      "step": 76500
    },
    {
      "epoch": 0.2649776360258889,
      "grad_norm": 0.04229854419827461,
      "learning_rate": 0.0002205067091922333,
      "loss": 0.5295,
      "step": 76600
    },
    {
      "epoch": 0.2653235598327112,
      "grad_norm": 0.0007215763325802982,
      "learning_rate": 0.0002204029320501866,
      "loss": 0.4384,
      "step": 76700
    },
    {
      "epoch": 0.2656694836395336,
      "grad_norm": 0.6785639524459839,
      "learning_rate": 0.00022029915490813992,
      "loss": 0.5196,
      "step": 76800
    },
    {
      "epoch": 0.2660154074463559,
      "grad_norm": 17.824453353881836,
      "learning_rate": 0.00022019537776609322,
      "loss": 0.4166,
      "step": 76900
    },
    {
      "epoch": 0.2663613312531782,
      "grad_norm": 11.6563720703125,
      "learning_rate": 0.00022009160062404651,
      "loss": 0.3957,
      "step": 77000
    },
    {
      "epoch": 0.2667072550600005,
      "grad_norm": 36.73861312866211,
      "learning_rate": 0.00021998782348199984,
      "loss": 0.4797,
      "step": 77100
    },
    {
      "epoch": 0.2670531788668228,
      "grad_norm": 0.054514478892087936,
      "learning_rate": 0.00021988404633995314,
      "loss": 0.5417,
      "step": 77200
    },
    {
      "epoch": 0.2673991026736451,
      "grad_norm": 0.002716182265430689,
      "learning_rate": 0.00021978026919790647,
      "loss": 0.4923,
      "step": 77300
    },
    {
      "epoch": 0.2677450264804674,
      "grad_norm": 0.0045869480818510056,
      "learning_rate": 0.00021967649205585976,
      "loss": 0.4905,
      "step": 77400
    },
    {
      "epoch": 0.26809095028728974,
      "grad_norm": 0.0005124956951476634,
      "learning_rate": 0.00021957271491381306,
      "loss": 0.4049,
      "step": 77500
    },
    {
      "epoch": 0.26843687409411204,
      "grad_norm": 1.0346784591674805,
      "learning_rate": 0.0002194689377717664,
      "loss": 0.5091,
      "step": 77600
    },
    {
      "epoch": 0.26878279790093434,
      "grad_norm": 12.896824836730957,
      "learning_rate": 0.0002193651606297197,
      "loss": 0.5188,
      "step": 77700
    },
    {
      "epoch": 0.26912872170775665,
      "grad_norm": 0.327773779630661,
      "learning_rate": 0.00021926138348767296,
      "loss": 0.5446,
      "step": 77800
    },
    {
      "epoch": 0.26947464551457895,
      "grad_norm": 0.019419265910983086,
      "learning_rate": 0.00021915760634562629,
      "loss": 0.5412,
      "step": 77900
    },
    {
      "epoch": 0.26982056932140125,
      "grad_norm": 0.016916213557124138,
      "learning_rate": 0.00021905382920357958,
      "loss": 0.4202,
      "step": 78000
    },
    {
      "epoch": 0.27016649312822355,
      "grad_norm": 117.40719604492188,
      "learning_rate": 0.0002189500520615329,
      "loss": 0.5411,
      "step": 78100
    },
    {
      "epoch": 0.2705124169350459,
      "grad_norm": 0.4235781133174896,
      "learning_rate": 0.0002188462749194862,
      "loss": 0.3446,
      "step": 78200
    },
    {
      "epoch": 0.2708583407418682,
      "grad_norm": 8.19957160949707,
      "learning_rate": 0.0002187424977774395,
      "loss": 0.3397,
      "step": 78300
    },
    {
      "epoch": 0.2712042645486905,
      "grad_norm": 0.00494789844378829,
      "learning_rate": 0.00021863872063539283,
      "loss": 0.3085,
      "step": 78400
    },
    {
      "epoch": 0.2715501883555128,
      "grad_norm": 0.014069954864680767,
      "learning_rate": 0.00021853494349334613,
      "loss": 0.5611,
      "step": 78500
    },
    {
      "epoch": 0.2718961121623351,
      "grad_norm": 0.0031331509817391634,
      "learning_rate": 0.00021843116635129946,
      "loss": 0.6703,
      "step": 78600
    },
    {
      "epoch": 0.2722420359691574,
      "grad_norm": 2.2074601650238037,
      "learning_rate": 0.00021832738920925276,
      "loss": 0.5513,
      "step": 78700
    },
    {
      "epoch": 0.2725879597759797,
      "grad_norm": 46.827430725097656,
      "learning_rate": 0.00021822361206720606,
      "loss": 0.3044,
      "step": 78800
    },
    {
      "epoch": 0.2729338835828021,
      "grad_norm": 0.012806919403374195,
      "learning_rate": 0.00021811983492515938,
      "loss": 0.2691,
      "step": 78900
    },
    {
      "epoch": 0.2732798073896244,
      "grad_norm": 106.83234405517578,
      "learning_rate": 0.00021801605778311268,
      "loss": 0.5353,
      "step": 79000
    },
    {
      "epoch": 0.2736257311964467,
      "grad_norm": 32.99807357788086,
      "learning_rate": 0.000217912280641066,
      "loss": 0.6094,
      "step": 79100
    },
    {
      "epoch": 0.273971655003269,
      "grad_norm": 0.01698201894760132,
      "learning_rate": 0.00021780850349901928,
      "loss": 0.4653,
      "step": 79200
    },
    {
      "epoch": 0.2743175788100913,
      "grad_norm": 0.46731656789779663,
      "learning_rate": 0.00021770472635697258,
      "loss": 0.3813,
      "step": 79300
    },
    {
      "epoch": 0.2746635026169136,
      "grad_norm": 163.5322723388672,
      "learning_rate": 0.0002176009492149259,
      "loss": 0.4061,
      "step": 79400
    },
    {
      "epoch": 0.2750094264237359,
      "grad_norm": 0.0008772240253165364,
      "learning_rate": 0.0002174971720728792,
      "loss": 0.3701,
      "step": 79500
    },
    {
      "epoch": 0.27535535023055824,
      "grad_norm": 0.2539866268634796,
      "learning_rate": 0.0002173933949308325,
      "loss": 0.482,
      "step": 79600
    },
    {
      "epoch": 0.27570127403738054,
      "grad_norm": 0.004203712102025747,
      "learning_rate": 0.00021728961778878583,
      "loss": 0.5249,
      "step": 79700
    },
    {
      "epoch": 0.27604719784420284,
      "grad_norm": 0.16788838803768158,
      "learning_rate": 0.00021718584064673912,
      "loss": 0.4161,
      "step": 79800
    },
    {
      "epoch": 0.27639312165102514,
      "grad_norm": 68.63973236083984,
      "learning_rate": 0.00021708206350469245,
      "loss": 0.4997,
      "step": 79900
    },
    {
      "epoch": 0.27673904545784744,
      "grad_norm": 4.4972662925720215,
      "learning_rate": 0.00021697828636264575,
      "loss": 0.6159,
      "step": 80000
    },
    {
      "epoch": 0.27708496926466974,
      "grad_norm": 0.0029017827473580837,
      "learning_rate": 0.00021687450922059905,
      "loss": 0.621,
      "step": 80100
    },
    {
      "epoch": 0.27743089307149205,
      "grad_norm": 8.545161247253418,
      "learning_rate": 0.00021677073207855237,
      "loss": 0.6509,
      "step": 80200
    },
    {
      "epoch": 0.2777768168783144,
      "grad_norm": 0.011300344951450825,
      "learning_rate": 0.00021666695493650567,
      "loss": 0.4241,
      "step": 80300
    },
    {
      "epoch": 0.2781227406851367,
      "grad_norm": 6.841170310974121,
      "learning_rate": 0.000216563177794459,
      "loss": 0.3695,
      "step": 80400
    },
    {
      "epoch": 0.278468664491959,
      "grad_norm": 8.239614544436336e-05,
      "learning_rate": 0.0002164594006524123,
      "loss": 0.4634,
      "step": 80500
    },
    {
      "epoch": 0.2788145882987813,
      "grad_norm": 0.00016216354561038315,
      "learning_rate": 0.00021635562351036557,
      "loss": 0.6126,
      "step": 80600
    },
    {
      "epoch": 0.2791605121056036,
      "grad_norm": 0.1761474311351776,
      "learning_rate": 0.0002162518463683189,
      "loss": 0.4187,
      "step": 80700
    },
    {
      "epoch": 0.2795064359124259,
      "grad_norm": 0.04560410603880882,
      "learning_rate": 0.0002161480692262722,
      "loss": 0.3395,
      "step": 80800
    },
    {
      "epoch": 0.2798523597192482,
      "grad_norm": 0.00043657279456965625,
      "learning_rate": 0.0002160442920842255,
      "loss": 0.16,
      "step": 80900
    },
    {
      "epoch": 0.28019828352607057,
      "grad_norm": 0.01436292752623558,
      "learning_rate": 0.00021594051494217882,
      "loss": 0.2225,
      "step": 81000
    },
    {
      "epoch": 0.28054420733289287,
      "grad_norm": 0.004465459380298853,
      "learning_rate": 0.00021583673780013212,
      "loss": 0.5843,
      "step": 81100
    },
    {
      "epoch": 0.28089013113971517,
      "grad_norm": 0.09705888479948044,
      "learning_rate": 0.00021573296065808544,
      "loss": 0.488,
      "step": 81200
    },
    {
      "epoch": 0.2812360549465375,
      "grad_norm": 0.00020884856348857284,
      "learning_rate": 0.00021562918351603874,
      "loss": 0.4986,
      "step": 81300
    },
    {
      "epoch": 0.2815819787533598,
      "grad_norm": 0.15232060849666595,
      "learning_rate": 0.00021552540637399204,
      "loss": 0.5004,
      "step": 81400
    },
    {
      "epoch": 0.2819279025601821,
      "grad_norm": 1.0243401527404785,
      "learning_rate": 0.00021542162923194537,
      "loss": 0.5398,
      "step": 81500
    },
    {
      "epoch": 0.2822738263670044,
      "grad_norm": 0.7414504289627075,
      "learning_rate": 0.00021531785208989867,
      "loss": 0.4061,
      "step": 81600
    },
    {
      "epoch": 0.28261975017382673,
      "grad_norm": 0.0007693697698414326,
      "learning_rate": 0.00021521407494785196,
      "loss": 0.5193,
      "step": 81700
    },
    {
      "epoch": 0.28296567398064904,
      "grad_norm": 13.449666023254395,
      "learning_rate": 0.0002151102978058053,
      "loss": 0.6668,
      "step": 81800
    },
    {
      "epoch": 0.28331159778747134,
      "grad_norm": 0.027346057817339897,
      "learning_rate": 0.0002150065206637586,
      "loss": 0.3951,
      "step": 81900
    },
    {
      "epoch": 0.28365752159429364,
      "grad_norm": 0.0014750687405467033,
      "learning_rate": 0.00021490274352171186,
      "loss": 0.6814,
      "step": 82000
    },
    {
      "epoch": 0.28400344540111594,
      "grad_norm": 0.009059330448508263,
      "learning_rate": 0.0002147989663796652,
      "loss": 0.4768,
      "step": 82100
    },
    {
      "epoch": 0.28434936920793824,
      "grad_norm": 45.874088287353516,
      "learning_rate": 0.00021469518923761849,
      "loss": 0.5625,
      "step": 82200
    },
    {
      "epoch": 0.28469529301476054,
      "grad_norm": 0.3985210657119751,
      "learning_rate": 0.0002145914120955718,
      "loss": 0.4643,
      "step": 82300
    },
    {
      "epoch": 0.2850412168215829,
      "grad_norm": 0.03149085491895676,
      "learning_rate": 0.0002144876349535251,
      "loss": 0.3559,
      "step": 82400
    },
    {
      "epoch": 0.2853871406284052,
      "grad_norm": 55.411991119384766,
      "learning_rate": 0.0002143838578114784,
      "loss": 0.5422,
      "step": 82500
    },
    {
      "epoch": 0.2857330644352275,
      "grad_norm": 14.725448608398438,
      "learning_rate": 0.00021428008066943173,
      "loss": 0.3983,
      "step": 82600
    },
    {
      "epoch": 0.2860789882420498,
      "grad_norm": 2.3763675689697266,
      "learning_rate": 0.00021417630352738503,
      "loss": 0.5369,
      "step": 82700
    },
    {
      "epoch": 0.2864249120488721,
      "grad_norm": 0.0020248412620276213,
      "learning_rate": 0.00021407252638533836,
      "loss": 0.7098,
      "step": 82800
    },
    {
      "epoch": 0.2867708358556944,
      "grad_norm": 0.2646898627281189,
      "learning_rate": 0.00021396874924329166,
      "loss": 0.3613,
      "step": 82900
    },
    {
      "epoch": 0.2871167596625167,
      "grad_norm": 0.0035751673858612776,
      "learning_rate": 0.00021386497210124496,
      "loss": 0.4469,
      "step": 83000
    },
    {
      "epoch": 0.28746268346933906,
      "grad_norm": 0.0039495001547038555,
      "learning_rate": 0.00021376119495919828,
      "loss": 0.4194,
      "step": 83100
    },
    {
      "epoch": 0.28780860727616137,
      "grad_norm": 0.9296226501464844,
      "learning_rate": 0.00021365741781715158,
      "loss": 0.4579,
      "step": 83200
    },
    {
      "epoch": 0.28815453108298367,
      "grad_norm": 0.0006005242466926575,
      "learning_rate": 0.0002135536406751049,
      "loss": 0.6686,
      "step": 83300
    },
    {
      "epoch": 0.28850045488980597,
      "grad_norm": 0.32662251591682434,
      "learning_rate": 0.00021344986353305818,
      "loss": 0.3549,
      "step": 83400
    },
    {
      "epoch": 0.28884637869662827,
      "grad_norm": 0.21320366859436035,
      "learning_rate": 0.00021334608639101148,
      "loss": 0.5625,
      "step": 83500
    },
    {
      "epoch": 0.28919230250345057,
      "grad_norm": 0.011448523961007595,
      "learning_rate": 0.0002132423092489648,
      "loss": 0.4834,
      "step": 83600
    },
    {
      "epoch": 0.2895382263102729,
      "grad_norm": 5.96795654296875,
      "learning_rate": 0.0002131385321069181,
      "loss": 0.2949,
      "step": 83700
    },
    {
      "epoch": 0.28988415011709523,
      "grad_norm": 0.7565997838973999,
      "learning_rate": 0.0002130347549648714,
      "loss": 0.3871,
      "step": 83800
    },
    {
      "epoch": 0.29023007392391753,
      "grad_norm": 9.332304000854492,
      "learning_rate": 0.00021293097782282473,
      "loss": 0.4768,
      "step": 83900
    },
    {
      "epoch": 0.29057599773073983,
      "grad_norm": 41.503700256347656,
      "learning_rate": 0.00021282720068077803,
      "loss": 0.2929,
      "step": 84000
    },
    {
      "epoch": 0.29092192153756213,
      "grad_norm": 0.03883202373981476,
      "learning_rate": 0.00021272342353873135,
      "loss": 0.5037,
      "step": 84100
    },
    {
      "epoch": 0.29126784534438444,
      "grad_norm": 0.00089354527881369,
      "learning_rate": 0.00021261964639668465,
      "loss": 0.3331,
      "step": 84200
    },
    {
      "epoch": 0.29161376915120674,
      "grad_norm": 0.01631179451942444,
      "learning_rate": 0.00021251586925463795,
      "loss": 0.5011,
      "step": 84300
    },
    {
      "epoch": 0.29195969295802904,
      "grad_norm": 4.80499267578125,
      "learning_rate": 0.00021241209211259128,
      "loss": 0.3395,
      "step": 84400
    },
    {
      "epoch": 0.2923056167648514,
      "grad_norm": 0.01058531366288662,
      "learning_rate": 0.00021230831497054457,
      "loss": 0.5429,
      "step": 84500
    },
    {
      "epoch": 0.2926515405716737,
      "grad_norm": 22.049976348876953,
      "learning_rate": 0.0002122045378284979,
      "loss": 0.5463,
      "step": 84600
    },
    {
      "epoch": 0.292997464378496,
      "grad_norm": 0.02805197611451149,
      "learning_rate": 0.0002121007606864512,
      "loss": 0.3228,
      "step": 84700
    },
    {
      "epoch": 0.2933433881853183,
      "grad_norm": 0.00035988030140288174,
      "learning_rate": 0.00021199698354440447,
      "loss": 0.2923,
      "step": 84800
    },
    {
      "epoch": 0.2936893119921406,
      "grad_norm": 0.002412810456007719,
      "learning_rate": 0.0002118932064023578,
      "loss": 0.5849,
      "step": 84900
    },
    {
      "epoch": 0.2940352357989629,
      "grad_norm": 0.0014238186413422227,
      "learning_rate": 0.0002117894292603111,
      "loss": 0.4586,
      "step": 85000
    },
    {
      "epoch": 0.2943811596057852,
      "grad_norm": 0.00013431141269393265,
      "learning_rate": 0.0002116856521182644,
      "loss": 0.5249,
      "step": 85100
    },
    {
      "epoch": 0.29472708341260756,
      "grad_norm": 0.0001643518771743402,
      "learning_rate": 0.00021158187497621772,
      "loss": 0.6385,
      "step": 85200
    },
    {
      "epoch": 0.29507300721942986,
      "grad_norm": 11.866848945617676,
      "learning_rate": 0.00021147809783417102,
      "loss": 0.5355,
      "step": 85300
    },
    {
      "epoch": 0.29541893102625216,
      "grad_norm": 0.7281704545021057,
      "learning_rate": 0.00021137432069212434,
      "loss": 0.487,
      "step": 85400
    },
    {
      "epoch": 0.29576485483307446,
      "grad_norm": 0.17716047167778015,
      "learning_rate": 0.00021127054355007764,
      "loss": 0.3651,
      "step": 85500
    },
    {
      "epoch": 0.29611077863989677,
      "grad_norm": 0.006453886162489653,
      "learning_rate": 0.00021116676640803094,
      "loss": 0.599,
      "step": 85600
    },
    {
      "epoch": 0.29645670244671907,
      "grad_norm": 0.001352741033770144,
      "learning_rate": 0.00021106298926598427,
      "loss": 0.5586,
      "step": 85700
    },
    {
      "epoch": 0.29680262625354137,
      "grad_norm": 0.00974605418741703,
      "learning_rate": 0.00021095921212393757,
      "loss": 0.2625,
      "step": 85800
    },
    {
      "epoch": 0.2971485500603637,
      "grad_norm": 0.0999138355255127,
      "learning_rate": 0.0002108554349818909,
      "loss": 0.7419,
      "step": 85900
    },
    {
      "epoch": 0.297494473867186,
      "grad_norm": 0.0018649608828127384,
      "learning_rate": 0.0002107516578398442,
      "loss": 0.5561,
      "step": 86000
    },
    {
      "epoch": 0.29784039767400833,
      "grad_norm": 15.159180641174316,
      "learning_rate": 0.0002106478806977975,
      "loss": 0.5595,
      "step": 86100
    },
    {
      "epoch": 0.29818632148083063,
      "grad_norm": 0.8732423782348633,
      "learning_rate": 0.0002105441035557508,
      "loss": 0.4181,
      "step": 86200
    },
    {
      "epoch": 0.29853224528765293,
      "grad_norm": 0.552398145198822,
      "learning_rate": 0.0002104403264137041,
      "loss": 0.5578,
      "step": 86300
    },
    {
      "epoch": 0.29887816909447523,
      "grad_norm": 1.3611319065093994,
      "learning_rate": 0.0002103365492716574,
      "loss": 0.5748,
      "step": 86400
    },
    {
      "epoch": 0.29922409290129753,
      "grad_norm": 0.0030532570090144873,
      "learning_rate": 0.0002102327721296107,
      "loss": 0.3423,
      "step": 86500
    },
    {
      "epoch": 0.2995700167081199,
      "grad_norm": 0.010648743249475956,
      "learning_rate": 0.000210128994987564,
      "loss": 0.465,
      "step": 86600
    },
    {
      "epoch": 0.2999159405149422,
      "grad_norm": 0.005754495970904827,
      "learning_rate": 0.00021002521784551734,
      "loss": 0.3486,
      "step": 86700
    },
    {
      "epoch": 0.3002618643217645,
      "grad_norm": 134.01589965820312,
      "learning_rate": 0.00020992144070347064,
      "loss": 0.3113,
      "step": 86800
    },
    {
      "epoch": 0.3006077881285868,
      "grad_norm": 0.0006942422478459775,
      "learning_rate": 0.00020981766356142393,
      "loss": 0.4249,
      "step": 86900
    },
    {
      "epoch": 0.3009537119354091,
      "grad_norm": 0.006767049431800842,
      "learning_rate": 0.00020971388641937726,
      "loss": 0.6137,
      "step": 87000
    },
    {
      "epoch": 0.3012996357422314,
      "grad_norm": 6.965563774108887,
      "learning_rate": 0.00020961010927733056,
      "loss": 0.5126,
      "step": 87100
    },
    {
      "epoch": 0.3016455595490537,
      "grad_norm": 87.81153869628906,
      "learning_rate": 0.00020950633213528389,
      "loss": 0.4393,
      "step": 87200
    },
    {
      "epoch": 0.30199148335587606,
      "grad_norm": 0.0021744482219219208,
      "learning_rate": 0.00020940255499323718,
      "loss": 0.4639,
      "step": 87300
    },
    {
      "epoch": 0.30233740716269836,
      "grad_norm": 0.0015398731920868158,
      "learning_rate": 0.00020929877785119048,
      "loss": 0.4637,
      "step": 87400
    },
    {
      "epoch": 0.30268333096952066,
      "grad_norm": 3.9186651706695557,
      "learning_rate": 0.0002091950007091438,
      "loss": 0.4456,
      "step": 87500
    },
    {
      "epoch": 0.30302925477634296,
      "grad_norm": 0.0020747901871800423,
      "learning_rate": 0.00020909122356709708,
      "loss": 0.3251,
      "step": 87600
    },
    {
      "epoch": 0.30337517858316526,
      "grad_norm": 0.00028527123504318297,
      "learning_rate": 0.00020898744642505038,
      "loss": 0.5753,
      "step": 87700
    },
    {
      "epoch": 0.30372110238998756,
      "grad_norm": 0.005137463565915823,
      "learning_rate": 0.0002088836692830037,
      "loss": 0.345,
      "step": 87800
    },
    {
      "epoch": 0.30406702619680986,
      "grad_norm": 0.652236819267273,
      "learning_rate": 0.000208779892140957,
      "loss": 0.5966,
      "step": 87900
    },
    {
      "epoch": 0.3044129500036322,
      "grad_norm": 0.00892083439975977,
      "learning_rate": 0.0002086761149989103,
      "loss": 0.3852,
      "step": 88000
    },
    {
      "epoch": 0.3047588738104545,
      "grad_norm": 0.004970945417881012,
      "learning_rate": 0.00020857233785686363,
      "loss": 0.6328,
      "step": 88100
    },
    {
      "epoch": 0.3051047976172768,
      "grad_norm": 15.30628776550293,
      "learning_rate": 0.00020846856071481693,
      "loss": 0.3817,
      "step": 88200
    },
    {
      "epoch": 0.3054507214240991,
      "grad_norm": 52.18401336669922,
      "learning_rate": 0.00020836478357277025,
      "loss": 0.4539,
      "step": 88300
    },
    {
      "epoch": 0.3057966452309214,
      "grad_norm": 0.00014333735452964902,
      "learning_rate": 0.00020826100643072355,
      "loss": 0.7446,
      "step": 88400
    },
    {
      "epoch": 0.30614256903774373,
      "grad_norm": 1.3841472864151,
      "learning_rate": 0.00020815722928867685,
      "loss": 0.5478,
      "step": 88500
    },
    {
      "epoch": 0.30648849284456603,
      "grad_norm": 26.323389053344727,
      "learning_rate": 0.00020805345214663018,
      "loss": 0.2908,
      "step": 88600
    },
    {
      "epoch": 0.3068344166513884,
      "grad_norm": 0.007254750933498144,
      "learning_rate": 0.00020794967500458348,
      "loss": 0.2903,
      "step": 88700
    },
    {
      "epoch": 0.3071803404582107,
      "grad_norm": 61.27436828613281,
      "learning_rate": 0.0002078458978625368,
      "loss": 0.6688,
      "step": 88800
    },
    {
      "epoch": 0.307526264265033,
      "grad_norm": 0.9554393887519836,
      "learning_rate": 0.0002077421207204901,
      "loss": 0.619,
      "step": 88900
    },
    {
      "epoch": 0.3078721880718553,
      "grad_norm": 0.010090898722410202,
      "learning_rate": 0.00020763834357844337,
      "loss": 0.3999,
      "step": 89000
    },
    {
      "epoch": 0.3082181118786776,
      "grad_norm": 0.529274046421051,
      "learning_rate": 0.0002075345664363967,
      "loss": 0.2954,
      "step": 89100
    },
    {
      "epoch": 0.3085640356854999,
      "grad_norm": 14.267189025878906,
      "learning_rate": 0.00020743078929435,
      "loss": 0.4762,
      "step": 89200
    },
    {
      "epoch": 0.3089099594923222,
      "grad_norm": 0.6066594123840332,
      "learning_rate": 0.0002073270121523033,
      "loss": 0.5808,
      "step": 89300
    },
    {
      "epoch": 0.30925588329914455,
      "grad_norm": 0.012918139807879925,
      "learning_rate": 0.00020722323501025662,
      "loss": 0.3919,
      "step": 89400
    },
    {
      "epoch": 0.30960180710596685,
      "grad_norm": 0.0020466807764023542,
      "learning_rate": 0.00020711945786820992,
      "loss": 0.2353,
      "step": 89500
    },
    {
      "epoch": 0.30994773091278915,
      "grad_norm": 0.1555097997188568,
      "learning_rate": 0.00020701568072616325,
      "loss": 0.6625,
      "step": 89600
    },
    {
      "epoch": 0.31029365471961146,
      "grad_norm": 0.09072808921337128,
      "learning_rate": 0.00020691190358411654,
      "loss": 0.5878,
      "step": 89700
    },
    {
      "epoch": 0.31063957852643376,
      "grad_norm": 0.0007376486901193857,
      "learning_rate": 0.00020680812644206984,
      "loss": 0.3615,
      "step": 89800
    },
    {
      "epoch": 0.31098550233325606,
      "grad_norm": 0.008394584991037846,
      "learning_rate": 0.00020670434930002317,
      "loss": 0.3754,
      "step": 89900
    },
    {
      "epoch": 0.31133142614007836,
      "grad_norm": 0.017121704295277596,
      "learning_rate": 0.00020660057215797647,
      "loss": 0.4031,
      "step": 90000
    },
    {
      "epoch": 0.3116773499469007,
      "grad_norm": 0.04280339926481247,
      "learning_rate": 0.0002064967950159298,
      "loss": 0.3854,
      "step": 90100
    },
    {
      "epoch": 0.312023273753723,
      "grad_norm": 0.0023554558865725994,
      "learning_rate": 0.0002063930178738831,
      "loss": 0.4248,
      "step": 90200
    },
    {
      "epoch": 0.3123691975605453,
      "grad_norm": 0.00031546485843136907,
      "learning_rate": 0.0002062892407318364,
      "loss": 0.3666,
      "step": 90300
    },
    {
      "epoch": 0.3127151213673676,
      "grad_norm": 0.4434429109096527,
      "learning_rate": 0.0002061854635897897,
      "loss": 0.5534,
      "step": 90400
    },
    {
      "epoch": 0.3130610451741899,
      "grad_norm": 0.000615938100963831,
      "learning_rate": 0.000206081686447743,
      "loss": 0.5397,
      "step": 90500
    },
    {
      "epoch": 0.3134069689810122,
      "grad_norm": 0.28157299757003784,
      "learning_rate": 0.0002059779093056963,
      "loss": 0.2811,
      "step": 90600
    },
    {
      "epoch": 0.3137528927878345,
      "grad_norm": 106.642822265625,
      "learning_rate": 0.00020587413216364961,
      "loss": 0.4945,
      "step": 90700
    },
    {
      "epoch": 0.3140988165946569,
      "grad_norm": 0.0017693833215162158,
      "learning_rate": 0.0002057703550216029,
      "loss": 0.4141,
      "step": 90800
    },
    {
      "epoch": 0.3144447404014792,
      "grad_norm": 0.0009533936390653253,
      "learning_rate": 0.00020566657787955624,
      "loss": 0.4815,
      "step": 90900
    },
    {
      "epoch": 0.3147906642083015,
      "grad_norm": 37.38413619995117,
      "learning_rate": 0.00020556280073750954,
      "loss": 0.3714,
      "step": 91000
    },
    {
      "epoch": 0.3151365880151238,
      "grad_norm": 0.19995903968811035,
      "learning_rate": 0.00020545902359546284,
      "loss": 0.5061,
      "step": 91100
    },
    {
      "epoch": 0.3154825118219461,
      "grad_norm": 0.10771327465772629,
      "learning_rate": 0.00020535524645341616,
      "loss": 0.312,
      "step": 91200
    },
    {
      "epoch": 0.3158284356287684,
      "grad_norm": 20.16145896911621,
      "learning_rate": 0.00020525146931136946,
      "loss": 0.5993,
      "step": 91300
    },
    {
      "epoch": 0.3161743594355907,
      "grad_norm": 14.076981544494629,
      "learning_rate": 0.0002051476921693228,
      "loss": 0.6119,
      "step": 91400
    },
    {
      "epoch": 0.31652028324241305,
      "grad_norm": 0.0009153099381364882,
      "learning_rate": 0.00020504391502727609,
      "loss": 0.3565,
      "step": 91500
    },
    {
      "epoch": 0.31686620704923535,
      "grad_norm": 0.0004980142111890018,
      "learning_rate": 0.00020494013788522938,
      "loss": 0.5288,
      "step": 91600
    },
    {
      "epoch": 0.31721213085605765,
      "grad_norm": 0.03266850486397743,
      "learning_rate": 0.0002048363607431827,
      "loss": 0.3524,
      "step": 91700
    },
    {
      "epoch": 0.31755805466287995,
      "grad_norm": 0.0045202188193798065,
      "learning_rate": 0.00020473258360113598,
      "loss": 0.3921,
      "step": 91800
    },
    {
      "epoch": 0.31790397846970225,
      "grad_norm": 0.07774268090724945,
      "learning_rate": 0.00020462880645908928,
      "loss": 0.5077,
      "step": 91900
    },
    {
      "epoch": 0.31824990227652455,
      "grad_norm": 0.05545008182525635,
      "learning_rate": 0.0002045250293170426,
      "loss": 0.4593,
      "step": 92000
    },
    {
      "epoch": 0.31859582608334686,
      "grad_norm": 64.18097686767578,
      "learning_rate": 0.0002044212521749959,
      "loss": 0.2725,
      "step": 92100
    },
    {
      "epoch": 0.3189417498901692,
      "grad_norm": 110.79529571533203,
      "learning_rate": 0.00020431747503294923,
      "loss": 0.5494,
      "step": 92200
    },
    {
      "epoch": 0.3192876736969915,
      "grad_norm": 0.03409374877810478,
      "learning_rate": 0.00020421369789090253,
      "loss": 0.2468,
      "step": 92300
    },
    {
      "epoch": 0.3196335975038138,
      "grad_norm": 0.007978213019669056,
      "learning_rate": 0.00020410992074885583,
      "loss": 0.468,
      "step": 92400
    },
    {
      "epoch": 0.3199795213106361,
      "grad_norm": 0.02820350043475628,
      "learning_rate": 0.00020400614360680915,
      "loss": 0.4452,
      "step": 92500
    },
    {
      "epoch": 0.3203254451174584,
      "grad_norm": 0.015835678204894066,
      "learning_rate": 0.00020390236646476245,
      "loss": 0.2101,
      "step": 92600
    },
    {
      "epoch": 0.3206713689242807,
      "grad_norm": 0.05715141072869301,
      "learning_rate": 0.00020379858932271578,
      "loss": 0.3052,
      "step": 92700
    },
    {
      "epoch": 0.321017292731103,
      "grad_norm": 0.0031784542370587587,
      "learning_rate": 0.00020369481218066908,
      "loss": 0.3337,
      "step": 92800
    },
    {
      "epoch": 0.3213632165379254,
      "grad_norm": 0.09416836500167847,
      "learning_rate": 0.00020359103503862238,
      "loss": 0.5725,
      "step": 92900
    },
    {
      "epoch": 0.3217091403447477,
      "grad_norm": 22.561674118041992,
      "learning_rate": 0.0002034872578965757,
      "loss": 0.494,
      "step": 93000
    },
    {
      "epoch": 0.32205506415157,
      "grad_norm": 10.106722831726074,
      "learning_rate": 0.000203383480754529,
      "loss": 0.4521,
      "step": 93100
    },
    {
      "epoch": 0.3224009879583923,
      "grad_norm": 0.0020577276591211557,
      "learning_rate": 0.00020327970361248227,
      "loss": 0.7362,
      "step": 93200
    },
    {
      "epoch": 0.3227469117652146,
      "grad_norm": 0.006631530821323395,
      "learning_rate": 0.0002031759264704356,
      "loss": 0.3714,
      "step": 93300
    },
    {
      "epoch": 0.3230928355720369,
      "grad_norm": 0.0007247844478115439,
      "learning_rate": 0.0002030721493283889,
      "loss": 0.5093,
      "step": 93400
    },
    {
      "epoch": 0.3234387593788592,
      "grad_norm": 0.008855837397277355,
      "learning_rate": 0.0002029683721863422,
      "loss": 0.327,
      "step": 93500
    },
    {
      "epoch": 0.32378468318568154,
      "grad_norm": 0.0006963599007576704,
      "learning_rate": 0.00020286459504429552,
      "loss": 0.4855,
      "step": 93600
    },
    {
      "epoch": 0.32413060699250384,
      "grad_norm": 0.0026415791362524033,
      "learning_rate": 0.00020276081790224882,
      "loss": 0.5886,
      "step": 93700
    },
    {
      "epoch": 0.32447653079932615,
      "grad_norm": 0.01564027927815914,
      "learning_rate": 0.00020265704076020215,
      "loss": 0.598,
      "step": 93800
    },
    {
      "epoch": 0.32482245460614845,
      "grad_norm": 0.005424770526587963,
      "learning_rate": 0.00020255326361815545,
      "loss": 0.4378,
      "step": 93900
    },
    {
      "epoch": 0.32516837841297075,
      "grad_norm": 14.87329387664795,
      "learning_rate": 0.00020244948647610874,
      "loss": 0.5649,
      "step": 94000
    },
    {
      "epoch": 0.32551430221979305,
      "grad_norm": 0.2102496474981308,
      "learning_rate": 0.00020234570933406207,
      "loss": 0.4416,
      "step": 94100
    },
    {
      "epoch": 0.32586022602661535,
      "grad_norm": 31.162723541259766,
      "learning_rate": 0.00020224193219201537,
      "loss": 0.4611,
      "step": 94200
    },
    {
      "epoch": 0.3262061498334377,
      "grad_norm": 0.0013410075334832072,
      "learning_rate": 0.0002021381550499687,
      "loss": 0.5485,
      "step": 94300
    },
    {
      "epoch": 0.32655207364026,
      "grad_norm": 31.627347946166992,
      "learning_rate": 0.000202034377907922,
      "loss": 0.4363,
      "step": 94400
    },
    {
      "epoch": 0.3268979974470823,
      "grad_norm": 49.69048309326172,
      "learning_rate": 0.0002019306007658753,
      "loss": 0.2859,
      "step": 94500
    },
    {
      "epoch": 0.3272439212539046,
      "grad_norm": 0.012971802614629269,
      "learning_rate": 0.0002018268236238286,
      "loss": 0.5735,
      "step": 94600
    },
    {
      "epoch": 0.3275898450607269,
      "grad_norm": 0.0047227549366652966,
      "learning_rate": 0.0002017230464817819,
      "loss": 0.4079,
      "step": 94700
    },
    {
      "epoch": 0.3279357688675492,
      "grad_norm": 0.08565603196620941,
      "learning_rate": 0.0002016192693397352,
      "loss": 0.396,
      "step": 94800
    },
    {
      "epoch": 0.3282816926743715,
      "grad_norm": 0.003270415822044015,
      "learning_rate": 0.00020151549219768851,
      "loss": 0.3281,
      "step": 94900
    },
    {
      "epoch": 0.3286276164811939,
      "grad_norm": 66.80685424804688,
      "learning_rate": 0.00020141171505564181,
      "loss": 0.2643,
      "step": 95000
    },
    {
      "epoch": 0.3289735402880162,
      "grad_norm": 0.0044914912432432175,
      "learning_rate": 0.00020130793791359514,
      "loss": 0.4843,
      "step": 95100
    },
    {
      "epoch": 0.3293194640948385,
      "grad_norm": 0.011027947999536991,
      "learning_rate": 0.00020120416077154844,
      "loss": 0.2893,
      "step": 95200
    },
    {
      "epoch": 0.3296653879016608,
      "grad_norm": 49.21659851074219,
      "learning_rate": 0.00020110038362950174,
      "loss": 0.4897,
      "step": 95300
    },
    {
      "epoch": 0.3300113117084831,
      "grad_norm": 0.03540452569723129,
      "learning_rate": 0.00020099660648745506,
      "loss": 0.5228,
      "step": 95400
    },
    {
      "epoch": 0.3303572355153054,
      "grad_norm": 6.499373912811279,
      "learning_rate": 0.00020089282934540836,
      "loss": 0.3655,
      "step": 95500
    },
    {
      "epoch": 0.3307031593221277,
      "grad_norm": 17.385662078857422,
      "learning_rate": 0.0002007890522033617,
      "loss": 0.7298,
      "step": 95600
    },
    {
      "epoch": 0.33104908312895004,
      "grad_norm": 0.0013621461112052202,
      "learning_rate": 0.000200685275061315,
      "loss": 0.5232,
      "step": 95700
    },
    {
      "epoch": 0.33139500693577234,
      "grad_norm": 0.7722116112709045,
      "learning_rate": 0.00020058149791926829,
      "loss": 0.2946,
      "step": 95800
    },
    {
      "epoch": 0.33174093074259464,
      "grad_norm": 37.78562927246094,
      "learning_rate": 0.0002004777207772216,
      "loss": 0.5431,
      "step": 95900
    },
    {
      "epoch": 0.33208685454941694,
      "grad_norm": 0.0005394138279370964,
      "learning_rate": 0.00020037394363517488,
      "loss": 0.4948,
      "step": 96000
    },
    {
      "epoch": 0.33243277835623924,
      "grad_norm": 0.8951252698898315,
      "learning_rate": 0.00020027016649312818,
      "loss": 0.3434,
      "step": 96100
    },
    {
      "epoch": 0.33277870216306155,
      "grad_norm": 0.0014593021478503942,
      "learning_rate": 0.0002001663893510815,
      "loss": 0.4461,
      "step": 96200
    },
    {
      "epoch": 0.33312462596988385,
      "grad_norm": 0.013031852431595325,
      "learning_rate": 0.0002000626122090348,
      "loss": 0.4693,
      "step": 96300
    },
    {
      "epoch": 0.3334705497767062,
      "grad_norm": 0.0004087146371603012,
      "learning_rate": 0.00019995883506698813,
      "loss": 0.6254,
      "step": 96400
    },
    {
      "epoch": 0.3338164735835285,
      "grad_norm": 147.1669464111328,
      "learning_rate": 0.00019985505792494143,
      "loss": 0.3058,
      "step": 96500
    },
    {
      "epoch": 0.3341623973903508,
      "grad_norm": 0.0038129957392811775,
      "learning_rate": 0.00019975128078289473,
      "loss": 0.4565,
      "step": 96600
    },
    {
      "epoch": 0.3345083211971731,
      "grad_norm": 0.003462654771283269,
      "learning_rate": 0.00019964750364084806,
      "loss": 0.4994,
      "step": 96700
    },
    {
      "epoch": 0.3348542450039954,
      "grad_norm": 0.0043543544597923756,
      "learning_rate": 0.00019954372649880135,
      "loss": 0.3344,
      "step": 96800
    },
    {
      "epoch": 0.3352001688108177,
      "grad_norm": 0.015545797534286976,
      "learning_rate": 0.00019943994935675468,
      "loss": 0.546,
      "step": 96900
    },
    {
      "epoch": 0.33554609261764,
      "grad_norm": 0.18577316403388977,
      "learning_rate": 0.00019933617221470798,
      "loss": 0.4355,
      "step": 97000
    },
    {
      "epoch": 0.33589201642446237,
      "grad_norm": 0.14327427744865417,
      "learning_rate": 0.00019923239507266128,
      "loss": 0.5117,
      "step": 97100
    },
    {
      "epoch": 0.33623794023128467,
      "grad_norm": 0.0571526400744915,
      "learning_rate": 0.0001991286179306146,
      "loss": 0.2355,
      "step": 97200
    },
    {
      "epoch": 0.336583864038107,
      "grad_norm": 0.031523216515779495,
      "learning_rate": 0.0001990248407885679,
      "loss": 0.6231,
      "step": 97300
    },
    {
      "epoch": 0.3369297878449293,
      "grad_norm": 0.0014653370017185807,
      "learning_rate": 0.00019892106364652117,
      "loss": 0.5695,
      "step": 97400
    },
    {
      "epoch": 0.3372757116517516,
      "grad_norm": 51.48422622680664,
      "learning_rate": 0.0001988172865044745,
      "loss": 0.5323,
      "step": 97500
    },
    {
      "epoch": 0.3376216354585739,
      "grad_norm": 0.0020672858227044344,
      "learning_rate": 0.0001987135093624278,
      "loss": 0.4993,
      "step": 97600
    },
    {
      "epoch": 0.3379675592653962,
      "grad_norm": 0.07383421808481216,
      "learning_rate": 0.00019860973222038112,
      "loss": 0.4247,
      "step": 97700
    },
    {
      "epoch": 0.33831348307221853,
      "grad_norm": 0.8285261392593384,
      "learning_rate": 0.00019850595507833442,
      "loss": 0.4605,
      "step": 97800
    },
    {
      "epoch": 0.33865940687904084,
      "grad_norm": 42.451416015625,
      "learning_rate": 0.00019840217793628772,
      "loss": 0.4162,
      "step": 97900
    },
    {
      "epoch": 0.33900533068586314,
      "grad_norm": 0.002243128139525652,
      "learning_rate": 0.00019829840079424105,
      "loss": 0.5913,
      "step": 98000
    },
    {
      "epoch": 0.33935125449268544,
      "grad_norm": 29.881591796875,
      "learning_rate": 0.00019819462365219435,
      "loss": 0.2878,
      "step": 98100
    },
    {
      "epoch": 0.33969717829950774,
      "grad_norm": 0.17884498834609985,
      "learning_rate": 0.00019809084651014767,
      "loss": 0.4352,
      "step": 98200
    },
    {
      "epoch": 0.34004310210633004,
      "grad_norm": 73.30358123779297,
      "learning_rate": 0.00019798706936810097,
      "loss": 0.4103,
      "step": 98300
    },
    {
      "epoch": 0.34038902591315234,
      "grad_norm": 28.324935913085938,
      "learning_rate": 0.00019788329222605427,
      "loss": 0.5303,
      "step": 98400
    },
    {
      "epoch": 0.3407349497199747,
      "grad_norm": 0.009864049963653088,
      "learning_rate": 0.0001977795150840076,
      "loss": 0.3743,
      "step": 98500
    },
    {
      "epoch": 0.341080873526797,
      "grad_norm": 8.400607109069824,
      "learning_rate": 0.0001976757379419609,
      "loss": 0.2846,
      "step": 98600
    },
    {
      "epoch": 0.3414267973336193,
      "grad_norm": 0.18075093626976013,
      "learning_rate": 0.00019757196079991422,
      "loss": 0.4,
      "step": 98700
    },
    {
      "epoch": 0.3417727211404416,
      "grad_norm": 0.01178490836173296,
      "learning_rate": 0.0001974681836578675,
      "loss": 0.5229,
      "step": 98800
    },
    {
      "epoch": 0.3421186449472639,
      "grad_norm": 17.046751022338867,
      "learning_rate": 0.0001973644065158208,
      "loss": 0.4982,
      "step": 98900
    },
    {
      "epoch": 0.3424645687540862,
      "grad_norm": 0.0018210792914032936,
      "learning_rate": 0.00019726062937377412,
      "loss": 0.3245,
      "step": 99000
    },
    {
      "epoch": 0.3428104925609085,
      "grad_norm": 0.00027552255778573453,
      "learning_rate": 0.00019715685223172742,
      "loss": 0.3594,
      "step": 99100
    },
    {
      "epoch": 0.34315641636773087,
      "grad_norm": 3.728059768676758,
      "learning_rate": 0.00019705307508968071,
      "loss": 0.3761,
      "step": 99200
    },
    {
      "epoch": 0.34350234017455317,
      "grad_norm": 0.005618926137685776,
      "learning_rate": 0.00019694929794763404,
      "loss": 0.311,
      "step": 99300
    },
    {
      "epoch": 0.34384826398137547,
      "grad_norm": 4.198405742645264,
      "learning_rate": 0.00019684552080558734,
      "loss": 0.5323,
      "step": 99400
    },
    {
      "epoch": 0.34419418778819777,
      "grad_norm": 3.855179786682129,
      "learning_rate": 0.00019674174366354064,
      "loss": 0.4075,
      "step": 99500
    },
    {
      "epoch": 0.34454011159502007,
      "grad_norm": 0.0009772636694833636,
      "learning_rate": 0.00019663796652149396,
      "loss": 0.4225,
      "step": 99600
    },
    {
      "epoch": 0.3448860354018424,
      "grad_norm": 0.0061696600168943405,
      "learning_rate": 0.00019653418937944726,
      "loss": 0.2789,
      "step": 99700
    },
    {
      "epoch": 0.3452319592086647,
      "grad_norm": 0.03779660537838936,
      "learning_rate": 0.0001964304122374006,
      "loss": 0.2676,
      "step": 99800
    },
    {
      "epoch": 0.34557788301548703,
      "grad_norm": 0.0035261674784123898,
      "learning_rate": 0.0001963266350953539,
      "loss": 0.3371,
      "step": 99900
    },
    {
      "epoch": 0.34592380682230933,
      "grad_norm": 0.00032826996175572276,
      "learning_rate": 0.0001962228579533072,
      "loss": 0.5699,
      "step": 100000
    },
    {
      "epoch": 0.34626973062913163,
      "grad_norm": 0.0004040040075778961,
      "learning_rate": 0.0001961190808112605,
      "loss": 0.1573,
      "step": 100100
    },
    {
      "epoch": 0.34661565443595393,
      "grad_norm": 0.00834321416914463,
      "learning_rate": 0.00019601530366921378,
      "loss": 0.4201,
      "step": 100200
    },
    {
      "epoch": 0.34696157824277624,
      "grad_norm": 0.0013687002938240767,
      "learning_rate": 0.00019591152652716708,
      "loss": 0.4393,
      "step": 100300
    },
    {
      "epoch": 0.34730750204959854,
      "grad_norm": 0.010754354298114777,
      "learning_rate": 0.0001958077493851204,
      "loss": 0.5031,
      "step": 100400
    },
    {
      "epoch": 0.34765342585642084,
      "grad_norm": 11.169886589050293,
      "learning_rate": 0.0001957039722430737,
      "loss": 0.4545,
      "step": 100500
    },
    {
      "epoch": 0.3479993496632432,
      "grad_norm": 0.16636979579925537,
      "learning_rate": 0.00019560019510102703,
      "loss": 0.584,
      "step": 100600
    },
    {
      "epoch": 0.3483452734700655,
      "grad_norm": 0.007279229816049337,
      "learning_rate": 0.00019549641795898033,
      "loss": 0.5202,
      "step": 100700
    },
    {
      "epoch": 0.3486911972768878,
      "grad_norm": 20.267749786376953,
      "learning_rate": 0.00019539264081693363,
      "loss": 0.4239,
      "step": 100800
    },
    {
      "epoch": 0.3490371210837101,
      "grad_norm": 65.07335662841797,
      "learning_rate": 0.00019528886367488696,
      "loss": 0.5087,
      "step": 100900
    },
    {
      "epoch": 0.3493830448905324,
      "grad_norm": 7.30684757232666,
      "learning_rate": 0.00019518508653284026,
      "loss": 0.3286,
      "step": 101000
    },
    {
      "epoch": 0.3497289686973547,
      "grad_norm": 0.010030163452029228,
      "learning_rate": 0.00019508130939079358,
      "loss": 0.36,
      "step": 101100
    },
    {
      "epoch": 0.350074892504177,
      "grad_norm": 0.0008685293723829091,
      "learning_rate": 0.00019497753224874688,
      "loss": 0.3479,
      "step": 101200
    },
    {
      "epoch": 0.35042081631099936,
      "grad_norm": 2.278184175491333,
      "learning_rate": 0.00019487375510670018,
      "loss": 0.3192,
      "step": 101300
    },
    {
      "epoch": 0.35076674011782166,
      "grad_norm": 0.002051661955192685,
      "learning_rate": 0.0001947699779646535,
      "loss": 0.3024,
      "step": 101400
    },
    {
      "epoch": 0.35111266392464396,
      "grad_norm": 0.03421748802065849,
      "learning_rate": 0.0001946662008226068,
      "loss": 0.4147,
      "step": 101500
    },
    {
      "epoch": 0.35145858773146627,
      "grad_norm": 0.0007688881014473736,
      "learning_rate": 0.00019456242368056008,
      "loss": 0.4175,
      "step": 101600
    },
    {
      "epoch": 0.35180451153828857,
      "grad_norm": 1.7183525562286377,
      "learning_rate": 0.0001944586465385134,
      "loss": 0.429,
      "step": 101700
    },
    {
      "epoch": 0.35215043534511087,
      "grad_norm": 4.811400890350342,
      "learning_rate": 0.0001943548693964667,
      "loss": 0.4494,
      "step": 101800
    },
    {
      "epoch": 0.35249635915193317,
      "grad_norm": 0.024607349187135696,
      "learning_rate": 0.00019425109225442003,
      "loss": 0.3424,
      "step": 101900
    },
    {
      "epoch": 0.3528422829587555,
      "grad_norm": 0.0024728269781917334,
      "learning_rate": 0.00019414731511237332,
      "loss": 0.217,
      "step": 102000
    },
    {
      "epoch": 0.35318820676557783,
      "grad_norm": 0.43821150064468384,
      "learning_rate": 0.00019404353797032662,
      "loss": 0.6349,
      "step": 102100
    },
    {
      "epoch": 0.35353413057240013,
      "grad_norm": 0.03358454629778862,
      "learning_rate": 0.00019393976082827995,
      "loss": 0.3737,
      "step": 102200
    },
    {
      "epoch": 0.35388005437922243,
      "grad_norm": 0.009425820782780647,
      "learning_rate": 0.00019383598368623325,
      "loss": 0.3073,
      "step": 102300
    },
    {
      "epoch": 0.35422597818604473,
      "grad_norm": 0.01536113116890192,
      "learning_rate": 0.00019373220654418657,
      "loss": 0.4688,
      "step": 102400
    },
    {
      "epoch": 0.35457190199286703,
      "grad_norm": 0.001724265399388969,
      "learning_rate": 0.00019362842940213987,
      "loss": 0.3344,
      "step": 102500
    },
    {
      "epoch": 0.35491782579968933,
      "grad_norm": 0.008497952483594418,
      "learning_rate": 0.00019352465226009317,
      "loss": 0.4056,
      "step": 102600
    },
    {
      "epoch": 0.3552637496065117,
      "grad_norm": 0.002792074577882886,
      "learning_rate": 0.0001934208751180465,
      "loss": 0.5247,
      "step": 102700
    },
    {
      "epoch": 0.355609673413334,
      "grad_norm": 0.0012624366208910942,
      "learning_rate": 0.0001933170979759998,
      "loss": 0.3629,
      "step": 102800
    },
    {
      "epoch": 0.3559555972201563,
      "grad_norm": 0.9485748410224915,
      "learning_rate": 0.00019321332083395312,
      "loss": 0.5332,
      "step": 102900
    },
    {
      "epoch": 0.3563015210269786,
      "grad_norm": 0.03761884197592735,
      "learning_rate": 0.0001931095436919064,
      "loss": 0.4834,
      "step": 103000
    },
    {
      "epoch": 0.3566474448338009,
      "grad_norm": 0.05257768556475639,
      "learning_rate": 0.0001930057665498597,
      "loss": 0.4047,
      "step": 103100
    },
    {
      "epoch": 0.3569933686406232,
      "grad_norm": 0.0005282980855554342,
      "learning_rate": 0.00019290198940781302,
      "loss": 0.7065,
      "step": 103200
    },
    {
      "epoch": 0.3573392924474455,
      "grad_norm": 37.72459030151367,
      "learning_rate": 0.00019279821226576632,
      "loss": 0.4016,
      "step": 103300
    },
    {
      "epoch": 0.35768521625426786,
      "grad_norm": 0.00045790799777023494,
      "learning_rate": 0.00019269443512371962,
      "loss": 0.4004,
      "step": 103400
    },
    {
      "epoch": 0.35803114006109016,
      "grad_norm": 0.000432950328104198,
      "learning_rate": 0.00019259065798167294,
      "loss": 0.2621,
      "step": 103500
    },
    {
      "epoch": 0.35837706386791246,
      "grad_norm": 0.10899928212165833,
      "learning_rate": 0.00019248688083962624,
      "loss": 0.4079,
      "step": 103600
    },
    {
      "epoch": 0.35872298767473476,
      "grad_norm": 0.023810992017388344,
      "learning_rate": 0.00019238310369757957,
      "loss": 0.4707,
      "step": 103700
    },
    {
      "epoch": 0.35906891148155706,
      "grad_norm": 0.03887894004583359,
      "learning_rate": 0.00019227932655553287,
      "loss": 0.4928,
      "step": 103800
    },
    {
      "epoch": 0.35941483528837936,
      "grad_norm": 0.2118716686964035,
      "learning_rate": 0.00019217554941348616,
      "loss": 0.4841,
      "step": 103900
    },
    {
      "epoch": 0.35976075909520167,
      "grad_norm": 0.0008253874839283526,
      "learning_rate": 0.0001920717722714395,
      "loss": 0.4594,
      "step": 104000
    },
    {
      "epoch": 0.360106682902024,
      "grad_norm": 8.552175521850586,
      "learning_rate": 0.0001919679951293928,
      "loss": 0.4358,
      "step": 104100
    },
    {
      "epoch": 0.3604526067088463,
      "grad_norm": 0.0018756826175376773,
      "learning_rate": 0.00019186421798734611,
      "loss": 0.4884,
      "step": 104200
    },
    {
      "epoch": 0.3607985305156686,
      "grad_norm": 70.36481475830078,
      "learning_rate": 0.00019176044084529941,
      "loss": 0.3449,
      "step": 104300
    },
    {
      "epoch": 0.3611444543224909,
      "grad_norm": 0.027365997433662415,
      "learning_rate": 0.00019165666370325269,
      "loss": 0.563,
      "step": 104400
    },
    {
      "epoch": 0.36149037812931323,
      "grad_norm": 0.031569596379995346,
      "learning_rate": 0.000191552886561206,
      "loss": 0.3433,
      "step": 104500
    },
    {
      "epoch": 0.36183630193613553,
      "grad_norm": 0.00036191928666085005,
      "learning_rate": 0.0001914491094191593,
      "loss": 0.588,
      "step": 104600
    },
    {
      "epoch": 0.36218222574295783,
      "grad_norm": 67.5819091796875,
      "learning_rate": 0.0001913453322771126,
      "loss": 0.5437,
      "step": 104700
    },
    {
      "epoch": 0.3625281495497802,
      "grad_norm": 0.0025402416940778494,
      "learning_rate": 0.00019124155513506593,
      "loss": 0.2979,
      "step": 104800
    },
    {
      "epoch": 0.3628740733566025,
      "grad_norm": 0.0025596392806619406,
      "learning_rate": 0.00019113777799301923,
      "loss": 0.4675,
      "step": 104900
    },
    {
      "epoch": 0.3632199971634248,
      "grad_norm": 0.018728280439972878,
      "learning_rate": 0.00019103400085097256,
      "loss": 0.2988,
      "step": 105000
    },
    {
      "epoch": 0.3635659209702471,
      "grad_norm": 8.377484321594238,
      "learning_rate": 0.00019093022370892586,
      "loss": 0.3958,
      "step": 105100
    },
    {
      "epoch": 0.3639118447770694,
      "grad_norm": 6.954623699188232,
      "learning_rate": 0.00019082644656687916,
      "loss": 0.3352,
      "step": 105200
    },
    {
      "epoch": 0.3642577685838917,
      "grad_norm": 0.0005465648719109595,
      "learning_rate": 0.00019072266942483248,
      "loss": 0.5531,
      "step": 105300
    },
    {
      "epoch": 0.364603692390714,
      "grad_norm": 0.0009755983483046293,
      "learning_rate": 0.00019061889228278578,
      "loss": 0.2974,
      "step": 105400
    },
    {
      "epoch": 0.36494961619753635,
      "grad_norm": 19.735454559326172,
      "learning_rate": 0.0001905151151407391,
      "loss": 0.4663,
      "step": 105500
    },
    {
      "epoch": 0.36529554000435865,
      "grad_norm": 0.0002620051382109523,
      "learning_rate": 0.0001904113379986924,
      "loss": 0.3845,
      "step": 105600
    },
    {
      "epoch": 0.36564146381118096,
      "grad_norm": 0.017201250419020653,
      "learning_rate": 0.0001903075608566457,
      "loss": 0.5122,
      "step": 105700
    },
    {
      "epoch": 0.36598738761800326,
      "grad_norm": 0.9092563986778259,
      "learning_rate": 0.00019020378371459898,
      "loss": 0.5588,
      "step": 105800
    },
    {
      "epoch": 0.36633331142482556,
      "grad_norm": 0.0011312848655506968,
      "learning_rate": 0.0001901000065725523,
      "loss": 0.346,
      "step": 105900
    },
    {
      "epoch": 0.36667923523164786,
      "grad_norm": 2.6632843017578125,
      "learning_rate": 0.0001899962294305056,
      "loss": 0.5084,
      "step": 106000
    },
    {
      "epoch": 0.36702515903847016,
      "grad_norm": 0.015108690597116947,
      "learning_rate": 0.00018989245228845893,
      "loss": 0.3601,
      "step": 106100
    },
    {
      "epoch": 0.3673710828452925,
      "grad_norm": 53.08470916748047,
      "learning_rate": 0.00018978867514641223,
      "loss": 0.4148,
      "step": 106200
    },
    {
      "epoch": 0.3677170066521148,
      "grad_norm": 0.0021276611369103193,
      "learning_rate": 0.00018968489800436552,
      "loss": 0.3681,
      "step": 106300
    },
    {
      "epoch": 0.3680629304589371,
      "grad_norm": 2.481844186782837,
      "learning_rate": 0.00018958112086231885,
      "loss": 0.3955,
      "step": 106400
    },
    {
      "epoch": 0.3684088542657594,
      "grad_norm": 0.01113246101886034,
      "learning_rate": 0.00018947734372027215,
      "loss": 0.4365,
      "step": 106500
    },
    {
      "epoch": 0.3687547780725817,
      "grad_norm": 3.103203058242798,
      "learning_rate": 0.00018937356657822548,
      "loss": 0.5231,
      "step": 106600
    },
    {
      "epoch": 0.369100701879404,
      "grad_norm": 0.027970237657427788,
      "learning_rate": 0.00018926978943617877,
      "loss": 0.5407,
      "step": 106700
    },
    {
      "epoch": 0.3694466256862263,
      "grad_norm": 0.00011215254926355556,
      "learning_rate": 0.00018916601229413207,
      "loss": 0.3284,
      "step": 106800
    },
    {
      "epoch": 0.3697925494930487,
      "grad_norm": 0.04243289306759834,
      "learning_rate": 0.0001890622351520854,
      "loss": 0.3649,
      "step": 106900
    },
    {
      "epoch": 0.370138473299871,
      "grad_norm": 8.325793266296387,
      "learning_rate": 0.0001889584580100387,
      "loss": 0.4836,
      "step": 107000
    },
    {
      "epoch": 0.3704843971066933,
      "grad_norm": 11.480283737182617,
      "learning_rate": 0.00018885468086799202,
      "loss": 0.4461,
      "step": 107100
    },
    {
      "epoch": 0.3708303209135156,
      "grad_norm": 0.004718593321740627,
      "learning_rate": 0.0001887509037259453,
      "loss": 0.5355,
      "step": 107200
    },
    {
      "epoch": 0.3711762447203379,
      "grad_norm": 0.05550847575068474,
      "learning_rate": 0.0001886471265838986,
      "loss": 0.1393,
      "step": 107300
    },
    {
      "epoch": 0.3715221685271602,
      "grad_norm": 0.00011410000297473744,
      "learning_rate": 0.00018854334944185192,
      "loss": 0.4417,
      "step": 107400
    },
    {
      "epoch": 0.3718680923339825,
      "grad_norm": 0.0021079378202557564,
      "learning_rate": 0.00018843957229980522,
      "loss": 0.3671,
      "step": 107500
    },
    {
      "epoch": 0.37221401614080485,
      "grad_norm": 0.003199372673407197,
      "learning_rate": 0.00018833579515775852,
      "loss": 0.162,
      "step": 107600
    },
    {
      "epoch": 0.37255993994762715,
      "grad_norm": 0.617895245552063,
      "learning_rate": 0.00018823201801571184,
      "loss": 0.6201,
      "step": 107700
    },
    {
      "epoch": 0.37290586375444945,
      "grad_norm": 0.004262709524482489,
      "learning_rate": 0.00018812824087366514,
      "loss": 0.4108,
      "step": 107800
    },
    {
      "epoch": 0.37325178756127175,
      "grad_norm": 0.000816165644209832,
      "learning_rate": 0.00018802446373161847,
      "loss": 0.436,
      "step": 107900
    },
    {
      "epoch": 0.37359771136809405,
      "grad_norm": 0.0006547327502630651,
      "learning_rate": 0.00018792068658957177,
      "loss": 0.5048,
      "step": 108000
    },
    {
      "epoch": 0.37394363517491636,
      "grad_norm": 0.0010998283978551626,
      "learning_rate": 0.00018781690944752507,
      "loss": 0.3888,
      "step": 108100
    },
    {
      "epoch": 0.37428955898173866,
      "grad_norm": 0.00020379522175062448,
      "learning_rate": 0.0001877131323054784,
      "loss": 0.3271,
      "step": 108200
    },
    {
      "epoch": 0.374635482788561,
      "grad_norm": 0.0018930071964859962,
      "learning_rate": 0.0001876093551634317,
      "loss": 0.5234,
      "step": 108300
    },
    {
      "epoch": 0.3749814065953833,
      "grad_norm": 0.00017260797903873026,
      "learning_rate": 0.00018750557802138502,
      "loss": 0.4508,
      "step": 108400
    },
    {
      "epoch": 0.3753273304022056,
      "grad_norm": 0.5844569802284241,
      "learning_rate": 0.00018740180087933831,
      "loss": 0.2329,
      "step": 108500
    },
    {
      "epoch": 0.3756732542090279,
      "grad_norm": 0.03144237771630287,
      "learning_rate": 0.0001872980237372916,
      "loss": 0.4345,
      "step": 108600
    },
    {
      "epoch": 0.3760191780158502,
      "grad_norm": 0.002730570500716567,
      "learning_rate": 0.0001871942465952449,
      "loss": 0.4087,
      "step": 108700
    },
    {
      "epoch": 0.3763651018226725,
      "grad_norm": 0.04290701076388359,
      "learning_rate": 0.0001870904694531982,
      "loss": 0.306,
      "step": 108800
    },
    {
      "epoch": 0.3767110256294948,
      "grad_norm": 0.0013167253928259015,
      "learning_rate": 0.0001869866923111515,
      "loss": 0.3555,
      "step": 108900
    },
    {
      "epoch": 0.3770569494363172,
      "grad_norm": 42.775978088378906,
      "learning_rate": 0.00018688291516910484,
      "loss": 0.4198,
      "step": 109000
    },
    {
      "epoch": 0.3774028732431395,
      "grad_norm": 0.0035565656144171953,
      "learning_rate": 0.00018677913802705813,
      "loss": 0.345,
      "step": 109100
    },
    {
      "epoch": 0.3777487970499618,
      "grad_norm": 0.00023959141981322318,
      "learning_rate": 0.00018667536088501146,
      "loss": 0.3547,
      "step": 109200
    },
    {
      "epoch": 0.3780947208567841,
      "grad_norm": 0.022492550313472748,
      "learning_rate": 0.00018657158374296476,
      "loss": 0.2238,
      "step": 109300
    },
    {
      "epoch": 0.3784406446636064,
      "grad_norm": 0.0035671647638082504,
      "learning_rate": 0.00018646780660091806,
      "loss": 0.414,
      "step": 109400
    },
    {
      "epoch": 0.3787865684704287,
      "grad_norm": 0.0031280627008527517,
      "learning_rate": 0.00018636402945887138,
      "loss": 0.3228,
      "step": 109500
    },
    {
      "epoch": 0.379132492277251,
      "grad_norm": 1.957658052444458,
      "learning_rate": 0.00018626025231682468,
      "loss": 0.334,
      "step": 109600
    },
    {
      "epoch": 0.37947841608407334,
      "grad_norm": 0.02089269831776619,
      "learning_rate": 0.000186156475174778,
      "loss": 0.4856,
      "step": 109700
    },
    {
      "epoch": 0.37982433989089565,
      "grad_norm": 0.0011830219300463796,
      "learning_rate": 0.0001860526980327313,
      "loss": 0.364,
      "step": 109800
    },
    {
      "epoch": 0.38017026369771795,
      "grad_norm": 0.0028161201626062393,
      "learning_rate": 0.0001859489208906846,
      "loss": 0.3338,
      "step": 109900
    },
    {
      "epoch": 0.38051618750454025,
      "grad_norm": 111.020751953125,
      "learning_rate": 0.0001858451437486379,
      "loss": 0.4978,
      "step": 110000
    },
    {
      "epoch": 0.38086211131136255,
      "grad_norm": 0.00022447638912126422,
      "learning_rate": 0.0001857413666065912,
      "loss": 0.4603,
      "step": 110100
    },
    {
      "epoch": 0.38120803511818485,
      "grad_norm": 0.038657378405332565,
      "learning_rate": 0.0001856375894645445,
      "loss": 0.4543,
      "step": 110200
    },
    {
      "epoch": 0.38155395892500715,
      "grad_norm": 0.12659844756126404,
      "learning_rate": 0.00018553381232249783,
      "loss": 0.3602,
      "step": 110300
    },
    {
      "epoch": 0.3818998827318295,
      "grad_norm": 25.22786521911621,
      "learning_rate": 0.00018543003518045113,
      "loss": 0.7107,
      "step": 110400
    },
    {
      "epoch": 0.3822458065386518,
      "grad_norm": 0.004766775295138359,
      "learning_rate": 0.00018532625803840445,
      "loss": 0.3587,
      "step": 110500
    },
    {
      "epoch": 0.3825917303454741,
      "grad_norm": 0.00013317028060555458,
      "learning_rate": 0.00018522248089635775,
      "loss": 0.5616,
      "step": 110600
    },
    {
      "epoch": 0.3829376541522964,
      "grad_norm": 85.04090118408203,
      "learning_rate": 0.00018511870375431105,
      "loss": 0.464,
      "step": 110700
    },
    {
      "epoch": 0.3832835779591187,
      "grad_norm": 0.00014148061745800078,
      "learning_rate": 0.00018501492661226438,
      "loss": 0.4077,
      "step": 110800
    },
    {
      "epoch": 0.383629501765941,
      "grad_norm": 0.00041398743633180857,
      "learning_rate": 0.00018491114947021768,
      "loss": 0.3068,
      "step": 110900
    },
    {
      "epoch": 0.3839754255727633,
      "grad_norm": 5.981945514678955,
      "learning_rate": 0.000184807372328171,
      "loss": 0.4472,
      "step": 111000
    },
    {
      "epoch": 0.3843213493795857,
      "grad_norm": 1.1345627307891846,
      "learning_rate": 0.0001847035951861243,
      "loss": 0.3034,
      "step": 111100
    },
    {
      "epoch": 0.384667273186408,
      "grad_norm": 0.21806125342845917,
      "learning_rate": 0.0001845998180440776,
      "loss": 0.2084,
      "step": 111200
    },
    {
      "epoch": 0.3850131969932303,
      "grad_norm": 5.6003570556640625,
      "learning_rate": 0.00018449604090203092,
      "loss": 0.4419,
      "step": 111300
    },
    {
      "epoch": 0.3853591208000526,
      "grad_norm": 0.07825500518083572,
      "learning_rate": 0.0001843922637599842,
      "loss": 0.1497,
      "step": 111400
    },
    {
      "epoch": 0.3857050446068749,
      "grad_norm": 0.0004683778388425708,
      "learning_rate": 0.0001842884866179375,
      "loss": 0.4792,
      "step": 111500
    },
    {
      "epoch": 0.3860509684136972,
      "grad_norm": 92.7283935546875,
      "learning_rate": 0.00018418470947589082,
      "loss": 0.3178,
      "step": 111600
    },
    {
      "epoch": 0.3863968922205195,
      "grad_norm": 12.642173767089844,
      "learning_rate": 0.00018408093233384412,
      "loss": 0.4764,
      "step": 111700
    },
    {
      "epoch": 0.38674281602734184,
      "grad_norm": 0.006561342626810074,
      "learning_rate": 0.00018397715519179742,
      "loss": 0.3729,
      "step": 111800
    },
    {
      "epoch": 0.38708873983416414,
      "grad_norm": 9.798783302307129,
      "learning_rate": 0.00018387337804975074,
      "loss": 0.5879,
      "step": 111900
    },
    {
      "epoch": 0.38743466364098644,
      "grad_norm": 0.003975902684032917,
      "learning_rate": 0.00018376960090770404,
      "loss": 0.3479,
      "step": 112000
    },
    {
      "epoch": 0.38778058744780874,
      "grad_norm": 44.3991584777832,
      "learning_rate": 0.00018366582376565737,
      "loss": 0.4985,
      "step": 112100
    },
    {
      "epoch": 0.38812651125463105,
      "grad_norm": 152.04147338867188,
      "learning_rate": 0.00018356204662361067,
      "loss": 0.3586,
      "step": 112200
    },
    {
      "epoch": 0.38847243506145335,
      "grad_norm": 0.008378221653401852,
      "learning_rate": 0.00018345826948156397,
      "loss": 0.4499,
      "step": 112300
    },
    {
      "epoch": 0.38881835886827565,
      "grad_norm": 0.04422304779291153,
      "learning_rate": 0.0001833544923395173,
      "loss": 0.3513,
      "step": 112400
    },
    {
      "epoch": 0.389164282675098,
      "grad_norm": 0.15693509578704834,
      "learning_rate": 0.0001832507151974706,
      "loss": 0.5923,
      "step": 112500
    },
    {
      "epoch": 0.3895102064819203,
      "grad_norm": 0.0021348008885979652,
      "learning_rate": 0.00018314693805542392,
      "loss": 0.5377,
      "step": 112600
    },
    {
      "epoch": 0.3898561302887426,
      "grad_norm": 0.0010771696688607335,
      "learning_rate": 0.00018304316091337722,
      "loss": 0.3905,
      "step": 112700
    },
    {
      "epoch": 0.3902020540955649,
      "grad_norm": 0.001968602417036891,
      "learning_rate": 0.0001829393837713305,
      "loss": 0.3698,
      "step": 112800
    },
    {
      "epoch": 0.3905479779023872,
      "grad_norm": 0.00202932464890182,
      "learning_rate": 0.00018283560662928381,
      "loss": 0.4541,
      "step": 112900
    },
    {
      "epoch": 0.3908939017092095,
      "grad_norm": 0.00012324703857302666,
      "learning_rate": 0.0001827318294872371,
      "loss": 0.371,
      "step": 113000
    },
    {
      "epoch": 0.3912398255160318,
      "grad_norm": 24.528642654418945,
      "learning_rate": 0.0001826280523451904,
      "loss": 0.4569,
      "step": 113100
    },
    {
      "epoch": 0.39158574932285417,
      "grad_norm": 0.0007585377315990627,
      "learning_rate": 0.00018252427520314374,
      "loss": 0.2629,
      "step": 113200
    },
    {
      "epoch": 0.39193167312967647,
      "grad_norm": 71.19308471679688,
      "learning_rate": 0.00018242049806109704,
      "loss": 0.4062,
      "step": 113300
    },
    {
      "epoch": 0.3922775969364988,
      "grad_norm": 0.00030927403713576496,
      "learning_rate": 0.00018231672091905036,
      "loss": 0.3512,
      "step": 113400
    },
    {
      "epoch": 0.3926235207433211,
      "grad_norm": 17.495996475219727,
      "learning_rate": 0.00018221294377700366,
      "loss": 0.4488,
      "step": 113500
    },
    {
      "epoch": 0.3929694445501434,
      "grad_norm": 0.13754215836524963,
      "learning_rate": 0.00018210916663495696,
      "loss": 0.3702,
      "step": 113600
    },
    {
      "epoch": 0.3933153683569657,
      "grad_norm": 0.0006987937958911061,
      "learning_rate": 0.00018200538949291029,
      "loss": 0.4414,
      "step": 113700
    },
    {
      "epoch": 0.393661292163788,
      "grad_norm": 0.0005547832115553319,
      "learning_rate": 0.00018190161235086358,
      "loss": 0.3684,
      "step": 113800
    },
    {
      "epoch": 0.39400721597061034,
      "grad_norm": 50.80821990966797,
      "learning_rate": 0.0001817978352088169,
      "loss": 0.4593,
      "step": 113900
    },
    {
      "epoch": 0.39435313977743264,
      "grad_norm": 70.2503433227539,
      "learning_rate": 0.0001816940580667702,
      "loss": 0.4296,
      "step": 114000
    },
    {
      "epoch": 0.39469906358425494,
      "grad_norm": 61.00039291381836,
      "learning_rate": 0.0001815902809247235,
      "loss": 0.6298,
      "step": 114100
    },
    {
      "epoch": 0.39504498739107724,
      "grad_norm": 31.491294860839844,
      "learning_rate": 0.0001814865037826768,
      "loss": 0.3675,
      "step": 114200
    },
    {
      "epoch": 0.39539091119789954,
      "grad_norm": 0.00011051780165871605,
      "learning_rate": 0.0001813827266406301,
      "loss": 0.4918,
      "step": 114300
    },
    {
      "epoch": 0.39573683500472184,
      "grad_norm": 0.6294242739677429,
      "learning_rate": 0.0001812789494985834,
      "loss": 0.3514,
      "step": 114400
    },
    {
      "epoch": 0.39608275881154414,
      "grad_norm": 39.055633544921875,
      "learning_rate": 0.00018117517235653673,
      "loss": 0.3361,
      "step": 114500
    },
    {
      "epoch": 0.3964286826183665,
      "grad_norm": 13.23000431060791,
      "learning_rate": 0.00018107139521449003,
      "loss": 0.4474,
      "step": 114600
    },
    {
      "epoch": 0.3967746064251888,
      "grad_norm": 0.0024891875218600035,
      "learning_rate": 0.00018096761807244335,
      "loss": 0.2217,
      "step": 114700
    },
    {
      "epoch": 0.3971205302320111,
      "grad_norm": 0.013164511881768703,
      "learning_rate": 0.00018086384093039665,
      "loss": 0.3824,
      "step": 114800
    },
    {
      "epoch": 0.3974664540388334,
      "grad_norm": 4.525479316711426,
      "learning_rate": 0.00018076006378834995,
      "loss": 0.4226,
      "step": 114900
    },
    {
      "epoch": 0.3978123778456557,
      "grad_norm": 1.1000794172286987,
      "learning_rate": 0.00018065628664630328,
      "loss": 0.2627,
      "step": 115000
    },
    {
      "epoch": 0.398158301652478,
      "grad_norm": 0.0029807144310325384,
      "learning_rate": 0.00018055250950425658,
      "loss": 0.5311,
      "step": 115100
    },
    {
      "epoch": 0.3985042254593003,
      "grad_norm": 2.5641703605651855,
      "learning_rate": 0.0001804487323622099,
      "loss": 0.4385,
      "step": 115200
    },
    {
      "epoch": 0.39885014926612267,
      "grad_norm": 0.0013555536279454827,
      "learning_rate": 0.0001803449552201632,
      "loss": 0.3606,
      "step": 115300
    },
    {
      "epoch": 0.39919607307294497,
      "grad_norm": 5.46791410446167,
      "learning_rate": 0.0001802411780781165,
      "loss": 0.5812,
      "step": 115400
    },
    {
      "epoch": 0.39954199687976727,
      "grad_norm": 0.006769648753106594,
      "learning_rate": 0.00018013740093606983,
      "loss": 0.6653,
      "step": 115500
    },
    {
      "epoch": 0.39988792068658957,
      "grad_norm": 10.34431266784668,
      "learning_rate": 0.0001800336237940231,
      "loss": 0.3961,
      "step": 115600
    },
    {
      "epoch": 0.40023384449341187,
      "grad_norm": 0.01687142439186573,
      "learning_rate": 0.0001799298466519764,
      "loss": 0.3269,
      "step": 115700
    },
    {
      "epoch": 0.4005797683002342,
      "grad_norm": 0.0019754539243876934,
      "learning_rate": 0.00017982606950992972,
      "loss": 0.3627,
      "step": 115800
    },
    {
      "epoch": 0.4009256921070565,
      "grad_norm": 0.02558344602584839,
      "learning_rate": 0.00017972229236788302,
      "loss": 0.2985,
      "step": 115900
    },
    {
      "epoch": 0.40127161591387883,
      "grad_norm": 16.006567001342773,
      "learning_rate": 0.00017961851522583635,
      "loss": 0.335,
      "step": 116000
    },
    {
      "epoch": 0.40161753972070113,
      "grad_norm": 0.01770891435444355,
      "learning_rate": 0.00017951473808378965,
      "loss": 0.4516,
      "step": 116100
    },
    {
      "epoch": 0.40196346352752343,
      "grad_norm": 50.32313537597656,
      "learning_rate": 0.00017941096094174294,
      "loss": 0.4114,
      "step": 116200
    },
    {
      "epoch": 0.40230938733434574,
      "grad_norm": 0.0020312881097197533,
      "learning_rate": 0.00017930718379969627,
      "loss": 0.4035,
      "step": 116300
    },
    {
      "epoch": 0.40265531114116804,
      "grad_norm": 0.23879864811897278,
      "learning_rate": 0.00017920340665764957,
      "loss": 0.3986,
      "step": 116400
    },
    {
      "epoch": 0.40300123494799034,
      "grad_norm": 0.0017550252377986908,
      "learning_rate": 0.0001790996295156029,
      "loss": 0.3449,
      "step": 116500
    },
    {
      "epoch": 0.40334715875481264,
      "grad_norm": 0.007028838619589806,
      "learning_rate": 0.0001789958523735562,
      "loss": 0.5332,
      "step": 116600
    },
    {
      "epoch": 0.403693082561635,
      "grad_norm": 0.0010005362564697862,
      "learning_rate": 0.0001788920752315095,
      "loss": 0.5901,
      "step": 116700
    },
    {
      "epoch": 0.4040390063684573,
      "grad_norm": 0.011938033625483513,
      "learning_rate": 0.00017878829808946282,
      "loss": 0.478,
      "step": 116800
    },
    {
      "epoch": 0.4043849301752796,
      "grad_norm": 0.005601853132247925,
      "learning_rate": 0.00017868452094741612,
      "loss": 0.2915,
      "step": 116900
    },
    {
      "epoch": 0.4047308539821019,
      "grad_norm": 0.006377535406500101,
      "learning_rate": 0.0001785807438053694,
      "loss": 0.3396,
      "step": 117000
    },
    {
      "epoch": 0.4050767777889242,
      "grad_norm": 7.001809120178223,
      "learning_rate": 0.00017847696666332272,
      "loss": 0.2341,
      "step": 117100
    },
    {
      "epoch": 0.4054227015957465,
      "grad_norm": 0.08802241086959839,
      "learning_rate": 0.00017837318952127601,
      "loss": 0.2489,
      "step": 117200
    },
    {
      "epoch": 0.4057686254025688,
      "grad_norm": 0.0001025678138830699,
      "learning_rate": 0.0001782694123792293,
      "loss": 0.3343,
      "step": 117300
    },
    {
      "epoch": 0.40611454920939116,
      "grad_norm": 0.005469439085572958,
      "learning_rate": 0.00017816563523718264,
      "loss": 0.4779,
      "step": 117400
    },
    {
      "epoch": 0.40646047301621346,
      "grad_norm": 0.004474450834095478,
      "learning_rate": 0.00017806185809513594,
      "loss": 0.3506,
      "step": 117500
    },
    {
      "epoch": 0.40680639682303577,
      "grad_norm": 0.0005382724339142442,
      "learning_rate": 0.00017795808095308926,
      "loss": 0.3685,
      "step": 117600
    },
    {
      "epoch": 0.40715232062985807,
      "grad_norm": 0.0010913594160228968,
      "learning_rate": 0.00017785430381104256,
      "loss": 0.4809,
      "step": 117700
    },
    {
      "epoch": 0.40749824443668037,
      "grad_norm": 0.0005581019213423133,
      "learning_rate": 0.00017775052666899586,
      "loss": 0.2497,
      "step": 117800
    },
    {
      "epoch": 0.40784416824350267,
      "grad_norm": 0.0011477564694359899,
      "learning_rate": 0.0001776467495269492,
      "loss": 0.5197,
      "step": 117900
    },
    {
      "epoch": 0.40819009205032497,
      "grad_norm": 0.3698447346687317,
      "learning_rate": 0.00017754297238490249,
      "loss": 0.618,
      "step": 118000
    },
    {
      "epoch": 0.4085360158571473,
      "grad_norm": 0.1205902248620987,
      "learning_rate": 0.0001774391952428558,
      "loss": 0.3967,
      "step": 118100
    },
    {
      "epoch": 0.40888193966396963,
      "grad_norm": 14.541021347045898,
      "learning_rate": 0.0001773354181008091,
      "loss": 0.3906,
      "step": 118200
    },
    {
      "epoch": 0.40922786347079193,
      "grad_norm": 0.21652452647686005,
      "learning_rate": 0.0001772316409587624,
      "loss": 0.191,
      "step": 118300
    },
    {
      "epoch": 0.40957378727761423,
      "grad_norm": 63.6966552734375,
      "learning_rate": 0.0001771278638167157,
      "loss": 0.3572,
      "step": 118400
    },
    {
      "epoch": 0.40991971108443653,
      "grad_norm": 0.0009635124006308615,
      "learning_rate": 0.000177024086674669,
      "loss": 0.2379,
      "step": 118500
    },
    {
      "epoch": 0.41026563489125883,
      "grad_norm": 0.00016812185640446842,
      "learning_rate": 0.0001769203095326223,
      "loss": 0.2604,
      "step": 118600
    },
    {
      "epoch": 0.41061155869808114,
      "grad_norm": 86.07362365722656,
      "learning_rate": 0.00017681653239057563,
      "loss": 0.5247,
      "step": 118700
    },
    {
      "epoch": 0.4109574825049035,
      "grad_norm": 0.0037270253524184227,
      "learning_rate": 0.00017671275524852893,
      "loss": 0.3609,
      "step": 118800
    },
    {
      "epoch": 0.4113034063117258,
      "grad_norm": 0.00044926482951268554,
      "learning_rate": 0.00017660897810648226,
      "loss": 0.3136,
      "step": 118900
    },
    {
      "epoch": 0.4116493301185481,
      "grad_norm": 0.015003676526248455,
      "learning_rate": 0.00017650520096443555,
      "loss": 0.4817,
      "step": 119000
    },
    {
      "epoch": 0.4119952539253704,
      "grad_norm": 0.0009536166908219457,
      "learning_rate": 0.00017640142382238885,
      "loss": 0.4553,
      "step": 119100
    },
    {
      "epoch": 0.4123411777321927,
      "grad_norm": 52.100502014160156,
      "learning_rate": 0.00017629764668034218,
      "loss": 0.4204,
      "step": 119200
    },
    {
      "epoch": 0.412687101539015,
      "grad_norm": 0.014087678864598274,
      "learning_rate": 0.00017619386953829548,
      "loss": 0.3387,
      "step": 119300
    },
    {
      "epoch": 0.4130330253458373,
      "grad_norm": 1.6078228950500488,
      "learning_rate": 0.0001760900923962488,
      "loss": 0.3492,
      "step": 119400
    },
    {
      "epoch": 0.41337894915265966,
      "grad_norm": 0.0510387122631073,
      "learning_rate": 0.0001759863152542021,
      "loss": 0.4187,
      "step": 119500
    },
    {
      "epoch": 0.41372487295948196,
      "grad_norm": 50.320892333984375,
      "learning_rate": 0.0001758825381121554,
      "loss": 0.5063,
      "step": 119600
    },
    {
      "epoch": 0.41407079676630426,
      "grad_norm": 0.009083941578865051,
      "learning_rate": 0.00017577876097010873,
      "loss": 0.6536,
      "step": 119700
    },
    {
      "epoch": 0.41441672057312656,
      "grad_norm": 0.0012061541201546788,
      "learning_rate": 0.000175674983828062,
      "loss": 0.3968,
      "step": 119800
    },
    {
      "epoch": 0.41476264437994886,
      "grad_norm": 1.1044751405715942,
      "learning_rate": 0.0001755712066860153,
      "loss": 0.3673,
      "step": 119900
    },
    {
      "epoch": 0.41510856818677117,
      "grad_norm": 0.0026981078553944826,
      "learning_rate": 0.00017546742954396862,
      "loss": 0.5334,
      "step": 120000
    },
    {
      "epoch": 0.41545449199359347,
      "grad_norm": 31.014530181884766,
      "learning_rate": 0.00017536365240192192,
      "loss": 0.4737,
      "step": 120100
    },
    {
      "epoch": 0.4158004158004158,
      "grad_norm": 32.36199188232422,
      "learning_rate": 0.00017525987525987525,
      "loss": 0.3296,
      "step": 120200
    },
    {
      "epoch": 0.4161463396072381,
      "grad_norm": 0.004479582887142897,
      "learning_rate": 0.00017515609811782855,
      "loss": 0.4176,
      "step": 120300
    },
    {
      "epoch": 0.4164922634140604,
      "grad_norm": 3.4008047580718994,
      "learning_rate": 0.00017505232097578185,
      "loss": 0.2646,
      "step": 120400
    },
    {
      "epoch": 0.4168381872208827,
      "grad_norm": 3.8852787017822266,
      "learning_rate": 0.00017494854383373517,
      "loss": 0.1588,
      "step": 120500
    },
    {
      "epoch": 0.41718411102770503,
      "grad_norm": 0.0040874057449400425,
      "learning_rate": 0.00017484476669168847,
      "loss": 0.4177,
      "step": 120600
    },
    {
      "epoch": 0.41753003483452733,
      "grad_norm": 0.4229324758052826,
      "learning_rate": 0.0001747409895496418,
      "loss": 0.3698,
      "step": 120700
    },
    {
      "epoch": 0.41787595864134963,
      "grad_norm": 0.0005407940479926765,
      "learning_rate": 0.0001746372124075951,
      "loss": 0.4475,
      "step": 120800
    },
    {
      "epoch": 0.418221882448172,
      "grad_norm": 14.570695877075195,
      "learning_rate": 0.0001745334352655484,
      "loss": 0.3092,
      "step": 120900
    },
    {
      "epoch": 0.4185678062549943,
      "grad_norm": 0.055923428386449814,
      "learning_rate": 0.00017442965812350172,
      "loss": 0.4705,
      "step": 121000
    },
    {
      "epoch": 0.4189137300618166,
      "grad_norm": 0.01340927928686142,
      "learning_rate": 0.00017432588098145502,
      "loss": 0.3073,
      "step": 121100
    },
    {
      "epoch": 0.4192596538686389,
      "grad_norm": 0.014685040339827538,
      "learning_rate": 0.0001742221038394083,
      "loss": 0.5916,
      "step": 121200
    },
    {
      "epoch": 0.4196055776754612,
      "grad_norm": 0.0005150623037479818,
      "learning_rate": 0.00017411832669736162,
      "loss": 0.3492,
      "step": 121300
    },
    {
      "epoch": 0.4199515014822835,
      "grad_norm": 0.0007060334901325405,
      "learning_rate": 0.00017401454955531492,
      "loss": 0.1733,
      "step": 121400
    },
    {
      "epoch": 0.4202974252891058,
      "grad_norm": 0.001003903802484274,
      "learning_rate": 0.00017391077241326824,
      "loss": 0.3289,
      "step": 121500
    },
    {
      "epoch": 0.42064334909592815,
      "grad_norm": 0.00031743416911922395,
      "learning_rate": 0.00017380699527122154,
      "loss": 0.3638,
      "step": 121600
    },
    {
      "epoch": 0.42098927290275046,
      "grad_norm": 0.011873411945998669,
      "learning_rate": 0.00017370321812917484,
      "loss": 0.3926,
      "step": 121700
    },
    {
      "epoch": 0.42133519670957276,
      "grad_norm": 0.001206978689879179,
      "learning_rate": 0.00017359944098712816,
      "loss": 0.2425,
      "step": 121800
    },
    {
      "epoch": 0.42168112051639506,
      "grad_norm": 0.7340449690818787,
      "learning_rate": 0.00017349566384508146,
      "loss": 0.2722,
      "step": 121900
    },
    {
      "epoch": 0.42202704432321736,
      "grad_norm": 0.0006929529481567442,
      "learning_rate": 0.0001733918867030348,
      "loss": 0.415,
      "step": 122000
    },
    {
      "epoch": 0.42237296813003966,
      "grad_norm": 0.2080674171447754,
      "learning_rate": 0.0001732881095609881,
      "loss": 0.4344,
      "step": 122100
    },
    {
      "epoch": 0.42271889193686196,
      "grad_norm": 0.6301329135894775,
      "learning_rate": 0.0001731843324189414,
      "loss": 0.637,
      "step": 122200
    },
    {
      "epoch": 0.4230648157436843,
      "grad_norm": 0.021638359874486923,
      "learning_rate": 0.0001730805552768947,
      "loss": 0.4392,
      "step": 122300
    },
    {
      "epoch": 0.4234107395505066,
      "grad_norm": 15.61635971069336,
      "learning_rate": 0.000172976778134848,
      "loss": 0.5119,
      "step": 122400
    },
    {
      "epoch": 0.4237566633573289,
      "grad_norm": 0.0013851950643584132,
      "learning_rate": 0.00017287300099280134,
      "loss": 0.3986,
      "step": 122500
    },
    {
      "epoch": 0.4241025871641512,
      "grad_norm": 0.009768891148269176,
      "learning_rate": 0.0001727692238507546,
      "loss": 0.3757,
      "step": 122600
    },
    {
      "epoch": 0.4244485109709735,
      "grad_norm": 0.005382377654314041,
      "learning_rate": 0.0001726654467087079,
      "loss": 0.3817,
      "step": 122700
    },
    {
      "epoch": 0.4247944347777958,
      "grad_norm": 0.0006286464631557465,
      "learning_rate": 0.00017256166956666123,
      "loss": 0.4348,
      "step": 122800
    },
    {
      "epoch": 0.4251403585846181,
      "grad_norm": 3.7272186279296875,
      "learning_rate": 0.00017245789242461453,
      "loss": 0.4428,
      "step": 122900
    },
    {
      "epoch": 0.4254862823914405,
      "grad_norm": 0.0004907977418042719,
      "learning_rate": 0.00017235411528256783,
      "loss": 0.2708,
      "step": 123000
    },
    {
      "epoch": 0.4258322061982628,
      "grad_norm": 33.833770751953125,
      "learning_rate": 0.00017225033814052116,
      "loss": 0.2778,
      "step": 123100
    },
    {
      "epoch": 0.4261781300050851,
      "grad_norm": 0.020891200751066208,
      "learning_rate": 0.00017214656099847446,
      "loss": 0.3765,
      "step": 123200
    },
    {
      "epoch": 0.4265240538119074,
      "grad_norm": 0.006607542280107737,
      "learning_rate": 0.00017204278385642778,
      "loss": 0.3138,
      "step": 123300
    },
    {
      "epoch": 0.4268699776187297,
      "grad_norm": 0.3774091899394989,
      "learning_rate": 0.00017193900671438108,
      "loss": 0.2087,
      "step": 123400
    },
    {
      "epoch": 0.427215901425552,
      "grad_norm": 0.0012867561308667064,
      "learning_rate": 0.00017183522957233438,
      "loss": 0.4011,
      "step": 123500
    },
    {
      "epoch": 0.4275618252323743,
      "grad_norm": 36.109317779541016,
      "learning_rate": 0.0001717314524302877,
      "loss": 0.3678,
      "step": 123600
    },
    {
      "epoch": 0.42790774903919665,
      "grad_norm": 0.7594357132911682,
      "learning_rate": 0.000171627675288241,
      "loss": 0.4269,
      "step": 123700
    },
    {
      "epoch": 0.42825367284601895,
      "grad_norm": 0.0158808846026659,
      "learning_rate": 0.00017152389814619433,
      "loss": 0.3743,
      "step": 123800
    },
    {
      "epoch": 0.42859959665284125,
      "grad_norm": 0.009992209263145924,
      "learning_rate": 0.00017142012100414763,
      "loss": 0.5033,
      "step": 123900
    },
    {
      "epoch": 0.42894552045966355,
      "grad_norm": 0.0003379535337444395,
      "learning_rate": 0.0001713163438621009,
      "loss": 0.3672,
      "step": 124000
    },
    {
      "epoch": 0.42929144426648586,
      "grad_norm": 0.001188519410789013,
      "learning_rate": 0.0001712125667200542,
      "loss": 0.423,
      "step": 124100
    },
    {
      "epoch": 0.42963736807330816,
      "grad_norm": 0.0007942670490592718,
      "learning_rate": 0.00017110878957800752,
      "loss": 0.3548,
      "step": 124200
    },
    {
      "epoch": 0.42998329188013046,
      "grad_norm": 0.0011439741356298327,
      "learning_rate": 0.00017100501243596082,
      "loss": 0.3455,
      "step": 124300
    },
    {
      "epoch": 0.4303292156869528,
      "grad_norm": 0.0009663469973020256,
      "learning_rate": 0.00017090123529391415,
      "loss": 0.1848,
      "step": 124400
    },
    {
      "epoch": 0.4306751394937751,
      "grad_norm": 0.028089845553040504,
      "learning_rate": 0.00017079745815186745,
      "loss": 0.4214,
      "step": 124500
    },
    {
      "epoch": 0.4310210633005974,
      "grad_norm": 0.0008477325900457799,
      "learning_rate": 0.00017069368100982075,
      "loss": 0.4442,
      "step": 124600
    },
    {
      "epoch": 0.4313669871074197,
      "grad_norm": 0.007220389321446419,
      "learning_rate": 0.00017058990386777407,
      "loss": 0.217,
      "step": 124700
    },
    {
      "epoch": 0.431712910914242,
      "grad_norm": 0.00014467757137026638,
      "learning_rate": 0.00017048612672572737,
      "loss": 0.2535,
      "step": 124800
    },
    {
      "epoch": 0.4320588347210643,
      "grad_norm": 0.0007979885558597744,
      "learning_rate": 0.0001703823495836807,
      "loss": 0.4794,
      "step": 124900
    },
    {
      "epoch": 0.4324047585278866,
      "grad_norm": 0.0018439836567267776,
      "learning_rate": 0.000170278572441634,
      "loss": 0.5349,
      "step": 125000
    },
    {
      "epoch": 0.432750682334709,
      "grad_norm": 0.002807053504511714,
      "learning_rate": 0.0001701747952995873,
      "loss": 0.24,
      "step": 125100
    },
    {
      "epoch": 0.4330966061415313,
      "grad_norm": 0.05209914222359657,
      "learning_rate": 0.00017007101815754062,
      "loss": 0.382,
      "step": 125200
    },
    {
      "epoch": 0.4334425299483536,
      "grad_norm": 40.745948791503906,
      "learning_rate": 0.00016996724101549392,
      "loss": 0.3876,
      "step": 125300
    },
    {
      "epoch": 0.4337884537551759,
      "grad_norm": 0.0008506974554620683,
      "learning_rate": 0.0001698634638734472,
      "loss": 0.6081,
      "step": 125400
    },
    {
      "epoch": 0.4341343775619982,
      "grad_norm": 38.96316146850586,
      "learning_rate": 0.00016975968673140052,
      "loss": 0.2375,
      "step": 125500
    },
    {
      "epoch": 0.4344803013688205,
      "grad_norm": 64.01087951660156,
      "learning_rate": 0.00016965590958935382,
      "loss": 0.3821,
      "step": 125600
    },
    {
      "epoch": 0.4348262251756428,
      "grad_norm": 0.021509403362870216,
      "learning_rate": 0.00016955213244730714,
      "loss": 0.464,
      "step": 125700
    },
    {
      "epoch": 0.43517214898246515,
      "grad_norm": 0.14729171991348267,
      "learning_rate": 0.00016944835530526044,
      "loss": 0.3675,
      "step": 125800
    },
    {
      "epoch": 0.43551807278928745,
      "grad_norm": 0.00040622364031150937,
      "learning_rate": 0.00016934457816321374,
      "loss": 0.4562,
      "step": 125900
    },
    {
      "epoch": 0.43586399659610975,
      "grad_norm": 0.2492407113313675,
      "learning_rate": 0.00016924080102116707,
      "loss": 0.3758,
      "step": 126000
    },
    {
      "epoch": 0.43620992040293205,
      "grad_norm": 0.017375223338603973,
      "learning_rate": 0.00016913702387912036,
      "loss": 0.2736,
      "step": 126100
    },
    {
      "epoch": 0.43655584420975435,
      "grad_norm": 0.0008311176206916571,
      "learning_rate": 0.0001690332467370737,
      "loss": 0.3472,
      "step": 126200
    },
    {
      "epoch": 0.43690176801657665,
      "grad_norm": 0.00016798962315078825,
      "learning_rate": 0.000168929469595027,
      "loss": 0.3758,
      "step": 126300
    },
    {
      "epoch": 0.43724769182339895,
      "grad_norm": 0.005787706468254328,
      "learning_rate": 0.0001688256924529803,
      "loss": 0.4445,
      "step": 126400
    },
    {
      "epoch": 0.4375936156302213,
      "grad_norm": 1.3457722663879395,
      "learning_rate": 0.00016872191531093361,
      "loss": 0.4111,
      "step": 126500
    },
    {
      "epoch": 0.4379395394370436,
      "grad_norm": 0.00938426423817873,
      "learning_rate": 0.0001686181381688869,
      "loss": 0.3866,
      "step": 126600
    },
    {
      "epoch": 0.4382854632438659,
      "grad_norm": 0.15074734389781952,
      "learning_rate": 0.00016851436102684024,
      "loss": 0.2172,
      "step": 126700
    },
    {
      "epoch": 0.4386313870506882,
      "grad_norm": 25.44544792175293,
      "learning_rate": 0.0001684105838847935,
      "loss": 0.7441,
      "step": 126800
    },
    {
      "epoch": 0.4389773108575105,
      "grad_norm": 14.491817474365234,
      "learning_rate": 0.0001683068067427468,
      "loss": 0.373,
      "step": 126900
    },
    {
      "epoch": 0.4393232346643328,
      "grad_norm": 61.48424530029297,
      "learning_rate": 0.00016820302960070013,
      "loss": 0.3839,
      "step": 127000
    },
    {
      "epoch": 0.4396691584711551,
      "grad_norm": 0.0005492986529134214,
      "learning_rate": 0.00016809925245865343,
      "loss": 0.3145,
      "step": 127100
    },
    {
      "epoch": 0.4400150822779775,
      "grad_norm": 0.000166474434081465,
      "learning_rate": 0.00016799547531660673,
      "loss": 0.2868,
      "step": 127200
    },
    {
      "epoch": 0.4403610060847998,
      "grad_norm": 0.05128468945622444,
      "learning_rate": 0.00016789169817456006,
      "loss": 0.3983,
      "step": 127300
    },
    {
      "epoch": 0.4407069298916221,
      "grad_norm": 0.00032755653955973685,
      "learning_rate": 0.00016778792103251336,
      "loss": 0.2295,
      "step": 127400
    },
    {
      "epoch": 0.4410528536984444,
      "grad_norm": 0.007412138860672712,
      "learning_rate": 0.00016768414389046668,
      "loss": 0.2425,
      "step": 127500
    },
    {
      "epoch": 0.4413987775052667,
      "grad_norm": 57.116546630859375,
      "learning_rate": 0.00016758036674841998,
      "loss": 0.2812,
      "step": 127600
    },
    {
      "epoch": 0.441744701312089,
      "grad_norm": 0.20022481679916382,
      "learning_rate": 0.00016747658960637328,
      "loss": 0.6252,
      "step": 127700
    },
    {
      "epoch": 0.4420906251189113,
      "grad_norm": 54.60050582885742,
      "learning_rate": 0.0001673728124643266,
      "loss": 0.4092,
      "step": 127800
    },
    {
      "epoch": 0.44243654892573364,
      "grad_norm": 0.0030690862331539392,
      "learning_rate": 0.0001672690353222799,
      "loss": 0.3046,
      "step": 127900
    },
    {
      "epoch": 0.44278247273255594,
      "grad_norm": 0.000514891988132149,
      "learning_rate": 0.00016716525818023323,
      "loss": 0.2561,
      "step": 128000
    },
    {
      "epoch": 0.44312839653937824,
      "grad_norm": 24.848451614379883,
      "learning_rate": 0.00016706148103818653,
      "loss": 0.3812,
      "step": 128100
    },
    {
      "epoch": 0.44347432034620055,
      "grad_norm": 0.2065449208021164,
      "learning_rate": 0.0001669577038961398,
      "loss": 0.2588,
      "step": 128200
    },
    {
      "epoch": 0.44382024415302285,
      "grad_norm": 23.429283142089844,
      "learning_rate": 0.00016685392675409313,
      "loss": 0.4324,
      "step": 128300
    },
    {
      "epoch": 0.44416616795984515,
      "grad_norm": 0.011021086014807224,
      "learning_rate": 0.00016675014961204643,
      "loss": 0.3309,
      "step": 128400
    },
    {
      "epoch": 0.44451209176666745,
      "grad_norm": 0.00040662719402462244,
      "learning_rate": 0.00016664637246999972,
      "loss": 0.2848,
      "step": 128500
    },
    {
      "epoch": 0.4448580155734898,
      "grad_norm": 0.0009709940641187131,
      "learning_rate": 0.00016654259532795305,
      "loss": 0.3491,
      "step": 128600
    },
    {
      "epoch": 0.4452039393803121,
      "grad_norm": 0.00035513684269972146,
      "learning_rate": 0.00016643881818590635,
      "loss": 0.435,
      "step": 128700
    },
    {
      "epoch": 0.4455498631871344,
      "grad_norm": 0.024577613919973373,
      "learning_rate": 0.00016633504104385968,
      "loss": 0.3034,
      "step": 128800
    },
    {
      "epoch": 0.4458957869939567,
      "grad_norm": 0.0016963138477876782,
      "learning_rate": 0.00016623126390181297,
      "loss": 0.346,
      "step": 128900
    },
    {
      "epoch": 0.446241710800779,
      "grad_norm": 0.0008360302890650928,
      "learning_rate": 0.00016612748675976627,
      "loss": 0.3523,
      "step": 129000
    },
    {
      "epoch": 0.4465876346076013,
      "grad_norm": 0.0018073362298309803,
      "learning_rate": 0.0001660237096177196,
      "loss": 0.2982,
      "step": 129100
    },
    {
      "epoch": 0.4469335584144236,
      "grad_norm": 0.04844145104289055,
      "learning_rate": 0.0001659199324756729,
      "loss": 0.3276,
      "step": 129200
    },
    {
      "epoch": 0.44727948222124597,
      "grad_norm": 0.0025984079111367464,
      "learning_rate": 0.00016581615533362622,
      "loss": 0.5787,
      "step": 129300
    },
    {
      "epoch": 0.4476254060280683,
      "grad_norm": 39.840431213378906,
      "learning_rate": 0.00016571237819157952,
      "loss": 0.2634,
      "step": 129400
    },
    {
      "epoch": 0.4479713298348906,
      "grad_norm": 0.0015649184351786971,
      "learning_rate": 0.00016560860104953282,
      "loss": 0.3897,
      "step": 129500
    },
    {
      "epoch": 0.4483172536417129,
      "grad_norm": 0.5242612957954407,
      "learning_rate": 0.0001655048239074861,
      "loss": 0.3438,
      "step": 129600
    },
    {
      "epoch": 0.4486631774485352,
      "grad_norm": 0.05460081622004509,
      "learning_rate": 0.00016540104676543942,
      "loss": 0.243,
      "step": 129700
    },
    {
      "epoch": 0.4490091012553575,
      "grad_norm": 0.0020787864923477173,
      "learning_rate": 0.00016529726962339272,
      "loss": 0.2688,
      "step": 129800
    },
    {
      "epoch": 0.4493550250621798,
      "grad_norm": 1.076244592666626,
      "learning_rate": 0.00016519349248134604,
      "loss": 0.2461,
      "step": 129900
    },
    {
      "epoch": 0.44970094886900214,
      "grad_norm": 64.69209289550781,
      "learning_rate": 0.00016508971533929934,
      "loss": 0.4725,
      "step": 130000
    },
    {
      "epoch": 0.45004687267582444,
      "grad_norm": 4.473177909851074,
      "learning_rate": 0.00016498593819725264,
      "loss": 0.3479,
      "step": 130100
    },
    {
      "epoch": 0.45039279648264674,
      "grad_norm": 0.001755053410306573,
      "learning_rate": 0.00016488216105520597,
      "loss": 0.5123,
      "step": 130200
    },
    {
      "epoch": 0.45073872028946904,
      "grad_norm": 39.19499588012695,
      "learning_rate": 0.00016477838391315927,
      "loss": 0.2485,
      "step": 130300
    },
    {
      "epoch": 0.45108464409629134,
      "grad_norm": 0.008749526925384998,
      "learning_rate": 0.0001646746067711126,
      "loss": 0.3378,
      "step": 130400
    },
    {
      "epoch": 0.45143056790311364,
      "grad_norm": 0.0017525047296658158,
      "learning_rate": 0.0001645708296290659,
      "loss": 0.396,
      "step": 130500
    },
    {
      "epoch": 0.45177649170993595,
      "grad_norm": 0.01002687867730856,
      "learning_rate": 0.0001644670524870192,
      "loss": 0.2374,
      "step": 130600
    },
    {
      "epoch": 0.4521224155167583,
      "grad_norm": 0.008004231378436089,
      "learning_rate": 0.00016436327534497252,
      "loss": 0.2674,
      "step": 130700
    },
    {
      "epoch": 0.4524683393235806,
      "grad_norm": 0.002328861504793167,
      "learning_rate": 0.00016425949820292581,
      "loss": 0.4561,
      "step": 130800
    },
    {
      "epoch": 0.4528142631304029,
      "grad_norm": 10.788005828857422,
      "learning_rate": 0.00016415572106087914,
      "loss": 0.2643,
      "step": 130900
    },
    {
      "epoch": 0.4531601869372252,
      "grad_norm": 0.001101754023693502,
      "learning_rate": 0.0001640519439188324,
      "loss": 0.3382,
      "step": 131000
    },
    {
      "epoch": 0.4535061107440475,
      "grad_norm": 0.0008981175487861037,
      "learning_rate": 0.0001639481667767857,
      "loss": 0.4884,
      "step": 131100
    },
    {
      "epoch": 0.4538520345508698,
      "grad_norm": 6.375790596008301,
      "learning_rate": 0.00016384438963473904,
      "loss": 0.3694,
      "step": 131200
    },
    {
      "epoch": 0.4541979583576921,
      "grad_norm": 0.33040374517440796,
      "learning_rate": 0.00016374061249269233,
      "loss": 0.3402,
      "step": 131300
    },
    {
      "epoch": 0.45454388216451447,
      "grad_norm": 0.013200018554925919,
      "learning_rate": 0.00016363683535064563,
      "loss": 0.5189,
      "step": 131400
    },
    {
      "epoch": 0.45488980597133677,
      "grad_norm": 0.016081610694527626,
      "learning_rate": 0.00016353305820859896,
      "loss": 0.3351,
      "step": 131500
    },
    {
      "epoch": 0.45523572977815907,
      "grad_norm": 0.04715718701481819,
      "learning_rate": 0.00016342928106655226,
      "loss": 0.3087,
      "step": 131600
    },
    {
      "epoch": 0.45558165358498137,
      "grad_norm": 20.0455379486084,
      "learning_rate": 0.00016332550392450558,
      "loss": 0.3976,
      "step": 131700
    },
    {
      "epoch": 0.4559275773918037,
      "grad_norm": 0.03332437202334404,
      "learning_rate": 0.00016322172678245888,
      "loss": 0.1784,
      "step": 131800
    },
    {
      "epoch": 0.456273501198626,
      "grad_norm": 0.029915185645222664,
      "learning_rate": 0.00016311794964041218,
      "loss": 0.4653,
      "step": 131900
    },
    {
      "epoch": 0.4566194250054483,
      "grad_norm": 0.004283522255718708,
      "learning_rate": 0.0001630141724983655,
      "loss": 0.36,
      "step": 132000
    },
    {
      "epoch": 0.45696534881227063,
      "grad_norm": 0.00015046655607875437,
      "learning_rate": 0.0001629103953563188,
      "loss": 0.2166,
      "step": 132100
    },
    {
      "epoch": 0.45731127261909293,
      "grad_norm": 0.012746200896799564,
      "learning_rate": 0.00016280661821427213,
      "loss": 0.2839,
      "step": 132200
    },
    {
      "epoch": 0.45765719642591524,
      "grad_norm": 12.724218368530273,
      "learning_rate": 0.00016270284107222543,
      "loss": 0.5474,
      "step": 132300
    },
    {
      "epoch": 0.45800312023273754,
      "grad_norm": 23.641389846801758,
      "learning_rate": 0.00016259906393017873,
      "loss": 0.3725,
      "step": 132400
    },
    {
      "epoch": 0.45834904403955984,
      "grad_norm": 25.7764835357666,
      "learning_rate": 0.00016249528678813203,
      "loss": 0.5191,
      "step": 132500
    },
    {
      "epoch": 0.45869496784638214,
      "grad_norm": 0.838318407535553,
      "learning_rate": 0.00016239150964608533,
      "loss": 0.3022,
      "step": 132600
    },
    {
      "epoch": 0.45904089165320444,
      "grad_norm": 14.41869068145752,
      "learning_rate": 0.00016228773250403863,
      "loss": 0.4114,
      "step": 132700
    },
    {
      "epoch": 0.4593868154600268,
      "grad_norm": 0.00042661174666136503,
      "learning_rate": 0.00016218395536199195,
      "loss": 0.2663,
      "step": 132800
    },
    {
      "epoch": 0.4597327392668491,
      "grad_norm": 0.0008567424374632537,
      "learning_rate": 0.00016208017821994525,
      "loss": 0.1644,
      "step": 132900
    },
    {
      "epoch": 0.4600786630736714,
      "grad_norm": 78.14590454101562,
      "learning_rate": 0.00016197640107789858,
      "loss": 0.2302,
      "step": 133000
    },
    {
      "epoch": 0.4604245868804937,
      "grad_norm": 0.0060411118902266026,
      "learning_rate": 0.00016187262393585188,
      "loss": 0.3516,
      "step": 133100
    },
    {
      "epoch": 0.460770510687316,
      "grad_norm": 86.35145568847656,
      "learning_rate": 0.00016176884679380517,
      "loss": 0.3381,
      "step": 133200
    },
    {
      "epoch": 0.4611164344941383,
      "grad_norm": 29.65572166442871,
      "learning_rate": 0.0001616650696517585,
      "loss": 0.3605,
      "step": 133300
    },
    {
      "epoch": 0.4614623583009606,
      "grad_norm": 0.0011296828743070364,
      "learning_rate": 0.0001615612925097118,
      "loss": 0.2143,
      "step": 133400
    },
    {
      "epoch": 0.46180828210778296,
      "grad_norm": 0.0031590014696121216,
      "learning_rate": 0.00016145751536766512,
      "loss": 0.3873,
      "step": 133500
    },
    {
      "epoch": 0.46215420591460527,
      "grad_norm": 0.00846929382532835,
      "learning_rate": 0.00016135373822561842,
      "loss": 0.5171,
      "step": 133600
    },
    {
      "epoch": 0.46250012972142757,
      "grad_norm": 3.1012325286865234,
      "learning_rate": 0.00016124996108357172,
      "loss": 0.4304,
      "step": 133700
    },
    {
      "epoch": 0.46284605352824987,
      "grad_norm": 0.0006501544849015772,
      "learning_rate": 0.00016114618394152505,
      "loss": 0.2121,
      "step": 133800
    },
    {
      "epoch": 0.46319197733507217,
      "grad_norm": 0.0005126179312355816,
      "learning_rate": 0.00016104240679947832,
      "loss": 0.2406,
      "step": 133900
    },
    {
      "epoch": 0.46353790114189447,
      "grad_norm": 8.526181773049757e-05,
      "learning_rate": 0.00016093862965743162,
      "loss": 0.3597,
      "step": 134000
    },
    {
      "epoch": 0.46388382494871677,
      "grad_norm": 0.004555634688585997,
      "learning_rate": 0.00016083485251538494,
      "loss": 0.3059,
      "step": 134100
    },
    {
      "epoch": 0.46422974875553913,
      "grad_norm": 22.458498001098633,
      "learning_rate": 0.00016073107537333824,
      "loss": 0.3977,
      "step": 134200
    },
    {
      "epoch": 0.46457567256236143,
      "grad_norm": 0.0014631521189585328,
      "learning_rate": 0.00016062729823129157,
      "loss": 0.3777,
      "step": 134300
    },
    {
      "epoch": 0.46492159636918373,
      "grad_norm": 0.0024101652670651674,
      "learning_rate": 0.00016052352108924487,
      "loss": 0.4179,
      "step": 134400
    },
    {
      "epoch": 0.46526752017600603,
      "grad_norm": 0.009756487794220448,
      "learning_rate": 0.00016041974394719817,
      "loss": 0.4078,
      "step": 134500
    },
    {
      "epoch": 0.46561344398282833,
      "grad_norm": 0.0018269651336595416,
      "learning_rate": 0.0001603159668051515,
      "loss": 0.468,
      "step": 134600
    },
    {
      "epoch": 0.46595936778965064,
      "grad_norm": 0.17953985929489136,
      "learning_rate": 0.0001602121896631048,
      "loss": 0.3041,
      "step": 134700
    },
    {
      "epoch": 0.46630529159647294,
      "grad_norm": 0.026202354580163956,
      "learning_rate": 0.00016010841252105812,
      "loss": 0.2433,
      "step": 134800
    },
    {
      "epoch": 0.4666512154032953,
      "grad_norm": 0.0028031994588673115,
      "learning_rate": 0.00016000463537901142,
      "loss": 0.384,
      "step": 134900
    },
    {
      "epoch": 0.4669971392101176,
      "grad_norm": 0.0003052922838833183,
      "learning_rate": 0.00015990085823696472,
      "loss": 0.4054,
      "step": 135000
    },
    {
      "epoch": 0.4673430630169399,
      "grad_norm": 0.0010627314914017916,
      "learning_rate": 0.00015979708109491804,
      "loss": 0.2806,
      "step": 135100
    },
    {
      "epoch": 0.4676889868237622,
      "grad_norm": 0.08969539403915405,
      "learning_rate": 0.00015969330395287134,
      "loss": 0.4863,
      "step": 135200
    },
    {
      "epoch": 0.4680349106305845,
      "grad_norm": 0.014475044794380665,
      "learning_rate": 0.0001595895268108246,
      "loss": 0.3491,
      "step": 135300
    },
    {
      "epoch": 0.4683808344374068,
      "grad_norm": 80.03287506103516,
      "learning_rate": 0.00015948574966877794,
      "loss": 0.377,
      "step": 135400
    },
    {
      "epoch": 0.4687267582442291,
      "grad_norm": 0.01038284506648779,
      "learning_rate": 0.00015938197252673124,
      "loss": 0.2733,
      "step": 135500
    },
    {
      "epoch": 0.46907268205105146,
      "grad_norm": 0.0018735408084467053,
      "learning_rate": 0.00015927819538468453,
      "loss": 0.5839,
      "step": 135600
    },
    {
      "epoch": 0.46941860585787376,
      "grad_norm": 0.04919108375906944,
      "learning_rate": 0.00015917441824263786,
      "loss": 0.2109,
      "step": 135700
    },
    {
      "epoch": 0.46976452966469606,
      "grad_norm": 0.0008419299847446382,
      "learning_rate": 0.00015907064110059116,
      "loss": 0.1733,
      "step": 135800
    },
    {
      "epoch": 0.47011045347151836,
      "grad_norm": 0.006599807646125555,
      "learning_rate": 0.00015896686395854449,
      "loss": 0.3308,
      "step": 135900
    },
    {
      "epoch": 0.47045637727834066,
      "grad_norm": 0.04632424935698509,
      "learning_rate": 0.00015886308681649778,
      "loss": 0.3542,
      "step": 136000
    },
    {
      "epoch": 0.47080230108516297,
      "grad_norm": 0.008663516491651535,
      "learning_rate": 0.00015875930967445108,
      "loss": 0.3472,
      "step": 136100
    },
    {
      "epoch": 0.47114822489198527,
      "grad_norm": 0.007102600298821926,
      "learning_rate": 0.0001586555325324044,
      "loss": 0.4135,
      "step": 136200
    },
    {
      "epoch": 0.4714941486988076,
      "grad_norm": 0.005352529231458902,
      "learning_rate": 0.0001585517553903577,
      "loss": 0.3188,
      "step": 136300
    },
    {
      "epoch": 0.4718400725056299,
      "grad_norm": 0.003929643891751766,
      "learning_rate": 0.00015844797824831103,
      "loss": 0.2078,
      "step": 136400
    },
    {
      "epoch": 0.4721859963124522,
      "grad_norm": 0.00043768377508968115,
      "learning_rate": 0.00015834420110626433,
      "loss": 0.3496,
      "step": 136500
    },
    {
      "epoch": 0.47253192011927453,
      "grad_norm": 0.0009893822716549039,
      "learning_rate": 0.00015824042396421763,
      "loss": 0.2041,
      "step": 136600
    },
    {
      "epoch": 0.47287784392609683,
      "grad_norm": 0.00024806466535665095,
      "learning_rate": 0.00015813664682217093,
      "loss": 0.2939,
      "step": 136700
    },
    {
      "epoch": 0.47322376773291913,
      "grad_norm": 0.0008659504819661379,
      "learning_rate": 0.00015803286968012423,
      "loss": 0.437,
      "step": 136800
    },
    {
      "epoch": 0.47356969153974143,
      "grad_norm": 9.075776100158691,
      "learning_rate": 0.00015792909253807753,
      "loss": 0.2518,
      "step": 136900
    },
    {
      "epoch": 0.4739156153465638,
      "grad_norm": 21.58637237548828,
      "learning_rate": 0.00015782531539603085,
      "loss": 0.3055,
      "step": 137000
    },
    {
      "epoch": 0.4742615391533861,
      "grad_norm": 54.274017333984375,
      "learning_rate": 0.00015772153825398415,
      "loss": 0.4278,
      "step": 137100
    },
    {
      "epoch": 0.4746074629602084,
      "grad_norm": 0.0005155006656423211,
      "learning_rate": 0.00015761776111193748,
      "loss": 0.6951,
      "step": 137200
    },
    {
      "epoch": 0.4749533867670307,
      "grad_norm": 0.03251640126109123,
      "learning_rate": 0.00015751398396989078,
      "loss": 0.3662,
      "step": 137300
    },
    {
      "epoch": 0.475299310573853,
      "grad_norm": 0.09407588094472885,
      "learning_rate": 0.00015741020682784408,
      "loss": 0.4038,
      "step": 137400
    },
    {
      "epoch": 0.4756452343806753,
      "grad_norm": 0.0016203674022108316,
      "learning_rate": 0.0001573064296857974,
      "loss": 0.3462,
      "step": 137500
    },
    {
      "epoch": 0.4759911581874976,
      "grad_norm": 0.00546534638851881,
      "learning_rate": 0.0001572026525437507,
      "loss": 0.3734,
      "step": 137600
    },
    {
      "epoch": 0.47633708199431996,
      "grad_norm": 0.003932235296815634,
      "learning_rate": 0.00015709887540170403,
      "loss": 0.4024,
      "step": 137700
    },
    {
      "epoch": 0.47668300580114226,
      "grad_norm": 19.154693603515625,
      "learning_rate": 0.00015699509825965732,
      "loss": 0.3616,
      "step": 137800
    },
    {
      "epoch": 0.47702892960796456,
      "grad_norm": 0.001120127853937447,
      "learning_rate": 0.00015689132111761062,
      "loss": 0.2797,
      "step": 137900
    },
    {
      "epoch": 0.47737485341478686,
      "grad_norm": 0.6842442154884338,
      "learning_rate": 0.00015678754397556395,
      "loss": 0.2552,
      "step": 138000
    },
    {
      "epoch": 0.47772077722160916,
      "grad_norm": 0.01901782676577568,
      "learning_rate": 0.00015668376683351722,
      "loss": 0.2848,
      "step": 138100
    },
    {
      "epoch": 0.47806670102843146,
      "grad_norm": 0.05422263965010643,
      "learning_rate": 0.00015657998969147052,
      "loss": 0.4162,
      "step": 138200
    },
    {
      "epoch": 0.47841262483525376,
      "grad_norm": 9.901714324951172,
      "learning_rate": 0.00015647621254942385,
      "loss": 0.3209,
      "step": 138300
    },
    {
      "epoch": 0.4787585486420761,
      "grad_norm": 0.0006836700486019254,
      "learning_rate": 0.00015637243540737714,
      "loss": 0.3058,
      "step": 138400
    },
    {
      "epoch": 0.4791044724488984,
      "grad_norm": 0.014827647246420383,
      "learning_rate": 0.00015626865826533047,
      "loss": 0.3185,
      "step": 138500
    },
    {
      "epoch": 0.4794503962557207,
      "grad_norm": 0.003694494953379035,
      "learning_rate": 0.00015616488112328377,
      "loss": 0.2274,
      "step": 138600
    },
    {
      "epoch": 0.479796320062543,
      "grad_norm": 0.010890095494687557,
      "learning_rate": 0.00015606110398123707,
      "loss": 0.4491,
      "step": 138700
    },
    {
      "epoch": 0.4801422438693653,
      "grad_norm": 0.4774719774723053,
      "learning_rate": 0.0001559573268391904,
      "loss": 0.3902,
      "step": 138800
    },
    {
      "epoch": 0.4804881676761876,
      "grad_norm": 7.667632102966309,
      "learning_rate": 0.0001558535496971437,
      "loss": 0.2852,
      "step": 138900
    },
    {
      "epoch": 0.48083409148300993,
      "grad_norm": 0.00024019343254622072,
      "learning_rate": 0.00015574977255509702,
      "loss": 0.2854,
      "step": 139000
    },
    {
      "epoch": 0.4811800152898323,
      "grad_norm": 49.02022933959961,
      "learning_rate": 0.00015564599541305032,
      "loss": 0.4027,
      "step": 139100
    },
    {
      "epoch": 0.4815259390966546,
      "grad_norm": 1.8320295810699463,
      "learning_rate": 0.00015554221827100362,
      "loss": 0.3095,
      "step": 139200
    },
    {
      "epoch": 0.4818718629034769,
      "grad_norm": 0.007835986092686653,
      "learning_rate": 0.00015543844112895694,
      "loss": 0.0852,
      "step": 139300
    },
    {
      "epoch": 0.4822177867102992,
      "grad_norm": 0.00016180802776943892,
      "learning_rate": 0.00015533466398691024,
      "loss": 0.4733,
      "step": 139400
    },
    {
      "epoch": 0.4825637105171215,
      "grad_norm": 0.0012795130023732781,
      "learning_rate": 0.0001552308868448635,
      "loss": 0.4756,
      "step": 139500
    },
    {
      "epoch": 0.4829096343239438,
      "grad_norm": 0.0017418869538232684,
      "learning_rate": 0.00015512710970281684,
      "loss": 0.2284,
      "step": 139600
    },
    {
      "epoch": 0.4832555581307661,
      "grad_norm": 0.11323820799589157,
      "learning_rate": 0.00015502333256077014,
      "loss": 0.3112,
      "step": 139700
    },
    {
      "epoch": 0.48360148193758845,
      "grad_norm": 14.812111854553223,
      "learning_rate": 0.00015491955541872346,
      "loss": 0.2427,
      "step": 139800
    },
    {
      "epoch": 0.48394740574441075,
      "grad_norm": 0.017353178933262825,
      "learning_rate": 0.00015481577827667676,
      "loss": 0.3289,
      "step": 139900
    },
    {
      "epoch": 0.48429332955123305,
      "grad_norm": 0.0002664150670170784,
      "learning_rate": 0.00015471200113463006,
      "loss": 0.4541,
      "step": 140000
    },
    {
      "epoch": 0.48463925335805536,
      "grad_norm": 0.011530239135026932,
      "learning_rate": 0.0001546082239925834,
      "loss": 0.1798,
      "step": 140100
    },
    {
      "epoch": 0.48498517716487766,
      "grad_norm": 0.010050274431705475,
      "learning_rate": 0.00015450444685053669,
      "loss": 0.3997,
      "step": 140200
    },
    {
      "epoch": 0.48533110097169996,
      "grad_norm": 0.0010555170010775328,
      "learning_rate": 0.00015440066970849,
      "loss": 0.3745,
      "step": 140300
    },
    {
      "epoch": 0.48567702477852226,
      "grad_norm": 58.13873291015625,
      "learning_rate": 0.0001542968925664433,
      "loss": 0.3688,
      "step": 140400
    },
    {
      "epoch": 0.4860229485853446,
      "grad_norm": 63.6955680847168,
      "learning_rate": 0.0001541931154243966,
      "loss": 0.2713,
      "step": 140500
    },
    {
      "epoch": 0.4863688723921669,
      "grad_norm": 0.0004627992457244545,
      "learning_rate": 0.00015408933828234993,
      "loss": 0.3641,
      "step": 140600
    },
    {
      "epoch": 0.4867147961989892,
      "grad_norm": 0.000824995688162744,
      "learning_rate": 0.00015398556114030323,
      "loss": 0.4009,
      "step": 140700
    },
    {
      "epoch": 0.4870607200058115,
      "grad_norm": 0.002927006920799613,
      "learning_rate": 0.00015388178399825656,
      "loss": 0.3658,
      "step": 140800
    },
    {
      "epoch": 0.4874066438126338,
      "grad_norm": 30.505388259887695,
      "learning_rate": 0.00015377800685620983,
      "loss": 0.2815,
      "step": 140900
    },
    {
      "epoch": 0.4877525676194561,
      "grad_norm": 112.31756591796875,
      "learning_rate": 0.00015367422971416313,
      "loss": 0.3844,
      "step": 141000
    },
    {
      "epoch": 0.4880984914262784,
      "grad_norm": 2.5642566680908203,
      "learning_rate": 0.00015357045257211646,
      "loss": 0.2539,
      "step": 141100
    },
    {
      "epoch": 0.4884444152331008,
      "grad_norm": 1.6615406274795532,
      "learning_rate": 0.00015346667543006975,
      "loss": 0.2114,
      "step": 141200
    },
    {
      "epoch": 0.4887903390399231,
      "grad_norm": 0.0014836207265034318,
      "learning_rate": 0.00015336289828802305,
      "loss": 0.4155,
      "step": 141300
    },
    {
      "epoch": 0.4891362628467454,
      "grad_norm": 0.0005727363750338554,
      "learning_rate": 0.00015325912114597638,
      "loss": 0.3323,
      "step": 141400
    },
    {
      "epoch": 0.4894821866535677,
      "grad_norm": 0.0022467658855021,
      "learning_rate": 0.00015315534400392968,
      "loss": 0.3693,
      "step": 141500
    },
    {
      "epoch": 0.48982811046039,
      "grad_norm": 0.004017055034637451,
      "learning_rate": 0.000153051566861883,
      "loss": 0.5819,
      "step": 141600
    },
    {
      "epoch": 0.4901740342672123,
      "grad_norm": 0.0008801922085694969,
      "learning_rate": 0.0001529477897198363,
      "loss": 0.2262,
      "step": 141700
    },
    {
      "epoch": 0.4905199580740346,
      "grad_norm": 0.00025224333512596786,
      "learning_rate": 0.0001528440125777896,
      "loss": 0.2785,
      "step": 141800
    },
    {
      "epoch": 0.49086588188085695,
      "grad_norm": 0.003447446972131729,
      "learning_rate": 0.00015274023543574293,
      "loss": 0.2577,
      "step": 141900
    },
    {
      "epoch": 0.49121180568767925,
      "grad_norm": 40.473365783691406,
      "learning_rate": 0.00015263645829369623,
      "loss": 0.3589,
      "step": 142000
    },
    {
      "epoch": 0.49155772949450155,
      "grad_norm": 0.005828952416777611,
      "learning_rate": 0.00015253268115164953,
      "loss": 0.2296,
      "step": 142100
    },
    {
      "epoch": 0.49190365330132385,
      "grad_norm": 0.0028808985371142626,
      "learning_rate": 0.00015242890400960285,
      "loss": 0.1453,
      "step": 142200
    },
    {
      "epoch": 0.49224957710814615,
      "grad_norm": 0.0015108464285731316,
      "learning_rate": 0.00015232512686755612,
      "loss": 0.134,
      "step": 142300
    },
    {
      "epoch": 0.49259550091496845,
      "grad_norm": 26.226844787597656,
      "learning_rate": 0.00015222134972550942,
      "loss": 0.3301,
      "step": 142400
    },
    {
      "epoch": 0.49294142472179076,
      "grad_norm": 0.0003795679658651352,
      "learning_rate": 0.00015211757258346275,
      "loss": 0.3518,
      "step": 142500
    },
    {
      "epoch": 0.4932873485286131,
      "grad_norm": 0.0005135576357133687,
      "learning_rate": 0.00015201379544141605,
      "loss": 0.2489,
      "step": 142600
    },
    {
      "epoch": 0.4936332723354354,
      "grad_norm": 0.00034029423841275275,
      "learning_rate": 0.00015191001829936937,
      "loss": 0.3378,
      "step": 142700
    },
    {
      "epoch": 0.4939791961422577,
      "grad_norm": 0.0018115965649485588,
      "learning_rate": 0.00015180624115732267,
      "loss": 0.2823,
      "step": 142800
    },
    {
      "epoch": 0.49432511994908,
      "grad_norm": 0.007184075657278299,
      "learning_rate": 0.00015170246401527597,
      "loss": 0.481,
      "step": 142900
    },
    {
      "epoch": 0.4946710437559023,
      "grad_norm": 0.0036054516676813364,
      "learning_rate": 0.0001515986868732293,
      "loss": 0.3777,
      "step": 143000
    },
    {
      "epoch": 0.4950169675627246,
      "grad_norm": 26.2647762298584,
      "learning_rate": 0.0001514949097311826,
      "loss": 0.3358,
      "step": 143100
    },
    {
      "epoch": 0.4953628913695469,
      "grad_norm": 0.2778758406639099,
      "learning_rate": 0.00015139113258913592,
      "loss": 0.3361,
      "step": 143200
    },
    {
      "epoch": 0.4957088151763693,
      "grad_norm": 0.06826730072498322,
      "learning_rate": 0.00015128735544708922,
      "loss": 0.5024,
      "step": 143300
    },
    {
      "epoch": 0.4960547389831916,
      "grad_norm": 0.0013124628458172083,
      "learning_rate": 0.00015118357830504252,
      "loss": 0.3969,
      "step": 143400
    },
    {
      "epoch": 0.4964006627900139,
      "grad_norm": 0.005499102175235748,
      "learning_rate": 0.00015107980116299584,
      "loss": 0.2327,
      "step": 143500
    },
    {
      "epoch": 0.4967465865968362,
      "grad_norm": 9.831058502197266,
      "learning_rate": 0.00015097602402094914,
      "loss": 0.3422,
      "step": 143600
    },
    {
      "epoch": 0.4970925104036585,
      "grad_norm": 18.801746368408203,
      "learning_rate": 0.00015087224687890241,
      "loss": 0.5282,
      "step": 143700
    },
    {
      "epoch": 0.4974384342104808,
      "grad_norm": 0.025577332824468613,
      "learning_rate": 0.00015076846973685574,
      "loss": 0.3035,
      "step": 143800
    },
    {
      "epoch": 0.4977843580173031,
      "grad_norm": 0.0008926730952225626,
      "learning_rate": 0.00015066469259480904,
      "loss": 0.4023,
      "step": 143900
    },
    {
      "epoch": 0.49813028182412544,
      "grad_norm": 0.001233958755619824,
      "learning_rate": 0.00015056091545276236,
      "loss": 0.5728,
      "step": 144000
    },
    {
      "epoch": 0.49847620563094774,
      "grad_norm": 0.0017987898318096995,
      "learning_rate": 0.00015045713831071566,
      "loss": 0.284,
      "step": 144100
    },
    {
      "epoch": 0.49882212943777005,
      "grad_norm": 0.0011753109283745289,
      "learning_rate": 0.00015035336116866896,
      "loss": 0.3213,
      "step": 144200
    },
    {
      "epoch": 0.49916805324459235,
      "grad_norm": 266.5769348144531,
      "learning_rate": 0.0001502495840266223,
      "loss": 0.2434,
      "step": 144300
    },
    {
      "epoch": 0.49951397705141465,
      "grad_norm": 0.0011682697804644704,
      "learning_rate": 0.0001501458068845756,
      "loss": 0.3622,
      "step": 144400
    },
    {
      "epoch": 0.49985990085823695,
      "grad_norm": 0.0015054041286930442,
      "learning_rate": 0.0001500420297425289,
      "loss": 0.4584,
      "step": 144500
    },
    {
      "epoch": 0.5002058246650593,
      "grad_norm": 0.47497180104255676,
      "learning_rate": 0.0001499382526004822,
      "loss": 0.434,
      "step": 144600
    },
    {
      "epoch": 0.5005517484718816,
      "grad_norm": 15.270817756652832,
      "learning_rate": 0.0001498344754584355,
      "loss": 0.4338,
      "step": 144700
    },
    {
      "epoch": 0.5008976722787039,
      "grad_norm": 0.0006517106085084379,
      "learning_rate": 0.0001497306983163888,
      "loss": 0.2143,
      "step": 144800
    },
    {
      "epoch": 0.5012435960855262,
      "grad_norm": 0.00134209543466568,
      "learning_rate": 0.00014962692117434213,
      "loss": 0.4707,
      "step": 144900
    },
    {
      "epoch": 0.5015895198923485,
      "grad_norm": 0.0001997818035306409,
      "learning_rate": 0.00014952314403229543,
      "loss": 0.1602,
      "step": 145000
    },
    {
      "epoch": 0.5019354436991709,
      "grad_norm": 0.006938604172319174,
      "learning_rate": 0.00014941936689024873,
      "loss": 0.2676,
      "step": 145100
    },
    {
      "epoch": 0.5022813675059932,
      "grad_norm": 0.0018169645918533206,
      "learning_rate": 0.00014931558974820206,
      "loss": 0.3179,
      "step": 145200
    },
    {
      "epoch": 0.5026272913128155,
      "grad_norm": 1.029383897781372,
      "learning_rate": 0.00014921181260615536,
      "loss": 0.3247,
      "step": 145300
    },
    {
      "epoch": 0.5029732151196378,
      "grad_norm": 0.00145804975181818,
      "learning_rate": 0.00014910803546410866,
      "loss": 0.3641,
      "step": 145400
    },
    {
      "epoch": 0.5033191389264601,
      "grad_norm": 32.597557067871094,
      "learning_rate": 0.00014900425832206195,
      "loss": 0.2538,
      "step": 145500
    },
    {
      "epoch": 0.5036650627332824,
      "grad_norm": 0.0026553587522357702,
      "learning_rate": 0.00014890048118001528,
      "loss": 0.2506,
      "step": 145600
    },
    {
      "epoch": 0.5040109865401047,
      "grad_norm": 0.008641806431114674,
      "learning_rate": 0.00014879670403796858,
      "loss": 0.4342,
      "step": 145700
    },
    {
      "epoch": 0.504356910346927,
      "grad_norm": 0.11062224209308624,
      "learning_rate": 0.0001486929268959219,
      "loss": 0.4729,
      "step": 145800
    },
    {
      "epoch": 0.5047028341537493,
      "grad_norm": 0.000468310434371233,
      "learning_rate": 0.0001485891497538752,
      "loss": 0.4377,
      "step": 145900
    },
    {
      "epoch": 0.5050487579605716,
      "grad_norm": 17.550304412841797,
      "learning_rate": 0.0001484853726118285,
      "loss": 0.188,
      "step": 146000
    },
    {
      "epoch": 0.5053946817673939,
      "grad_norm": 0.011592303402721882,
      "learning_rate": 0.0001483815954697818,
      "loss": 0.4427,
      "step": 146100
    },
    {
      "epoch": 0.5057406055742162,
      "grad_norm": 2.784658908843994,
      "learning_rate": 0.00014827781832773513,
      "loss": 0.6454,
      "step": 146200
    },
    {
      "epoch": 0.5060865293810385,
      "grad_norm": 0.0005538025870919228,
      "learning_rate": 0.00014817404118568843,
      "loss": 0.3961,
      "step": 146300
    },
    {
      "epoch": 0.5064324531878608,
      "grad_norm": 0.011706344783306122,
      "learning_rate": 0.00014807026404364173,
      "loss": 0.2401,
      "step": 146400
    },
    {
      "epoch": 0.5067783769946832,
      "grad_norm": 0.0002335419412702322,
      "learning_rate": 0.00014796648690159505,
      "loss": 0.2758,
      "step": 146500
    },
    {
      "epoch": 0.5071243008015055,
      "grad_norm": 0.11680949479341507,
      "learning_rate": 0.00014786270975954835,
      "loss": 0.4887,
      "step": 146600
    },
    {
      "epoch": 0.5074702246083278,
      "grad_norm": 0.6066623330116272,
      "learning_rate": 0.00014775893261750168,
      "loss": 0.357,
      "step": 146700
    },
    {
      "epoch": 0.5078161484151501,
      "grad_norm": 83.05113983154297,
      "learning_rate": 0.00014765515547545495,
      "loss": 0.2956,
      "step": 146800
    },
    {
      "epoch": 0.5081620722219724,
      "grad_norm": 0.7650474309921265,
      "learning_rate": 0.00014755137833340827,
      "loss": 0.4335,
      "step": 146900
    },
    {
      "epoch": 0.5085079960287947,
      "grad_norm": 0.2392531931400299,
      "learning_rate": 0.00014744760119136157,
      "loss": 0.2215,
      "step": 147000
    },
    {
      "epoch": 0.508853919835617,
      "grad_norm": 0.06281158328056335,
      "learning_rate": 0.0001473438240493149,
      "loss": 0.277,
      "step": 147100
    },
    {
      "epoch": 0.5091998436424393,
      "grad_norm": 0.0062499139457941055,
      "learning_rate": 0.0001472400469072682,
      "loss": 0.4826,
      "step": 147200
    },
    {
      "epoch": 0.5095457674492616,
      "grad_norm": 0.00235155550763011,
      "learning_rate": 0.0001471362697652215,
      "loss": 0.3268,
      "step": 147300
    },
    {
      "epoch": 0.5098916912560839,
      "grad_norm": 5.347597122192383,
      "learning_rate": 0.00014703249262317482,
      "loss": 0.3243,
      "step": 147400
    },
    {
      "epoch": 0.5102376150629062,
      "grad_norm": 0.001476706936955452,
      "learning_rate": 0.0001469287154811281,
      "loss": 0.2299,
      "step": 147500
    },
    {
      "epoch": 0.5105835388697285,
      "grad_norm": 0.30003437399864197,
      "learning_rate": 0.00014682493833908142,
      "loss": 0.4615,
      "step": 147600
    },
    {
      "epoch": 0.5109294626765508,
      "grad_norm": 0.01173057034611702,
      "learning_rate": 0.00014672116119703472,
      "loss": 0.3824,
      "step": 147700
    },
    {
      "epoch": 0.5112753864833731,
      "grad_norm": 0.013223964720964432,
      "learning_rate": 0.00014661738405498804,
      "loss": 0.2888,
      "step": 147800
    },
    {
      "epoch": 0.5116213102901955,
      "grad_norm": 9.792561531066895,
      "learning_rate": 0.00014651360691294134,
      "loss": 0.2551,
      "step": 147900
    },
    {
      "epoch": 0.5119672340970178,
      "grad_norm": 0.010988043621182442,
      "learning_rate": 0.00014640982977089464,
      "loss": 0.3313,
      "step": 148000
    },
    {
      "epoch": 0.5123131579038401,
      "grad_norm": 0.04209190234541893,
      "learning_rate": 0.00014630605262884797,
      "loss": 0.2091,
      "step": 148100
    },
    {
      "epoch": 0.5126590817106624,
      "grad_norm": 6.318711280822754,
      "learning_rate": 0.00014620227548680127,
      "loss": 0.2633,
      "step": 148200
    },
    {
      "epoch": 0.5130050055174847,
      "grad_norm": 0.014301271177828312,
      "learning_rate": 0.00014609849834475456,
      "loss": 0.1374,
      "step": 148300
    },
    {
      "epoch": 0.513350929324307,
      "grad_norm": 0.0028044029604643583,
      "learning_rate": 0.00014599472120270786,
      "loss": 0.1591,
      "step": 148400
    },
    {
      "epoch": 0.5136968531311293,
      "grad_norm": 0.0071253469213843346,
      "learning_rate": 0.0001458909440606612,
      "loss": 0.2687,
      "step": 148500
    },
    {
      "epoch": 0.5140427769379516,
      "grad_norm": 0.0011349916458129883,
      "learning_rate": 0.0001457871669186145,
      "loss": 0.3182,
      "step": 148600
    },
    {
      "epoch": 0.5143887007447739,
      "grad_norm": 13.597378730773926,
      "learning_rate": 0.00014568338977656781,
      "loss": 0.44,
      "step": 148700
    },
    {
      "epoch": 0.5147346245515962,
      "grad_norm": 0.006315537262707949,
      "learning_rate": 0.0001455796126345211,
      "loss": 0.2834,
      "step": 148800
    },
    {
      "epoch": 0.5150805483584185,
      "grad_norm": 2.2428295612335205,
      "learning_rate": 0.0001454758354924744,
      "loss": 0.4321,
      "step": 148900
    },
    {
      "epoch": 0.5154264721652408,
      "grad_norm": 0.00019474820874165744,
      "learning_rate": 0.0001453720583504277,
      "loss": 0.1608,
      "step": 149000
    },
    {
      "epoch": 0.5157723959720631,
      "grad_norm": 0.0003724759444594383,
      "learning_rate": 0.00014526828120838104,
      "loss": 0.2556,
      "step": 149100
    },
    {
      "epoch": 0.5161183197788854,
      "grad_norm": 0.0020529143512248993,
      "learning_rate": 0.00014516450406633433,
      "loss": 0.2737,
      "step": 149200
    },
    {
      "epoch": 0.5164642435857079,
      "grad_norm": 0.0007346191559918225,
      "learning_rate": 0.00014506072692428763,
      "loss": 0.2558,
      "step": 149300
    },
    {
      "epoch": 0.5168101673925302,
      "grad_norm": 0.0015414258232340217,
      "learning_rate": 0.00014495694978224096,
      "loss": 0.3798,
      "step": 149400
    },
    {
      "epoch": 0.5171560911993525,
      "grad_norm": 109.79740142822266,
      "learning_rate": 0.00014485317264019426,
      "loss": 0.1903,
      "step": 149500
    },
    {
      "epoch": 0.5175020150061748,
      "grad_norm": 0.00032732903491705656,
      "learning_rate": 0.00014474939549814756,
      "loss": 0.3113,
      "step": 149600
    },
    {
      "epoch": 0.5178479388129971,
      "grad_norm": 0.0007130590965971351,
      "learning_rate": 0.00014464561835610086,
      "loss": 0.2772,
      "step": 149700
    },
    {
      "epoch": 0.5181938626198194,
      "grad_norm": 0.0020269989036023617,
      "learning_rate": 0.00014454184121405418,
      "loss": 0.3668,
      "step": 149800
    },
    {
      "epoch": 0.5185397864266417,
      "grad_norm": 0.0009674782049842179,
      "learning_rate": 0.00014443806407200748,
      "loss": 0.2344,
      "step": 149900
    },
    {
      "epoch": 0.518885710233464,
      "grad_norm": 0.0011166094336658716,
      "learning_rate": 0.0001443342869299608,
      "loss": 0.318,
      "step": 150000
    },
    {
      "epoch": 0.5192316340402863,
      "grad_norm": 0.0001899077178677544,
      "learning_rate": 0.0001442305097879141,
      "loss": 0.2653,
      "step": 150100
    },
    {
      "epoch": 0.5195775578471086,
      "grad_norm": 0.00014810734137427062,
      "learning_rate": 0.0001441267326458674,
      "loss": 0.2529,
      "step": 150200
    },
    {
      "epoch": 0.5199234816539309,
      "grad_norm": 13.58260726928711,
      "learning_rate": 0.0001440229555038207,
      "loss": 0.2164,
      "step": 150300
    },
    {
      "epoch": 0.5202694054607532,
      "grad_norm": 0.00017943551938515157,
      "learning_rate": 0.00014391917836177403,
      "loss": 0.3193,
      "step": 150400
    },
    {
      "epoch": 0.5206153292675755,
      "grad_norm": 0.30697983503341675,
      "learning_rate": 0.00014381540121972733,
      "loss": 0.4037,
      "step": 150500
    },
    {
      "epoch": 0.5209612530743978,
      "grad_norm": 0.0015033397357910872,
      "learning_rate": 0.00014371162407768063,
      "loss": 0.5345,
      "step": 150600
    },
    {
      "epoch": 0.5213071768812202,
      "grad_norm": 0.0023763959761708975,
      "learning_rate": 0.00014360784693563395,
      "loss": 0.2379,
      "step": 150700
    },
    {
      "epoch": 0.5216531006880425,
      "grad_norm": 0.0009203007211908698,
      "learning_rate": 0.00014350406979358725,
      "loss": 0.1934,
      "step": 150800
    },
    {
      "epoch": 0.5219990244948648,
      "grad_norm": 0.00031608034623786807,
      "learning_rate": 0.00014340029265154058,
      "loss": 0.3217,
      "step": 150900
    },
    {
      "epoch": 0.5223449483016871,
      "grad_norm": 0.40759238600730896,
      "learning_rate": 0.00014329651550949385,
      "loss": 0.2663,
      "step": 151000
    },
    {
      "epoch": 0.5226908721085094,
      "grad_norm": 0.001598138827830553,
      "learning_rate": 0.00014319273836744717,
      "loss": 0.3209,
      "step": 151100
    },
    {
      "epoch": 0.5230367959153317,
      "grad_norm": 0.00024989532539620996,
      "learning_rate": 0.00014308896122540047,
      "loss": 0.2399,
      "step": 151200
    },
    {
      "epoch": 0.523382719722154,
      "grad_norm": 0.0019883308559656143,
      "learning_rate": 0.0001429851840833538,
      "loss": 0.3002,
      "step": 151300
    },
    {
      "epoch": 0.5237286435289763,
      "grad_norm": 0.0018741292878985405,
      "learning_rate": 0.0001428814069413071,
      "loss": 0.2412,
      "step": 151400
    },
    {
      "epoch": 0.5240745673357986,
      "grad_norm": 0.08942308276891708,
      "learning_rate": 0.0001427776297992604,
      "loss": 0.2256,
      "step": 151500
    },
    {
      "epoch": 0.5244204911426209,
      "grad_norm": 0.0029181630816310644,
      "learning_rate": 0.00014267385265721372,
      "loss": 0.2581,
      "step": 151600
    },
    {
      "epoch": 0.5247664149494432,
      "grad_norm": 0.15614329278469086,
      "learning_rate": 0.00014257007551516702,
      "loss": 0.355,
      "step": 151700
    },
    {
      "epoch": 0.5251123387562655,
      "grad_norm": 56.48361587524414,
      "learning_rate": 0.00014246629837312032,
      "loss": 0.3577,
      "step": 151800
    },
    {
      "epoch": 0.5254582625630878,
      "grad_norm": 0.013127313926815987,
      "learning_rate": 0.00014236252123107362,
      "loss": 0.3303,
      "step": 151900
    },
    {
      "epoch": 0.5258041863699101,
      "grad_norm": 0.0014917039079591632,
      "learning_rate": 0.00014225874408902694,
      "loss": 0.174,
      "step": 152000
    },
    {
      "epoch": 0.5261501101767325,
      "grad_norm": 2.5515189170837402,
      "learning_rate": 0.00014215496694698024,
      "loss": 0.4145,
      "step": 152100
    },
    {
      "epoch": 0.5264960339835548,
      "grad_norm": 8.79547119140625,
      "learning_rate": 0.00014205118980493357,
      "loss": 0.3738,
      "step": 152200
    },
    {
      "epoch": 0.5268419577903771,
      "grad_norm": 0.008035204373300076,
      "learning_rate": 0.00014194741266288687,
      "loss": 0.3342,
      "step": 152300
    },
    {
      "epoch": 0.5271878815971994,
      "grad_norm": 0.005535752512514591,
      "learning_rate": 0.00014184363552084017,
      "loss": 0.177,
      "step": 152400
    },
    {
      "epoch": 0.5275338054040217,
      "grad_norm": 0.0004862583882641047,
      "learning_rate": 0.00014173985837879347,
      "loss": 0.3484,
      "step": 152500
    },
    {
      "epoch": 0.527879729210844,
      "grad_norm": 0.0041024694219231606,
      "learning_rate": 0.0001416360812367468,
      "loss": 0.1542,
      "step": 152600
    },
    {
      "epoch": 0.5282256530176663,
      "grad_norm": 0.85494065284729,
      "learning_rate": 0.0001415323040947001,
      "loss": 0.4803,
      "step": 152700
    },
    {
      "epoch": 0.5285715768244886,
      "grad_norm": 0.0017632843228057027,
      "learning_rate": 0.0001414285269526534,
      "loss": 0.3495,
      "step": 152800
    },
    {
      "epoch": 0.5289175006313109,
      "grad_norm": 0.004090780392289162,
      "learning_rate": 0.00014132474981060672,
      "loss": 0.2225,
      "step": 152900
    },
    {
      "epoch": 0.5292634244381332,
      "grad_norm": 0.07683783024549484,
      "learning_rate": 0.00014122097266856001,
      "loss": 0.4655,
      "step": 153000
    },
    {
      "epoch": 0.5296093482449555,
      "grad_norm": 0.005350003018975258,
      "learning_rate": 0.0001411171955265133,
      "loss": 0.5342,
      "step": 153100
    },
    {
      "epoch": 0.5299552720517778,
      "grad_norm": 0.00235030148178339,
      "learning_rate": 0.0001410134183844666,
      "loss": 0.384,
      "step": 153200
    },
    {
      "epoch": 0.5303011958586001,
      "grad_norm": 0.13195279240608215,
      "learning_rate": 0.00014090964124241994,
      "loss": 0.2522,
      "step": 153300
    },
    {
      "epoch": 0.5306471196654224,
      "grad_norm": 0.0001502834929851815,
      "learning_rate": 0.00014080586410037324,
      "loss": 0.2715,
      "step": 153400
    },
    {
      "epoch": 0.5309930434722449,
      "grad_norm": 0.00393861997872591,
      "learning_rate": 0.00014070208695832656,
      "loss": 0.2179,
      "step": 153500
    },
    {
      "epoch": 0.5313389672790672,
      "grad_norm": 0.003659251146018505,
      "learning_rate": 0.00014059830981627986,
      "loss": 0.3152,
      "step": 153600
    },
    {
      "epoch": 0.5316848910858895,
      "grad_norm": 0.004854896571487188,
      "learning_rate": 0.00014049453267423316,
      "loss": 0.1975,
      "step": 153700
    },
    {
      "epoch": 0.5320308148927118,
      "grad_norm": 0.029950987547636032,
      "learning_rate": 0.00014039075553218646,
      "loss": 0.2813,
      "step": 153800
    },
    {
      "epoch": 0.5323767386995341,
      "grad_norm": 3.0883443355560303,
      "learning_rate": 0.00014028697839013976,
      "loss": 0.517,
      "step": 153900
    },
    {
      "epoch": 0.5327226625063564,
      "grad_norm": 20.46616554260254,
      "learning_rate": 0.00014018320124809308,
      "loss": 0.3935,
      "step": 154000
    },
    {
      "epoch": 0.5330685863131787,
      "grad_norm": 0.0019439614843577147,
      "learning_rate": 0.00014007942410604638,
      "loss": 0.325,
      "step": 154100
    },
    {
      "epoch": 0.533414510120001,
      "grad_norm": 0.005044552031904459,
      "learning_rate": 0.0001399756469639997,
      "loss": 0.2769,
      "step": 154200
    },
    {
      "epoch": 0.5337604339268233,
      "grad_norm": 0.0008118717814795673,
      "learning_rate": 0.000139871869821953,
      "loss": 0.3321,
      "step": 154300
    },
    {
      "epoch": 0.5341063577336456,
      "grad_norm": 0.2413497269153595,
      "learning_rate": 0.0001397680926799063,
      "loss": 0.5173,
      "step": 154400
    },
    {
      "epoch": 0.5344522815404679,
      "grad_norm": 2.705655813217163,
      "learning_rate": 0.0001396643155378596,
      "loss": 0.7543,
      "step": 154500
    },
    {
      "epoch": 0.5347982053472902,
      "grad_norm": 0.0006388147594407201,
      "learning_rate": 0.00013956053839581293,
      "loss": 0.4208,
      "step": 154600
    },
    {
      "epoch": 0.5351441291541125,
      "grad_norm": 0.0017339482437819242,
      "learning_rate": 0.00013945676125376623,
      "loss": 0.3224,
      "step": 154700
    },
    {
      "epoch": 0.5354900529609348,
      "grad_norm": 0.5232107043266296,
      "learning_rate": 0.00013935298411171953,
      "loss": 0.2658,
      "step": 154800
    },
    {
      "epoch": 0.5358359767677572,
      "grad_norm": 0.01230329368263483,
      "learning_rate": 0.00013924920696967285,
      "loss": 0.2407,
      "step": 154900
    },
    {
      "epoch": 0.5361819005745795,
      "grad_norm": 0.00030572767718695104,
      "learning_rate": 0.00013914542982762615,
      "loss": 0.4093,
      "step": 155000
    },
    {
      "epoch": 0.5365278243814018,
      "grad_norm": 0.00018588123202789575,
      "learning_rate": 0.00013904165268557948,
      "loss": 0.2246,
      "step": 155100
    },
    {
      "epoch": 0.5368737481882241,
      "grad_norm": 0.0007624161662533879,
      "learning_rate": 0.00013893787554353275,
      "loss": 0.2789,
      "step": 155200
    },
    {
      "epoch": 0.5372196719950464,
      "grad_norm": 0.002844500355422497,
      "learning_rate": 0.00013883409840148608,
      "loss": 0.2083,
      "step": 155300
    },
    {
      "epoch": 0.5375655958018687,
      "grad_norm": 46.834407806396484,
      "learning_rate": 0.00013873032125943937,
      "loss": 0.4462,
      "step": 155400
    },
    {
      "epoch": 0.537911519608691,
      "grad_norm": 0.2145286202430725,
      "learning_rate": 0.0001386265441173927,
      "loss": 0.1621,
      "step": 155500
    },
    {
      "epoch": 0.5382574434155133,
      "grad_norm": 0.0008641339954920113,
      "learning_rate": 0.000138522766975346,
      "loss": 0.3829,
      "step": 155600
    },
    {
      "epoch": 0.5386033672223356,
      "grad_norm": 0.10942593961954117,
      "learning_rate": 0.0001384189898332993,
      "loss": 0.339,
      "step": 155700
    },
    {
      "epoch": 0.5389492910291579,
      "grad_norm": 0.001016790047287941,
      "learning_rate": 0.00013831521269125262,
      "loss": 0.2518,
      "step": 155800
    },
    {
      "epoch": 0.5392952148359802,
      "grad_norm": 0.010501601733267307,
      "learning_rate": 0.00013821143554920592,
      "loss": 0.3197,
      "step": 155900
    },
    {
      "epoch": 0.5396411386428025,
      "grad_norm": 0.004602767527103424,
      "learning_rate": 0.00013810765840715922,
      "loss": 0.31,
      "step": 156000
    },
    {
      "epoch": 0.5399870624496248,
      "grad_norm": 0.007801272440701723,
      "learning_rate": 0.00013800388126511252,
      "loss": 0.191,
      "step": 156100
    },
    {
      "epoch": 0.5403329862564471,
      "grad_norm": 27.228246688842773,
      "learning_rate": 0.00013790010412306585,
      "loss": 0.391,
      "step": 156200
    },
    {
      "epoch": 0.5406789100632695,
      "grad_norm": 0.023709982633590698,
      "learning_rate": 0.00013779632698101914,
      "loss": 0.2599,
      "step": 156300
    },
    {
      "epoch": 0.5410248338700918,
      "grad_norm": 2.169964075088501,
      "learning_rate": 0.00013769254983897247,
      "loss": 0.3413,
      "step": 156400
    },
    {
      "epoch": 0.5413707576769141,
      "grad_norm": 0.0003873216628562659,
      "learning_rate": 0.00013758877269692577,
      "loss": 0.351,
      "step": 156500
    },
    {
      "epoch": 0.5417166814837364,
      "grad_norm": 0.00503583112731576,
      "learning_rate": 0.00013748499555487907,
      "loss": 0.2655,
      "step": 156600
    },
    {
      "epoch": 0.5420626052905587,
      "grad_norm": 0.00818678643554449,
      "learning_rate": 0.00013738121841283237,
      "loss": 0.4031,
      "step": 156700
    },
    {
      "epoch": 0.542408529097381,
      "grad_norm": 17.866363525390625,
      "learning_rate": 0.0001372774412707857,
      "loss": 0.3953,
      "step": 156800
    },
    {
      "epoch": 0.5427544529042033,
      "grad_norm": 0.41942283511161804,
      "learning_rate": 0.000137173664128739,
      "loss": 0.2351,
      "step": 156900
    },
    {
      "epoch": 0.5431003767110256,
      "grad_norm": 0.9555957317352295,
      "learning_rate": 0.0001370698869866923,
      "loss": 0.3093,
      "step": 157000
    },
    {
      "epoch": 0.5434463005178479,
      "grad_norm": 0.002072257688269019,
      "learning_rate": 0.00013696610984464562,
      "loss": 0.5195,
      "step": 157100
    },
    {
      "epoch": 0.5437922243246702,
      "grad_norm": 0.0009147073142230511,
      "learning_rate": 0.00013686233270259892,
      "loss": 0.3742,
      "step": 157200
    },
    {
      "epoch": 0.5441381481314925,
      "grad_norm": 0.19613824784755707,
      "learning_rate": 0.00013675855556055221,
      "loss": 0.2803,
      "step": 157300
    },
    {
      "epoch": 0.5444840719383148,
      "grad_norm": 0.0006080198218114674,
      "learning_rate": 0.0001366547784185055,
      "loss": 0.269,
      "step": 157400
    },
    {
      "epoch": 0.5448299957451371,
      "grad_norm": 0.012998275458812714,
      "learning_rate": 0.00013655100127645884,
      "loss": 0.1939,
      "step": 157500
    },
    {
      "epoch": 0.5451759195519594,
      "grad_norm": 0.00033054198138415813,
      "learning_rate": 0.00013644722413441214,
      "loss": 0.4159,
      "step": 157600
    },
    {
      "epoch": 0.5455218433587818,
      "grad_norm": 0.0008915509097278118,
      "learning_rate": 0.00013634344699236546,
      "loss": 0.2119,
      "step": 157700
    },
    {
      "epoch": 0.5458677671656041,
      "grad_norm": 0.007582623511552811,
      "learning_rate": 0.00013623966985031876,
      "loss": 0.2935,
      "step": 157800
    },
    {
      "epoch": 0.5462136909724264,
      "grad_norm": 45.915401458740234,
      "learning_rate": 0.00013613589270827206,
      "loss": 0.3507,
      "step": 157900
    },
    {
      "epoch": 0.5465596147792487,
      "grad_norm": 0.1716012954711914,
      "learning_rate": 0.00013603211556622536,
      "loss": 0.37,
      "step": 158000
    },
    {
      "epoch": 0.546905538586071,
      "grad_norm": 0.0008251030230894685,
      "learning_rate": 0.00013592833842417869,
      "loss": 0.3112,
      "step": 158100
    },
    {
      "epoch": 0.5472514623928934,
      "grad_norm": 0.004591083154082298,
      "learning_rate": 0.00013582456128213198,
      "loss": 0.2808,
      "step": 158200
    },
    {
      "epoch": 0.5475973861997157,
      "grad_norm": 0.0009516743011772633,
      "learning_rate": 0.00013572078414008528,
      "loss": 0.2758,
      "step": 158300
    },
    {
      "epoch": 0.547943310006538,
      "grad_norm": 0.003530861809849739,
      "learning_rate": 0.0001356170069980386,
      "loss": 0.508,
      "step": 158400
    },
    {
      "epoch": 0.5482892338133603,
      "grad_norm": 0.010902284644544125,
      "learning_rate": 0.0001355132298559919,
      "loss": 0.3954,
      "step": 158500
    },
    {
      "epoch": 0.5486351576201826,
      "grad_norm": 0.03592603653669357,
      "learning_rate": 0.00013540945271394523,
      "loss": 0.29,
      "step": 158600
    },
    {
      "epoch": 0.5489810814270049,
      "grad_norm": 0.0009071064414456487,
      "learning_rate": 0.0001353056755718985,
      "loss": 0.188,
      "step": 158700
    },
    {
      "epoch": 0.5493270052338272,
      "grad_norm": 0.0027186793740838766,
      "learning_rate": 0.00013520189842985183,
      "loss": 0.1257,
      "step": 158800
    },
    {
      "epoch": 0.5496729290406495,
      "grad_norm": 0.013444734737277031,
      "learning_rate": 0.00013509812128780513,
      "loss": 0.4004,
      "step": 158900
    },
    {
      "epoch": 0.5500188528474718,
      "grad_norm": 0.0011370350839570165,
      "learning_rate": 0.00013499434414575846,
      "loss": 0.1904,
      "step": 159000
    },
    {
      "epoch": 0.5503647766542942,
      "grad_norm": 0.004539447370916605,
      "learning_rate": 0.00013489056700371175,
      "loss": 0.4645,
      "step": 159100
    },
    {
      "epoch": 0.5507107004611165,
      "grad_norm": 0.012415838427841663,
      "learning_rate": 0.00013478678986166505,
      "loss": 0.3902,
      "step": 159200
    },
    {
      "epoch": 0.5510566242679388,
      "grad_norm": 16.907297134399414,
      "learning_rate": 0.00013468301271961838,
      "loss": 0.299,
      "step": 159300
    },
    {
      "epoch": 0.5514025480747611,
      "grad_norm": 0.0012386931339278817,
      "learning_rate": 0.00013457923557757168,
      "loss": 0.2626,
      "step": 159400
    },
    {
      "epoch": 0.5517484718815834,
      "grad_norm": 0.0003755785583052784,
      "learning_rate": 0.00013447545843552498,
      "loss": 0.4226,
      "step": 159500
    },
    {
      "epoch": 0.5520943956884057,
      "grad_norm": 0.0035122146364301443,
      "learning_rate": 0.00013437168129347828,
      "loss": 0.3137,
      "step": 159600
    },
    {
      "epoch": 0.552440319495228,
      "grad_norm": 36.94603729248047,
      "learning_rate": 0.0001342679041514316,
      "loss": 0.2861,
      "step": 159700
    },
    {
      "epoch": 0.5527862433020503,
      "grad_norm": 7.504627704620361,
      "learning_rate": 0.0001341641270093849,
      "loss": 0.1865,
      "step": 159800
    },
    {
      "epoch": 0.5531321671088726,
      "grad_norm": 0.00041291024535894394,
      "learning_rate": 0.0001340603498673382,
      "loss": 0.1779,
      "step": 159900
    },
    {
      "epoch": 0.5534780909156949,
      "grad_norm": 0.0011301712365821004,
      "learning_rate": 0.00013395657272529153,
      "loss": 0.3337,
      "step": 160000
    },
    {
      "epoch": 0.5538240147225172,
      "grad_norm": 0.0052583180367946625,
      "learning_rate": 0.00013385279558324482,
      "loss": 0.2693,
      "step": 160100
    },
    {
      "epoch": 0.5541699385293395,
      "grad_norm": 0.001224398729391396,
      "learning_rate": 0.00013374901844119812,
      "loss": 0.3742,
      "step": 160200
    },
    {
      "epoch": 0.5545158623361618,
      "grad_norm": 79.90834045410156,
      "learning_rate": 0.00013364524129915142,
      "loss": 0.4624,
      "step": 160300
    },
    {
      "epoch": 0.5548617861429841,
      "grad_norm": 0.00016370303637813777,
      "learning_rate": 0.00013354146415710475,
      "loss": 0.2605,
      "step": 160400
    },
    {
      "epoch": 0.5552077099498065,
      "grad_norm": 83.2128677368164,
      "learning_rate": 0.00013343768701505805,
      "loss": 0.4223,
      "step": 160500
    },
    {
      "epoch": 0.5555536337566288,
      "grad_norm": 33.52151870727539,
      "learning_rate": 0.00013333390987301137,
      "loss": 0.377,
      "step": 160600
    },
    {
      "epoch": 0.5558995575634511,
      "grad_norm": 0.35448843240737915,
      "learning_rate": 0.00013323013273096467,
      "loss": 0.1876,
      "step": 160700
    },
    {
      "epoch": 0.5562454813702734,
      "grad_norm": 0.46992525458335876,
      "learning_rate": 0.00013312635558891797,
      "loss": 0.472,
      "step": 160800
    },
    {
      "epoch": 0.5565914051770957,
      "grad_norm": 0.00263406615704298,
      "learning_rate": 0.00013302257844687127,
      "loss": 0.2383,
      "step": 160900
    },
    {
      "epoch": 0.556937328983918,
      "grad_norm": 0.0069523402489721775,
      "learning_rate": 0.0001329188013048246,
      "loss": 0.3963,
      "step": 161000
    },
    {
      "epoch": 0.5572832527907403,
      "grad_norm": 0.03909822180867195,
      "learning_rate": 0.0001328150241627779,
      "loss": 0.2881,
      "step": 161100
    },
    {
      "epoch": 0.5576291765975626,
      "grad_norm": 0.00246211769990623,
      "learning_rate": 0.0001327112470207312,
      "loss": 0.4553,
      "step": 161200
    },
    {
      "epoch": 0.5579751004043849,
      "grad_norm": 0.008024744689464569,
      "learning_rate": 0.00013260746987868452,
      "loss": 0.4357,
      "step": 161300
    },
    {
      "epoch": 0.5583210242112072,
      "grad_norm": 0.3989991247653961,
      "learning_rate": 0.00013250369273663782,
      "loss": 0.347,
      "step": 161400
    },
    {
      "epoch": 0.5586669480180295,
      "grad_norm": 0.000511318736243993,
      "learning_rate": 0.00013239991559459112,
      "loss": 0.2523,
      "step": 161500
    },
    {
      "epoch": 0.5590128718248518,
      "grad_norm": 0.0003360107948537916,
      "learning_rate": 0.00013229613845254441,
      "loss": 0.3852,
      "step": 161600
    },
    {
      "epoch": 0.5593587956316741,
      "grad_norm": 0.01660076528787613,
      "learning_rate": 0.00013219236131049774,
      "loss": 0.2676,
      "step": 161700
    },
    {
      "epoch": 0.5597047194384964,
      "grad_norm": 38.50989532470703,
      "learning_rate": 0.00013208858416845104,
      "loss": 0.3074,
      "step": 161800
    },
    {
      "epoch": 0.5600506432453188,
      "grad_norm": 0.0013964568497613072,
      "learning_rate": 0.00013198480702640436,
      "loss": 0.2996,
      "step": 161900
    },
    {
      "epoch": 0.5603965670521411,
      "grad_norm": 0.001546346815302968,
      "learning_rate": 0.00013188102988435766,
      "loss": 0.274,
      "step": 162000
    },
    {
      "epoch": 0.5607424908589634,
      "grad_norm": 46.845760345458984,
      "learning_rate": 0.00013177725274231096,
      "loss": 0.4,
      "step": 162100
    },
    {
      "epoch": 0.5610884146657857,
      "grad_norm": 16.67609214782715,
      "learning_rate": 0.00013167347560026426,
      "loss": 0.3681,
      "step": 162200
    },
    {
      "epoch": 0.561434338472608,
      "grad_norm": 0.18711034953594208,
      "learning_rate": 0.0001315696984582176,
      "loss": 0.2911,
      "step": 162300
    },
    {
      "epoch": 0.5617802622794303,
      "grad_norm": 0.002538936212658882,
      "learning_rate": 0.00013146592131617089,
      "loss": 0.3121,
      "step": 162400
    },
    {
      "epoch": 0.5621261860862526,
      "grad_norm": 5.417739391326904,
      "learning_rate": 0.00013136214417412418,
      "loss": 0.2426,
      "step": 162500
    },
    {
      "epoch": 0.562472109893075,
      "grad_norm": 0.00020319317991379648,
      "learning_rate": 0.0001312583670320775,
      "loss": 0.3381,
      "step": 162600
    },
    {
      "epoch": 0.5628180336998972,
      "grad_norm": 0.0015677871415391564,
      "learning_rate": 0.0001311545898900308,
      "loss": 0.2665,
      "step": 162700
    },
    {
      "epoch": 0.5631639575067195,
      "grad_norm": 0.419265478849411,
      "learning_rate": 0.00013105081274798413,
      "loss": 0.4266,
      "step": 162800
    },
    {
      "epoch": 0.5635098813135418,
      "grad_norm": 0.0006939638988114893,
      "learning_rate": 0.0001309470356059374,
      "loss": 0.2759,
      "step": 162900
    },
    {
      "epoch": 0.5638558051203642,
      "grad_norm": 0.23627087473869324,
      "learning_rate": 0.00013084325846389073,
      "loss": 0.2782,
      "step": 163000
    },
    {
      "epoch": 0.5642017289271865,
      "grad_norm": 0.0011124188313260674,
      "learning_rate": 0.00013073948132184403,
      "loss": 0.1723,
      "step": 163100
    },
    {
      "epoch": 0.5645476527340088,
      "grad_norm": 0.00023083851556293666,
      "learning_rate": 0.00013063570417979736,
      "loss": 0.2917,
      "step": 163200
    },
    {
      "epoch": 0.5648935765408312,
      "grad_norm": 4.8602494644001126e-05,
      "learning_rate": 0.00013053192703775066,
      "loss": 0.2783,
      "step": 163300
    },
    {
      "epoch": 0.5652395003476535,
      "grad_norm": 4.653181167668663e-05,
      "learning_rate": 0.00013042814989570395,
      "loss": 0.2855,
      "step": 163400
    },
    {
      "epoch": 0.5655854241544758,
      "grad_norm": 0.0002822772366926074,
      "learning_rate": 0.00013032437275365728,
      "loss": 0.2709,
      "step": 163500
    },
    {
      "epoch": 0.5659313479612981,
      "grad_norm": 0.0003431523800827563,
      "learning_rate": 0.00013022059561161058,
      "loss": 0.4378,
      "step": 163600
    },
    {
      "epoch": 0.5662772717681204,
      "grad_norm": 66.64796447753906,
      "learning_rate": 0.00013011681846956388,
      "loss": 0.2384,
      "step": 163700
    },
    {
      "epoch": 0.5666231955749427,
      "grad_norm": 0.0005046318983659148,
      "learning_rate": 0.00013001304132751718,
      "loss": 0.273,
      "step": 163800
    },
    {
      "epoch": 0.566969119381765,
      "grad_norm": 45.77109909057617,
      "learning_rate": 0.0001299092641854705,
      "loss": 0.2569,
      "step": 163900
    },
    {
      "epoch": 0.5673150431885873,
      "grad_norm": 0.0018443672452121973,
      "learning_rate": 0.0001298054870434238,
      "loss": 0.188,
      "step": 164000
    },
    {
      "epoch": 0.5676609669954096,
      "grad_norm": 0.06708382070064545,
      "learning_rate": 0.00012970170990137713,
      "loss": 0.314,
      "step": 164100
    },
    {
      "epoch": 0.5680068908022319,
      "grad_norm": 0.021604541689157486,
      "learning_rate": 0.00012959793275933043,
      "loss": 0.4451,
      "step": 164200
    },
    {
      "epoch": 0.5683528146090542,
      "grad_norm": 0.000807550735771656,
      "learning_rate": 0.00012949415561728373,
      "loss": 0.1581,
      "step": 164300
    },
    {
      "epoch": 0.5686987384158765,
      "grad_norm": 0.0004862415371462703,
      "learning_rate": 0.00012939037847523702,
      "loss": 0.3126,
      "step": 164400
    },
    {
      "epoch": 0.5690446622226988,
      "grad_norm": 0.001692880061455071,
      "learning_rate": 0.00012928660133319035,
      "loss": 0.307,
      "step": 164500
    },
    {
      "epoch": 0.5693905860295211,
      "grad_norm": 0.0011977336835116148,
      "learning_rate": 0.00012918282419114365,
      "loss": 0.3695,
      "step": 164600
    },
    {
      "epoch": 0.5697365098363435,
      "grad_norm": 72.47615814208984,
      "learning_rate": 0.00012907904704909695,
      "loss": 0.2818,
      "step": 164700
    },
    {
      "epoch": 0.5700824336431658,
      "grad_norm": 0.14048337936401367,
      "learning_rate": 0.00012897526990705027,
      "loss": 0.2622,
      "step": 164800
    },
    {
      "epoch": 0.5704283574499881,
      "grad_norm": 0.0016484096413478255,
      "learning_rate": 0.00012887149276500357,
      "loss": 0.3198,
      "step": 164900
    },
    {
      "epoch": 0.5707742812568104,
      "grad_norm": 5.046945571899414,
      "learning_rate": 0.00012876771562295687,
      "loss": 0.248,
      "step": 165000
    },
    {
      "epoch": 0.5711202050636327,
      "grad_norm": 46.56935119628906,
      "learning_rate": 0.00012866393848091017,
      "loss": 0.2981,
      "step": 165100
    },
    {
      "epoch": 0.571466128870455,
      "grad_norm": 0.00795733742415905,
      "learning_rate": 0.0001285601613388635,
      "loss": 0.2384,
      "step": 165200
    },
    {
      "epoch": 0.5718120526772773,
      "grad_norm": 0.021592000499367714,
      "learning_rate": 0.0001284563841968168,
      "loss": 0.1999,
      "step": 165300
    },
    {
      "epoch": 0.5721579764840996,
      "grad_norm": 0.005461025983095169,
      "learning_rate": 0.00012835260705477012,
      "loss": 0.2296,
      "step": 165400
    },
    {
      "epoch": 0.5725039002909219,
      "grad_norm": 11.31456184387207,
      "learning_rate": 0.00012824882991272342,
      "loss": 0.2225,
      "step": 165500
    },
    {
      "epoch": 0.5728498240977442,
      "grad_norm": 0.6540834903717041,
      "learning_rate": 0.00012814505277067672,
      "loss": 0.2507,
      "step": 165600
    },
    {
      "epoch": 0.5731957479045665,
      "grad_norm": 0.0017545681912451982,
      "learning_rate": 0.00012804127562863002,
      "loss": 0.2824,
      "step": 165700
    },
    {
      "epoch": 0.5735416717113888,
      "grad_norm": 0.001792030525393784,
      "learning_rate": 0.00012793749848658332,
      "loss": 0.2778,
      "step": 165800
    },
    {
      "epoch": 0.5738875955182111,
      "grad_norm": 0.0005496841622516513,
      "learning_rate": 0.00012783372134453664,
      "loss": 0.3241,
      "step": 165900
    },
    {
      "epoch": 0.5742335193250334,
      "grad_norm": 0.005104213021695614,
      "learning_rate": 0.00012772994420248994,
      "loss": 0.1836,
      "step": 166000
    },
    {
      "epoch": 0.5745794431318558,
      "grad_norm": 2.0919606685638428,
      "learning_rate": 0.00012762616706044327,
      "loss": 0.2068,
      "step": 166100
    },
    {
      "epoch": 0.5749253669386781,
      "grad_norm": 0.004206869751214981,
      "learning_rate": 0.00012752238991839656,
      "loss": 0.2792,
      "step": 166200
    },
    {
      "epoch": 0.5752712907455004,
      "grad_norm": 0.0001206733868457377,
      "learning_rate": 0.00012741861277634986,
      "loss": 0.378,
      "step": 166300
    },
    {
      "epoch": 0.5756172145523227,
      "grad_norm": 0.001294909743592143,
      "learning_rate": 0.00012731483563430316,
      "loss": 0.2772,
      "step": 166400
    },
    {
      "epoch": 0.575963138359145,
      "grad_norm": 0.0013406443176791072,
      "learning_rate": 0.0001272110584922565,
      "loss": 0.2016,
      "step": 166500
    },
    {
      "epoch": 0.5763090621659673,
      "grad_norm": 0.23839114606380463,
      "learning_rate": 0.0001271072813502098,
      "loss": 0.3598,
      "step": 166600
    },
    {
      "epoch": 0.5766549859727896,
      "grad_norm": 0.013683510012924671,
      "learning_rate": 0.00012700350420816309,
      "loss": 0.5027,
      "step": 166700
    },
    {
      "epoch": 0.5770009097796119,
      "grad_norm": 18.0090389251709,
      "learning_rate": 0.0001268997270661164,
      "loss": 0.2734,
      "step": 166800
    },
    {
      "epoch": 0.5773468335864342,
      "grad_norm": 0.011444014497101307,
      "learning_rate": 0.0001267959499240697,
      "loss": 0.2296,
      "step": 166900
    },
    {
      "epoch": 0.5776927573932565,
      "grad_norm": 97.0057373046875,
      "learning_rate": 0.00012669217278202304,
      "loss": 0.3671,
      "step": 167000
    },
    {
      "epoch": 0.5780386812000788,
      "grad_norm": 0.010669303126633167,
      "learning_rate": 0.0001265883956399763,
      "loss": 0.1086,
      "step": 167100
    },
    {
      "epoch": 0.5783846050069011,
      "grad_norm": 0.0010106759145855904,
      "learning_rate": 0.00012648461849792963,
      "loss": 0.1963,
      "step": 167200
    },
    {
      "epoch": 0.5787305288137234,
      "grad_norm": 0.00198902259580791,
      "learning_rate": 0.00012638084135588293,
      "loss": 0.3203,
      "step": 167300
    },
    {
      "epoch": 0.5790764526205457,
      "grad_norm": 0.00027938649873249233,
      "learning_rate": 0.00012627706421383626,
      "loss": 0.2316,
      "step": 167400
    },
    {
      "epoch": 0.5794223764273682,
      "grad_norm": 0.00038774547283537686,
      "learning_rate": 0.00012617328707178956,
      "loss": 0.3443,
      "step": 167500
    },
    {
      "epoch": 0.5797683002341905,
      "grad_norm": 0.0003858701093122363,
      "learning_rate": 0.00012606950992974286,
      "loss": 0.2928,
      "step": 167600
    },
    {
      "epoch": 0.5801142240410128,
      "grad_norm": 0.0022097742184996605,
      "learning_rate": 0.00012596573278769618,
      "loss": 0.1683,
      "step": 167700
    },
    {
      "epoch": 0.5804601478478351,
      "grad_norm": 0.002062710002064705,
      "learning_rate": 0.00012586195564564948,
      "loss": 0.4054,
      "step": 167800
    },
    {
      "epoch": 0.5808060716546574,
      "grad_norm": 0.004216080065816641,
      "learning_rate": 0.00012575817850360278,
      "loss": 0.5719,
      "step": 167900
    },
    {
      "epoch": 0.5811519954614797,
      "grad_norm": 45.557273864746094,
      "learning_rate": 0.00012565440136155608,
      "loss": 0.3472,
      "step": 168000
    },
    {
      "epoch": 0.581497919268302,
      "grad_norm": 1.2012442350387573,
      "learning_rate": 0.0001255506242195094,
      "loss": 0.2752,
      "step": 168100
    },
    {
      "epoch": 0.5818438430751243,
      "grad_norm": 0.0020509432069957256,
      "learning_rate": 0.0001254468470774627,
      "loss": 0.2356,
      "step": 168200
    },
    {
      "epoch": 0.5821897668819466,
      "grad_norm": 0.0027805257122963667,
      "learning_rate": 0.00012534306993541603,
      "loss": 0.2272,
      "step": 168300
    },
    {
      "epoch": 0.5825356906887689,
      "grad_norm": 0.08739843219518661,
      "learning_rate": 0.00012523929279336933,
      "loss": 0.2077,
      "step": 168400
    },
    {
      "epoch": 0.5828816144955912,
      "grad_norm": 0.007839750498533249,
      "learning_rate": 0.00012513551565132263,
      "loss": 0.295,
      "step": 168500
    },
    {
      "epoch": 0.5832275383024135,
      "grad_norm": 0.00046560115879401565,
      "learning_rate": 0.00012503173850927593,
      "loss": 0.3133,
      "step": 168600
    },
    {
      "epoch": 0.5835734621092358,
      "grad_norm": 0.002791594248265028,
      "learning_rate": 0.00012492796136722925,
      "loss": 0.1419,
      "step": 168700
    },
    {
      "epoch": 0.5839193859160581,
      "grad_norm": 0.007975501008331776,
      "learning_rate": 0.00012482418422518255,
      "loss": 0.2204,
      "step": 168800
    },
    {
      "epoch": 0.5842653097228805,
      "grad_norm": 0.0011549802729859948,
      "learning_rate": 0.00012472040708313585,
      "loss": 0.2088,
      "step": 168900
    },
    {
      "epoch": 0.5846112335297028,
      "grad_norm": 0.02167079970240593,
      "learning_rate": 0.00012461662994108917,
      "loss": 0.3561,
      "step": 169000
    },
    {
      "epoch": 0.5849571573365251,
      "grad_norm": 0.000193635409232229,
      "learning_rate": 0.00012451285279904247,
      "loss": 0.2514,
      "step": 169100
    },
    {
      "epoch": 0.5853030811433474,
      "grad_norm": 1.3588614463806152,
      "learning_rate": 0.00012440907565699577,
      "loss": 0.3248,
      "step": 169200
    },
    {
      "epoch": 0.5856490049501697,
      "grad_norm": 0.0002704195794649422,
      "learning_rate": 0.00012430529851494907,
      "loss": 0.5528,
      "step": 169300
    },
    {
      "epoch": 0.585994928756992,
      "grad_norm": 0.004640232305973768,
      "learning_rate": 0.0001242015213729024,
      "loss": 0.2496,
      "step": 169400
    },
    {
      "epoch": 0.5863408525638143,
      "grad_norm": 0.009324043057858944,
      "learning_rate": 0.0001240977442308557,
      "loss": 0.292,
      "step": 169500
    },
    {
      "epoch": 0.5866867763706366,
      "grad_norm": 0.003196007339283824,
      "learning_rate": 0.00012399396708880902,
      "loss": 0.3377,
      "step": 169600
    },
    {
      "epoch": 0.5870327001774589,
      "grad_norm": 0.009468129836022854,
      "learning_rate": 0.00012389018994676232,
      "loss": 0.249,
      "step": 169700
    },
    {
      "epoch": 0.5873786239842812,
      "grad_norm": 0.41879695653915405,
      "learning_rate": 0.00012378641280471562,
      "loss": 0.3418,
      "step": 169800
    },
    {
      "epoch": 0.5877245477911035,
      "grad_norm": 60.727806091308594,
      "learning_rate": 0.00012368263566266892,
      "loss": 0.2129,
      "step": 169900
    },
    {
      "epoch": 0.5880704715979258,
      "grad_norm": 45.933189392089844,
      "learning_rate": 0.00012357885852062224,
      "loss": 0.3179,
      "step": 170000
    },
    {
      "epoch": 0.5884163954047481,
      "grad_norm": 0.011540076695382595,
      "learning_rate": 0.00012347508137857554,
      "loss": 0.4781,
      "step": 170100
    },
    {
      "epoch": 0.5887623192115704,
      "grad_norm": 0.00013684839359484613,
      "learning_rate": 0.00012337130423652884,
      "loss": 0.2492,
      "step": 170200
    },
    {
      "epoch": 0.5891082430183928,
      "grad_norm": 0.0022483314387500286,
      "learning_rate": 0.00012326752709448217,
      "loss": 0.1134,
      "step": 170300
    },
    {
      "epoch": 0.5894541668252151,
      "grad_norm": 74.55641174316406,
      "learning_rate": 0.00012316374995243547,
      "loss": 0.3393,
      "step": 170400
    },
    {
      "epoch": 0.5898000906320374,
      "grad_norm": 9.66327953338623,
      "learning_rate": 0.0001230599728103888,
      "loss": 0.2492,
      "step": 170500
    },
    {
      "epoch": 0.5901460144388597,
      "grad_norm": 0.0009409365593455732,
      "learning_rate": 0.00012295619566834206,
      "loss": 0.3255,
      "step": 170600
    },
    {
      "epoch": 0.590491938245682,
      "grad_norm": 0.001497482182458043,
      "learning_rate": 0.0001228524185262954,
      "loss": 0.2693,
      "step": 170700
    },
    {
      "epoch": 0.5908378620525043,
      "grad_norm": 0.0003943145275115967,
      "learning_rate": 0.0001227486413842487,
      "loss": 0.1859,
      "step": 170800
    },
    {
      "epoch": 0.5911837858593266,
      "grad_norm": 26.408615112304688,
      "learning_rate": 0.00012264486424220201,
      "loss": 0.3748,
      "step": 170900
    },
    {
      "epoch": 0.5915297096661489,
      "grad_norm": 0.0038273183163255453,
      "learning_rate": 0.0001225410871001553,
      "loss": 0.3863,
      "step": 171000
    },
    {
      "epoch": 0.5918756334729712,
      "grad_norm": 0.0003347220772411674,
      "learning_rate": 0.0001224373099581086,
      "loss": 0.3916,
      "step": 171100
    },
    {
      "epoch": 0.5922215572797935,
      "grad_norm": 0.027995437383651733,
      "learning_rate": 0.00012233353281606194,
      "loss": 0.3688,
      "step": 171200
    },
    {
      "epoch": 0.5925674810866158,
      "grad_norm": 83.1059799194336,
      "learning_rate": 0.00012222975567401524,
      "loss": 0.3127,
      "step": 171300
    },
    {
      "epoch": 0.5929134048934381,
      "grad_norm": 7.270205969689414e-05,
      "learning_rate": 0.00012212597853196854,
      "loss": 0.3791,
      "step": 171400
    },
    {
      "epoch": 0.5932593287002604,
      "grad_norm": 0.03849361836910248,
      "learning_rate": 0.00012202220138992185,
      "loss": 0.2594,
      "step": 171500
    },
    {
      "epoch": 0.5936052525070827,
      "grad_norm": 34.612728118896484,
      "learning_rate": 0.00012191842424787515,
      "loss": 0.175,
      "step": 171600
    },
    {
      "epoch": 0.5939511763139051,
      "grad_norm": 0.00033933279337361455,
      "learning_rate": 0.00012181464710582846,
      "loss": 0.339,
      "step": 171700
    },
    {
      "epoch": 0.5942971001207275,
      "grad_norm": 0.010495650582015514,
      "learning_rate": 0.00012171086996378177,
      "loss": 0.2045,
      "step": 171800
    },
    {
      "epoch": 0.5946430239275498,
      "grad_norm": 0.0009138504974544048,
      "learning_rate": 0.00012160709282173508,
      "loss": 0.2413,
      "step": 171900
    },
    {
      "epoch": 0.594988947734372,
      "grad_norm": 0.00023522468109149486,
      "learning_rate": 0.00012150331567968837,
      "loss": 0.301,
      "step": 172000
    },
    {
      "epoch": 0.5953348715411944,
      "grad_norm": 35.89408874511719,
      "learning_rate": 0.00012139953853764168,
      "loss": 0.2788,
      "step": 172100
    },
    {
      "epoch": 0.5956807953480167,
      "grad_norm": 14.170454025268555,
      "learning_rate": 0.00012129576139559499,
      "loss": 0.2971,
      "step": 172200
    },
    {
      "epoch": 0.596026719154839,
      "grad_norm": 0.0019686927553266287,
      "learning_rate": 0.0001211919842535483,
      "loss": 0.4003,
      "step": 172300
    },
    {
      "epoch": 0.5963726429616613,
      "grad_norm": 2.6430413722991943,
      "learning_rate": 0.00012108820711150162,
      "loss": 0.4367,
      "step": 172400
    },
    {
      "epoch": 0.5967185667684836,
      "grad_norm": 0.0024546661879867315,
      "learning_rate": 0.00012098442996945492,
      "loss": 0.1826,
      "step": 172500
    },
    {
      "epoch": 0.5970644905753059,
      "grad_norm": 0.00019052127026952803,
      "learning_rate": 0.00012088065282740823,
      "loss": 0.2888,
      "step": 172600
    },
    {
      "epoch": 0.5974104143821282,
      "grad_norm": 0.10595925897359848,
      "learning_rate": 0.00012077687568536153,
      "loss": 0.3463,
      "step": 172700
    },
    {
      "epoch": 0.5977563381889505,
      "grad_norm": 0.00016474838776048273,
      "learning_rate": 0.00012067309854331484,
      "loss": 0.3362,
      "step": 172800
    },
    {
      "epoch": 0.5981022619957728,
      "grad_norm": 0.0016336283879354596,
      "learning_rate": 0.00012056932140126814,
      "loss": 0.3371,
      "step": 172900
    },
    {
      "epoch": 0.5984481858025951,
      "grad_norm": 8.000924572115764e-05,
      "learning_rate": 0.00012046554425922145,
      "loss": 0.2474,
      "step": 173000
    },
    {
      "epoch": 0.5987941096094175,
      "grad_norm": 0.0012168391840532422,
      "learning_rate": 0.00012036176711717476,
      "loss": 0.2428,
      "step": 173100
    },
    {
      "epoch": 0.5991400334162398,
      "grad_norm": 0.0538577139377594,
      "learning_rate": 0.00012025798997512808,
      "loss": 0.1496,
      "step": 173200
    },
    {
      "epoch": 0.5994859572230621,
      "grad_norm": 14.108614921569824,
      "learning_rate": 0.00012015421283308139,
      "loss": 0.2346,
      "step": 173300
    },
    {
      "epoch": 0.5998318810298844,
      "grad_norm": 53.00269317626953,
      "learning_rate": 0.00012005043569103467,
      "loss": 0.2636,
      "step": 173400
    },
    {
      "epoch": 0.6001778048367067,
      "grad_norm": 18.526283264160156,
      "learning_rate": 0.00011994665854898799,
      "loss": 0.1829,
      "step": 173500
    },
    {
      "epoch": 0.600523728643529,
      "grad_norm": 0.0003499826998449862,
      "learning_rate": 0.0001198428814069413,
      "loss": 0.2378,
      "step": 173600
    },
    {
      "epoch": 0.6008696524503513,
      "grad_norm": 0.04546476900577545,
      "learning_rate": 0.0001197391042648946,
      "loss": 0.4221,
      "step": 173700
    },
    {
      "epoch": 0.6012155762571736,
      "grad_norm": 0.0002811495796777308,
      "learning_rate": 0.00011963532712284791,
      "loss": 0.4325,
      "step": 173800
    },
    {
      "epoch": 0.6015615000639959,
      "grad_norm": 9.725458145141602,
      "learning_rate": 0.00011953154998080122,
      "loss": 0.2142,
      "step": 173900
    },
    {
      "epoch": 0.6019074238708182,
      "grad_norm": 4.2269392013549805,
      "learning_rate": 0.00011942777283875453,
      "loss": 0.3122,
      "step": 174000
    },
    {
      "epoch": 0.6022533476776405,
      "grad_norm": 0.0026132601778954268,
      "learning_rate": 0.00011932399569670782,
      "loss": 0.3037,
      "step": 174100
    },
    {
      "epoch": 0.6025992714844628,
      "grad_norm": 0.0014545454178005457,
      "learning_rate": 0.00011922021855466113,
      "loss": 0.1914,
      "step": 174200
    },
    {
      "epoch": 0.6029451952912851,
      "grad_norm": 0.4411905109882355,
      "learning_rate": 0.00011911644141261444,
      "loss": 0.2807,
      "step": 174300
    },
    {
      "epoch": 0.6032911190981074,
      "grad_norm": 0.0002653718984220177,
      "learning_rate": 0.00011901266427056776,
      "loss": 0.1615,
      "step": 174400
    },
    {
      "epoch": 0.6036370429049298,
      "grad_norm": 4.3684667616616935e-05,
      "learning_rate": 0.00011890888712852107,
      "loss": 0.3851,
      "step": 174500
    },
    {
      "epoch": 0.6039829667117521,
      "grad_norm": 0.009565428830683231,
      "learning_rate": 0.00011880510998647437,
      "loss": 0.274,
      "step": 174600
    },
    {
      "epoch": 0.6043288905185744,
      "grad_norm": 0.000691099266987294,
      "learning_rate": 0.00011870133284442768,
      "loss": 0.2914,
      "step": 174700
    },
    {
      "epoch": 0.6046748143253967,
      "grad_norm": 0.008216437883675098,
      "learning_rate": 0.00011859755570238098,
      "loss": 0.2909,
      "step": 174800
    },
    {
      "epoch": 0.605020738132219,
      "grad_norm": 2.0888006687164307,
      "learning_rate": 0.00011849377856033429,
      "loss": 0.3255,
      "step": 174900
    },
    {
      "epoch": 0.6053666619390413,
      "grad_norm": 0.07404082268476486,
      "learning_rate": 0.00011839000141828759,
      "loss": 0.3571,
      "step": 175000
    },
    {
      "epoch": 0.6057125857458636,
      "grad_norm": 0.0005670237587764859,
      "learning_rate": 0.0001182862242762409,
      "loss": 0.2282,
      "step": 175100
    },
    {
      "epoch": 0.6060585095526859,
      "grad_norm": 0.015239693224430084,
      "learning_rate": 0.00011818244713419421,
      "loss": 0.3334,
      "step": 175200
    },
    {
      "epoch": 0.6064044333595082,
      "grad_norm": 0.0027129012160003185,
      "learning_rate": 0.00011807866999214753,
      "loss": 0.2935,
      "step": 175300
    },
    {
      "epoch": 0.6067503571663305,
      "grad_norm": 3.493429539958015e-05,
      "learning_rate": 0.00011797489285010084,
      "loss": 0.4125,
      "step": 175400
    },
    {
      "epoch": 0.6070962809731528,
      "grad_norm": 0.0011645887279883027,
      "learning_rate": 0.00011787111570805412,
      "loss": 0.1764,
      "step": 175500
    },
    {
      "epoch": 0.6074422047799751,
      "grad_norm": 0.0015666238032281399,
      "learning_rate": 0.00011776733856600744,
      "loss": 0.3301,
      "step": 175600
    },
    {
      "epoch": 0.6077881285867974,
      "grad_norm": 0.0004757725109811872,
      "learning_rate": 0.00011766356142396075,
      "loss": 0.2525,
      "step": 175700
    },
    {
      "epoch": 0.6081340523936197,
      "grad_norm": 0.00022023073688615113,
      "learning_rate": 0.00011755978428191406,
      "loss": 0.4765,
      "step": 175800
    },
    {
      "epoch": 0.6084799762004421,
      "grad_norm": 24.6500301361084,
      "learning_rate": 0.00011745600713986736,
      "loss": 0.1972,
      "step": 175900
    },
    {
      "epoch": 0.6088259000072644,
      "grad_norm": 0.00016785449406597763,
      "learning_rate": 0.00011735222999782067,
      "loss": 0.3942,
      "step": 176000
    },
    {
      "epoch": 0.6091718238140867,
      "grad_norm": 0.024079572409391403,
      "learning_rate": 0.00011724845285577398,
      "loss": 0.2336,
      "step": 176100
    },
    {
      "epoch": 0.609517747620909,
      "grad_norm": 20.257034301757812,
      "learning_rate": 0.00011714467571372727,
      "loss": 0.2998,
      "step": 176200
    },
    {
      "epoch": 0.6098636714277313,
      "grad_norm": 8.872201919555664,
      "learning_rate": 0.00011704089857168058,
      "loss": 0.4018,
      "step": 176300
    },
    {
      "epoch": 0.6102095952345536,
      "grad_norm": 0.003245857311412692,
      "learning_rate": 0.0001169371214296339,
      "loss": 0.3185,
      "step": 176400
    },
    {
      "epoch": 0.610555519041376,
      "grad_norm": 0.0026013001333922148,
      "learning_rate": 0.0001168333442875872,
      "loss": 0.2596,
      "step": 176500
    },
    {
      "epoch": 0.6109014428481983,
      "grad_norm": 0.00043291569454595447,
      "learning_rate": 0.00011672956714554052,
      "loss": 0.2124,
      "step": 176600
    },
    {
      "epoch": 0.6112473666550206,
      "grad_norm": 46.942317962646484,
      "learning_rate": 0.00011662579000349382,
      "loss": 0.2962,
      "step": 176700
    },
    {
      "epoch": 0.6115932904618429,
      "grad_norm": 18.729827880859375,
      "learning_rate": 0.00011652201286144713,
      "loss": 0.2583,
      "step": 176800
    },
    {
      "epoch": 0.6119392142686652,
      "grad_norm": 0.008063821122050285,
      "learning_rate": 0.00011641823571940043,
      "loss": 0.3521,
      "step": 176900
    },
    {
      "epoch": 0.6122851380754875,
      "grad_norm": 11.001562118530273,
      "learning_rate": 0.00011631445857735374,
      "loss": 0.2382,
      "step": 177000
    },
    {
      "epoch": 0.6126310618823098,
      "grad_norm": 0.0027972606476396322,
      "learning_rate": 0.00011621068143530704,
      "loss": 0.2163,
      "step": 177100
    },
    {
      "epoch": 0.6129769856891321,
      "grad_norm": 0.3143710196018219,
      "learning_rate": 0.00011610690429326035,
      "loss": 0.2087,
      "step": 177200
    },
    {
      "epoch": 0.6133229094959545,
      "grad_norm": 0.00028061409830115736,
      "learning_rate": 0.00011600312715121366,
      "loss": 0.2243,
      "step": 177300
    },
    {
      "epoch": 0.6136688333027768,
      "grad_norm": 0.0013279120903462172,
      "learning_rate": 0.00011589935000916698,
      "loss": 0.431,
      "step": 177400
    },
    {
      "epoch": 0.6140147571095991,
      "grad_norm": 0.07477908581495285,
      "learning_rate": 0.00011579557286712029,
      "loss": 0.1561,
      "step": 177500
    },
    {
      "epoch": 0.6143606809164214,
      "grad_norm": 106.82190704345703,
      "learning_rate": 0.00011569179572507357,
      "loss": 0.1335,
      "step": 177600
    },
    {
      "epoch": 0.6147066047232437,
      "grad_norm": 0.0012568725505843759,
      "learning_rate": 0.00011558801858302689,
      "loss": 0.2725,
      "step": 177700
    },
    {
      "epoch": 0.615052528530066,
      "grad_norm": 0.0019698089454323053,
      "learning_rate": 0.0001154842414409802,
      "loss": 0.2604,
      "step": 177800
    },
    {
      "epoch": 0.6153984523368883,
      "grad_norm": 0.002873222343623638,
      "learning_rate": 0.00011538046429893351,
      "loss": 0.2343,
      "step": 177900
    },
    {
      "epoch": 0.6157443761437106,
      "grad_norm": 0.008029063232243061,
      "learning_rate": 0.00011527668715688681,
      "loss": 0.4091,
      "step": 178000
    },
    {
      "epoch": 0.6160902999505329,
      "grad_norm": 0.0006414022645913064,
      "learning_rate": 0.00011517291001484012,
      "loss": 0.158,
      "step": 178100
    },
    {
      "epoch": 0.6164362237573552,
      "grad_norm": 0.000786133052315563,
      "learning_rate": 0.00011506913287279344,
      "loss": 0.2159,
      "step": 178200
    },
    {
      "epoch": 0.6167821475641775,
      "grad_norm": 0.009537190198898315,
      "learning_rate": 0.00011496535573074673,
      "loss": 0.4113,
      "step": 178300
    },
    {
      "epoch": 0.6171280713709998,
      "grad_norm": 4.311567783355713,
      "learning_rate": 0.00011486157858870003,
      "loss": 0.2046,
      "step": 178400
    },
    {
      "epoch": 0.6174739951778221,
      "grad_norm": 0.014966387301683426,
      "learning_rate": 0.00011475780144665334,
      "loss": 0.2504,
      "step": 178500
    },
    {
      "epoch": 0.6178199189846444,
      "grad_norm": 0.3966809809207916,
      "learning_rate": 0.00011465402430460666,
      "loss": 0.288,
      "step": 178600
    },
    {
      "epoch": 0.6181658427914668,
      "grad_norm": 0.0010435008443892002,
      "learning_rate": 0.00011455024716255997,
      "loss": 0.2598,
      "step": 178700
    },
    {
      "epoch": 0.6185117665982891,
      "grad_norm": 0.0016301078721880913,
      "learning_rate": 0.00011444647002051328,
      "loss": 0.372,
      "step": 178800
    },
    {
      "epoch": 0.6188576904051114,
      "grad_norm": 0.0016313630621880293,
      "learning_rate": 0.00011434269287846658,
      "loss": 0.3795,
      "step": 178900
    },
    {
      "epoch": 0.6192036142119337,
      "grad_norm": 1.053320050239563,
      "learning_rate": 0.00011423891573641988,
      "loss": 0.1971,
      "step": 179000
    },
    {
      "epoch": 0.619549538018756,
      "grad_norm": 0.012612923048436642,
      "learning_rate": 0.00011413513859437319,
      "loss": 0.2809,
      "step": 179100
    },
    {
      "epoch": 0.6198954618255783,
      "grad_norm": 2.1002144813537598,
      "learning_rate": 0.00011403136145232649,
      "loss": 0.3155,
      "step": 179200
    },
    {
      "epoch": 0.6202413856324006,
      "grad_norm": 0.0025052481796592474,
      "learning_rate": 0.0001139275843102798,
      "loss": 0.151,
      "step": 179300
    },
    {
      "epoch": 0.6205873094392229,
      "grad_norm": 0.04892076179385185,
      "learning_rate": 0.00011382380716823312,
      "loss": 0.1675,
      "step": 179400
    },
    {
      "epoch": 0.6209332332460452,
      "grad_norm": 0.06911326944828033,
      "learning_rate": 0.00011372003002618643,
      "loss": 0.2233,
      "step": 179500
    },
    {
      "epoch": 0.6212791570528675,
      "grad_norm": 11.007966041564941,
      "learning_rate": 0.00011361625288413974,
      "loss": 0.3326,
      "step": 179600
    },
    {
      "epoch": 0.6216250808596898,
      "grad_norm": 0.0011299537727609277,
      "learning_rate": 0.00011351247574209303,
      "loss": 0.3733,
      "step": 179700
    },
    {
      "epoch": 0.6219710046665121,
      "grad_norm": 0.0011720459442585707,
      "learning_rate": 0.00011340869860004634,
      "loss": 0.3531,
      "step": 179800
    },
    {
      "epoch": 0.6223169284733344,
      "grad_norm": 0.009388716891407967,
      "learning_rate": 0.00011330492145799965,
      "loss": 0.1922,
      "step": 179900
    },
    {
      "epoch": 0.6226628522801567,
      "grad_norm": 3.822603464126587,
      "learning_rate": 0.00011320114431595296,
      "loss": 0.2379,
      "step": 180000
    },
    {
      "epoch": 0.6230087760869791,
      "grad_norm": 0.0018434373196214437,
      "learning_rate": 0.00011309736717390626,
      "loss": 0.443,
      "step": 180100
    },
    {
      "epoch": 0.6233546998938014,
      "grad_norm": 15.331565856933594,
      "learning_rate": 0.00011299359003185957,
      "loss": 0.4738,
      "step": 180200
    },
    {
      "epoch": 0.6237006237006237,
      "grad_norm": 0.007320162374526262,
      "learning_rate": 0.00011288981288981289,
      "loss": 0.267,
      "step": 180300
    },
    {
      "epoch": 0.624046547507446,
      "grad_norm": 0.0003992931451648474,
      "learning_rate": 0.00011278603574776618,
      "loss": 0.2604,
      "step": 180400
    },
    {
      "epoch": 0.6243924713142683,
      "grad_norm": 0.002271453384310007,
      "learning_rate": 0.00011268225860571948,
      "loss": 0.334,
      "step": 180500
    },
    {
      "epoch": 0.6247383951210906,
      "grad_norm": 0.005236652214080095,
      "learning_rate": 0.0001125784814636728,
      "loss": 0.263,
      "step": 180600
    },
    {
      "epoch": 0.6250843189279129,
      "grad_norm": 3.8966734409332275,
      "learning_rate": 0.00011247470432162611,
      "loss": 0.306,
      "step": 180700
    },
    {
      "epoch": 0.6254302427347352,
      "grad_norm": 0.0003689589793793857,
      "learning_rate": 0.00011237092717957942,
      "loss": 0.2831,
      "step": 180800
    },
    {
      "epoch": 0.6257761665415575,
      "grad_norm": 0.0017447867430746555,
      "learning_rate": 0.00011226715003753273,
      "loss": 0.3816,
      "step": 180900
    },
    {
      "epoch": 0.6261220903483798,
      "grad_norm": 0.07064596563577652,
      "learning_rate": 0.00011216337289548603,
      "loss": 0.3274,
      "step": 181000
    },
    {
      "epoch": 0.6264680141552021,
      "grad_norm": 19.5748233795166,
      "learning_rate": 0.00011205959575343933,
      "loss": 0.3137,
      "step": 181100
    },
    {
      "epoch": 0.6268139379620244,
      "grad_norm": 0.09351344406604767,
      "learning_rate": 0.00011195581861139264,
      "loss": 0.2801,
      "step": 181200
    },
    {
      "epoch": 0.6271598617688467,
      "grad_norm": 0.003525499487295747,
      "learning_rate": 0.00011185204146934595,
      "loss": 0.3577,
      "step": 181300
    },
    {
      "epoch": 0.627505785575669,
      "grad_norm": 6.673649311065674,
      "learning_rate": 0.00011174826432729925,
      "loss": 0.389,
      "step": 181400
    },
    {
      "epoch": 0.6278517093824915,
      "grad_norm": 0.06564301997423172,
      "learning_rate": 0.00011164448718525257,
      "loss": 0.2122,
      "step": 181500
    },
    {
      "epoch": 0.6281976331893138,
      "grad_norm": 35.204864501953125,
      "learning_rate": 0.00011154071004320588,
      "loss": 0.2853,
      "step": 181600
    },
    {
      "epoch": 0.6285435569961361,
      "grad_norm": 0.004922586493194103,
      "learning_rate": 0.00011143693290115919,
      "loss": 0.3212,
      "step": 181700
    },
    {
      "epoch": 0.6288894808029584,
      "grad_norm": 0.0055463542230427265,
      "learning_rate": 0.00011133315575911248,
      "loss": 0.2744,
      "step": 181800
    },
    {
      "epoch": 0.6292354046097807,
      "grad_norm": 0.0024671207647770643,
      "learning_rate": 0.00011122937861706579,
      "loss": 0.1659,
      "step": 181900
    },
    {
      "epoch": 0.629581328416603,
      "grad_norm": 0.0008297578897327185,
      "learning_rate": 0.0001111256014750191,
      "loss": 0.197,
      "step": 182000
    },
    {
      "epoch": 0.6299272522234253,
      "grad_norm": 0.005977631080895662,
      "learning_rate": 0.00011102182433297241,
      "loss": 0.1849,
      "step": 182100
    },
    {
      "epoch": 0.6302731760302476,
      "grad_norm": 0.007310586050152779,
      "learning_rate": 0.00011091804719092573,
      "loss": 0.1859,
      "step": 182200
    },
    {
      "epoch": 0.6306190998370699,
      "grad_norm": 0.0036195488646626472,
      "learning_rate": 0.00011081427004887902,
      "loss": 0.2655,
      "step": 182300
    },
    {
      "epoch": 0.6309650236438922,
      "grad_norm": 0.04317035898566246,
      "learning_rate": 0.00011071049290683234,
      "loss": 0.3237,
      "step": 182400
    },
    {
      "epoch": 0.6313109474507145,
      "grad_norm": 96.15696716308594,
      "learning_rate": 0.00011060671576478564,
      "loss": 0.3443,
      "step": 182500
    },
    {
      "epoch": 0.6316568712575368,
      "grad_norm": 0.0596427246928215,
      "learning_rate": 0.00011050293862273893,
      "loss": 0.2656,
      "step": 182600
    },
    {
      "epoch": 0.6320027950643591,
      "grad_norm": 0.09703025221824646,
      "learning_rate": 0.00011039916148069225,
      "loss": 0.4308,
      "step": 182700
    },
    {
      "epoch": 0.6323487188711814,
      "grad_norm": 0.0012256185291334987,
      "learning_rate": 0.00011029538433864556,
      "loss": 0.2246,
      "step": 182800
    },
    {
      "epoch": 0.6326946426780038,
      "grad_norm": 0.029800737276673317,
      "learning_rate": 0.00011019160719659887,
      "loss": 0.2826,
      "step": 182900
    },
    {
      "epoch": 0.6330405664848261,
      "grad_norm": 0.0001748742797644809,
      "learning_rate": 0.00011008783005455218,
      "loss": 0.3954,
      "step": 183000
    },
    {
      "epoch": 0.6333864902916484,
      "grad_norm": 0.00011013435869244859,
      "learning_rate": 0.00010998405291250548,
      "loss": 0.212,
      "step": 183100
    },
    {
      "epoch": 0.6337324140984707,
      "grad_norm": 0.00044066080590710044,
      "learning_rate": 0.00010988027577045878,
      "loss": 0.1963,
      "step": 183200
    },
    {
      "epoch": 0.634078337905293,
      "grad_norm": 0.00011046044528484344,
      "learning_rate": 0.00010977649862841209,
      "loss": 0.2348,
      "step": 183300
    },
    {
      "epoch": 0.6344242617121153,
      "grad_norm": 0.002596493111923337,
      "learning_rate": 0.0001096727214863654,
      "loss": 0.2467,
      "step": 183400
    },
    {
      "epoch": 0.6347701855189376,
      "grad_norm": 0.0016530239954590797,
      "learning_rate": 0.0001095689443443187,
      "loss": 0.2198,
      "step": 183500
    },
    {
      "epoch": 0.6351161093257599,
      "grad_norm": 0.0012907061027362943,
      "learning_rate": 0.00010946516720227202,
      "loss": 0.2187,
      "step": 183600
    },
    {
      "epoch": 0.6354620331325822,
      "grad_norm": 0.4471355974674225,
      "learning_rate": 0.00010936139006022533,
      "loss": 0.3508,
      "step": 183700
    },
    {
      "epoch": 0.6358079569394045,
      "grad_norm": 0.08299577236175537,
      "learning_rate": 0.00010925761291817864,
      "loss": 0.1519,
      "step": 183800
    },
    {
      "epoch": 0.6361538807462268,
      "grad_norm": 0.0014279050519689918,
      "learning_rate": 0.00010915383577613193,
      "loss": 0.2181,
      "step": 183900
    },
    {
      "epoch": 0.6364998045530491,
      "grad_norm": 0.004913657903671265,
      "learning_rate": 0.00010905005863408524,
      "loss": 0.3143,
      "step": 184000
    },
    {
      "epoch": 0.6368457283598714,
      "grad_norm": 0.03200367093086243,
      "learning_rate": 0.00010894628149203855,
      "loss": 0.2424,
      "step": 184100
    },
    {
      "epoch": 0.6371916521666937,
      "grad_norm": 0.6800384521484375,
      "learning_rate": 0.00010884250434999186,
      "loss": 0.195,
      "step": 184200
    },
    {
      "epoch": 0.6375375759735161,
      "grad_norm": 0.00039999562432058156,
      "learning_rate": 0.00010873872720794518,
      "loss": 0.312,
      "step": 184300
    },
    {
      "epoch": 0.6378834997803384,
      "grad_norm": 0.002945269225165248,
      "learning_rate": 0.00010863495006589847,
      "loss": 0.3456,
      "step": 184400
    },
    {
      "epoch": 0.6382294235871607,
      "grad_norm": 0.0002660636673681438,
      "learning_rate": 0.00010853117292385179,
      "loss": 0.3019,
      "step": 184500
    },
    {
      "epoch": 0.638575347393983,
      "grad_norm": 0.0022034761495888233,
      "learning_rate": 0.00010842739578180509,
      "loss": 0.3269,
      "step": 184600
    },
    {
      "epoch": 0.6389212712008053,
      "grad_norm": 0.00034329594927839935,
      "learning_rate": 0.0001083236186397584,
      "loss": 0.3211,
      "step": 184700
    },
    {
      "epoch": 0.6392671950076276,
      "grad_norm": 0.004897375125437975,
      "learning_rate": 0.0001082198414977117,
      "loss": 0.1883,
      "step": 184800
    },
    {
      "epoch": 0.6396131188144499,
      "grad_norm": 0.0020221190061420202,
      "learning_rate": 0.00010811606435566501,
      "loss": 0.399,
      "step": 184900
    },
    {
      "epoch": 0.6399590426212722,
      "grad_norm": 23.343246459960938,
      "learning_rate": 0.00010801228721361832,
      "loss": 0.2573,
      "step": 185000
    },
    {
      "epoch": 0.6403049664280945,
      "grad_norm": 0.03453563526272774,
      "learning_rate": 0.00010790851007157163,
      "loss": 0.2549,
      "step": 185100
    },
    {
      "epoch": 0.6406508902349168,
      "grad_norm": 0.642883837223053,
      "learning_rate": 0.00010780473292952495,
      "loss": 0.3058,
      "step": 185200
    },
    {
      "epoch": 0.6409968140417391,
      "grad_norm": 0.044065285474061966,
      "learning_rate": 0.00010770095578747823,
      "loss": 0.1808,
      "step": 185300
    },
    {
      "epoch": 0.6413427378485614,
      "grad_norm": 6.381266575772315e-05,
      "learning_rate": 0.00010759717864543154,
      "loss": 0.2919,
      "step": 185400
    },
    {
      "epoch": 0.6416886616553837,
      "grad_norm": 0.01391245611011982,
      "learning_rate": 0.00010749340150338486,
      "loss": 0.2622,
      "step": 185500
    },
    {
      "epoch": 0.642034585462206,
      "grad_norm": 0.08007338643074036,
      "learning_rate": 0.00010738962436133815,
      "loss": 0.1946,
      "step": 185600
    },
    {
      "epoch": 0.6423805092690285,
      "grad_norm": 0.0001612502965144813,
      "learning_rate": 0.00010728584721929147,
      "loss": 0.1439,
      "step": 185700
    },
    {
      "epoch": 0.6427264330758508,
      "grad_norm": 0.00018252119480166584,
      "learning_rate": 0.00010718207007724478,
      "loss": 0.19,
      "step": 185800
    },
    {
      "epoch": 0.6430723568826731,
      "grad_norm": 0.001492287265136838,
      "learning_rate": 0.00010707829293519809,
      "loss": 0.1148,
      "step": 185900
    },
    {
      "epoch": 0.6434182806894954,
      "grad_norm": 0.004291076213121414,
      "learning_rate": 0.00010697451579315138,
      "loss": 0.3707,
      "step": 186000
    },
    {
      "epoch": 0.6437642044963177,
      "grad_norm": 39.544376373291016,
      "learning_rate": 0.00010687073865110469,
      "loss": 0.3636,
      "step": 186100
    },
    {
      "epoch": 0.64411012830314,
      "grad_norm": 1.2237943410873413,
      "learning_rate": 0.000106766961509058,
      "loss": 0.3162,
      "step": 186200
    },
    {
      "epoch": 0.6444560521099623,
      "grad_norm": 138.0146484375,
      "learning_rate": 0.00010666318436701131,
      "loss": 0.1096,
      "step": 186300
    },
    {
      "epoch": 0.6448019759167846,
      "grad_norm": 0.10173290967941284,
      "learning_rate": 0.00010655940722496463,
      "loss": 0.3501,
      "step": 186400
    },
    {
      "epoch": 0.6451478997236069,
      "grad_norm": 0.0070671034045517445,
      "learning_rate": 0.00010645563008291793,
      "loss": 0.3171,
      "step": 186500
    },
    {
      "epoch": 0.6454938235304292,
      "grad_norm": 7.300893306732178,
      "learning_rate": 0.00010635185294087124,
      "loss": 0.1563,
      "step": 186600
    },
    {
      "epoch": 0.6458397473372515,
      "grad_norm": 11.23073673248291,
      "learning_rate": 0.00010624807579882454,
      "loss": 0.1866,
      "step": 186700
    },
    {
      "epoch": 0.6461856711440738,
      "grad_norm": 0.00012198719923617318,
      "learning_rate": 0.00010614429865677785,
      "loss": 0.1287,
      "step": 186800
    },
    {
      "epoch": 0.6465315949508961,
      "grad_norm": 31.18277931213379,
      "learning_rate": 0.00010604052151473115,
      "loss": 0.1997,
      "step": 186900
    },
    {
      "epoch": 0.6468775187577184,
      "grad_norm": 0.08728720247745514,
      "learning_rate": 0.00010593674437268446,
      "loss": 0.3297,
      "step": 187000
    },
    {
      "epoch": 0.6472234425645408,
      "grad_norm": 0.020504934713244438,
      "learning_rate": 0.00010583296723063777,
      "loss": 0.1781,
      "step": 187100
    },
    {
      "epoch": 0.6475693663713631,
      "grad_norm": 0.0015097458381205797,
      "learning_rate": 0.00010572919008859108,
      "loss": 0.3013,
      "step": 187200
    },
    {
      "epoch": 0.6479152901781854,
      "grad_norm": 0.01058760192245245,
      "learning_rate": 0.0001056254129465444,
      "loss": 0.2898,
      "step": 187300
    },
    {
      "epoch": 0.6482612139850077,
      "grad_norm": 7.023302168818191e-05,
      "learning_rate": 0.00010552163580449768,
      "loss": 0.2739,
      "step": 187400
    },
    {
      "epoch": 0.64860713779183,
      "grad_norm": 4.718293190002441,
      "learning_rate": 0.000105417858662451,
      "loss": 0.2068,
      "step": 187500
    },
    {
      "epoch": 0.6489530615986523,
      "grad_norm": 0.00041172158671543,
      "learning_rate": 0.0001053140815204043,
      "loss": 0.1794,
      "step": 187600
    },
    {
      "epoch": 0.6492989854054746,
      "grad_norm": 0.0015332583570852876,
      "learning_rate": 0.00010521030437835762,
      "loss": 0.3438,
      "step": 187700
    },
    {
      "epoch": 0.6496449092122969,
      "grad_norm": 0.00016887416131794453,
      "learning_rate": 0.00010510652723631092,
      "loss": 0.2283,
      "step": 187800
    },
    {
      "epoch": 0.6499908330191192,
      "grad_norm": 0.0013072683941572905,
      "learning_rate": 0.00010500275009426423,
      "loss": 0.3638,
      "step": 187900
    },
    {
      "epoch": 0.6503367568259415,
      "grad_norm": 0.0056532141752541065,
      "learning_rate": 0.00010489897295221754,
      "loss": 0.2941,
      "step": 188000
    },
    {
      "epoch": 0.6506826806327638,
      "grad_norm": 0.0009681711089797318,
      "learning_rate": 0.00010479519581017083,
      "loss": 0.4305,
      "step": 188100
    },
    {
      "epoch": 0.6510286044395861,
      "grad_norm": 10.217503547668457,
      "learning_rate": 0.00010469141866812414,
      "loss": 0.1788,
      "step": 188200
    },
    {
      "epoch": 0.6513745282464084,
      "grad_norm": 0.017176441848278046,
      "learning_rate": 0.00010458764152607745,
      "loss": 0.3074,
      "step": 188300
    },
    {
      "epoch": 0.6517204520532307,
      "grad_norm": 0.003049764083698392,
      "learning_rate": 0.00010448386438403076,
      "loss": 0.3719,
      "step": 188400
    },
    {
      "epoch": 0.6520663758600531,
      "grad_norm": 0.0021772931795567274,
      "learning_rate": 0.00010438008724198408,
      "loss": 0.2463,
      "step": 188500
    },
    {
      "epoch": 0.6524122996668754,
      "grad_norm": 0.6451401114463806,
      "learning_rate": 0.00010427631009993738,
      "loss": 0.279,
      "step": 188600
    },
    {
      "epoch": 0.6527582234736977,
      "grad_norm": 0.011281084269285202,
      "learning_rate": 0.00010417253295789069,
      "loss": 0.1787,
      "step": 188700
    },
    {
      "epoch": 0.65310414728052,
      "grad_norm": 0.0001603765122126788,
      "learning_rate": 0.00010406875581584399,
      "loss": 0.1761,
      "step": 188800
    },
    {
      "epoch": 0.6534500710873423,
      "grad_norm": 24.323156356811523,
      "learning_rate": 0.0001039649786737973,
      "loss": 0.2917,
      "step": 188900
    },
    {
      "epoch": 0.6537959948941646,
      "grad_norm": 3.3758928775787354,
      "learning_rate": 0.0001038612015317506,
      "loss": 0.3148,
      "step": 189000
    },
    {
      "epoch": 0.6541419187009869,
      "grad_norm": 0.007563531864434481,
      "learning_rate": 0.00010375742438970391,
      "loss": 0.2916,
      "step": 189100
    },
    {
      "epoch": 0.6544878425078092,
      "grad_norm": 8.783554221736267e-05,
      "learning_rate": 0.00010365364724765722,
      "loss": 0.3208,
      "step": 189200
    },
    {
      "epoch": 0.6548337663146315,
      "grad_norm": 0.0002955661911983043,
      "learning_rate": 0.00010354987010561054,
      "loss": 0.1186,
      "step": 189300
    },
    {
      "epoch": 0.6551796901214538,
      "grad_norm": 0.0034859017468988895,
      "learning_rate": 0.00010344609296356385,
      "loss": 0.2935,
      "step": 189400
    },
    {
      "epoch": 0.6555256139282761,
      "grad_norm": 0.0026281203608959913,
      "learning_rate": 0.00010334231582151713,
      "loss": 0.3492,
      "step": 189500
    },
    {
      "epoch": 0.6558715377350984,
      "grad_norm": 3.574789524078369,
      "learning_rate": 0.00010323853867947044,
      "loss": 0.2424,
      "step": 189600
    },
    {
      "epoch": 0.6562174615419207,
      "grad_norm": 66.88475799560547,
      "learning_rate": 0.00010313476153742376,
      "loss": 0.156,
      "step": 189700
    },
    {
      "epoch": 0.656563385348743,
      "grad_norm": 0.0053741117008030415,
      "learning_rate": 0.00010303098439537707,
      "loss": 0.253,
      "step": 189800
    },
    {
      "epoch": 0.6569093091555654,
      "grad_norm": 0.002176189562305808,
      "learning_rate": 0.00010292720725333037,
      "loss": 0.1683,
      "step": 189900
    },
    {
      "epoch": 0.6572552329623877,
      "grad_norm": 2.0211546421051025,
      "learning_rate": 0.00010282343011128368,
      "loss": 0.1901,
      "step": 190000
    },
    {
      "epoch": 0.65760115676921,
      "grad_norm": 0.1666540950536728,
      "learning_rate": 0.00010271965296923699,
      "loss": 0.203,
      "step": 190100
    },
    {
      "epoch": 0.6579470805760324,
      "grad_norm": 0.02286980301141739,
      "learning_rate": 0.00010261587582719029,
      "loss": 0.1405,
      "step": 190200
    },
    {
      "epoch": 0.6582930043828547,
      "grad_norm": 0.0009483580943197012,
      "learning_rate": 0.00010251209868514359,
      "loss": 0.1322,
      "step": 190300
    },
    {
      "epoch": 0.658638928189677,
      "grad_norm": 0.19391192495822906,
      "learning_rate": 0.0001024083215430969,
      "loss": 0.3536,
      "step": 190400
    },
    {
      "epoch": 0.6589848519964993,
      "grad_norm": 0.0010628881864249706,
      "learning_rate": 0.00010230454440105022,
      "loss": 0.4361,
      "step": 190500
    },
    {
      "epoch": 0.6593307758033216,
      "grad_norm": 0.002711981302127242,
      "learning_rate": 0.00010220076725900353,
      "loss": 0.429,
      "step": 190600
    },
    {
      "epoch": 0.6596766996101439,
      "grad_norm": 0.008667959831655025,
      "learning_rate": 0.00010209699011695684,
      "loss": 0.1766,
      "step": 190700
    },
    {
      "epoch": 0.6600226234169662,
      "grad_norm": 0.0006723364349454641,
      "learning_rate": 0.00010199321297491014,
      "loss": 0.3437,
      "step": 190800
    },
    {
      "epoch": 0.6603685472237885,
      "grad_norm": 0.0001490127615397796,
      "learning_rate": 0.00010188943583286344,
      "loss": 0.2325,
      "step": 190900
    },
    {
      "epoch": 0.6607144710306108,
      "grad_norm": 7.711397171020508,
      "learning_rate": 0.00010178565869081675,
      "loss": 0.2902,
      "step": 191000
    },
    {
      "epoch": 0.6610603948374331,
      "grad_norm": 19.706756591796875,
      "learning_rate": 0.00010168188154877006,
      "loss": 0.2988,
      "step": 191100
    },
    {
      "epoch": 0.6614063186442554,
      "grad_norm": 0.001636213855817914,
      "learning_rate": 0.00010157810440672336,
      "loss": 0.2128,
      "step": 191200
    },
    {
      "epoch": 0.6617522424510778,
      "grad_norm": 0.0007553265313617885,
      "learning_rate": 0.00010147432726467667,
      "loss": 0.2429,
      "step": 191300
    },
    {
      "epoch": 0.6620981662579001,
      "grad_norm": 0.0012572879204526544,
      "learning_rate": 0.00010137055012262999,
      "loss": 0.102,
      "step": 191400
    },
    {
      "epoch": 0.6624440900647224,
      "grad_norm": 5.028809547424316,
      "learning_rate": 0.0001012667729805833,
      "loss": 0.0823,
      "step": 191500
    },
    {
      "epoch": 0.6627900138715447,
      "grad_norm": 23.64603614807129,
      "learning_rate": 0.00010116299583853658,
      "loss": 0.2735,
      "step": 191600
    },
    {
      "epoch": 0.663135937678367,
      "grad_norm": 0.06968748569488525,
      "learning_rate": 0.0001010592186964899,
      "loss": 0.2691,
      "step": 191700
    },
    {
      "epoch": 0.6634818614851893,
      "grad_norm": 29.034080505371094,
      "learning_rate": 0.00010095544155444321,
      "loss": 0.328,
      "step": 191800
    },
    {
      "epoch": 0.6638277852920116,
      "grad_norm": 3.623469352722168,
      "learning_rate": 0.00010085166441239652,
      "loss": 0.2984,
      "step": 191900
    },
    {
      "epoch": 0.6641737090988339,
      "grad_norm": 4.483438897295855e-05,
      "learning_rate": 0.00010074788727034982,
      "loss": 0.3676,
      "step": 192000
    },
    {
      "epoch": 0.6645196329056562,
      "grad_norm": 0.0026563205756247044,
      "learning_rate": 0.00010064411012830313,
      "loss": 0.2199,
      "step": 192100
    },
    {
      "epoch": 0.6648655567124785,
      "grad_norm": 124.44480895996094,
      "learning_rate": 0.00010054033298625644,
      "loss": 0.5108,
      "step": 192200
    },
    {
      "epoch": 0.6652114805193008,
      "grad_norm": 0.0002033789933193475,
      "learning_rate": 0.00010043655584420974,
      "loss": 0.1879,
      "step": 192300
    },
    {
      "epoch": 0.6655574043261231,
      "grad_norm": 0.20984269678592682,
      "learning_rate": 0.00010033277870216304,
      "loss": 0.226,
      "step": 192400
    },
    {
      "epoch": 0.6659033281329454,
      "grad_norm": 0.032937176525592804,
      "learning_rate": 0.00010022900156011635,
      "loss": 0.4184,
      "step": 192500
    },
    {
      "epoch": 0.6662492519397677,
      "grad_norm": 0.18675480782985687,
      "learning_rate": 0.00010012522441806967,
      "loss": 0.2276,
      "step": 192600
    },
    {
      "epoch": 0.6665951757465901,
      "grad_norm": 0.013651051558554173,
      "learning_rate": 0.00010002144727602298,
      "loss": 0.2254,
      "step": 192700
    },
    {
      "epoch": 0.6669410995534124,
      "grad_norm": 0.005970844998955727,
      "learning_rate": 9.991767013397629e-05,
      "loss": 0.2417,
      "step": 192800
    },
    {
      "epoch": 0.6672870233602347,
      "grad_norm": 0.00019331744988448918,
      "learning_rate": 9.981389299192959e-05,
      "loss": 0.1423,
      "step": 192900
    },
    {
      "epoch": 0.667632947167057,
      "grad_norm": 0.00037065401556901634,
      "learning_rate": 9.971011584988289e-05,
      "loss": 0.1837,
      "step": 193000
    },
    {
      "epoch": 0.6679788709738793,
      "grad_norm": 0.12333590537309647,
      "learning_rate": 9.96063387078362e-05,
      "loss": 0.3434,
      "step": 193100
    },
    {
      "epoch": 0.6683247947807016,
      "grad_norm": 0.004152404610067606,
      "learning_rate": 9.950256156578951e-05,
      "loss": 0.2766,
      "step": 193200
    },
    {
      "epoch": 0.6686707185875239,
      "grad_norm": 12.37098217010498,
      "learning_rate": 9.939878442374281e-05,
      "loss": 0.1472,
      "step": 193300
    },
    {
      "epoch": 0.6690166423943462,
      "grad_norm": 0.013708505779504776,
      "learning_rate": 9.929500728169612e-05,
      "loss": 0.2749,
      "step": 193400
    },
    {
      "epoch": 0.6693625662011685,
      "grad_norm": 9.292370668845251e-05,
      "learning_rate": 9.919123013964944e-05,
      "loss": 0.1697,
      "step": 193500
    },
    {
      "epoch": 0.6697084900079908,
      "grad_norm": 0.010471034795045853,
      "learning_rate": 9.908745299760275e-05,
      "loss": 0.2764,
      "step": 193600
    },
    {
      "epoch": 0.6700544138148131,
      "grad_norm": 0.11534157395362854,
      "learning_rate": 9.898367585555603e-05,
      "loss": 0.2972,
      "step": 193700
    },
    {
      "epoch": 0.6704003376216354,
      "grad_norm": 0.00479361554607749,
      "learning_rate": 9.887989871350935e-05,
      "loss": 0.1443,
      "step": 193800
    },
    {
      "epoch": 0.6707462614284577,
      "grad_norm": 8.294326782226562,
      "learning_rate": 9.877612157146266e-05,
      "loss": 0.2744,
      "step": 193900
    },
    {
      "epoch": 0.67109218523528,
      "grad_norm": 0.0011785636888816953,
      "learning_rate": 9.867234442941597e-05,
      "loss": 0.1842,
      "step": 194000
    },
    {
      "epoch": 0.6714381090421024,
      "grad_norm": 0.06717246770858765,
      "learning_rate": 9.856856728736928e-05,
      "loss": 0.2593,
      "step": 194100
    },
    {
      "epoch": 0.6717840328489247,
      "grad_norm": 0.004589382093399763,
      "learning_rate": 9.846479014532258e-05,
      "loss": 0.2548,
      "step": 194200
    },
    {
      "epoch": 0.672129956655747,
      "grad_norm": 2.313915319973603e-05,
      "learning_rate": 9.83610130032759e-05,
      "loss": 0.3461,
      "step": 194300
    },
    {
      "epoch": 0.6724758804625693,
      "grad_norm": 0.009435785003006458,
      "learning_rate": 9.825723586122919e-05,
      "loss": 0.2942,
      "step": 194400
    },
    {
      "epoch": 0.6728218042693916,
      "grad_norm": 0.3239208459854126,
      "learning_rate": 9.815345871918249e-05,
      "loss": 0.2172,
      "step": 194500
    },
    {
      "epoch": 0.673167728076214,
      "grad_norm": 0.0016669502947479486,
      "learning_rate": 9.80496815771358e-05,
      "loss": 0.1863,
      "step": 194600
    },
    {
      "epoch": 0.6735136518830362,
      "grad_norm": 0.5955703854560852,
      "learning_rate": 9.794590443508912e-05,
      "loss": 0.2668,
      "step": 194700
    },
    {
      "epoch": 0.6738595756898585,
      "grad_norm": 0.029677221551537514,
      "learning_rate": 9.784212729304243e-05,
      "loss": 0.1573,
      "step": 194800
    },
    {
      "epoch": 0.6742054994966808,
      "grad_norm": 97.15090942382812,
      "learning_rate": 9.773835015099574e-05,
      "loss": 0.2061,
      "step": 194900
    },
    {
      "epoch": 0.6745514233035032,
      "grad_norm": 0.0012261365773156285,
      "learning_rate": 9.763457300894904e-05,
      "loss": 0.2639,
      "step": 195000
    },
    {
      "epoch": 0.6748973471103255,
      "grad_norm": 0.0022728415206074715,
      "learning_rate": 9.753079586690234e-05,
      "loss": 0.3332,
      "step": 195100
    },
    {
      "epoch": 0.6752432709171478,
      "grad_norm": 0.17051967978477478,
      "learning_rate": 9.742701872485565e-05,
      "loss": 0.4175,
      "step": 195200
    },
    {
      "epoch": 0.67558919472397,
      "grad_norm": 0.003305695252493024,
      "learning_rate": 9.732324158280896e-05,
      "loss": 0.1862,
      "step": 195300
    },
    {
      "epoch": 0.6759351185307924,
      "grad_norm": 0.0008775025489740074,
      "learning_rate": 9.721946444076226e-05,
      "loss": 0.1252,
      "step": 195400
    },
    {
      "epoch": 0.6762810423376148,
      "grad_norm": 1.740295171737671,
      "learning_rate": 9.711568729871557e-05,
      "loss": 0.2597,
      "step": 195500
    },
    {
      "epoch": 0.6766269661444371,
      "grad_norm": 0.000675673654768616,
      "learning_rate": 9.701191015666889e-05,
      "loss": 0.537,
      "step": 195600
    },
    {
      "epoch": 0.6769728899512594,
      "grad_norm": 0.0004880251653958112,
      "learning_rate": 9.69081330146222e-05,
      "loss": 0.2494,
      "step": 195700
    },
    {
      "epoch": 0.6773188137580817,
      "grad_norm": 0.8896518349647522,
      "learning_rate": 9.680435587257548e-05,
      "loss": 0.3504,
      "step": 195800
    },
    {
      "epoch": 0.677664737564904,
      "grad_norm": 0.0034562915097922087,
      "learning_rate": 9.67005787305288e-05,
      "loss": 0.307,
      "step": 195900
    },
    {
      "epoch": 0.6780106613717263,
      "grad_norm": 0.0002566407201811671,
      "learning_rate": 9.659680158848211e-05,
      "loss": 0.1952,
      "step": 196000
    },
    {
      "epoch": 0.6783565851785486,
      "grad_norm": 0.0007661359268240631,
      "learning_rate": 9.649302444643542e-05,
      "loss": 0.3498,
      "step": 196100
    },
    {
      "epoch": 0.6787025089853709,
      "grad_norm": 0.00030409847386181355,
      "learning_rate": 9.638924730438873e-05,
      "loss": 0.3607,
      "step": 196200
    },
    {
      "epoch": 0.6790484327921932,
      "grad_norm": 0.0019088417757302523,
      "learning_rate": 9.628547016234203e-05,
      "loss": 0.2283,
      "step": 196300
    },
    {
      "epoch": 0.6793943565990155,
      "grad_norm": 0.013335369527339935,
      "learning_rate": 9.618169302029535e-05,
      "loss": 0.2512,
      "step": 196400
    },
    {
      "epoch": 0.6797402804058378,
      "grad_norm": 0.001800667610950768,
      "learning_rate": 9.607791587824864e-05,
      "loss": 0.3731,
      "step": 196500
    },
    {
      "epoch": 0.6800862042126601,
      "grad_norm": 0.0005545034073293209,
      "learning_rate": 9.597413873620196e-05,
      "loss": 0.2706,
      "step": 196600
    },
    {
      "epoch": 0.6804321280194824,
      "grad_norm": 0.0502224825322628,
      "learning_rate": 9.587036159415525e-05,
      "loss": 0.2573,
      "step": 196700
    },
    {
      "epoch": 0.6807780518263047,
      "grad_norm": 0.006437064614146948,
      "learning_rate": 9.576658445210857e-05,
      "loss": 0.0983,
      "step": 196800
    },
    {
      "epoch": 0.6811239756331271,
      "grad_norm": 0.0004391965630929917,
      "learning_rate": 9.566280731006188e-05,
      "loss": 0.1768,
      "step": 196900
    },
    {
      "epoch": 0.6814698994399494,
      "grad_norm": 51.72438430786133,
      "learning_rate": 9.555903016801519e-05,
      "loss": 0.1907,
      "step": 197000
    },
    {
      "epoch": 0.6818158232467717,
      "grad_norm": 0.0021209493279457092,
      "learning_rate": 9.54552530259685e-05,
      "loss": 0.2453,
      "step": 197100
    },
    {
      "epoch": 0.682161747053594,
      "grad_norm": 0.00029303040355443954,
      "learning_rate": 9.535147588392179e-05,
      "loss": 0.2352,
      "step": 197200
    },
    {
      "epoch": 0.6825076708604163,
      "grad_norm": 0.0025188385043293238,
      "learning_rate": 9.52476987418751e-05,
      "loss": 0.1719,
      "step": 197300
    },
    {
      "epoch": 0.6828535946672386,
      "grad_norm": 0.019504355266690254,
      "learning_rate": 9.514392159982841e-05,
      "loss": 0.2315,
      "step": 197400
    },
    {
      "epoch": 0.6831995184740609,
      "grad_norm": 0.0027220998890697956,
      "learning_rate": 9.504014445778171e-05,
      "loss": 0.4507,
      "step": 197500
    },
    {
      "epoch": 0.6835454422808832,
      "grad_norm": 7.1570611000061035,
      "learning_rate": 9.493636731573503e-05,
      "loss": 0.1598,
      "step": 197600
    },
    {
      "epoch": 0.6838913660877055,
      "grad_norm": 0.02324683591723442,
      "learning_rate": 9.483259017368834e-05,
      "loss": 0.4506,
      "step": 197700
    },
    {
      "epoch": 0.6842372898945278,
      "grad_norm": 0.0019267433090135455,
      "learning_rate": 9.472881303164165e-05,
      "loss": 0.2916,
      "step": 197800
    },
    {
      "epoch": 0.6845832137013501,
      "grad_norm": 0.0004398094897624105,
      "learning_rate": 9.462503588959494e-05,
      "loss": 0.1053,
      "step": 197900
    },
    {
      "epoch": 0.6849291375081724,
      "grad_norm": 92.3294906616211,
      "learning_rate": 9.452125874754825e-05,
      "loss": 0.33,
      "step": 198000
    },
    {
      "epoch": 0.6852750613149947,
      "grad_norm": 6.152137211756781e-05,
      "learning_rate": 9.441748160550156e-05,
      "loss": 0.2731,
      "step": 198100
    },
    {
      "epoch": 0.685620985121817,
      "grad_norm": 0.0002817656786646694,
      "learning_rate": 9.431370446345487e-05,
      "loss": 0.2112,
      "step": 198200
    },
    {
      "epoch": 0.6859669089286394,
      "grad_norm": 0.009198677726089954,
      "learning_rate": 9.420992732140818e-05,
      "loss": 0.2148,
      "step": 198300
    },
    {
      "epoch": 0.6863128327354617,
      "grad_norm": 0.0027883180882781744,
      "learning_rate": 9.410615017936148e-05,
      "loss": 0.2589,
      "step": 198400
    },
    {
      "epoch": 0.686658756542284,
      "grad_norm": 0.0006675468757748604,
      "learning_rate": 9.40023730373148e-05,
      "loss": 0.2609,
      "step": 198500
    },
    {
      "epoch": 0.6870046803491063,
      "grad_norm": 0.00046196641051210463,
      "learning_rate": 9.389859589526811e-05,
      "loss": 0.2197,
      "step": 198600
    },
    {
      "epoch": 0.6873506041559286,
      "grad_norm": 0.0013838184531778097,
      "learning_rate": 9.37948187532214e-05,
      "loss": 0.4021,
      "step": 198700
    },
    {
      "epoch": 0.6876965279627509,
      "grad_norm": 0.00549726327881217,
      "learning_rate": 9.36910416111747e-05,
      "loss": 0.3837,
      "step": 198800
    },
    {
      "epoch": 0.6880424517695732,
      "grad_norm": 0.1192728728055954,
      "learning_rate": 9.358726446912802e-05,
      "loss": 0.3355,
      "step": 198900
    },
    {
      "epoch": 0.6883883755763955,
      "grad_norm": 0.15015286207199097,
      "learning_rate": 9.348348732708133e-05,
      "loss": 0.1515,
      "step": 199000
    },
    {
      "epoch": 0.6887342993832178,
      "grad_norm": 0.3029220402240753,
      "learning_rate": 9.337971018503464e-05,
      "loss": 0.1397,
      "step": 199100
    },
    {
      "epoch": 0.6890802231900401,
      "grad_norm": 0.1339396983385086,
      "learning_rate": 9.327593304298795e-05,
      "loss": 0.3135,
      "step": 199200
    },
    {
      "epoch": 0.6894261469968624,
      "grad_norm": 0.007193233817815781,
      "learning_rate": 9.317215590094125e-05,
      "loss": 0.2329,
      "step": 199300
    },
    {
      "epoch": 0.6897720708036847,
      "grad_norm": 0.0014947126619517803,
      "learning_rate": 9.306837875889455e-05,
      "loss": 0.2666,
      "step": 199400
    },
    {
      "epoch": 0.690117994610507,
      "grad_norm": 0.0036586932837963104,
      "learning_rate": 9.296460161684786e-05,
      "loss": 0.1764,
      "step": 199500
    },
    {
      "epoch": 0.6904639184173293,
      "grad_norm": 0.00033414250356145203,
      "learning_rate": 9.286082447480118e-05,
      "loss": 0.3221,
      "step": 199600
    },
    {
      "epoch": 0.6908098422241518,
      "grad_norm": 0.16432268917560577,
      "learning_rate": 9.275704733275448e-05,
      "loss": 0.3321,
      "step": 199700
    },
    {
      "epoch": 0.6911557660309741,
      "grad_norm": 0.00032733153784647584,
      "learning_rate": 9.265327019070779e-05,
      "loss": 0.3608,
      "step": 199800
    },
    {
      "epoch": 0.6915016898377964,
      "grad_norm": 9.738369941711426,
      "learning_rate": 9.25494930486611e-05,
      "loss": 0.2072,
      "step": 199900
    },
    {
      "epoch": 0.6918476136446187,
      "grad_norm": 0.0031177466735243797,
      "learning_rate": 9.244571590661441e-05,
      "loss": 0.1739,
      "step": 200000
    },
    {
      "epoch": 0.692193537451441,
      "grad_norm": 0.0021244394592940807,
      "learning_rate": 9.23419387645677e-05,
      "loss": 0.2538,
      "step": 200100
    },
    {
      "epoch": 0.6925394612582633,
      "grad_norm": 0.0008576937252655625,
      "learning_rate": 9.223816162252101e-05,
      "loss": 0.1977,
      "step": 200200
    },
    {
      "epoch": 0.6928853850650856,
      "grad_norm": 0.0006802863208577037,
      "learning_rate": 9.213438448047432e-05,
      "loss": 0.2747,
      "step": 200300
    },
    {
      "epoch": 0.6932313088719079,
      "grad_norm": 20.11555290222168,
      "learning_rate": 9.203060733842764e-05,
      "loss": 0.2064,
      "step": 200400
    },
    {
      "epoch": 0.6935772326787302,
      "grad_norm": 49.5739631652832,
      "learning_rate": 9.192683019638093e-05,
      "loss": 0.271,
      "step": 200500
    },
    {
      "epoch": 0.6939231564855525,
      "grad_norm": 23.577768325805664,
      "learning_rate": 9.182305305433425e-05,
      "loss": 0.1877,
      "step": 200600
    },
    {
      "epoch": 0.6942690802923748,
      "grad_norm": 0.07718304544687271,
      "learning_rate": 9.171927591228756e-05,
      "loss": 0.1393,
      "step": 200700
    },
    {
      "epoch": 0.6946150040991971,
      "grad_norm": 8.697227895027027e-05,
      "learning_rate": 9.161549877024086e-05,
      "loss": 0.2144,
      "step": 200800
    },
    {
      "epoch": 0.6949609279060194,
      "grad_norm": 0.004708773456513882,
      "learning_rate": 9.151172162819416e-05,
      "loss": 0.3228,
      "step": 200900
    },
    {
      "epoch": 0.6953068517128417,
      "grad_norm": 0.0005740748019888997,
      "learning_rate": 9.140794448614747e-05,
      "loss": 0.1643,
      "step": 201000
    },
    {
      "epoch": 0.6956527755196641,
      "grad_norm": 0.0008380574290640652,
      "learning_rate": 9.130416734410078e-05,
      "loss": 0.1567,
      "step": 201100
    },
    {
      "epoch": 0.6959986993264864,
      "grad_norm": 0.16968798637390137,
      "learning_rate": 9.120039020205409e-05,
      "loss": 0.2024,
      "step": 201200
    },
    {
      "epoch": 0.6963446231333087,
      "grad_norm": 0.1406642496585846,
      "learning_rate": 9.10966130600074e-05,
      "loss": 0.0813,
      "step": 201300
    },
    {
      "epoch": 0.696690546940131,
      "grad_norm": 0.9083412885665894,
      "learning_rate": 9.09928359179607e-05,
      "loss": 0.4122,
      "step": 201400
    },
    {
      "epoch": 0.6970364707469533,
      "grad_norm": 35.07274627685547,
      "learning_rate": 9.0889058775914e-05,
      "loss": 0.2078,
      "step": 201500
    },
    {
      "epoch": 0.6973823945537756,
      "grad_norm": 0.1771952509880066,
      "learning_rate": 9.078528163386732e-05,
      "loss": 0.2468,
      "step": 201600
    },
    {
      "epoch": 0.6977283183605979,
      "grad_norm": 0.0021858413238078356,
      "learning_rate": 9.068150449182063e-05,
      "loss": 0.2886,
      "step": 201700
    },
    {
      "epoch": 0.6980742421674202,
      "grad_norm": 0.0033013366628438234,
      "learning_rate": 9.057772734977393e-05,
      "loss": 0.1793,
      "step": 201800
    },
    {
      "epoch": 0.6984201659742425,
      "grad_norm": 0.28058090806007385,
      "learning_rate": 9.047395020772724e-05,
      "loss": 0.315,
      "step": 201900
    },
    {
      "epoch": 0.6987660897810648,
      "grad_norm": 0.0047465101815760136,
      "learning_rate": 9.037017306568055e-05,
      "loss": 0.2409,
      "step": 202000
    },
    {
      "epoch": 0.6991120135878871,
      "grad_norm": 0.24565072357654572,
      "learning_rate": 9.026639592363386e-05,
      "loss": 0.3291,
      "step": 202100
    },
    {
      "epoch": 0.6994579373947094,
      "grad_norm": 0.07665104418992996,
      "learning_rate": 9.016261878158715e-05,
      "loss": 0.2412,
      "step": 202200
    },
    {
      "epoch": 0.6998038612015317,
      "grad_norm": 0.00282993889413774,
      "learning_rate": 9.005884163954046e-05,
      "loss": 0.3358,
      "step": 202300
    },
    {
      "epoch": 0.700149785008354,
      "grad_norm": 0.02389167621731758,
      "learning_rate": 8.995506449749377e-05,
      "loss": 0.3434,
      "step": 202400
    },
    {
      "epoch": 0.7004957088151764,
      "grad_norm": 0.0016429177485406399,
      "learning_rate": 8.985128735544709e-05,
      "loss": 0.3373,
      "step": 202500
    },
    {
      "epoch": 0.7008416326219987,
      "grad_norm": 1.0465724468231201,
      "learning_rate": 8.97475102134004e-05,
      "loss": 0.2915,
      "step": 202600
    },
    {
      "epoch": 0.701187556428821,
      "grad_norm": 0.037075307220220566,
      "learning_rate": 8.96437330713537e-05,
      "loss": 0.2546,
      "step": 202700
    },
    {
      "epoch": 0.7015334802356433,
      "grad_norm": 13.824766159057617,
      "learning_rate": 8.953995592930701e-05,
      "loss": 0.1305,
      "step": 202800
    },
    {
      "epoch": 0.7018794040424656,
      "grad_norm": 0.0026527484878897667,
      "learning_rate": 8.943617878726031e-05,
      "loss": 0.3285,
      "step": 202900
    },
    {
      "epoch": 0.7022253278492879,
      "grad_norm": 0.00040879775770008564,
      "learning_rate": 8.933240164521362e-05,
      "loss": 0.2159,
      "step": 203000
    },
    {
      "epoch": 0.7025712516561102,
      "grad_norm": 0.0004762168100569397,
      "learning_rate": 8.922862450316692e-05,
      "loss": 0.2312,
      "step": 203100
    },
    {
      "epoch": 0.7029171754629325,
      "grad_norm": 0.00010754059621831402,
      "learning_rate": 8.912484736112023e-05,
      "loss": 0.25,
      "step": 203200
    },
    {
      "epoch": 0.7032630992697548,
      "grad_norm": 0.06217252090573311,
      "learning_rate": 8.902107021907354e-05,
      "loss": 0.2987,
      "step": 203300
    },
    {
      "epoch": 0.7036090230765771,
      "grad_norm": 0.0005943614523857832,
      "learning_rate": 8.891729307702686e-05,
      "loss": 0.2685,
      "step": 203400
    },
    {
      "epoch": 0.7039549468833994,
      "grad_norm": 74.26728057861328,
      "learning_rate": 8.881351593498017e-05,
      "loss": 0.2544,
      "step": 203500
    },
    {
      "epoch": 0.7043008706902217,
      "grad_norm": 0.029292337596416473,
      "learning_rate": 8.870973879293345e-05,
      "loss": 0.229,
      "step": 203600
    },
    {
      "epoch": 0.704646794497044,
      "grad_norm": 0.0004558670916594565,
      "learning_rate": 8.860596165088677e-05,
      "loss": 0.2401,
      "step": 203700
    },
    {
      "epoch": 0.7049927183038663,
      "grad_norm": 0.0017914222553372383,
      "learning_rate": 8.850218450884008e-05,
      "loss": 0.1461,
      "step": 203800
    },
    {
      "epoch": 0.7053386421106888,
      "grad_norm": 0.42264169454574585,
      "learning_rate": 8.839840736679338e-05,
      "loss": 0.2396,
      "step": 203900
    },
    {
      "epoch": 0.705684565917511,
      "grad_norm": 0.15146343410015106,
      "learning_rate": 8.829463022474669e-05,
      "loss": 0.1833,
      "step": 204000
    },
    {
      "epoch": 0.7060304897243334,
      "grad_norm": 0.006027194205671549,
      "learning_rate": 8.81908530827e-05,
      "loss": 0.2426,
      "step": 204100
    },
    {
      "epoch": 0.7063764135311557,
      "grad_norm": 0.0004994419869035482,
      "learning_rate": 8.808707594065331e-05,
      "loss": 0.1861,
      "step": 204200
    },
    {
      "epoch": 0.706722337337978,
      "grad_norm": 0.0002146030165022239,
      "learning_rate": 8.79832987986066e-05,
      "loss": 0.2304,
      "step": 204300
    },
    {
      "epoch": 0.7070682611448003,
      "grad_norm": 0.0004972069291397929,
      "learning_rate": 8.787952165655991e-05,
      "loss": 0.2894,
      "step": 204400
    },
    {
      "epoch": 0.7074141849516226,
      "grad_norm": 0.000313509110128507,
      "learning_rate": 8.777574451451322e-05,
      "loss": 0.2071,
      "step": 204500
    },
    {
      "epoch": 0.7077601087584449,
      "grad_norm": 0.02789301984012127,
      "learning_rate": 8.767196737246654e-05,
      "loss": 0.3373,
      "step": 204600
    },
    {
      "epoch": 0.7081060325652672,
      "grad_norm": 0.059588368982076645,
      "learning_rate": 8.756819023041985e-05,
      "loss": 0.1891,
      "step": 204700
    },
    {
      "epoch": 0.7084519563720895,
      "grad_norm": 0.0006826085154898465,
      "learning_rate": 8.746441308837315e-05,
      "loss": 0.1454,
      "step": 204800
    },
    {
      "epoch": 0.7087978801789118,
      "grad_norm": 0.0039125666953623295,
      "learning_rate": 8.736063594632646e-05,
      "loss": 0.232,
      "step": 204900
    },
    {
      "epoch": 0.7091438039857341,
      "grad_norm": 0.04509122669696808,
      "learning_rate": 8.725685880427976e-05,
      "loss": 0.2886,
      "step": 205000
    },
    {
      "epoch": 0.7094897277925564,
      "grad_norm": 0.0027790588792413473,
      "learning_rate": 8.715308166223307e-05,
      "loss": 0.0915,
      "step": 205100
    },
    {
      "epoch": 0.7098356515993787,
      "grad_norm": 0.0009377195965498686,
      "learning_rate": 8.704930452018637e-05,
      "loss": 0.2704,
      "step": 205200
    },
    {
      "epoch": 0.7101815754062011,
      "grad_norm": 18.661663055419922,
      "learning_rate": 8.694552737813968e-05,
      "loss": 0.2897,
      "step": 205300
    },
    {
      "epoch": 0.7105274992130234,
      "grad_norm": 0.012966083362698555,
      "learning_rate": 8.6841750236093e-05,
      "loss": 0.25,
      "step": 205400
    },
    {
      "epoch": 0.7108734230198457,
      "grad_norm": 63.80060958862305,
      "learning_rate": 8.67379730940463e-05,
      "loss": 0.3591,
      "step": 205500
    },
    {
      "epoch": 0.711219346826668,
      "grad_norm": 0.0011181646259501576,
      "learning_rate": 8.663419595199962e-05,
      "loss": 0.2258,
      "step": 205600
    },
    {
      "epoch": 0.7115652706334903,
      "grad_norm": 12.196364402770996,
      "learning_rate": 8.65304188099529e-05,
      "loss": 0.0698,
      "step": 205700
    },
    {
      "epoch": 0.7119111944403126,
      "grad_norm": 0.012449676170945168,
      "learning_rate": 8.642664166790622e-05,
      "loss": 0.3426,
      "step": 205800
    },
    {
      "epoch": 0.7122571182471349,
      "grad_norm": 1.3830565214157104,
      "learning_rate": 8.632286452585953e-05,
      "loss": 0.2723,
      "step": 205900
    },
    {
      "epoch": 0.7126030420539572,
      "grad_norm": 0.011543279513716698,
      "learning_rate": 8.621908738381284e-05,
      "loss": 0.239,
      "step": 206000
    },
    {
      "epoch": 0.7129489658607795,
      "grad_norm": 0.0011293424759060144,
      "learning_rate": 8.611531024176614e-05,
      "loss": 0.3653,
      "step": 206100
    },
    {
      "epoch": 0.7132948896676018,
      "grad_norm": 0.008054116740822792,
      "learning_rate": 8.601153309971945e-05,
      "loss": 0.2501,
      "step": 206200
    },
    {
      "epoch": 0.7136408134744241,
      "grad_norm": 0.0002004401758313179,
      "learning_rate": 8.590775595767276e-05,
      "loss": 0.2382,
      "step": 206300
    },
    {
      "epoch": 0.7139867372812464,
      "grad_norm": 0.008375703357160091,
      "learning_rate": 8.580397881562605e-05,
      "loss": 0.0641,
      "step": 206400
    },
    {
      "epoch": 0.7143326610880687,
      "grad_norm": 0.0019433008274063468,
      "learning_rate": 8.570020167357936e-05,
      "loss": 0.3914,
      "step": 206500
    },
    {
      "epoch": 0.714678584894891,
      "grad_norm": 0.00016104122914839536,
      "learning_rate": 8.559642453153267e-05,
      "loss": 0.2588,
      "step": 206600
    },
    {
      "epoch": 0.7150245087017134,
      "grad_norm": 0.000864896981511265,
      "learning_rate": 8.549264738948599e-05,
      "loss": 0.3403,
      "step": 206700
    },
    {
      "epoch": 0.7153704325085357,
      "grad_norm": 8.044059359235689e-05,
      "learning_rate": 8.53888702474393e-05,
      "loss": 0.3078,
      "step": 206800
    },
    {
      "epoch": 0.715716356315358,
      "grad_norm": 0.00013417961599770933,
      "learning_rate": 8.52850931053926e-05,
      "loss": 0.3118,
      "step": 206900
    },
    {
      "epoch": 0.7160622801221803,
      "grad_norm": 0.00035418415791355073,
      "learning_rate": 8.518131596334591e-05,
      "loss": 0.2467,
      "step": 207000
    },
    {
      "epoch": 0.7164082039290026,
      "grad_norm": 29.536394119262695,
      "learning_rate": 8.507753882129921e-05,
      "loss": 0.3082,
      "step": 207100
    },
    {
      "epoch": 0.7167541277358249,
      "grad_norm": 0.0006319169769994915,
      "learning_rate": 8.497376167925252e-05,
      "loss": 0.2138,
      "step": 207200
    },
    {
      "epoch": 0.7171000515426472,
      "grad_norm": 0.0004785155178979039,
      "learning_rate": 8.486998453720582e-05,
      "loss": 0.2182,
      "step": 207300
    },
    {
      "epoch": 0.7174459753494695,
      "grad_norm": 0.04034581407904625,
      "learning_rate": 8.476620739515913e-05,
      "loss": 0.4135,
      "step": 207400
    },
    {
      "epoch": 0.7177918991562918,
      "grad_norm": 0.001372968778014183,
      "learning_rate": 8.466243025311245e-05,
      "loss": 0.1257,
      "step": 207500
    },
    {
      "epoch": 0.7181378229631141,
      "grad_norm": 5.7331646530656144e-05,
      "learning_rate": 8.455865311106576e-05,
      "loss": 0.3099,
      "step": 207600
    },
    {
      "epoch": 0.7184837467699364,
      "grad_norm": 0.001972861820831895,
      "learning_rate": 8.445487596901907e-05,
      "loss": 0.2066,
      "step": 207700
    },
    {
      "epoch": 0.7188296705767587,
      "grad_norm": 0.0049201310612261295,
      "learning_rate": 8.435109882697235e-05,
      "loss": 0.2864,
      "step": 207800
    },
    {
      "epoch": 0.719175594383581,
      "grad_norm": 0.0007249534246511757,
      "learning_rate": 8.424732168492567e-05,
      "loss": 0.2791,
      "step": 207900
    },
    {
      "epoch": 0.7195215181904033,
      "grad_norm": 0.0017072362825274467,
      "learning_rate": 8.414354454287898e-05,
      "loss": 0.3368,
      "step": 208000
    },
    {
      "epoch": 0.7198674419972257,
      "grad_norm": 0.0009431159705854952,
      "learning_rate": 8.403976740083229e-05,
      "loss": 0.275,
      "step": 208100
    },
    {
      "epoch": 0.720213365804048,
      "grad_norm": 6.591754913330078,
      "learning_rate": 8.393599025878559e-05,
      "loss": 0.2953,
      "step": 208200
    },
    {
      "epoch": 0.7205592896108703,
      "grad_norm": 0.00029890279984101653,
      "learning_rate": 8.38322131167389e-05,
      "loss": 0.1259,
      "step": 208300
    },
    {
      "epoch": 0.7209052134176926,
      "grad_norm": 0.0011398238129913807,
      "learning_rate": 8.372843597469222e-05,
      "loss": 0.2061,
      "step": 208400
    },
    {
      "epoch": 0.721251137224515,
      "grad_norm": 0.009470256976783276,
      "learning_rate": 8.362465883264551e-05,
      "loss": 0.2849,
      "step": 208500
    },
    {
      "epoch": 0.7215970610313373,
      "grad_norm": 10.779905319213867,
      "learning_rate": 8.352088169059881e-05,
      "loss": 0.2244,
      "step": 208600
    },
    {
      "epoch": 0.7219429848381596,
      "grad_norm": 0.0005581078585237265,
      "learning_rate": 8.341710454855213e-05,
      "loss": 0.1598,
      "step": 208700
    },
    {
      "epoch": 0.7222889086449819,
      "grad_norm": 2.2853596210479736,
      "learning_rate": 8.331332740650544e-05,
      "loss": 0.3044,
      "step": 208800
    },
    {
      "epoch": 0.7226348324518042,
      "grad_norm": 0.0009319866658188403,
      "learning_rate": 8.320955026445875e-05,
      "loss": 0.2076,
      "step": 208900
    },
    {
      "epoch": 0.7229807562586265,
      "grad_norm": 0.00040576845640316606,
      "learning_rate": 8.310577312241206e-05,
      "loss": 0.229,
      "step": 209000
    },
    {
      "epoch": 0.7233266800654488,
      "grad_norm": 7.846437802072614e-05,
      "learning_rate": 8.300199598036536e-05,
      "loss": 0.244,
      "step": 209100
    },
    {
      "epoch": 0.7236726038722711,
      "grad_norm": 0.0021551097743213177,
      "learning_rate": 8.289821883831866e-05,
      "loss": 0.2353,
      "step": 209200
    },
    {
      "epoch": 0.7240185276790934,
      "grad_norm": 0.002374724019318819,
      "learning_rate": 8.279444169627197e-05,
      "loss": 0.2511,
      "step": 209300
    },
    {
      "epoch": 0.7243644514859157,
      "grad_norm": 0.002825217554345727,
      "learning_rate": 8.269066455422527e-05,
      "loss": 0.3942,
      "step": 209400
    },
    {
      "epoch": 0.7247103752927381,
      "grad_norm": 70.37168884277344,
      "learning_rate": 8.258688741217858e-05,
      "loss": 0.2078,
      "step": 209500
    },
    {
      "epoch": 0.7250562990995604,
      "grad_norm": 0.0015983962221071124,
      "learning_rate": 8.24831102701319e-05,
      "loss": 0.2737,
      "step": 209600
    },
    {
      "epoch": 0.7254022229063827,
      "grad_norm": 0.00022762600565329194,
      "learning_rate": 8.237933312808521e-05,
      "loss": 0.247,
      "step": 209700
    },
    {
      "epoch": 0.725748146713205,
      "grad_norm": 0.0005372677114792168,
      "learning_rate": 8.227555598603852e-05,
      "loss": 0.1252,
      "step": 209800
    },
    {
      "epoch": 0.7260940705200273,
      "grad_norm": 0.002824127906933427,
      "learning_rate": 8.21717788439918e-05,
      "loss": 0.1761,
      "step": 209900
    },
    {
      "epoch": 0.7264399943268496,
      "grad_norm": 0.00023989714100025594,
      "learning_rate": 8.206800170194512e-05,
      "loss": 0.1472,
      "step": 210000
    },
    {
      "epoch": 0.7267859181336719,
      "grad_norm": 13.620844841003418,
      "learning_rate": 8.196422455989843e-05,
      "loss": 0.2029,
      "step": 210100
    },
    {
      "epoch": 0.7271318419404942,
      "grad_norm": 0.002279775682836771,
      "learning_rate": 8.186044741785174e-05,
      "loss": 0.1754,
      "step": 210200
    },
    {
      "epoch": 0.7274777657473165,
      "grad_norm": 0.004744799342006445,
      "learning_rate": 8.175667027580504e-05,
      "loss": 0.128,
      "step": 210300
    },
    {
      "epoch": 0.7278236895541388,
      "grad_norm": 0.00027614811551757157,
      "learning_rate": 8.165289313375835e-05,
      "loss": 0.2474,
      "step": 210400
    },
    {
      "epoch": 0.7281696133609611,
      "grad_norm": 69.06684112548828,
      "learning_rate": 8.154911599171167e-05,
      "loss": 0.2352,
      "step": 210500
    },
    {
      "epoch": 0.7285155371677834,
      "grad_norm": 0.003335363231599331,
      "learning_rate": 8.144533884966496e-05,
      "loss": 0.4288,
      "step": 210600
    },
    {
      "epoch": 0.7288614609746057,
      "grad_norm": 0.0005237197037786245,
      "learning_rate": 8.134156170761826e-05,
      "loss": 0.2034,
      "step": 210700
    },
    {
      "epoch": 0.729207384781428,
      "grad_norm": 0.0005231890827417374,
      "learning_rate": 8.123778456557158e-05,
      "loss": 0.329,
      "step": 210800
    },
    {
      "epoch": 0.7295533085882504,
      "grad_norm": 0.0020563991274684668,
      "learning_rate": 8.113400742352489e-05,
      "loss": 0.1894,
      "step": 210900
    },
    {
      "epoch": 0.7298992323950727,
      "grad_norm": 0.0017471519531682134,
      "learning_rate": 8.10302302814782e-05,
      "loss": 0.2205,
      "step": 211000
    },
    {
      "epoch": 0.730245156201895,
      "grad_norm": 7.200710296630859,
      "learning_rate": 8.092645313943151e-05,
      "loss": 0.4055,
      "step": 211100
    },
    {
      "epoch": 0.7305910800087173,
      "grad_norm": 0.9724754095077515,
      "learning_rate": 8.082267599738481e-05,
      "loss": 0.2434,
      "step": 211200
    },
    {
      "epoch": 0.7309370038155396,
      "grad_norm": 0.00013367063365876675,
      "learning_rate": 8.071889885533811e-05,
      "loss": 0.4765,
      "step": 211300
    },
    {
      "epoch": 0.7312829276223619,
      "grad_norm": 0.0014533433131873608,
      "learning_rate": 8.061512171329142e-05,
      "loss": 0.1359,
      "step": 211400
    },
    {
      "epoch": 0.7316288514291842,
      "grad_norm": 0.00020421543740667403,
      "learning_rate": 8.051134457124474e-05,
      "loss": 0.2349,
      "step": 211500
    },
    {
      "epoch": 0.7319747752360065,
      "grad_norm": 2.727944850921631,
      "learning_rate": 8.040756742919803e-05,
      "loss": 0.3581,
      "step": 211600
    },
    {
      "epoch": 0.7323206990428288,
      "grad_norm": 0.0007814758573658764,
      "learning_rate": 8.030379028715135e-05,
      "loss": 0.2304,
      "step": 211700
    },
    {
      "epoch": 0.7326666228496511,
      "grad_norm": 21.173259735107422,
      "learning_rate": 8.020001314510466e-05,
      "loss": 0.2562,
      "step": 211800
    },
    {
      "epoch": 0.7330125466564734,
      "grad_norm": 0.027999062091112137,
      "learning_rate": 8.009623600305797e-05,
      "loss": 0.3085,
      "step": 211900
    },
    {
      "epoch": 0.7333584704632957,
      "grad_norm": 11.530946731567383,
      "learning_rate": 7.999245886101126e-05,
      "loss": 0.2895,
      "step": 212000
    },
    {
      "epoch": 0.733704394270118,
      "grad_norm": 0.02733064629137516,
      "learning_rate": 7.988868171896457e-05,
      "loss": 0.1673,
      "step": 212100
    },
    {
      "epoch": 0.7340503180769403,
      "grad_norm": 0.002221511909738183,
      "learning_rate": 7.978490457691788e-05,
      "loss": 0.3414,
      "step": 212200
    },
    {
      "epoch": 0.7343962418837627,
      "grad_norm": 0.0013538240455091,
      "learning_rate": 7.96811274348712e-05,
      "loss": 0.3328,
      "step": 212300
    },
    {
      "epoch": 0.734742165690585,
      "grad_norm": 0.009274649433791637,
      "learning_rate": 7.95773502928245e-05,
      "loss": 0.2019,
      "step": 212400
    },
    {
      "epoch": 0.7350880894974073,
      "grad_norm": 0.0004134145565330982,
      "learning_rate": 7.94735731507778e-05,
      "loss": 0.5409,
      "step": 212500
    },
    {
      "epoch": 0.7354340133042296,
      "grad_norm": 6.630124568939209,
      "learning_rate": 7.936979600873112e-05,
      "loss": 0.2296,
      "step": 212600
    },
    {
      "epoch": 0.7357799371110519,
      "grad_norm": 0.005005234386771917,
      "learning_rate": 7.926601886668442e-05,
      "loss": 0.1092,
      "step": 212700
    },
    {
      "epoch": 0.7361258609178742,
      "grad_norm": 12.92651081085205,
      "learning_rate": 7.916224172463771e-05,
      "loss": 0.2763,
      "step": 212800
    },
    {
      "epoch": 0.7364717847246965,
      "grad_norm": 0.01978200115263462,
      "learning_rate": 7.905846458259103e-05,
      "loss": 0.3081,
      "step": 212900
    },
    {
      "epoch": 0.7368177085315188,
      "grad_norm": 28.280378341674805,
      "learning_rate": 7.895468744054434e-05,
      "loss": 0.2931,
      "step": 213000
    },
    {
      "epoch": 0.7371636323383411,
      "grad_norm": 1.4250612258911133,
      "learning_rate": 7.885091029849765e-05,
      "loss": 0.2199,
      "step": 213100
    },
    {
      "epoch": 0.7375095561451634,
      "grad_norm": 0.000640072044916451,
      "learning_rate": 7.874713315645096e-05,
      "loss": 0.2076,
      "step": 213200
    },
    {
      "epoch": 0.7378554799519857,
      "grad_norm": 0.002461033873260021,
      "learning_rate": 7.864335601440426e-05,
      "loss": 0.2838,
      "step": 213300
    },
    {
      "epoch": 0.738201403758808,
      "grad_norm": 0.0010979064973071218,
      "learning_rate": 7.853957887235756e-05,
      "loss": 0.2226,
      "step": 213400
    },
    {
      "epoch": 0.7385473275656304,
      "grad_norm": 0.0023835713509470224,
      "learning_rate": 7.843580173031087e-05,
      "loss": 0.1623,
      "step": 213500
    },
    {
      "epoch": 0.7388932513724527,
      "grad_norm": 0.33154383301734924,
      "learning_rate": 7.833202458826419e-05,
      "loss": 0.3274,
      "step": 213600
    },
    {
      "epoch": 0.7392391751792751,
      "grad_norm": 0.4241419732570648,
      "learning_rate": 7.822824744621748e-05,
      "loss": 0.2548,
      "step": 213700
    },
    {
      "epoch": 0.7395850989860974,
      "grad_norm": 0.0002119010459864512,
      "learning_rate": 7.81244703041708e-05,
      "loss": 0.1569,
      "step": 213800
    },
    {
      "epoch": 0.7399310227929197,
      "grad_norm": 11.577345848083496,
      "learning_rate": 7.802069316212411e-05,
      "loss": 0.2519,
      "step": 213900
    },
    {
      "epoch": 0.740276946599742,
      "grad_norm": 0.5052096843719482,
      "learning_rate": 7.791691602007742e-05,
      "loss": 0.3866,
      "step": 214000
    },
    {
      "epoch": 0.7406228704065643,
      "grad_norm": 0.005485915578901768,
      "learning_rate": 7.781313887803071e-05,
      "loss": 0.2432,
      "step": 214100
    },
    {
      "epoch": 0.7409687942133866,
      "grad_norm": 0.0022354715038090944,
      "learning_rate": 7.770936173598402e-05,
      "loss": 0.2398,
      "step": 214200
    },
    {
      "epoch": 0.7413147180202089,
      "grad_norm": 0.0001818592572817579,
      "learning_rate": 7.760558459393733e-05,
      "loss": 0.2151,
      "step": 214300
    },
    {
      "epoch": 0.7416606418270312,
      "grad_norm": 9.431572914123535,
      "learning_rate": 7.750180745189064e-05,
      "loss": 0.1898,
      "step": 214400
    },
    {
      "epoch": 0.7420065656338535,
      "grad_norm": 0.0008298753527924418,
      "learning_rate": 7.739803030984396e-05,
      "loss": 0.1114,
      "step": 214500
    },
    {
      "epoch": 0.7423524894406758,
      "grad_norm": 0.0009329632157459855,
      "learning_rate": 7.729425316779725e-05,
      "loss": 0.1124,
      "step": 214600
    },
    {
      "epoch": 0.7426984132474981,
      "grad_norm": 8.298676402773708e-05,
      "learning_rate": 7.719047602575057e-05,
      "loss": 0.3689,
      "step": 214700
    },
    {
      "epoch": 0.7430443370543204,
      "grad_norm": 27.900747299194336,
      "learning_rate": 7.708669888370387e-05,
      "loss": 0.2928,
      "step": 214800
    },
    {
      "epoch": 0.7433902608611427,
      "grad_norm": 0.000266569055384025,
      "learning_rate": 7.698292174165718e-05,
      "loss": 0.2534,
      "step": 214900
    },
    {
      "epoch": 0.743736184667965,
      "grad_norm": 0.0015722826356068254,
      "learning_rate": 7.687914459961048e-05,
      "loss": 0.2018,
      "step": 215000
    },
    {
      "epoch": 0.7440821084747874,
      "grad_norm": 0.0016019674949347973,
      "learning_rate": 7.677536745756379e-05,
      "loss": 0.2174,
      "step": 215100
    },
    {
      "epoch": 0.7444280322816097,
      "grad_norm": 0.009801254607737064,
      "learning_rate": 7.66715903155171e-05,
      "loss": 0.3538,
      "step": 215200
    },
    {
      "epoch": 0.744773956088432,
      "grad_norm": 0.06509547680616379,
      "learning_rate": 7.656781317347041e-05,
      "loss": 0.2482,
      "step": 215300
    },
    {
      "epoch": 0.7451198798952543,
      "grad_norm": 0.6518085598945618,
      "learning_rate": 7.646403603142373e-05,
      "loss": 0.1336,
      "step": 215400
    },
    {
      "epoch": 0.7454658037020766,
      "grad_norm": 0.0010870336554944515,
      "learning_rate": 7.636025888937701e-05,
      "loss": 0.2723,
      "step": 215500
    },
    {
      "epoch": 0.7458117275088989,
      "grad_norm": 0.016249772161245346,
      "learning_rate": 7.625648174733032e-05,
      "loss": 0.2188,
      "step": 215600
    },
    {
      "epoch": 0.7461576513157212,
      "grad_norm": 0.0004993953043594956,
      "learning_rate": 7.615270460528364e-05,
      "loss": 0.3581,
      "step": 215700
    },
    {
      "epoch": 0.7465035751225435,
      "grad_norm": 38.335227966308594,
      "learning_rate": 7.604892746323694e-05,
      "loss": 0.2137,
      "step": 215800
    },
    {
      "epoch": 0.7468494989293658,
      "grad_norm": 0.0030900246929377317,
      "learning_rate": 7.594515032119025e-05,
      "loss": 0.211,
      "step": 215900
    },
    {
      "epoch": 0.7471954227361881,
      "grad_norm": 0.0014088342431932688,
      "learning_rate": 7.584137317914356e-05,
      "loss": 0.1519,
      "step": 216000
    },
    {
      "epoch": 0.7475413465430104,
      "grad_norm": 0.004816293250769377,
      "learning_rate": 7.573759603709687e-05,
      "loss": 0.2073,
      "step": 216100
    },
    {
      "epoch": 0.7478872703498327,
      "grad_norm": 44.02126693725586,
      "learning_rate": 7.563381889505016e-05,
      "loss": 0.1763,
      "step": 216200
    },
    {
      "epoch": 0.748233194156655,
      "grad_norm": 0.03863571584224701,
      "learning_rate": 7.553004175300347e-05,
      "loss": 0.2918,
      "step": 216300
    },
    {
      "epoch": 0.7485791179634773,
      "grad_norm": 0.002148422645404935,
      "learning_rate": 7.542626461095678e-05,
      "loss": 0.2528,
      "step": 216400
    },
    {
      "epoch": 0.7489250417702997,
      "grad_norm": 0.001524386228993535,
      "learning_rate": 7.53224874689101e-05,
      "loss": 0.3058,
      "step": 216500
    },
    {
      "epoch": 0.749270965577122,
      "grad_norm": 7.965688564581797e-05,
      "learning_rate": 7.521871032686341e-05,
      "loss": 0.2406,
      "step": 216600
    },
    {
      "epoch": 0.7496168893839443,
      "grad_norm": 11.125316619873047,
      "learning_rate": 7.51149331848167e-05,
      "loss": 0.2556,
      "step": 216700
    },
    {
      "epoch": 0.7499628131907666,
      "grad_norm": 0.0002410726301604882,
      "learning_rate": 7.501115604277002e-05,
      "loss": 0.2286,
      "step": 216800
    },
    {
      "epoch": 0.7503087369975889,
      "grad_norm": 0.005179993808269501,
      "learning_rate": 7.490737890072332e-05,
      "loss": 0.295,
      "step": 216900
    },
    {
      "epoch": 0.7506546608044112,
      "grad_norm": 0.0025560820940881968,
      "learning_rate": 7.480360175867663e-05,
      "loss": 0.1974,
      "step": 217000
    },
    {
      "epoch": 0.7510005846112335,
      "grad_norm": 0.0017362330108880997,
      "learning_rate": 7.469982461662993e-05,
      "loss": 0.2074,
      "step": 217100
    },
    {
      "epoch": 0.7513465084180558,
      "grad_norm": 3.0865955352783203,
      "learning_rate": 7.459604747458324e-05,
      "loss": 0.3709,
      "step": 217200
    },
    {
      "epoch": 0.7516924322248781,
      "grad_norm": 49.0001220703125,
      "learning_rate": 7.449227033253655e-05,
      "loss": 0.3665,
      "step": 217300
    },
    {
      "epoch": 0.7520383560317004,
      "grad_norm": 0.00574069656431675,
      "learning_rate": 7.438849319048985e-05,
      "loss": 0.2636,
      "step": 217400
    },
    {
      "epoch": 0.7523842798385227,
      "grad_norm": 0.0018873753724619746,
      "learning_rate": 7.428471604844316e-05,
      "loss": 0.1612,
      "step": 217500
    },
    {
      "epoch": 0.752730203645345,
      "grad_norm": 0.002044796012341976,
      "learning_rate": 7.418093890639648e-05,
      "loss": 0.1802,
      "step": 217600
    },
    {
      "epoch": 0.7530761274521673,
      "grad_norm": 0.05670496076345444,
      "learning_rate": 7.407716176434977e-05,
      "loss": 0.2252,
      "step": 217700
    },
    {
      "epoch": 0.7534220512589896,
      "grad_norm": 0.0018671860452741385,
      "learning_rate": 7.397338462230309e-05,
      "loss": 0.2561,
      "step": 217800
    },
    {
      "epoch": 0.7537679750658121,
      "grad_norm": 3.249644214520231e-05,
      "learning_rate": 7.38696074802564e-05,
      "loss": 0.2932,
      "step": 217900
    },
    {
      "epoch": 0.7541138988726344,
      "grad_norm": 0.00035228292108513415,
      "learning_rate": 7.37658303382097e-05,
      "loss": 0.183,
      "step": 218000
    },
    {
      "epoch": 0.7544598226794567,
      "grad_norm": 0.00031108714756555855,
      "learning_rate": 7.366205319616301e-05,
      "loss": 0.1967,
      "step": 218100
    },
    {
      "epoch": 0.754805746486279,
      "grad_norm": 12.240005493164062,
      "learning_rate": 7.355827605411631e-05,
      "loss": 0.2462,
      "step": 218200
    },
    {
      "epoch": 0.7551516702931013,
      "grad_norm": 0.08201313763856888,
      "learning_rate": 7.345449891206962e-05,
      "loss": 0.1236,
      "step": 218300
    },
    {
      "epoch": 0.7554975940999236,
      "grad_norm": 0.0035442195367068052,
      "learning_rate": 7.335072177002292e-05,
      "loss": 0.3872,
      "step": 218400
    },
    {
      "epoch": 0.7558435179067459,
      "grad_norm": 0.0015291663585230708,
      "learning_rate": 7.324694462797623e-05,
      "loss": 0.1212,
      "step": 218500
    },
    {
      "epoch": 0.7561894417135682,
      "grad_norm": 0.30238935351371765,
      "learning_rate": 7.314316748592955e-05,
      "loss": 0.1833,
      "step": 218600
    },
    {
      "epoch": 0.7565353655203905,
      "grad_norm": 0.014769608154892921,
      "learning_rate": 7.303939034388286e-05,
      "loss": 0.2831,
      "step": 218700
    },
    {
      "epoch": 0.7568812893272128,
      "grad_norm": 0.010992903262376785,
      "learning_rate": 7.293561320183616e-05,
      "loss": 0.223,
      "step": 218800
    },
    {
      "epoch": 0.7572272131340351,
      "grad_norm": 9.93606518022716e-05,
      "learning_rate": 7.283183605978947e-05,
      "loss": 0.4825,
      "step": 218900
    },
    {
      "epoch": 0.7575731369408574,
      "grad_norm": 67.41093444824219,
      "learning_rate": 7.272805891774277e-05,
      "loss": 0.2508,
      "step": 219000
    },
    {
      "epoch": 0.7579190607476797,
      "grad_norm": 0.006601318251341581,
      "learning_rate": 7.262428177569608e-05,
      "loss": 0.1542,
      "step": 219100
    },
    {
      "epoch": 0.758264984554502,
      "grad_norm": 0.00033704485394991934,
      "learning_rate": 7.252050463364938e-05,
      "loss": 0.3107,
      "step": 219200
    },
    {
      "epoch": 0.7586109083613244,
      "grad_norm": 9.273285832023248e-05,
      "learning_rate": 7.241672749160269e-05,
      "loss": 0.2755,
      "step": 219300
    },
    {
      "epoch": 0.7589568321681467,
      "grad_norm": 0.022268176078796387,
      "learning_rate": 7.2312950349556e-05,
      "loss": 0.2724,
      "step": 219400
    },
    {
      "epoch": 0.759302755974969,
      "grad_norm": 0.19671380519866943,
      "learning_rate": 7.22091732075093e-05,
      "loss": 0.2167,
      "step": 219500
    },
    {
      "epoch": 0.7596486797817913,
      "grad_norm": 10.36100959777832,
      "learning_rate": 7.210539606546261e-05,
      "loss": 0.2092,
      "step": 219600
    },
    {
      "epoch": 0.7599946035886136,
      "grad_norm": 0.0005530809867195785,
      "learning_rate": 7.200161892341593e-05,
      "loss": 0.195,
      "step": 219700
    },
    {
      "epoch": 0.7603405273954359,
      "grad_norm": 0.0032197609543800354,
      "learning_rate": 7.189784178136923e-05,
      "loss": 0.2613,
      "step": 219800
    },
    {
      "epoch": 0.7606864512022582,
      "grad_norm": 0.0013419652823358774,
      "learning_rate": 7.179406463932254e-05,
      "loss": 0.188,
      "step": 219900
    },
    {
      "epoch": 0.7610323750090805,
      "grad_norm": 0.004407864063978195,
      "learning_rate": 7.169028749727585e-05,
      "loss": 0.3485,
      "step": 220000
    },
    {
      "epoch": 0.7613782988159028,
      "grad_norm": 0.0008420795202255249,
      "learning_rate": 7.158651035522915e-05,
      "loss": 0.1824,
      "step": 220100
    },
    {
      "epoch": 0.7617242226227251,
      "grad_norm": 0.0013946222607046366,
      "learning_rate": 7.148273321318246e-05,
      "loss": 0.2024,
      "step": 220200
    },
    {
      "epoch": 0.7620701464295474,
      "grad_norm": 0.08225834369659424,
      "learning_rate": 7.137895607113576e-05,
      "loss": 0.3398,
      "step": 220300
    },
    {
      "epoch": 0.7624160702363697,
      "grad_norm": 0.001085363095626235,
      "learning_rate": 7.127517892908907e-05,
      "loss": 0.1274,
      "step": 220400
    },
    {
      "epoch": 0.762761994043192,
      "grad_norm": 0.0032706267666071653,
      "learning_rate": 7.117140178704237e-05,
      "loss": 0.2549,
      "step": 220500
    },
    {
      "epoch": 0.7631079178500143,
      "grad_norm": 0.006085409317165613,
      "learning_rate": 7.106762464499568e-05,
      "loss": 0.1765,
      "step": 220600
    },
    {
      "epoch": 0.7634538416568367,
      "grad_norm": 0.17406731843948364,
      "learning_rate": 7.0963847502949e-05,
      "loss": 0.2844,
      "step": 220700
    },
    {
      "epoch": 0.763799765463659,
      "grad_norm": 75.79027557373047,
      "learning_rate": 7.086007036090231e-05,
      "loss": 0.2731,
      "step": 220800
    },
    {
      "epoch": 0.7641456892704813,
      "grad_norm": 0.0012714313343167305,
      "learning_rate": 7.075629321885561e-05,
      "loss": 0.26,
      "step": 220900
    },
    {
      "epoch": 0.7644916130773036,
      "grad_norm": 0.00044925318798050284,
      "learning_rate": 7.065251607680892e-05,
      "loss": 0.2379,
      "step": 221000
    },
    {
      "epoch": 0.7648375368841259,
      "grad_norm": 0.0006926195346750319,
      "learning_rate": 7.054873893476223e-05,
      "loss": 0.2418,
      "step": 221100
    },
    {
      "epoch": 0.7651834606909482,
      "grad_norm": 5.157688617706299,
      "learning_rate": 7.044496179271553e-05,
      "loss": 0.2602,
      "step": 221200
    },
    {
      "epoch": 0.7655293844977705,
      "grad_norm": 0.001521992264315486,
      "learning_rate": 7.034118465066884e-05,
      "loss": 0.1294,
      "step": 221300
    },
    {
      "epoch": 0.7658753083045928,
      "grad_norm": 21.922142028808594,
      "learning_rate": 7.023740750862214e-05,
      "loss": 0.1669,
      "step": 221400
    },
    {
      "epoch": 0.7662212321114151,
      "grad_norm": 0.00014553661458194256,
      "learning_rate": 7.013363036657545e-05,
      "loss": 0.1894,
      "step": 221500
    },
    {
      "epoch": 0.7665671559182374,
      "grad_norm": 0.0002413301117485389,
      "learning_rate": 7.002985322452875e-05,
      "loss": 0.2107,
      "step": 221600
    },
    {
      "epoch": 0.7669130797250597,
      "grad_norm": 0.0017735708970576525,
      "learning_rate": 6.992607608248206e-05,
      "loss": 0.2158,
      "step": 221700
    },
    {
      "epoch": 0.767259003531882,
      "grad_norm": 0.00023810582933947444,
      "learning_rate": 6.982229894043538e-05,
      "loss": 0.3156,
      "step": 221800
    },
    {
      "epoch": 0.7676049273387043,
      "grad_norm": 0.04477521404623985,
      "learning_rate": 6.971852179838868e-05,
      "loss": 0.1154,
      "step": 221900
    },
    {
      "epoch": 0.7679508511455266,
      "grad_norm": 0.0003645620308816433,
      "learning_rate": 6.961474465634199e-05,
      "loss": 0.3554,
      "step": 222000
    },
    {
      "epoch": 0.768296774952349,
      "grad_norm": 1.8804144859313965,
      "learning_rate": 6.95109675142953e-05,
      "loss": 0.2043,
      "step": 222100
    },
    {
      "epoch": 0.7686426987591713,
      "grad_norm": 0.0012991930125281215,
      "learning_rate": 6.94071903722486e-05,
      "loss": 0.16,
      "step": 222200
    },
    {
      "epoch": 0.7689886225659937,
      "grad_norm": 46.971832275390625,
      "learning_rate": 6.930341323020191e-05,
      "loss": 0.2575,
      "step": 222300
    },
    {
      "epoch": 0.769334546372816,
      "grad_norm": 0.10072346776723862,
      "learning_rate": 6.919963608815521e-05,
      "loss": 0.2306,
      "step": 222400
    },
    {
      "epoch": 0.7696804701796383,
      "grad_norm": 0.0056852721609175205,
      "learning_rate": 6.909585894610852e-05,
      "loss": 0.2288,
      "step": 222500
    },
    {
      "epoch": 0.7700263939864606,
      "grad_norm": 0.06198904663324356,
      "learning_rate": 6.899208180406182e-05,
      "loss": 0.3018,
      "step": 222600
    },
    {
      "epoch": 0.7703723177932829,
      "grad_norm": 0.6945422291755676,
      "learning_rate": 6.888830466201513e-05,
      "loss": 0.2345,
      "step": 222700
    },
    {
      "epoch": 0.7707182416001052,
      "grad_norm": 0.045823369175195694,
      "learning_rate": 6.878452751996845e-05,
      "loss": 0.2286,
      "step": 222800
    },
    {
      "epoch": 0.7710641654069275,
      "grad_norm": 54.262062072753906,
      "learning_rate": 6.868075037792176e-05,
      "loss": 0.1955,
      "step": 222900
    },
    {
      "epoch": 0.7714100892137498,
      "grad_norm": 11.739410400390625,
      "learning_rate": 6.857697323587506e-05,
      "loss": 0.3286,
      "step": 223000
    },
    {
      "epoch": 0.7717560130205721,
      "grad_norm": 0.003340069204568863,
      "learning_rate": 6.847319609382837e-05,
      "loss": 0.189,
      "step": 223100
    },
    {
      "epoch": 0.7721019368273944,
      "grad_norm": 47.91822814941406,
      "learning_rate": 6.836941895178168e-05,
      "loss": 0.2733,
      "step": 223200
    },
    {
      "epoch": 0.7724478606342167,
      "grad_norm": 0.0008751861751079559,
      "learning_rate": 6.826564180973498e-05,
      "loss": 0.2289,
      "step": 223300
    },
    {
      "epoch": 0.772793784441039,
      "grad_norm": 0.00047980586532503366,
      "learning_rate": 6.81618646676883e-05,
      "loss": 0.2601,
      "step": 223400
    },
    {
      "epoch": 0.7731397082478614,
      "grad_norm": 1.3008924722671509,
      "learning_rate": 6.805808752564159e-05,
      "loss": 0.1272,
      "step": 223500
    },
    {
      "epoch": 0.7734856320546837,
      "grad_norm": 0.0003839691053144634,
      "learning_rate": 6.79543103835949e-05,
      "loss": 0.1904,
      "step": 223600
    },
    {
      "epoch": 0.773831555861506,
      "grad_norm": 0.024487195536494255,
      "learning_rate": 6.78505332415482e-05,
      "loss": 0.2618,
      "step": 223700
    },
    {
      "epoch": 0.7741774796683283,
      "grad_norm": 0.0011421386152505875,
      "learning_rate": 6.774675609950152e-05,
      "loss": 0.1729,
      "step": 223800
    },
    {
      "epoch": 0.7745234034751506,
      "grad_norm": 0.0005153499078005552,
      "learning_rate": 6.764297895745483e-05,
      "loss": 0.1646,
      "step": 223900
    },
    {
      "epoch": 0.7748693272819729,
      "grad_norm": 16.658462524414062,
      "learning_rate": 6.753920181540813e-05,
      "loss": 0.3089,
      "step": 224000
    },
    {
      "epoch": 0.7752152510887952,
      "grad_norm": 0.0038653796073049307,
      "learning_rate": 6.743542467336144e-05,
      "loss": 0.1863,
      "step": 224100
    },
    {
      "epoch": 0.7755611748956175,
      "grad_norm": 0.00044359112507663667,
      "learning_rate": 6.733164753131475e-05,
      "loss": 0.1721,
      "step": 224200
    },
    {
      "epoch": 0.7759070987024398,
      "grad_norm": 0.001174387289211154,
      "learning_rate": 6.722787038926806e-05,
      "loss": 0.3697,
      "step": 224300
    },
    {
      "epoch": 0.7762530225092621,
      "grad_norm": 1.7164560556411743,
      "learning_rate": 6.712409324722136e-05,
      "loss": 0.2698,
      "step": 224400
    },
    {
      "epoch": 0.7765989463160844,
      "grad_norm": 0.0004973592003807425,
      "learning_rate": 6.702031610517466e-05,
      "loss": 0.175,
      "step": 224500
    },
    {
      "epoch": 0.7769448701229067,
      "grad_norm": 0.0004561817040666938,
      "learning_rate": 6.691653896312797e-05,
      "loss": 0.2569,
      "step": 224600
    },
    {
      "epoch": 0.777290793929729,
      "grad_norm": 0.0003453116223681718,
      "learning_rate": 6.681276182108127e-05,
      "loss": 0.1566,
      "step": 224700
    },
    {
      "epoch": 0.7776367177365513,
      "grad_norm": 11.383289337158203,
      "learning_rate": 6.670898467903458e-05,
      "loss": 0.2457,
      "step": 224800
    },
    {
      "epoch": 0.7779826415433737,
      "grad_norm": 8.553319931030273,
      "learning_rate": 6.66052075369879e-05,
      "loss": 0.2203,
      "step": 224900
    },
    {
      "epoch": 0.778328565350196,
      "grad_norm": 0.006386553402990103,
      "learning_rate": 6.650143039494121e-05,
      "loss": 0.2242,
      "step": 225000
    },
    {
      "epoch": 0.7786744891570183,
      "grad_norm": 0.0015056601259857416,
      "learning_rate": 6.639765325289451e-05,
      "loss": 0.2032,
      "step": 225100
    },
    {
      "epoch": 0.7790204129638406,
      "grad_norm": 0.0010812714463099837,
      "learning_rate": 6.629387611084782e-05,
      "loss": 0.1823,
      "step": 225200
    },
    {
      "epoch": 0.7793663367706629,
      "grad_norm": 45.734771728515625,
      "learning_rate": 6.619009896880113e-05,
      "loss": 0.3428,
      "step": 225300
    },
    {
      "epoch": 0.7797122605774852,
      "grad_norm": 11.562689781188965,
      "learning_rate": 6.608632182675443e-05,
      "loss": 0.1418,
      "step": 225400
    },
    {
      "epoch": 0.7800581843843075,
      "grad_norm": 1.3044097423553467,
      "learning_rate": 6.598254468470774e-05,
      "loss": 0.1677,
      "step": 225500
    },
    {
      "epoch": 0.7804041081911298,
      "grad_norm": 0.0014592773513868451,
      "learning_rate": 6.587876754266104e-05,
      "loss": 0.0983,
      "step": 225600
    },
    {
      "epoch": 0.7807500319979521,
      "grad_norm": 10.521354675292969,
      "learning_rate": 6.577499040061436e-05,
      "loss": 0.1524,
      "step": 225700
    },
    {
      "epoch": 0.7810959558047744,
      "grad_norm": 7.856025331420824e-05,
      "learning_rate": 6.567121325856765e-05,
      "loss": 0.2904,
      "step": 225800
    },
    {
      "epoch": 0.7814418796115967,
      "grad_norm": 0.009817413054406643,
      "learning_rate": 6.556743611652097e-05,
      "loss": 0.2042,
      "step": 225900
    },
    {
      "epoch": 0.781787803418419,
      "grad_norm": 26.23250961303711,
      "learning_rate": 6.546365897447428e-05,
      "loss": 0.2034,
      "step": 226000
    },
    {
      "epoch": 0.7821337272252413,
      "grad_norm": 0.003820032114163041,
      "learning_rate": 6.535988183242758e-05,
      "loss": 0.1976,
      "step": 226100
    },
    {
      "epoch": 0.7824796510320636,
      "grad_norm": 0.20521558821201324,
      "learning_rate": 6.525610469038089e-05,
      "loss": 0.3291,
      "step": 226200
    },
    {
      "epoch": 0.782825574838886,
      "grad_norm": 0.0007373189437203109,
      "learning_rate": 6.51523275483342e-05,
      "loss": 0.2832,
      "step": 226300
    },
    {
      "epoch": 0.7831714986457083,
      "grad_norm": 0.00045865471474826336,
      "learning_rate": 6.504855040628751e-05,
      "loss": 0.2823,
      "step": 226400
    },
    {
      "epoch": 0.7835174224525306,
      "grad_norm": 0.01092834398150444,
      "learning_rate": 6.494477326424081e-05,
      "loss": 0.2342,
      "step": 226500
    },
    {
      "epoch": 0.7838633462593529,
      "grad_norm": 0.0014809862477704883,
      "learning_rate": 6.484099612219413e-05,
      "loss": 0.129,
      "step": 226600
    },
    {
      "epoch": 0.7842092700661752,
      "grad_norm": 0.0030797484796494246,
      "learning_rate": 6.473721898014742e-05,
      "loss": 0.3304,
      "step": 226700
    },
    {
      "epoch": 0.7845551938729975,
      "grad_norm": 0.0018231065478175879,
      "learning_rate": 6.463344183810074e-05,
      "loss": 0.344,
      "step": 226800
    },
    {
      "epoch": 0.7849011176798198,
      "grad_norm": 0.0008229183149524033,
      "learning_rate": 6.452966469605404e-05,
      "loss": 0.2561,
      "step": 226900
    },
    {
      "epoch": 0.7852470414866421,
      "grad_norm": 1.2503873109817505,
      "learning_rate": 6.442588755400735e-05,
      "loss": 0.1892,
      "step": 227000
    },
    {
      "epoch": 0.7855929652934645,
      "grad_norm": 0.004181527998298407,
      "learning_rate": 6.432211041196066e-05,
      "loss": 0.1597,
      "step": 227100
    },
    {
      "epoch": 0.7859388891002868,
      "grad_norm": 80.43511962890625,
      "learning_rate": 6.421833326991396e-05,
      "loss": 0.1624,
      "step": 227200
    },
    {
      "epoch": 0.786284812907109,
      "grad_norm": 0.0014728654641658068,
      "learning_rate": 6.411455612786727e-05,
      "loss": 0.2087,
      "step": 227300
    },
    {
      "epoch": 0.7866307367139314,
      "grad_norm": 51.21416091918945,
      "learning_rate": 6.401077898582058e-05,
      "loss": 0.4156,
      "step": 227400
    },
    {
      "epoch": 0.7869766605207537,
      "grad_norm": 0.0023074329365044832,
      "learning_rate": 6.390700184377388e-05,
      "loss": 0.2607,
      "step": 227500
    },
    {
      "epoch": 0.787322584327576,
      "grad_norm": 0.0002994333044625819,
      "learning_rate": 6.38032247017272e-05,
      "loss": 0.1936,
      "step": 227600
    },
    {
      "epoch": 0.7876685081343984,
      "grad_norm": 0.004528034944087267,
      "learning_rate": 6.36994475596805e-05,
      "loss": 0.1583,
      "step": 227700
    },
    {
      "epoch": 0.7880144319412207,
      "grad_norm": 0.0007801902247592807,
      "learning_rate": 6.35956704176338e-05,
      "loss": 0.2153,
      "step": 227800
    },
    {
      "epoch": 0.788360355748043,
      "grad_norm": 0.00015575485303997993,
      "learning_rate": 6.34918932755871e-05,
      "loss": 0.3878,
      "step": 227900
    },
    {
      "epoch": 0.7887062795548653,
      "grad_norm": 93.267578125,
      "learning_rate": 6.338811613354042e-05,
      "loss": 0.1884,
      "step": 228000
    },
    {
      "epoch": 0.7890522033616876,
      "grad_norm": 0.0015012708026915789,
      "learning_rate": 6.328433899149373e-05,
      "loss": 0.2886,
      "step": 228100
    },
    {
      "epoch": 0.7893981271685099,
      "grad_norm": 0.0006583875510841608,
      "learning_rate": 6.318056184944703e-05,
      "loss": 0.2292,
      "step": 228200
    },
    {
      "epoch": 0.7897440509753322,
      "grad_norm": 49.842411041259766,
      "learning_rate": 6.307678470740034e-05,
      "loss": 0.3327,
      "step": 228300
    },
    {
      "epoch": 0.7900899747821545,
      "grad_norm": 0.5534716844558716,
      "learning_rate": 6.297300756535365e-05,
      "loss": 0.3092,
      "step": 228400
    },
    {
      "epoch": 0.7904358985889768,
      "grad_norm": 16.7432861328125,
      "learning_rate": 6.286923042330696e-05,
      "loss": 0.2275,
      "step": 228500
    },
    {
      "epoch": 0.7907818223957991,
      "grad_norm": 0.007486630696803331,
      "learning_rate": 6.276545328126026e-05,
      "loss": 0.2157,
      "step": 228600
    },
    {
      "epoch": 0.7911277462026214,
      "grad_norm": 0.0002815971674863249,
      "learning_rate": 6.266167613921358e-05,
      "loss": 0.1923,
      "step": 228700
    },
    {
      "epoch": 0.7914736700094437,
      "grad_norm": 0.00023306036018766463,
      "learning_rate": 6.255789899716687e-05,
      "loss": 0.2274,
      "step": 228800
    },
    {
      "epoch": 0.791819593816266,
      "grad_norm": 0.00021128071239218116,
      "learning_rate": 6.245412185512019e-05,
      "loss": 0.1936,
      "step": 228900
    },
    {
      "epoch": 0.7921655176230883,
      "grad_norm": 0.00020402306108735502,
      "learning_rate": 6.235034471307349e-05,
      "loss": 0.1512,
      "step": 229000
    },
    {
      "epoch": 0.7925114414299107,
      "grad_norm": 0.00037176013574935496,
      "learning_rate": 6.22465675710268e-05,
      "loss": 0.1218,
      "step": 229100
    },
    {
      "epoch": 0.792857365236733,
      "grad_norm": 0.007934873923659325,
      "learning_rate": 6.214279042898011e-05,
      "loss": 0.152,
      "step": 229200
    },
    {
      "epoch": 0.7932032890435553,
      "grad_norm": 0.0013386880746111274,
      "learning_rate": 6.203901328693341e-05,
      "loss": 0.1874,
      "step": 229300
    },
    {
      "epoch": 0.7935492128503776,
      "grad_norm": 0.0335087925195694,
      "learning_rate": 6.193523614488672e-05,
      "loss": 0.1838,
      "step": 229400
    },
    {
      "epoch": 0.7938951366571999,
      "grad_norm": 0.020354807376861572,
      "learning_rate": 6.183145900284003e-05,
      "loss": 0.1654,
      "step": 229500
    },
    {
      "epoch": 0.7942410604640222,
      "grad_norm": 0.0002636263961903751,
      "learning_rate": 6.172768186079333e-05,
      "loss": 0.276,
      "step": 229600
    },
    {
      "epoch": 0.7945869842708445,
      "grad_norm": 0.00043029943481087685,
      "learning_rate": 6.162390471874665e-05,
      "loss": 0.1519,
      "step": 229700
    },
    {
      "epoch": 0.7949329080776668,
      "grad_norm": 0.0014476559590548277,
      "learning_rate": 6.152012757669996e-05,
      "loss": 0.227,
      "step": 229800
    },
    {
      "epoch": 0.7952788318844891,
      "grad_norm": 118.52800750732422,
      "learning_rate": 6.141635043465326e-05,
      "loss": 0.2675,
      "step": 229900
    },
    {
      "epoch": 0.7956247556913114,
      "grad_norm": 0.01377637218683958,
      "learning_rate": 6.131257329260657e-05,
      "loss": 0.1573,
      "step": 230000
    },
    {
      "epoch": 0.7959706794981337,
      "grad_norm": 0.00021946406923234463,
      "learning_rate": 6.120879615055987e-05,
      "loss": 0.2079,
      "step": 230100
    },
    {
      "epoch": 0.796316603304956,
      "grad_norm": 0.0015076770214363933,
      "learning_rate": 6.110501900851318e-05,
      "loss": 0.2231,
      "step": 230200
    },
    {
      "epoch": 0.7966625271117783,
      "grad_norm": 0.10661277920007706,
      "learning_rate": 6.1001241866466485e-05,
      "loss": 0.2076,
      "step": 230300
    },
    {
      "epoch": 0.7970084509186006,
      "grad_norm": 0.0023633926175534725,
      "learning_rate": 6.089746472441979e-05,
      "loss": 0.1923,
      "step": 230400
    },
    {
      "epoch": 0.797354374725423,
      "grad_norm": 0.00041770999087020755,
      "learning_rate": 6.07936875823731e-05,
      "loss": 0.0495,
      "step": 230500
    },
    {
      "epoch": 0.7977002985322453,
      "grad_norm": 0.000693001551553607,
      "learning_rate": 6.068991044032641e-05,
      "loss": 0.1569,
      "step": 230600
    },
    {
      "epoch": 0.7980462223390676,
      "grad_norm": 52.76498031616211,
      "learning_rate": 6.0586133298279714e-05,
      "loss": 0.2575,
      "step": 230700
    },
    {
      "epoch": 0.7983921461458899,
      "grad_norm": 36.15171813964844,
      "learning_rate": 6.048235615623302e-05,
      "loss": 0.2854,
      "step": 230800
    },
    {
      "epoch": 0.7987380699527122,
      "grad_norm": 9.015192335937172e-05,
      "learning_rate": 6.037857901418633e-05,
      "loss": 0.252,
      "step": 230900
    },
    {
      "epoch": 0.7990839937595345,
      "grad_norm": 0.001971163786947727,
      "learning_rate": 6.027480187213963e-05,
      "loss": 0.2205,
      "step": 231000
    },
    {
      "epoch": 0.7994299175663568,
      "grad_norm": 0.003218619851395488,
      "learning_rate": 6.017102473009294e-05,
      "loss": 0.1696,
      "step": 231100
    },
    {
      "epoch": 0.7997758413731791,
      "grad_norm": 0.0003835346142295748,
      "learning_rate": 6.006724758804625e-05,
      "loss": 0.4237,
      "step": 231200
    },
    {
      "epoch": 0.8001217651800014,
      "grad_norm": 0.003018587129190564,
      "learning_rate": 5.996347044599956e-05,
      "loss": 0.2391,
      "step": 231300
    },
    {
      "epoch": 0.8004676889868237,
      "grad_norm": 1.0899728536605835,
      "learning_rate": 5.985969330395286e-05,
      "loss": 0.1285,
      "step": 231400
    },
    {
      "epoch": 0.800813612793646,
      "grad_norm": 0.1811642348766327,
      "learning_rate": 5.975591616190617e-05,
      "loss": 0.1383,
      "step": 231500
    },
    {
      "epoch": 0.8011595366004683,
      "grad_norm": 0.01575525477528572,
      "learning_rate": 5.9652139019859485e-05,
      "loss": 0.1322,
      "step": 231600
    },
    {
      "epoch": 0.8015054604072906,
      "grad_norm": 0.000263979280134663,
      "learning_rate": 5.954836187781279e-05,
      "loss": 0.2716,
      "step": 231700
    },
    {
      "epoch": 0.801851384214113,
      "grad_norm": 9.213150042342022e-05,
      "learning_rate": 5.9444584735766096e-05,
      "loss": 0.1621,
      "step": 231800
    },
    {
      "epoch": 0.8021973080209354,
      "grad_norm": 0.0004787865618709475,
      "learning_rate": 5.93408075937194e-05,
      "loss": 0.1462,
      "step": 231900
    },
    {
      "epoch": 0.8025432318277577,
      "grad_norm": 0.0008605228504166007,
      "learning_rate": 5.9237030451672714e-05,
      "loss": 0.3516,
      "step": 232000
    },
    {
      "epoch": 0.80288915563458,
      "grad_norm": 0.09859530627727509,
      "learning_rate": 5.913325330962601e-05,
      "loss": 0.1083,
      "step": 232100
    },
    {
      "epoch": 0.8032350794414023,
      "grad_norm": 0.0032248101197183132,
      "learning_rate": 5.9029476167579325e-05,
      "loss": 0.2534,
      "step": 232200
    },
    {
      "epoch": 0.8035810032482246,
      "grad_norm": 0.001074924715794623,
      "learning_rate": 5.892569902553263e-05,
      "loss": 0.2427,
      "step": 232300
    },
    {
      "epoch": 0.8039269270550469,
      "grad_norm": 0.0032934423070400953,
      "learning_rate": 5.882192188348594e-05,
      "loss": 0.251,
      "step": 232400
    },
    {
      "epoch": 0.8042728508618692,
      "grad_norm": 1.0062391757965088,
      "learning_rate": 5.871814474143924e-05,
      "loss": 0.1078,
      "step": 232500
    },
    {
      "epoch": 0.8046187746686915,
      "grad_norm": 35.98649597167969,
      "learning_rate": 5.8614367599392554e-05,
      "loss": 0.2823,
      "step": 232600
    },
    {
      "epoch": 0.8049646984755138,
      "grad_norm": 0.00043911661487072706,
      "learning_rate": 5.8510590457345866e-05,
      "loss": 0.2893,
      "step": 232700
    },
    {
      "epoch": 0.8053106222823361,
      "grad_norm": 73.97411346435547,
      "learning_rate": 5.8406813315299165e-05,
      "loss": 0.2109,
      "step": 232800
    },
    {
      "epoch": 0.8056565460891584,
      "grad_norm": 0.0003193213779013604,
      "learning_rate": 5.830303617325247e-05,
      "loss": 0.1751,
      "step": 232900
    },
    {
      "epoch": 0.8060024698959807,
      "grad_norm": 0.0013182259863242507,
      "learning_rate": 5.819925903120578e-05,
      "loss": 0.1694,
      "step": 233000
    },
    {
      "epoch": 0.806348393702803,
      "grad_norm": 0.004141791723668575,
      "learning_rate": 5.8095481889159095e-05,
      "loss": 0.2412,
      "step": 233100
    },
    {
      "epoch": 0.8066943175096253,
      "grad_norm": 0.012351453304290771,
      "learning_rate": 5.7991704747112394e-05,
      "loss": 0.238,
      "step": 233200
    },
    {
      "epoch": 0.8070402413164477,
      "grad_norm": 0.28429070115089417,
      "learning_rate": 5.7887927605065706e-05,
      "loss": 0.0978,
      "step": 233300
    },
    {
      "epoch": 0.80738616512327,
      "grad_norm": 0.00014417152851819992,
      "learning_rate": 5.778415046301901e-05,
      "loss": 0.0776,
      "step": 233400
    },
    {
      "epoch": 0.8077320889300923,
      "grad_norm": 1.1111897230148315,
      "learning_rate": 5.768037332097232e-05,
      "loss": 0.175,
      "step": 233500
    },
    {
      "epoch": 0.8080780127369146,
      "grad_norm": 0.008359044790267944,
      "learning_rate": 5.757659617892562e-05,
      "loss": 0.3327,
      "step": 233600
    },
    {
      "epoch": 0.8084239365437369,
      "grad_norm": 91.1173324584961,
      "learning_rate": 5.7472819036878935e-05,
      "loss": 0.1504,
      "step": 233700
    },
    {
      "epoch": 0.8087698603505592,
      "grad_norm": 0.0023594985250383615,
      "learning_rate": 5.736904189483224e-05,
      "loss": 0.2996,
      "step": 233800
    },
    {
      "epoch": 0.8091157841573815,
      "grad_norm": 0.004649089649319649,
      "learning_rate": 5.7265264752785546e-05,
      "loss": 0.2582,
      "step": 233900
    },
    {
      "epoch": 0.8094617079642038,
      "grad_norm": 0.0513446182012558,
      "learning_rate": 5.716148761073885e-05,
      "loss": 0.2955,
      "step": 234000
    },
    {
      "epoch": 0.8098076317710261,
      "grad_norm": 0.0016221364494413137,
      "learning_rate": 5.7057710468692164e-05,
      "loss": 0.3066,
      "step": 234100
    },
    {
      "epoch": 0.8101535555778484,
      "grad_norm": 0.0003909717779606581,
      "learning_rate": 5.695393332664546e-05,
      "loss": 0.1317,
      "step": 234200
    },
    {
      "epoch": 0.8104994793846707,
      "grad_norm": 0.004179335664957762,
      "learning_rate": 5.6850156184598775e-05,
      "loss": 0.2534,
      "step": 234300
    },
    {
      "epoch": 0.810845403191493,
      "grad_norm": 0.009886875748634338,
      "learning_rate": 5.674637904255208e-05,
      "loss": 0.2603,
      "step": 234400
    },
    {
      "epoch": 0.8111913269983153,
      "grad_norm": 0.01170943770557642,
      "learning_rate": 5.664260190050539e-05,
      "loss": 0.2126,
      "step": 234500
    },
    {
      "epoch": 0.8115372508051376,
      "grad_norm": 0.005752312485128641,
      "learning_rate": 5.653882475845869e-05,
      "loss": 0.2497,
      "step": 234600
    },
    {
      "epoch": 0.81188317461196,
      "grad_norm": 0.045203469693660736,
      "learning_rate": 5.6435047616412004e-05,
      "loss": 0.0947,
      "step": 234700
    },
    {
      "epoch": 0.8122290984187823,
      "grad_norm": 0.0002465414290782064,
      "learning_rate": 5.633127047436532e-05,
      "loss": 0.2858,
      "step": 234800
    },
    {
      "epoch": 0.8125750222256046,
      "grad_norm": 35.509674072265625,
      "learning_rate": 5.6227493332318616e-05,
      "loss": 0.3467,
      "step": 234900
    },
    {
      "epoch": 0.8129209460324269,
      "grad_norm": 0.001996993087232113,
      "learning_rate": 5.612371619027193e-05,
      "loss": 0.1879,
      "step": 235000
    },
    {
      "epoch": 0.8132668698392492,
      "grad_norm": 0.00037170431460253894,
      "learning_rate": 5.6019939048225233e-05,
      "loss": 0.2082,
      "step": 235100
    },
    {
      "epoch": 0.8136127936460715,
      "grad_norm": 0.00022155266196932644,
      "learning_rate": 5.5916161906178546e-05,
      "loss": 0.1165,
      "step": 235200
    },
    {
      "epoch": 0.8139587174528938,
      "grad_norm": 0.001223566709086299,
      "learning_rate": 5.5812384764131845e-05,
      "loss": 0.2852,
      "step": 235300
    },
    {
      "epoch": 0.8143046412597161,
      "grad_norm": 0.15758588910102844,
      "learning_rate": 5.570860762208516e-05,
      "loss": 0.195,
      "step": 235400
    },
    {
      "epoch": 0.8146505650665384,
      "grad_norm": 0.00026339213945902884,
      "learning_rate": 5.560483048003846e-05,
      "loss": 0.1917,
      "step": 235500
    },
    {
      "epoch": 0.8149964888733607,
      "grad_norm": 117.9399642944336,
      "learning_rate": 5.550105333799177e-05,
      "loss": 0.3161,
      "step": 235600
    },
    {
      "epoch": 0.815342412680183,
      "grad_norm": 8.947492599487305,
      "learning_rate": 5.5397276195945074e-05,
      "loss": 0.2339,
      "step": 235700
    },
    {
      "epoch": 0.8156883364870053,
      "grad_norm": 0.0007790864910930395,
      "learning_rate": 5.5293499053898386e-05,
      "loss": 0.2372,
      "step": 235800
    },
    {
      "epoch": 0.8160342602938276,
      "grad_norm": 16.384920120239258,
      "learning_rate": 5.518972191185169e-05,
      "loss": 0.3443,
      "step": 235900
    },
    {
      "epoch": 0.8163801841006499,
      "grad_norm": 0.0036481006536632776,
      "learning_rate": 5.5085944769805e-05,
      "loss": 0.1228,
      "step": 236000
    },
    {
      "epoch": 0.8167261079074724,
      "grad_norm": 0.00231760973110795,
      "learning_rate": 5.49821676277583e-05,
      "loss": 0.0867,
      "step": 236100
    },
    {
      "epoch": 0.8170720317142947,
      "grad_norm": 0.006230388302356005,
      "learning_rate": 5.4878390485711615e-05,
      "loss": 0.174,
      "step": 236200
    },
    {
      "epoch": 0.817417955521117,
      "grad_norm": 0.0008781712385825813,
      "learning_rate": 5.4774613343664914e-05,
      "loss": 0.2179,
      "step": 236300
    },
    {
      "epoch": 0.8177638793279393,
      "grad_norm": 0.0012008077464997768,
      "learning_rate": 5.4670836201618226e-05,
      "loss": 0.2371,
      "step": 236400
    },
    {
      "epoch": 0.8181098031347616,
      "grad_norm": 0.00015428724873345345,
      "learning_rate": 5.456705905957154e-05,
      "loss": 0.1458,
      "step": 236500
    },
    {
      "epoch": 0.8184557269415839,
      "grad_norm": 0.0003470320953056216,
      "learning_rate": 5.4463281917524844e-05,
      "loss": 0.2165,
      "step": 236600
    },
    {
      "epoch": 0.8188016507484062,
      "grad_norm": 9.222495282301679e-05,
      "learning_rate": 5.435950477547815e-05,
      "loss": 0.0646,
      "step": 236700
    },
    {
      "epoch": 0.8191475745552285,
      "grad_norm": 0.0006155341980047524,
      "learning_rate": 5.4255727633431455e-05,
      "loss": 0.2626,
      "step": 236800
    },
    {
      "epoch": 0.8194934983620508,
      "grad_norm": 19.171951293945312,
      "learning_rate": 5.415195049138477e-05,
      "loss": 0.166,
      "step": 236900
    },
    {
      "epoch": 0.8198394221688731,
      "grad_norm": 0.0038857513573020697,
      "learning_rate": 5.4048173349338066e-05,
      "loss": 0.2816,
      "step": 237000
    },
    {
      "epoch": 0.8201853459756954,
      "grad_norm": 9.879591380013153e-05,
      "learning_rate": 5.394439620729138e-05,
      "loss": 0.1679,
      "step": 237100
    },
    {
      "epoch": 0.8205312697825177,
      "grad_norm": 0.0010819496819749475,
      "learning_rate": 5.3840619065244684e-05,
      "loss": 0.2426,
      "step": 237200
    },
    {
      "epoch": 0.82087719358934,
      "grad_norm": 0.00024075455439742655,
      "learning_rate": 5.3736841923197996e-05,
      "loss": 0.2487,
      "step": 237300
    },
    {
      "epoch": 0.8212231173961623,
      "grad_norm": 0.00032863105298019946,
      "learning_rate": 5.3633064781151295e-05,
      "loss": 0.1659,
      "step": 237400
    },
    {
      "epoch": 0.8215690412029847,
      "grad_norm": 0.00043726706644520164,
      "learning_rate": 5.352928763910461e-05,
      "loss": 0.2216,
      "step": 237500
    },
    {
      "epoch": 0.821914965009807,
      "grad_norm": 0.00020797930483240634,
      "learning_rate": 5.342551049705791e-05,
      "loss": 0.1352,
      "step": 237600
    },
    {
      "epoch": 0.8222608888166293,
      "grad_norm": 0.0012283949181437492,
      "learning_rate": 5.332173335501122e-05,
      "loss": 0.221,
      "step": 237700
    },
    {
      "epoch": 0.8226068126234516,
      "grad_norm": 0.0005392262828536332,
      "learning_rate": 5.3217956212964524e-05,
      "loss": 0.3019,
      "step": 237800
    },
    {
      "epoch": 0.8229527364302739,
      "grad_norm": 0.003455208148807287,
      "learning_rate": 5.3114179070917837e-05,
      "loss": 0.1298,
      "step": 237900
    },
    {
      "epoch": 0.8232986602370962,
      "grad_norm": 0.030192244797945023,
      "learning_rate": 5.301040192887115e-05,
      "loss": 0.1317,
      "step": 238000
    },
    {
      "epoch": 0.8236445840439185,
      "grad_norm": 0.03631985932588577,
      "learning_rate": 5.290662478682445e-05,
      "loss": 0.3341,
      "step": 238100
    },
    {
      "epoch": 0.8239905078507408,
      "grad_norm": 0.00029581593116745353,
      "learning_rate": 5.280284764477776e-05,
      "loss": 0.1509,
      "step": 238200
    },
    {
      "epoch": 0.8243364316575631,
      "grad_norm": 0.0006937307771295309,
      "learning_rate": 5.2699070502731066e-05,
      "loss": 0.2014,
      "step": 238300
    },
    {
      "epoch": 0.8246823554643854,
      "grad_norm": 7.767304487060755e-05,
      "learning_rate": 5.259529336068437e-05,
      "loss": 0.3077,
      "step": 238400
    },
    {
      "epoch": 0.8250282792712077,
      "grad_norm": 0.0015148129314184189,
      "learning_rate": 5.249151621863768e-05,
      "loss": 0.1799,
      "step": 238500
    },
    {
      "epoch": 0.82537420307803,
      "grad_norm": 16.592823028564453,
      "learning_rate": 5.238773907659099e-05,
      "loss": 0.1453,
      "step": 238600
    },
    {
      "epoch": 0.8257201268848523,
      "grad_norm": 0.0004401032638270408,
      "learning_rate": 5.2283961934544295e-05,
      "loss": 0.1595,
      "step": 238700
    },
    {
      "epoch": 0.8260660506916746,
      "grad_norm": 0.0001792948751244694,
      "learning_rate": 5.21801847924976e-05,
      "loss": 0.2451,
      "step": 238800
    },
    {
      "epoch": 0.826411974498497,
      "grad_norm": 8.841188537189737e-05,
      "learning_rate": 5.2076407650450906e-05,
      "loss": 0.124,
      "step": 238900
    },
    {
      "epoch": 0.8267578983053193,
      "grad_norm": 15.233600616455078,
      "learning_rate": 5.197263050840422e-05,
      "loss": 0.2887,
      "step": 239000
    },
    {
      "epoch": 0.8271038221121416,
      "grad_norm": 0.00038683321326971054,
      "learning_rate": 5.186885336635752e-05,
      "loss": 0.2003,
      "step": 239100
    },
    {
      "epoch": 0.8274497459189639,
      "grad_norm": 0.006614866200834513,
      "learning_rate": 5.176507622431083e-05,
      "loss": 0.1645,
      "step": 239200
    },
    {
      "epoch": 0.8277956697257862,
      "grad_norm": 0.005901148542761803,
      "learning_rate": 5.1661299082264135e-05,
      "loss": 0.2084,
      "step": 239300
    },
    {
      "epoch": 0.8281415935326085,
      "grad_norm": 0.00025377943529747427,
      "learning_rate": 5.155752194021745e-05,
      "loss": 0.206,
      "step": 239400
    },
    {
      "epoch": 0.8284875173394308,
      "grad_norm": 0.011697886511683464,
      "learning_rate": 5.1453744798170746e-05,
      "loss": 0.2907,
      "step": 239500
    },
    {
      "epoch": 0.8288334411462531,
      "grad_norm": 0.0013246486196294427,
      "learning_rate": 5.134996765612406e-05,
      "loss": 0.1841,
      "step": 239600
    },
    {
      "epoch": 0.8291793649530754,
      "grad_norm": 0.006217782851308584,
      "learning_rate": 5.124619051407737e-05,
      "loss": 0.128,
      "step": 239700
    },
    {
      "epoch": 0.8295252887598977,
      "grad_norm": 1.465474009513855,
      "learning_rate": 5.114241337203067e-05,
      "loss": 0.2294,
      "step": 239800
    },
    {
      "epoch": 0.82987121256672,
      "grad_norm": 14.370424270629883,
      "learning_rate": 5.103863622998398e-05,
      "loss": 0.2982,
      "step": 239900
    },
    {
      "epoch": 0.8302171363735423,
      "grad_norm": 0.0004916759207844734,
      "learning_rate": 5.093485908793729e-05,
      "loss": 0.0804,
      "step": 240000
    },
    {
      "epoch": 0.8305630601803646,
      "grad_norm": 20.583118438720703,
      "learning_rate": 5.08310819458906e-05,
      "loss": 0.0971,
      "step": 240100
    },
    {
      "epoch": 0.8309089839871869,
      "grad_norm": 0.0006829105550423265,
      "learning_rate": 5.07273048038439e-05,
      "loss": 0.1737,
      "step": 240200
    },
    {
      "epoch": 0.8312549077940093,
      "grad_norm": 0.00025195343187078834,
      "learning_rate": 5.062352766179721e-05,
      "loss": 0.0906,
      "step": 240300
    },
    {
      "epoch": 0.8316008316008316,
      "grad_norm": 0.002477537374943495,
      "learning_rate": 5.0519750519750516e-05,
      "loss": 0.0925,
      "step": 240400
    },
    {
      "epoch": 0.831946755407654,
      "grad_norm": 0.0007970149163156748,
      "learning_rate": 5.041597337770382e-05,
      "loss": 0.1711,
      "step": 240500
    },
    {
      "epoch": 0.8322926792144762,
      "grad_norm": 3.214938163757324,
      "learning_rate": 5.031219623565713e-05,
      "loss": 0.2681,
      "step": 240600
    },
    {
      "epoch": 0.8326386030212986,
      "grad_norm": 0.000238549240748398,
      "learning_rate": 5.020841909361044e-05,
      "loss": 0.3694,
      "step": 240700
    },
    {
      "epoch": 0.8329845268281209,
      "grad_norm": 0.0007166676805354655,
      "learning_rate": 5.0104641951563745e-05,
      "loss": 0.1132,
      "step": 240800
    },
    {
      "epoch": 0.8333304506349432,
      "grad_norm": 154.153564453125,
      "learning_rate": 5.000086480951705e-05,
      "loss": 0.3293,
      "step": 240900
    },
    {
      "epoch": 0.8336763744417655,
      "grad_norm": 0.0008331696735695004,
      "learning_rate": 4.9897087667470356e-05,
      "loss": 0.2155,
      "step": 241000
    },
    {
      "epoch": 0.8340222982485878,
      "grad_norm": 0.00020418642088770866,
      "learning_rate": 4.979331052542367e-05,
      "loss": 0.163,
      "step": 241100
    },
    {
      "epoch": 0.8343682220554101,
      "grad_norm": 0.0001808870874810964,
      "learning_rate": 4.968953338337697e-05,
      "loss": 0.1152,
      "step": 241200
    },
    {
      "epoch": 0.8347141458622324,
      "grad_norm": 0.01686735264956951,
      "learning_rate": 4.958575624133028e-05,
      "loss": 0.0757,
      "step": 241300
    },
    {
      "epoch": 0.8350600696690547,
      "grad_norm": 0.001443546381779015,
      "learning_rate": 4.948197909928359e-05,
      "loss": 0.1755,
      "step": 241400
    },
    {
      "epoch": 0.835405993475877,
      "grad_norm": 0.00020122020214330405,
      "learning_rate": 4.93782019572369e-05,
      "loss": 0.2315,
      "step": 241500
    },
    {
      "epoch": 0.8357519172826993,
      "grad_norm": 0.03974495455622673,
      "learning_rate": 4.92744248151902e-05,
      "loss": 0.1741,
      "step": 241600
    },
    {
      "epoch": 0.8360978410895217,
      "grad_norm": 0.0012640584027394652,
      "learning_rate": 4.917064767314351e-05,
      "loss": 0.3072,
      "step": 241700
    },
    {
      "epoch": 0.836443764896344,
      "grad_norm": 7.188773452071473e-05,
      "learning_rate": 4.906687053109682e-05,
      "loss": 0.1547,
      "step": 241800
    },
    {
      "epoch": 0.8367896887031663,
      "grad_norm": 0.1454177349805832,
      "learning_rate": 4.896309338905012e-05,
      "loss": 0.2822,
      "step": 241900
    },
    {
      "epoch": 0.8371356125099886,
      "grad_norm": 0.0016913141589611769,
      "learning_rate": 4.885931624700343e-05,
      "loss": 0.3165,
      "step": 242000
    },
    {
      "epoch": 0.8374815363168109,
      "grad_norm": 0.00031562900403514504,
      "learning_rate": 4.875553910495674e-05,
      "loss": 0.1365,
      "step": 242100
    },
    {
      "epoch": 0.8378274601236332,
      "grad_norm": 0.0005424490664154291,
      "learning_rate": 4.865176196291005e-05,
      "loss": 0.2791,
      "step": 242200
    },
    {
      "epoch": 0.8381733839304555,
      "grad_norm": 0.0004983815015293658,
      "learning_rate": 4.854798482086335e-05,
      "loss": 0.1712,
      "step": 242300
    },
    {
      "epoch": 0.8385193077372778,
      "grad_norm": 0.00016098622290883213,
      "learning_rate": 4.844420767881666e-05,
      "loss": 0.1365,
      "step": 242400
    },
    {
      "epoch": 0.8388652315441001,
      "grad_norm": 0.01177900843322277,
      "learning_rate": 4.834043053676997e-05,
      "loss": 0.2271,
      "step": 242500
    },
    {
      "epoch": 0.8392111553509224,
      "grad_norm": 0.00030166853684931993,
      "learning_rate": 4.823665339472327e-05,
      "loss": 0.1886,
      "step": 242600
    },
    {
      "epoch": 0.8395570791577447,
      "grad_norm": 0.0022245123982429504,
      "learning_rate": 4.813287625267658e-05,
      "loss": 0.1781,
      "step": 242700
    },
    {
      "epoch": 0.839903002964567,
      "grad_norm": 0.007728695869445801,
      "learning_rate": 4.802909911062989e-05,
      "loss": 0.1898,
      "step": 242800
    },
    {
      "epoch": 0.8402489267713893,
      "grad_norm": 0.0008204902405850589,
      "learning_rate": 4.79253219685832e-05,
      "loss": 0.1158,
      "step": 242900
    },
    {
      "epoch": 0.8405948505782116,
      "grad_norm": 0.004994201473891735,
      "learning_rate": 4.78215448265365e-05,
      "loss": 0.1538,
      "step": 243000
    },
    {
      "epoch": 0.840940774385034,
      "grad_norm": 0.0003591187414713204,
      "learning_rate": 4.7717767684489814e-05,
      "loss": 0.1211,
      "step": 243100
    },
    {
      "epoch": 0.8412866981918563,
      "grad_norm": 4.435590381035581e-05,
      "learning_rate": 4.761399054244312e-05,
      "loss": 0.2782,
      "step": 243200
    },
    {
      "epoch": 0.8416326219986786,
      "grad_norm": 0.006161792203783989,
      "learning_rate": 4.751021340039642e-05,
      "loss": 0.2022,
      "step": 243300
    },
    {
      "epoch": 0.8419785458055009,
      "grad_norm": 0.0016596303321421146,
      "learning_rate": 4.740643625834973e-05,
      "loss": 0.1422,
      "step": 243400
    },
    {
      "epoch": 0.8423244696123232,
      "grad_norm": 5.2309042075648904e-05,
      "learning_rate": 4.730265911630304e-05,
      "loss": 0.2259,
      "step": 243500
    },
    {
      "epoch": 0.8426703934191455,
      "grad_norm": 11.542542457580566,
      "learning_rate": 4.719888197425635e-05,
      "loss": 0.1014,
      "step": 243600
    },
    {
      "epoch": 0.8430163172259678,
      "grad_norm": 0.004575156141072512,
      "learning_rate": 4.7095104832209654e-05,
      "loss": 0.1716,
      "step": 243700
    },
    {
      "epoch": 0.8433622410327901,
      "grad_norm": 0.02056979201734066,
      "learning_rate": 4.699132769016296e-05,
      "loss": 0.2598,
      "step": 243800
    },
    {
      "epoch": 0.8437081648396124,
      "grad_norm": 0.004368938039988279,
      "learning_rate": 4.688755054811627e-05,
      "loss": 0.1969,
      "step": 243900
    },
    {
      "epoch": 0.8440540886464347,
      "grad_norm": 11.107016563415527,
      "learning_rate": 4.678377340606957e-05,
      "loss": 0.2503,
      "step": 244000
    },
    {
      "epoch": 0.844400012453257,
      "grad_norm": 65.0730972290039,
      "learning_rate": 4.667999626402288e-05,
      "loss": 0.2182,
      "step": 244100
    },
    {
      "epoch": 0.8447459362600793,
      "grad_norm": 0.00015557032020296901,
      "learning_rate": 4.657621912197619e-05,
      "loss": 0.2404,
      "step": 244200
    },
    {
      "epoch": 0.8450918600669016,
      "grad_norm": 0.0001516580377938226,
      "learning_rate": 4.64724419799295e-05,
      "loss": 0.2939,
      "step": 244300
    },
    {
      "epoch": 0.8454377838737239,
      "grad_norm": 0.03566588833928108,
      "learning_rate": 4.63686648378828e-05,
      "loss": 0.2643,
      "step": 244400
    },
    {
      "epoch": 0.8457837076805463,
      "grad_norm": 16.47822380065918,
      "learning_rate": 4.626488769583611e-05,
      "loss": 0.2212,
      "step": 244500
    },
    {
      "epoch": 0.8461296314873686,
      "grad_norm": 0.00032777476008050144,
      "learning_rate": 4.6161110553789424e-05,
      "loss": 0.1084,
      "step": 244600
    },
    {
      "epoch": 0.8464755552941909,
      "grad_norm": 0.03323087841272354,
      "learning_rate": 4.605733341174272e-05,
      "loss": 0.149,
      "step": 244700
    },
    {
      "epoch": 0.8468214791010132,
      "grad_norm": 0.0005681965849362314,
      "learning_rate": 4.595355626969603e-05,
      "loss": 0.137,
      "step": 244800
    },
    {
      "epoch": 0.8471674029078355,
      "grad_norm": 0.005034005269408226,
      "learning_rate": 4.584977912764934e-05,
      "loss": 0.3092,
      "step": 244900
    },
    {
      "epoch": 0.8475133267146578,
      "grad_norm": 38.246639251708984,
      "learning_rate": 4.574600198560265e-05,
      "loss": 0.207,
      "step": 245000
    },
    {
      "epoch": 0.8478592505214801,
      "grad_norm": 0.014140993356704712,
      "learning_rate": 4.564222484355595e-05,
      "loss": 0.1416,
      "step": 245100
    },
    {
      "epoch": 0.8482051743283024,
      "grad_norm": 0.1905776858329773,
      "learning_rate": 4.5538447701509264e-05,
      "loss": 0.1199,
      "step": 245200
    },
    {
      "epoch": 0.8485510981351247,
      "grad_norm": 8.805585093796253e-05,
      "learning_rate": 4.543467055946257e-05,
      "loss": 0.1404,
      "step": 245300
    },
    {
      "epoch": 0.848897021941947,
      "grad_norm": 2.095531463623047,
      "learning_rate": 4.5330893417415875e-05,
      "loss": 0.1488,
      "step": 245400
    },
    {
      "epoch": 0.8492429457487694,
      "grad_norm": 0.00011919506505364552,
      "learning_rate": 4.522711627536918e-05,
      "loss": 0.1674,
      "step": 245500
    },
    {
      "epoch": 0.8495888695555917,
      "grad_norm": 0.011999767273664474,
      "learning_rate": 4.5123339133322493e-05,
      "loss": 0.1803,
      "step": 245600
    },
    {
      "epoch": 0.849934793362414,
      "grad_norm": 0.04832591861486435,
      "learning_rate": 4.50195619912758e-05,
      "loss": 0.1278,
      "step": 245700
    },
    {
      "epoch": 0.8502807171692363,
      "grad_norm": 17.19024085998535,
      "learning_rate": 4.4915784849229105e-05,
      "loss": 0.1399,
      "step": 245800
    },
    {
      "epoch": 0.8506266409760587,
      "grad_norm": 0.010588647797703743,
      "learning_rate": 4.481200770718241e-05,
      "loss": 0.1116,
      "step": 245900
    },
    {
      "epoch": 0.850972564782881,
      "grad_norm": 0.0019471157575026155,
      "learning_rate": 4.470823056513572e-05,
      "loss": 0.3153,
      "step": 246000
    },
    {
      "epoch": 0.8513184885897033,
      "grad_norm": 0.0003153487341478467,
      "learning_rate": 4.460445342308902e-05,
      "loss": 0.2807,
      "step": 246100
    },
    {
      "epoch": 0.8516644123965256,
      "grad_norm": 0.0018935154657810926,
      "learning_rate": 4.4500676281042334e-05,
      "loss": 0.1571,
      "step": 246200
    },
    {
      "epoch": 0.8520103362033479,
      "grad_norm": 0.001268866821192205,
      "learning_rate": 4.439689913899564e-05,
      "loss": 0.1799,
      "step": 246300
    },
    {
      "epoch": 0.8523562600101702,
      "grad_norm": 6.353553295135498,
      "learning_rate": 4.429312199694895e-05,
      "loss": 0.1259,
      "step": 246400
    },
    {
      "epoch": 0.8527021838169925,
      "grad_norm": 35.43003845214844,
      "learning_rate": 4.418934485490225e-05,
      "loss": 0.1568,
      "step": 246500
    },
    {
      "epoch": 0.8530481076238148,
      "grad_norm": 13.986807823181152,
      "learning_rate": 4.408556771285556e-05,
      "loss": 0.1474,
      "step": 246600
    },
    {
      "epoch": 0.8533940314306371,
      "grad_norm": 0.0005196930724196136,
      "learning_rate": 4.3981790570808875e-05,
      "loss": 0.194,
      "step": 246700
    },
    {
      "epoch": 0.8537399552374594,
      "grad_norm": 0.0004176282964181155,
      "learning_rate": 4.3878013428762174e-05,
      "loss": 0.1194,
      "step": 246800
    },
    {
      "epoch": 0.8540858790442817,
      "grad_norm": 0.0059665655717253685,
      "learning_rate": 4.3774236286715486e-05,
      "loss": 0.1298,
      "step": 246900
    },
    {
      "epoch": 0.854431802851104,
      "grad_norm": 0.0031306669116020203,
      "learning_rate": 4.367045914466879e-05,
      "loss": 0.3473,
      "step": 247000
    },
    {
      "epoch": 0.8547777266579263,
      "grad_norm": 14.927769660949707,
      "learning_rate": 4.3566682002622104e-05,
      "loss": 0.1323,
      "step": 247100
    },
    {
      "epoch": 0.8551236504647486,
      "grad_norm": 0.007477891631424427,
      "learning_rate": 4.34629048605754e-05,
      "loss": 0.1875,
      "step": 247200
    },
    {
      "epoch": 0.855469574271571,
      "grad_norm": 0.1315470188856125,
      "learning_rate": 4.3359127718528715e-05,
      "loss": 0.2826,
      "step": 247300
    },
    {
      "epoch": 0.8558154980783933,
      "grad_norm": 0.0038255369290709496,
      "learning_rate": 4.325535057648202e-05,
      "loss": 0.2697,
      "step": 247400
    },
    {
      "epoch": 0.8561614218852156,
      "grad_norm": 0.00319255911745131,
      "learning_rate": 4.3151573434435326e-05,
      "loss": 0.2213,
      "step": 247500
    },
    {
      "epoch": 0.8565073456920379,
      "grad_norm": 0.0001852224813774228,
      "learning_rate": 4.304779629238863e-05,
      "loss": 0.1262,
      "step": 247600
    },
    {
      "epoch": 0.8568532694988602,
      "grad_norm": 11.71245288848877,
      "learning_rate": 4.2944019150341944e-05,
      "loss": 0.2682,
      "step": 247700
    },
    {
      "epoch": 0.8571991933056825,
      "grad_norm": 0.0017589256167411804,
      "learning_rate": 4.2840242008295256e-05,
      "loss": 0.2513,
      "step": 247800
    },
    {
      "epoch": 0.8575451171125048,
      "grad_norm": 0.012644883245229721,
      "learning_rate": 4.2736464866248555e-05,
      "loss": 0.2617,
      "step": 247900
    },
    {
      "epoch": 0.8578910409193271,
      "grad_norm": 0.000447865022579208,
      "learning_rate": 4.263268772420186e-05,
      "loss": 0.2345,
      "step": 248000
    },
    {
      "epoch": 0.8582369647261494,
      "grad_norm": 0.0057487161830067635,
      "learning_rate": 4.252891058215517e-05,
      "loss": 0.2538,
      "step": 248100
    },
    {
      "epoch": 0.8585828885329717,
      "grad_norm": 0.0001083420793293044,
      "learning_rate": 4.242513344010847e-05,
      "loss": 0.3585,
      "step": 248200
    },
    {
      "epoch": 0.858928812339794,
      "grad_norm": 38.600547790527344,
      "learning_rate": 4.2321356298061784e-05,
      "loss": 0.388,
      "step": 248300
    },
    {
      "epoch": 0.8592747361466163,
      "grad_norm": 0.1388968676328659,
      "learning_rate": 4.2217579156015096e-05,
      "loss": 0.1655,
      "step": 248400
    },
    {
      "epoch": 0.8596206599534386,
      "grad_norm": 0.0011773828882724047,
      "learning_rate": 4.21138020139684e-05,
      "loss": 0.3309,
      "step": 248500
    },
    {
      "epoch": 0.8599665837602609,
      "grad_norm": 0.17006546258926392,
      "learning_rate": 4.201002487192171e-05,
      "loss": 0.1239,
      "step": 248600
    },
    {
      "epoch": 0.8603125075670833,
      "grad_norm": 0.0005737527972087264,
      "learning_rate": 4.190624772987501e-05,
      "loss": 0.1934,
      "step": 248700
    },
    {
      "epoch": 0.8606584313739056,
      "grad_norm": 122.91806030273438,
      "learning_rate": 4.1802470587828326e-05,
      "loss": 0.1463,
      "step": 248800
    },
    {
      "epoch": 0.8610043551807279,
      "grad_norm": 0.0030567257199436426,
      "learning_rate": 4.1698693445781624e-05,
      "loss": 0.2361,
      "step": 248900
    },
    {
      "epoch": 0.8613502789875502,
      "grad_norm": 0.000598203216213733,
      "learning_rate": 4.159491630373494e-05,
      "loss": 0.2038,
      "step": 249000
    },
    {
      "epoch": 0.8616962027943725,
      "grad_norm": 3.0418214797973633,
      "learning_rate": 4.149113916168824e-05,
      "loss": 0.2602,
      "step": 249100
    },
    {
      "epoch": 0.8620421266011948,
      "grad_norm": 0.0023320233449339867,
      "learning_rate": 4.1387362019641555e-05,
      "loss": 0.1194,
      "step": 249200
    },
    {
      "epoch": 0.8623880504080171,
      "grad_norm": 0.00030891693313606083,
      "learning_rate": 4.128358487759485e-05,
      "loss": 0.1295,
      "step": 249300
    },
    {
      "epoch": 0.8627339742148394,
      "grad_norm": 0.0007047441322356462,
      "learning_rate": 4.1179807735548166e-05,
      "loss": 0.3024,
      "step": 249400
    },
    {
      "epoch": 0.8630798980216617,
      "grad_norm": 0.0003256279160268605,
      "learning_rate": 4.107603059350147e-05,
      "loss": 0.3377,
      "step": 249500
    },
    {
      "epoch": 0.863425821828484,
      "grad_norm": 0.027986645698547363,
      "learning_rate": 4.097225345145478e-05,
      "loss": 0.1912,
      "step": 249600
    },
    {
      "epoch": 0.8637717456353063,
      "grad_norm": 0.0018647359684109688,
      "learning_rate": 4.086847630940808e-05,
      "loss": 0.1389,
      "step": 249700
    },
    {
      "epoch": 0.8641176694421286,
      "grad_norm": 0.003108382225036621,
      "learning_rate": 4.0764699167361395e-05,
      "loss": 0.1582,
      "step": 249800
    },
    {
      "epoch": 0.864463593248951,
      "grad_norm": 7.659349648747593e-05,
      "learning_rate": 4.066092202531471e-05,
      "loss": 0.1754,
      "step": 249900
    },
    {
      "epoch": 0.8648095170557732,
      "grad_norm": 0.0005154874525032938,
      "learning_rate": 4.0557144883268006e-05,
      "loss": 0.0984,
      "step": 250000
    },
    {
      "epoch": 0.8651554408625957,
      "grad_norm": 16.10444450378418,
      "learning_rate": 4.045336774122132e-05,
      "loss": 0.1993,
      "step": 250100
    },
    {
      "epoch": 0.865501364669418,
      "grad_norm": 0.07063490897417068,
      "learning_rate": 4.0349590599174624e-05,
      "loss": 0.2409,
      "step": 250200
    },
    {
      "epoch": 0.8658472884762403,
      "grad_norm": 0.05148167163133621,
      "learning_rate": 4.024581345712793e-05,
      "loss": 0.2067,
      "step": 250300
    },
    {
      "epoch": 0.8661932122830626,
      "grad_norm": 4.801406976184808e-05,
      "learning_rate": 4.0142036315081235e-05,
      "loss": 0.1707,
      "step": 250400
    },
    {
      "epoch": 0.8665391360898849,
      "grad_norm": 0.0005429539596661925,
      "learning_rate": 4.003825917303455e-05,
      "loss": 0.0846,
      "step": 250500
    },
    {
      "epoch": 0.8668850598967072,
      "grad_norm": 0.0008221253519877791,
      "learning_rate": 3.993448203098785e-05,
      "loss": 0.0826,
      "step": 250600
    },
    {
      "epoch": 0.8672309837035295,
      "grad_norm": 0.00014581959112547338,
      "learning_rate": 3.983070488894116e-05,
      "loss": 0.2762,
      "step": 250700
    },
    {
      "epoch": 0.8675769075103518,
      "grad_norm": 0.00018567818915471435,
      "learning_rate": 3.9726927746894464e-05,
      "loss": 0.21,
      "step": 250800
    },
    {
      "epoch": 0.8679228313171741,
      "grad_norm": 0.3606417179107666,
      "learning_rate": 3.9623150604847776e-05,
      "loss": 0.3566,
      "step": 250900
    },
    {
      "epoch": 0.8682687551239964,
      "grad_norm": 0.001110799377784133,
      "learning_rate": 3.9519373462801075e-05,
      "loss": 0.3097,
      "step": 251000
    },
    {
      "epoch": 0.8686146789308187,
      "grad_norm": 0.010696926154196262,
      "learning_rate": 3.941559632075439e-05,
      "loss": 0.226,
      "step": 251100
    },
    {
      "epoch": 0.868960602737641,
      "grad_norm": 0.06850811839103699,
      "learning_rate": 3.931181917870769e-05,
      "loss": 0.2422,
      "step": 251200
    },
    {
      "epoch": 0.8693065265444633,
      "grad_norm": 0.00031683678389526904,
      "learning_rate": 3.9208042036661005e-05,
      "loss": 0.1673,
      "step": 251300
    },
    {
      "epoch": 0.8696524503512856,
      "grad_norm": 0.0010058326879516244,
      "learning_rate": 3.9104264894614304e-05,
      "loss": 0.3438,
      "step": 251400
    },
    {
      "epoch": 0.869998374158108,
      "grad_norm": 9.165083885192871,
      "learning_rate": 3.9000487752567616e-05,
      "loss": 0.2133,
      "step": 251500
    },
    {
      "epoch": 0.8703442979649303,
      "grad_norm": 0.007990197278559208,
      "learning_rate": 3.889671061052093e-05,
      "loss": 0.3207,
      "step": 251600
    },
    {
      "epoch": 0.8706902217717526,
      "grad_norm": 0.9305726885795593,
      "learning_rate": 3.879293346847423e-05,
      "loss": 0.233,
      "step": 251700
    },
    {
      "epoch": 0.8710361455785749,
      "grad_norm": 0.0007719515124335885,
      "learning_rate": 3.868915632642754e-05,
      "loss": 0.2934,
      "step": 251800
    },
    {
      "epoch": 0.8713820693853972,
      "grad_norm": 0.005081825889647007,
      "learning_rate": 3.8585379184380845e-05,
      "loss": 0.1905,
      "step": 251900
    },
    {
      "epoch": 0.8717279931922195,
      "grad_norm": 0.7752124667167664,
      "learning_rate": 3.848160204233416e-05,
      "loss": 0.1783,
      "step": 252000
    },
    {
      "epoch": 0.8720739169990418,
      "grad_norm": 0.23334991931915283,
      "learning_rate": 3.8377824900287456e-05,
      "loss": 0.25,
      "step": 252100
    },
    {
      "epoch": 0.8724198408058641,
      "grad_norm": 0.000987535109743476,
      "learning_rate": 3.827404775824077e-05,
      "loss": 0.2242,
      "step": 252200
    },
    {
      "epoch": 0.8727657646126864,
      "grad_norm": 0.7422157526016235,
      "learning_rate": 3.8170270616194074e-05,
      "loss": 0.2154,
      "step": 252300
    },
    {
      "epoch": 0.8731116884195087,
      "grad_norm": 0.00016454547585453838,
      "learning_rate": 3.806649347414738e-05,
      "loss": 0.2969,
      "step": 252400
    },
    {
      "epoch": 0.873457612226331,
      "grad_norm": 0.00018292255117557943,
      "learning_rate": 3.7962716332100685e-05,
      "loss": 0.1148,
      "step": 252500
    },
    {
      "epoch": 0.8738035360331533,
      "grad_norm": 0.6193459033966064,
      "learning_rate": 3.7858939190054e-05,
      "loss": 0.1582,
      "step": 252600
    },
    {
      "epoch": 0.8741494598399756,
      "grad_norm": 0.0002956762909889221,
      "learning_rate": 3.77551620480073e-05,
      "loss": 0.2558,
      "step": 252700
    },
    {
      "epoch": 0.8744953836467979,
      "grad_norm": 0.0013607273576781154,
      "learning_rate": 3.765138490596061e-05,
      "loss": 0.101,
      "step": 252800
    },
    {
      "epoch": 0.8748413074536203,
      "grad_norm": 0.0005707702948711812,
      "learning_rate": 3.7547607763913914e-05,
      "loss": 0.1198,
      "step": 252900
    },
    {
      "epoch": 0.8751872312604426,
      "grad_norm": 0.00032387199462391436,
      "learning_rate": 3.744383062186722e-05,
      "loss": 0.2489,
      "step": 253000
    },
    {
      "epoch": 0.8755331550672649,
      "grad_norm": 0.3905504643917084,
      "learning_rate": 3.734005347982053e-05,
      "loss": 0.1622,
      "step": 253100
    },
    {
      "epoch": 0.8758790788740872,
      "grad_norm": 0.00010780948650790378,
      "learning_rate": 3.723627633777384e-05,
      "loss": 0.1585,
      "step": 253200
    },
    {
      "epoch": 0.8762250026809095,
      "grad_norm": 0.0003346451558172703,
      "learning_rate": 3.713249919572715e-05,
      "loss": 0.0877,
      "step": 253300
    },
    {
      "epoch": 0.8765709264877318,
      "grad_norm": 0.023579774424433708,
      "learning_rate": 3.7028722053680456e-05,
      "loss": 0.2228,
      "step": 253400
    },
    {
      "epoch": 0.8769168502945541,
      "grad_norm": 40.34743118286133,
      "learning_rate": 3.692494491163376e-05,
      "loss": 0.1906,
      "step": 253500
    },
    {
      "epoch": 0.8772627741013764,
      "grad_norm": 0.20069186389446259,
      "learning_rate": 3.682116776958707e-05,
      "loss": 0.1709,
      "step": 253600
    },
    {
      "epoch": 0.8776086979081987,
      "grad_norm": 0.0025969643611460924,
      "learning_rate": 3.671739062754037e-05,
      "loss": 0.2228,
      "step": 253700
    },
    {
      "epoch": 0.877954621715021,
      "grad_norm": 0.00029978316160850227,
      "learning_rate": 3.6613613485493685e-05,
      "loss": 0.2782,
      "step": 253800
    },
    {
      "epoch": 0.8783005455218433,
      "grad_norm": 0.0001790934766177088,
      "learning_rate": 3.650983634344699e-05,
      "loss": 0.2259,
      "step": 253900
    },
    {
      "epoch": 0.8786464693286656,
      "grad_norm": 0.0003145629307255149,
      "learning_rate": 3.6406059201400296e-05,
      "loss": 0.2207,
      "step": 254000
    },
    {
      "epoch": 0.8789923931354879,
      "grad_norm": 0.00038613902870565653,
      "learning_rate": 3.63022820593536e-05,
      "loss": 0.0559,
      "step": 254100
    },
    {
      "epoch": 0.8793383169423102,
      "grad_norm": 0.000330032198689878,
      "learning_rate": 3.6198504917306914e-05,
      "loss": 0.191,
      "step": 254200
    },
    {
      "epoch": 0.8796842407491327,
      "grad_norm": 0.09446586668491364,
      "learning_rate": 3.609472777526022e-05,
      "loss": 0.1719,
      "step": 254300
    },
    {
      "epoch": 0.880030164555955,
      "grad_norm": 0.00022971747966948897,
      "learning_rate": 3.5990950633213525e-05,
      "loss": 0.1561,
      "step": 254400
    },
    {
      "epoch": 0.8803760883627773,
      "grad_norm": 0.00013546034460887313,
      "learning_rate": 3.588717349116683e-05,
      "loss": 0.0672,
      "step": 254500
    },
    {
      "epoch": 0.8807220121695996,
      "grad_norm": 0.08607716113328934,
      "learning_rate": 3.5783396349120136e-05,
      "loss": 0.1449,
      "step": 254600
    },
    {
      "epoch": 0.8810679359764219,
      "grad_norm": 0.0007191845215857029,
      "learning_rate": 3.567961920707345e-05,
      "loss": 0.1972,
      "step": 254700
    },
    {
      "epoch": 0.8814138597832442,
      "grad_norm": 0.013043418526649475,
      "learning_rate": 3.5575842065026754e-05,
      "loss": 0.0941,
      "step": 254800
    },
    {
      "epoch": 0.8817597835900665,
      "grad_norm": 0.0038198535330593586,
      "learning_rate": 3.5472064922980066e-05,
      "loss": 0.1455,
      "step": 254900
    },
    {
      "epoch": 0.8821057073968888,
      "grad_norm": 0.000552240526303649,
      "learning_rate": 3.536828778093337e-05,
      "loss": 0.2789,
      "step": 255000
    },
    {
      "epoch": 0.8824516312037111,
      "grad_norm": 0.0002472764754202217,
      "learning_rate": 3.526451063888668e-05,
      "loss": 0.2872,
      "step": 255100
    },
    {
      "epoch": 0.8827975550105334,
      "grad_norm": 67.50244140625,
      "learning_rate": 3.516073349683998e-05,
      "loss": 0.169,
      "step": 255200
    },
    {
      "epoch": 0.8831434788173557,
      "grad_norm": 0.0012217647163197398,
      "learning_rate": 3.505695635479329e-05,
      "loss": 0.3115,
      "step": 255300
    },
    {
      "epoch": 0.883489402624178,
      "grad_norm": 2.9376407837844454e-05,
      "learning_rate": 3.49531792127466e-05,
      "loss": 0.1795,
      "step": 255400
    },
    {
      "epoch": 0.8838353264310003,
      "grad_norm": 0.0002807938144542277,
      "learning_rate": 3.4849402070699906e-05,
      "loss": 0.2034,
      "step": 255500
    },
    {
      "epoch": 0.8841812502378226,
      "grad_norm": 10.54299545288086,
      "learning_rate": 3.474562492865321e-05,
      "loss": 0.2246,
      "step": 255600
    },
    {
      "epoch": 0.884527174044645,
      "grad_norm": 0.0006659460486844182,
      "learning_rate": 3.464184778660652e-05,
      "loss": 0.1676,
      "step": 255700
    },
    {
      "epoch": 0.8848730978514673,
      "grad_norm": 0.015474118292331696,
      "learning_rate": 3.453807064455982e-05,
      "loss": 0.1468,
      "step": 255800
    },
    {
      "epoch": 0.8852190216582896,
      "grad_norm": 0.00035142945125699043,
      "learning_rate": 3.4434293502513135e-05,
      "loss": 0.1349,
      "step": 255900
    },
    {
      "epoch": 0.8855649454651119,
      "grad_norm": 23.668764114379883,
      "learning_rate": 3.433051636046644e-05,
      "loss": 0.2136,
      "step": 256000
    },
    {
      "epoch": 0.8859108692719342,
      "grad_norm": 0.011685621924698353,
      "learning_rate": 3.4226739218419747e-05,
      "loss": 0.2602,
      "step": 256100
    },
    {
      "epoch": 0.8862567930787565,
      "grad_norm": 0.0005333311855792999,
      "learning_rate": 3.412296207637305e-05,
      "loss": 0.0755,
      "step": 256200
    },
    {
      "epoch": 0.8866027168855788,
      "grad_norm": 0.0002881390100810677,
      "learning_rate": 3.4019184934326364e-05,
      "loss": 0.0756,
      "step": 256300
    },
    {
      "epoch": 0.8869486406924011,
      "grad_norm": 0.0006661674124188721,
      "learning_rate": 3.391540779227967e-05,
      "loss": 0.2332,
      "step": 256400
    },
    {
      "epoch": 0.8872945644992234,
      "grad_norm": 0.0002291430573677644,
      "learning_rate": 3.3811630650232976e-05,
      "loss": 0.1197,
      "step": 256500
    },
    {
      "epoch": 0.8876404883060457,
      "grad_norm": 0.0005180894513614476,
      "learning_rate": 3.370785350818629e-05,
      "loss": 0.132,
      "step": 256600
    },
    {
      "epoch": 0.887986412112868,
      "grad_norm": 0.006571090780198574,
      "learning_rate": 3.3604076366139593e-05,
      "loss": 0.1795,
      "step": 256700
    },
    {
      "epoch": 0.8883323359196903,
      "grad_norm": 0.00014559261035174131,
      "learning_rate": 3.35002992240929e-05,
      "loss": 0.2528,
      "step": 256800
    },
    {
      "epoch": 0.8886782597265126,
      "grad_norm": 0.022081492468714714,
      "learning_rate": 3.3396522082046205e-05,
      "loss": 0.1694,
      "step": 256900
    },
    {
      "epoch": 0.8890241835333349,
      "grad_norm": 0.0002503334835637361,
      "learning_rate": 3.329274493999952e-05,
      "loss": 0.3016,
      "step": 257000
    },
    {
      "epoch": 0.8893701073401573,
      "grad_norm": 0.008054551668465137,
      "learning_rate": 3.318896779795282e-05,
      "loss": 0.1334,
      "step": 257100
    },
    {
      "epoch": 0.8897160311469796,
      "grad_norm": 0.007007875945419073,
      "learning_rate": 3.308519065590613e-05,
      "loss": 0.2265,
      "step": 257200
    },
    {
      "epoch": 0.8900619549538019,
      "grad_norm": 0.0052526663057506084,
      "learning_rate": 3.2981413513859434e-05,
      "loss": 0.1753,
      "step": 257300
    },
    {
      "epoch": 0.8904078787606242,
      "grad_norm": 0.0005019968957640231,
      "learning_rate": 3.287763637181274e-05,
      "loss": 0.1176,
      "step": 257400
    },
    {
      "epoch": 0.8907538025674465,
      "grad_norm": 4.928018093109131,
      "learning_rate": 3.277385922976605e-05,
      "loss": 0.1857,
      "step": 257500
    },
    {
      "epoch": 0.8910997263742688,
      "grad_norm": 0.00012566731311380863,
      "learning_rate": 3.267008208771936e-05,
      "loss": 0.195,
      "step": 257600
    },
    {
      "epoch": 0.8914456501810911,
      "grad_norm": 12.548491477966309,
      "learning_rate": 3.256630494567266e-05,
      "loss": 0.1967,
      "step": 257700
    },
    {
      "epoch": 0.8917915739879134,
      "grad_norm": 0.004363502841442823,
      "learning_rate": 3.246252780362597e-05,
      "loss": 0.1902,
      "step": 257800
    },
    {
      "epoch": 0.8921374977947357,
      "grad_norm": 0.00023953466734383255,
      "learning_rate": 3.2358750661579274e-05,
      "loss": 0.1458,
      "step": 257900
    },
    {
      "epoch": 0.892483421601558,
      "grad_norm": 0.008085890673100948,
      "learning_rate": 3.2254973519532586e-05,
      "loss": 0.1755,
      "step": 258000
    },
    {
      "epoch": 0.8928293454083803,
      "grad_norm": 0.0002593199023976922,
      "learning_rate": 3.215119637748589e-05,
      "loss": 0.1132,
      "step": 258100
    },
    {
      "epoch": 0.8931752692152026,
      "grad_norm": 0.000323825137456879,
      "learning_rate": 3.2047419235439204e-05,
      "loss": 0.3164,
      "step": 258200
    },
    {
      "epoch": 0.8935211930220249,
      "grad_norm": 0.0011212145909667015,
      "learning_rate": 3.19436420933925e-05,
      "loss": 0.2483,
      "step": 258300
    },
    {
      "epoch": 0.8938671168288472,
      "grad_norm": 0.007529933005571365,
      "learning_rate": 3.1839864951345815e-05,
      "loss": 0.3104,
      "step": 258400
    },
    {
      "epoch": 0.8942130406356696,
      "grad_norm": 0.0008523809956386685,
      "learning_rate": 3.173608780929912e-05,
      "loss": 0.2295,
      "step": 258500
    },
    {
      "epoch": 0.8945589644424919,
      "grad_norm": 0.14123457670211792,
      "learning_rate": 3.1632310667252426e-05,
      "loss": 0.1749,
      "step": 258600
    },
    {
      "epoch": 0.8949048882493142,
      "grad_norm": 0.0003220461367163807,
      "learning_rate": 3.152853352520574e-05,
      "loss": 0.1805,
      "step": 258700
    },
    {
      "epoch": 0.8952508120561365,
      "grad_norm": 0.00017743332136888057,
      "learning_rate": 3.1424756383159044e-05,
      "loss": 0.1078,
      "step": 258800
    },
    {
      "epoch": 0.8955967358629588,
      "grad_norm": 0.4840031862258911,
      "learning_rate": 3.132097924111235e-05,
      "loss": 0.1897,
      "step": 258900
    },
    {
      "epoch": 0.8959426596697811,
      "grad_norm": 11.384650230407715,
      "learning_rate": 3.1217202099065655e-05,
      "loss": 0.133,
      "step": 259000
    },
    {
      "epoch": 0.8962885834766035,
      "grad_norm": 0.01599903218448162,
      "learning_rate": 3.111342495701897e-05,
      "loss": 0.1157,
      "step": 259100
    },
    {
      "epoch": 0.8966345072834258,
      "grad_norm": 0.00017934991046786308,
      "learning_rate": 3.100964781497227e-05,
      "loss": 0.1633,
      "step": 259200
    },
    {
      "epoch": 0.896980431090248,
      "grad_norm": 0.00034607553971000016,
      "learning_rate": 3.090587067292558e-05,
      "loss": 0.0801,
      "step": 259300
    },
    {
      "epoch": 0.8973263548970704,
      "grad_norm": 0.12464751303195953,
      "learning_rate": 3.0802093530878884e-05,
      "loss": 0.2239,
      "step": 259400
    },
    {
      "epoch": 0.8976722787038927,
      "grad_norm": 0.000342048384482041,
      "learning_rate": 3.069831638883219e-05,
      "loss": 0.2806,
      "step": 259500
    },
    {
      "epoch": 0.898018202510715,
      "grad_norm": 0.0004360902530606836,
      "learning_rate": 3.05945392467855e-05,
      "loss": 0.1745,
      "step": 259600
    },
    {
      "epoch": 0.8983641263175373,
      "grad_norm": 0.0004904724773950875,
      "learning_rate": 3.0490762104738808e-05,
      "loss": 0.2153,
      "step": 259700
    },
    {
      "epoch": 0.8987100501243596,
      "grad_norm": 0.0005600466975010931,
      "learning_rate": 3.0386984962692117e-05,
      "loss": 0.2757,
      "step": 259800
    },
    {
      "epoch": 0.899055973931182,
      "grad_norm": 0.005901513155549765,
      "learning_rate": 3.0283207820645422e-05,
      "loss": 0.261,
      "step": 259900
    },
    {
      "epoch": 0.8994018977380043,
      "grad_norm": 0.002079769503325224,
      "learning_rate": 3.0179430678598728e-05,
      "loss": 0.1384,
      "step": 260000
    },
    {
      "epoch": 0.8997478215448266,
      "grad_norm": 0.005951658356934786,
      "learning_rate": 3.0075653536552037e-05,
      "loss": 0.1795,
      "step": 260100
    },
    {
      "epoch": 0.9000937453516489,
      "grad_norm": 0.0007212627679109573,
      "learning_rate": 2.9971876394505342e-05,
      "loss": 0.2751,
      "step": 260200
    },
    {
      "epoch": 0.9004396691584712,
      "grad_norm": 0.0004406995140016079,
      "learning_rate": 2.986809925245865e-05,
      "loss": 0.214,
      "step": 260300
    },
    {
      "epoch": 0.9007855929652935,
      "grad_norm": 0.0003465650952421129,
      "learning_rate": 2.9764322110411957e-05,
      "loss": 0.3582,
      "step": 260400
    },
    {
      "epoch": 0.9011315167721158,
      "grad_norm": 0.0006872837548144162,
      "learning_rate": 2.9660544968365266e-05,
      "loss": 0.1568,
      "step": 260500
    },
    {
      "epoch": 0.9014774405789381,
      "grad_norm": 0.001017609378322959,
      "learning_rate": 2.955676782631857e-05,
      "loss": 0.2229,
      "step": 260600
    },
    {
      "epoch": 0.9018233643857604,
      "grad_norm": 0.0001328968210145831,
      "learning_rate": 2.9452990684271877e-05,
      "loss": 0.4086,
      "step": 260700
    },
    {
      "epoch": 0.9021692881925827,
      "grad_norm": 1.0493587255477905,
      "learning_rate": 2.934921354222519e-05,
      "loss": 0.1248,
      "step": 260800
    },
    {
      "epoch": 0.902515211999405,
      "grad_norm": 0.00533914752304554,
      "learning_rate": 2.924543640017849e-05,
      "loss": 0.2437,
      "step": 260900
    },
    {
      "epoch": 0.9028611358062273,
      "grad_norm": 0.029169626533985138,
      "learning_rate": 2.9141659258131804e-05,
      "loss": 0.1995,
      "step": 261000
    },
    {
      "epoch": 0.9032070596130496,
      "grad_norm": 0.0032546960283070803,
      "learning_rate": 2.903788211608511e-05,
      "loss": 0.1856,
      "step": 261100
    },
    {
      "epoch": 0.9035529834198719,
      "grad_norm": 0.5749192237854004,
      "learning_rate": 2.8934104974038418e-05,
      "loss": 0.1415,
      "step": 261200
    },
    {
      "epoch": 0.9038989072266943,
      "grad_norm": 2.795009136199951,
      "learning_rate": 2.8830327831991724e-05,
      "loss": 0.1765,
      "step": 261300
    },
    {
      "epoch": 0.9042448310335166,
      "grad_norm": 0.0011921200202777982,
      "learning_rate": 2.872655068994503e-05,
      "loss": 0.1923,
      "step": 261400
    },
    {
      "epoch": 0.9045907548403389,
      "grad_norm": 0.00701172323897481,
      "learning_rate": 2.8622773547898338e-05,
      "loss": 0.0701,
      "step": 261500
    },
    {
      "epoch": 0.9049366786471612,
      "grad_norm": 0.0004273124213796109,
      "learning_rate": 2.8518996405851644e-05,
      "loss": 0.2375,
      "step": 261600
    },
    {
      "epoch": 0.9052826024539835,
      "grad_norm": 0.00011566153989406303,
      "learning_rate": 2.8415219263804953e-05,
      "loss": 0.2025,
      "step": 261700
    },
    {
      "epoch": 0.9056285262608058,
      "grad_norm": 8.922294364310801e-05,
      "learning_rate": 2.8311442121758258e-05,
      "loss": 0.0638,
      "step": 261800
    },
    {
      "epoch": 0.9059744500676281,
      "grad_norm": 0.0018804054707288742,
      "learning_rate": 2.8207664979711567e-05,
      "loss": 0.1301,
      "step": 261900
    },
    {
      "epoch": 0.9063203738744504,
      "grad_norm": 0.00017941504484042525,
      "learning_rate": 2.8103887837664873e-05,
      "loss": 0.1995,
      "step": 262000
    },
    {
      "epoch": 0.9066662976812727,
      "grad_norm": 0.00023246463388204575,
      "learning_rate": 2.800011069561818e-05,
      "loss": 0.1384,
      "step": 262100
    },
    {
      "epoch": 0.907012221488095,
      "grad_norm": 0.008419357240200043,
      "learning_rate": 2.7896333553571487e-05,
      "loss": 0.1729,
      "step": 262200
    },
    {
      "epoch": 0.9073581452949173,
      "grad_norm": 0.000132269473397173,
      "learning_rate": 2.7792556411524793e-05,
      "loss": 0.2353,
      "step": 262300
    },
    {
      "epoch": 0.9077040691017396,
      "grad_norm": 0.0236427690833807,
      "learning_rate": 2.7688779269478105e-05,
      "loss": 0.1877,
      "step": 262400
    },
    {
      "epoch": 0.9080499929085619,
      "grad_norm": 0.00012670940486714244,
      "learning_rate": 2.7585002127431407e-05,
      "loss": 0.1708,
      "step": 262500
    },
    {
      "epoch": 0.9083959167153842,
      "grad_norm": 0.03799499198794365,
      "learning_rate": 2.748122498538472e-05,
      "loss": 0.1849,
      "step": 262600
    },
    {
      "epoch": 0.9087418405222066,
      "grad_norm": 20.09182357788086,
      "learning_rate": 2.7377447843338025e-05,
      "loss": 0.1739,
      "step": 262700
    },
    {
      "epoch": 0.9090877643290289,
      "grad_norm": 0.0005009279702790082,
      "learning_rate": 2.727367070129133e-05,
      "loss": 0.1672,
      "step": 262800
    },
    {
      "epoch": 0.9094336881358512,
      "grad_norm": 43.5472412109375,
      "learning_rate": 2.716989355924464e-05,
      "loss": 0.1309,
      "step": 262900
    },
    {
      "epoch": 0.9097796119426735,
      "grad_norm": 0.0005310976412147284,
      "learning_rate": 2.7066116417197945e-05,
      "loss": 0.2589,
      "step": 263000
    },
    {
      "epoch": 0.9101255357494958,
      "grad_norm": 13.420958518981934,
      "learning_rate": 2.6962339275151254e-05,
      "loss": 0.1996,
      "step": 263100
    },
    {
      "epoch": 0.9104714595563181,
      "grad_norm": 0.0007408302626572549,
      "learning_rate": 2.685856213310456e-05,
      "loss": 0.1569,
      "step": 263200
    },
    {
      "epoch": 0.9108173833631404,
      "grad_norm": 0.0006086031789891422,
      "learning_rate": 2.675478499105787e-05,
      "loss": 0.1332,
      "step": 263300
    },
    {
      "epoch": 0.9111633071699627,
      "grad_norm": 0.0006115375435911119,
      "learning_rate": 2.6651007849011174e-05,
      "loss": 0.1669,
      "step": 263400
    },
    {
      "epoch": 0.911509230976785,
      "grad_norm": 0.008318240754306316,
      "learning_rate": 2.654723070696448e-05,
      "loss": 0.1395,
      "step": 263500
    },
    {
      "epoch": 0.9118551547836073,
      "grad_norm": 0.00025658286176621914,
      "learning_rate": 2.644345356491779e-05,
      "loss": 0.265,
      "step": 263600
    },
    {
      "epoch": 0.9122010785904296,
      "grad_norm": 0.0008963727741502225,
      "learning_rate": 2.6339676422871094e-05,
      "loss": 0.066,
      "step": 263700
    },
    {
      "epoch": 0.912547002397252,
      "grad_norm": 0.0005985600873827934,
      "learning_rate": 2.6235899280824403e-05,
      "loss": 0.0936,
      "step": 263800
    },
    {
      "epoch": 0.9128929262040743,
      "grad_norm": 56.38349533081055,
      "learning_rate": 2.613212213877771e-05,
      "loss": 0.1829,
      "step": 263900
    },
    {
      "epoch": 0.9132388500108966,
      "grad_norm": 0.008739937096834183,
      "learning_rate": 2.6028344996731018e-05,
      "loss": 0.3253,
      "step": 264000
    },
    {
      "epoch": 0.9135847738177189,
      "grad_norm": 0.00014966557500883937,
      "learning_rate": 2.5924567854684323e-05,
      "loss": 0.1814,
      "step": 264100
    },
    {
      "epoch": 0.9139306976245413,
      "grad_norm": 1.0929534435272217,
      "learning_rate": 2.582079071263763e-05,
      "loss": 0.1504,
      "step": 264200
    },
    {
      "epoch": 0.9142766214313636,
      "grad_norm": 0.0009370654006488621,
      "learning_rate": 2.571701357059094e-05,
      "loss": 0.1863,
      "step": 264300
    },
    {
      "epoch": 0.9146225452381859,
      "grad_norm": 10.166496276855469,
      "learning_rate": 2.5613236428544247e-05,
      "loss": 0.2227,
      "step": 264400
    },
    {
      "epoch": 0.9149684690450082,
      "grad_norm": 9.952622413635254,
      "learning_rate": 2.5509459286497556e-05,
      "loss": 0.1367,
      "step": 264500
    },
    {
      "epoch": 0.9153143928518305,
      "grad_norm": 0.0015066300984472036,
      "learning_rate": 2.540568214445086e-05,
      "loss": 0.1276,
      "step": 264600
    },
    {
      "epoch": 0.9156603166586528,
      "grad_norm": 44.63572692871094,
      "learning_rate": 2.530190500240417e-05,
      "loss": 0.1786,
      "step": 264700
    },
    {
      "epoch": 0.9160062404654751,
      "grad_norm": 2.2877919673919678,
      "learning_rate": 2.5198127860357476e-05,
      "loss": 0.176,
      "step": 264800
    },
    {
      "epoch": 0.9163521642722974,
      "grad_norm": 0.0007076023030094802,
      "learning_rate": 2.509435071831078e-05,
      "loss": 0.2799,
      "step": 264900
    },
    {
      "epoch": 0.9166980880791197,
      "grad_norm": 0.0008320743800140917,
      "learning_rate": 2.499057357626409e-05,
      "loss": 0.1954,
      "step": 265000
    },
    {
      "epoch": 0.917044011885942,
      "grad_norm": 0.000293064396828413,
      "learning_rate": 2.4886796434217396e-05,
      "loss": 0.2754,
      "step": 265100
    },
    {
      "epoch": 0.9173899356927643,
      "grad_norm": 0.0010639468673616648,
      "learning_rate": 2.4783019292170705e-05,
      "loss": 0.0534,
      "step": 265200
    },
    {
      "epoch": 0.9177358594995866,
      "grad_norm": 0.02533172443509102,
      "learning_rate": 2.467924215012401e-05,
      "loss": 0.2877,
      "step": 265300
    },
    {
      "epoch": 0.9180817833064089,
      "grad_norm": 0.0004344498156569898,
      "learning_rate": 2.457546500807732e-05,
      "loss": 0.3192,
      "step": 265400
    },
    {
      "epoch": 0.9184277071132312,
      "grad_norm": 0.01032167300581932,
      "learning_rate": 2.4471687866030625e-05,
      "loss": 0.1822,
      "step": 265500
    },
    {
      "epoch": 0.9187736309200536,
      "grad_norm": 0.0009110994287766516,
      "learning_rate": 2.436791072398393e-05,
      "loss": 0.3199,
      "step": 265600
    },
    {
      "epoch": 0.9191195547268759,
      "grad_norm": 0.00043857950367964804,
      "learning_rate": 2.426413358193724e-05,
      "loss": 0.252,
      "step": 265700
    },
    {
      "epoch": 0.9194654785336982,
      "grad_norm": 0.0008294623694382608,
      "learning_rate": 2.4160356439890545e-05,
      "loss": 0.2669,
      "step": 265800
    },
    {
      "epoch": 0.9198114023405205,
      "grad_norm": 0.0001562825927976519,
      "learning_rate": 2.4056579297843857e-05,
      "loss": 0.1209,
      "step": 265900
    },
    {
      "epoch": 0.9201573261473428,
      "grad_norm": 16.170894622802734,
      "learning_rate": 2.3952802155797163e-05,
      "loss": 0.2024,
      "step": 266000
    },
    {
      "epoch": 0.9205032499541651,
      "grad_norm": 0.0003236339252907783,
      "learning_rate": 2.3849025013750472e-05,
      "loss": 0.2461,
      "step": 266100
    },
    {
      "epoch": 0.9208491737609874,
      "grad_norm": 0.0003542019403539598,
      "learning_rate": 2.3745247871703777e-05,
      "loss": 0.2045,
      "step": 266200
    },
    {
      "epoch": 0.9211950975678097,
      "grad_norm": 0.0008814642787910998,
      "learning_rate": 2.3641470729657083e-05,
      "loss": 0.2947,
      "step": 266300
    },
    {
      "epoch": 0.921541021374632,
      "grad_norm": 0.003201123094186187,
      "learning_rate": 2.3537693587610392e-05,
      "loss": 0.1054,
      "step": 266400
    },
    {
      "epoch": 0.9218869451814543,
      "grad_norm": 0.0048438976518809795,
      "learning_rate": 2.3433916445563698e-05,
      "loss": 0.1364,
      "step": 266500
    },
    {
      "epoch": 0.9222328689882766,
      "grad_norm": 0.0009795097867026925,
      "learning_rate": 2.3330139303517006e-05,
      "loss": 0.1041,
      "step": 266600
    },
    {
      "epoch": 0.9225787927950989,
      "grad_norm": 0.005600341130048037,
      "learning_rate": 2.3226362161470312e-05,
      "loss": 0.0874,
      "step": 266700
    },
    {
      "epoch": 0.9229247166019212,
      "grad_norm": 0.0003064870834350586,
      "learning_rate": 2.312258501942362e-05,
      "loss": 0.1852,
      "step": 266800
    },
    {
      "epoch": 0.9232706404087435,
      "grad_norm": 0.00021219922928139567,
      "learning_rate": 2.3018807877376927e-05,
      "loss": 0.1233,
      "step": 266900
    },
    {
      "epoch": 0.9236165642155659,
      "grad_norm": 18.549001693725586,
      "learning_rate": 2.2915030735330232e-05,
      "loss": 0.2585,
      "step": 267000
    },
    {
      "epoch": 0.9239624880223882,
      "grad_norm": 8.898806117940694e-05,
      "learning_rate": 2.281125359328354e-05,
      "loss": 0.1744,
      "step": 267100
    },
    {
      "epoch": 0.9243084118292105,
      "grad_norm": 0.0007263725274242461,
      "learning_rate": 2.2707476451236847e-05,
      "loss": 0.1251,
      "step": 267200
    },
    {
      "epoch": 0.9246543356360328,
      "grad_norm": 0.009706325829029083,
      "learning_rate": 2.2603699309190156e-05,
      "loss": 0.243,
      "step": 267300
    },
    {
      "epoch": 0.9250002594428551,
      "grad_norm": 0.0004302828456275165,
      "learning_rate": 2.249992216714346e-05,
      "loss": 0.1902,
      "step": 267400
    },
    {
      "epoch": 0.9253461832496774,
      "grad_norm": 4.389293193817139,
      "learning_rate": 2.2396145025096773e-05,
      "loss": 0.0814,
      "step": 267500
    },
    {
      "epoch": 0.9256921070564997,
      "grad_norm": 0.000669808650854975,
      "learning_rate": 2.229236788305008e-05,
      "loss": 0.0678,
      "step": 267600
    },
    {
      "epoch": 0.926038030863322,
      "grad_norm": 0.0013789760414510965,
      "learning_rate": 2.218859074100338e-05,
      "loss": 0.1653,
      "step": 267700
    },
    {
      "epoch": 0.9263839546701443,
      "grad_norm": 6.38045821688138e-05,
      "learning_rate": 2.2084813598956694e-05,
      "loss": 0.1873,
      "step": 267800
    },
    {
      "epoch": 0.9267298784769666,
      "grad_norm": 0.0019250131445005536,
      "learning_rate": 2.198103645691e-05,
      "loss": 0.2077,
      "step": 267900
    },
    {
      "epoch": 0.9270758022837889,
      "grad_norm": 0.0011157214175909758,
      "learning_rate": 2.1877259314863308e-05,
      "loss": 0.1214,
      "step": 268000
    },
    {
      "epoch": 0.9274217260906112,
      "grad_norm": 18.732501983642578,
      "learning_rate": 2.1773482172816614e-05,
      "loss": 0.157,
      "step": 268100
    },
    {
      "epoch": 0.9277676498974335,
      "grad_norm": 107.16639709472656,
      "learning_rate": 2.1669705030769923e-05,
      "loss": 0.2038,
      "step": 268200
    },
    {
      "epoch": 0.9281135737042558,
      "grad_norm": 0.0025553726591169834,
      "learning_rate": 2.1565927888723228e-05,
      "loss": 0.1952,
      "step": 268300
    },
    {
      "epoch": 0.9284594975110783,
      "grad_norm": 0.0006176046445034444,
      "learning_rate": 2.1462150746676534e-05,
      "loss": 0.1087,
      "step": 268400
    },
    {
      "epoch": 0.9288054213179006,
      "grad_norm": 0.0002494369400665164,
      "learning_rate": 2.1358373604629843e-05,
      "loss": 0.142,
      "step": 268500
    },
    {
      "epoch": 0.9291513451247229,
      "grad_norm": 0.0004088830610271543,
      "learning_rate": 2.1254596462583148e-05,
      "loss": 0.2309,
      "step": 268600
    },
    {
      "epoch": 0.9294972689315452,
      "grad_norm": 0.0004512088489718735,
      "learning_rate": 2.1150819320536457e-05,
      "loss": 0.2167,
      "step": 268700
    },
    {
      "epoch": 0.9298431927383675,
      "grad_norm": 0.0008930944022722542,
      "learning_rate": 2.1047042178489763e-05,
      "loss": 0.1941,
      "step": 268800
    },
    {
      "epoch": 0.9301891165451898,
      "grad_norm": 0.0012570504331961274,
      "learning_rate": 2.094326503644307e-05,
      "loss": 0.2447,
      "step": 268900
    },
    {
      "epoch": 0.9305350403520121,
      "grad_norm": 0.5173380970954895,
      "learning_rate": 2.0839487894396377e-05,
      "loss": 0.2262,
      "step": 269000
    },
    {
      "epoch": 0.9308809641588344,
      "grad_norm": 0.009403363801538944,
      "learning_rate": 2.0735710752349683e-05,
      "loss": 0.1815,
      "step": 269100
    },
    {
      "epoch": 0.9312268879656567,
      "grad_norm": 0.0001870209234766662,
      "learning_rate": 2.0631933610302995e-05,
      "loss": 0.064,
      "step": 269200
    },
    {
      "epoch": 0.931572811772479,
      "grad_norm": 0.002241929993033409,
      "learning_rate": 2.0528156468256297e-05,
      "loss": 0.1162,
      "step": 269300
    },
    {
      "epoch": 0.9319187355793013,
      "grad_norm": 0.00028728737379424274,
      "learning_rate": 2.042437932620961e-05,
      "loss": 0.0807,
      "step": 269400
    },
    {
      "epoch": 0.9322646593861236,
      "grad_norm": 0.228598952293396,
      "learning_rate": 2.0320602184162915e-05,
      "loss": 0.3811,
      "step": 269500
    },
    {
      "epoch": 0.9326105831929459,
      "grad_norm": 22.136503219604492,
      "learning_rate": 2.0216825042116224e-05,
      "loss": 0.2775,
      "step": 269600
    },
    {
      "epoch": 0.9329565069997682,
      "grad_norm": 3.026726722717285,
      "learning_rate": 2.011304790006953e-05,
      "loss": 0.212,
      "step": 269700
    },
    {
      "epoch": 0.9333024308065906,
      "grad_norm": 0.00030739925568923354,
      "learning_rate": 2.0009270758022835e-05,
      "loss": 0.1564,
      "step": 269800
    },
    {
      "epoch": 0.9336483546134129,
      "grad_norm": 0.001018986920826137,
      "learning_rate": 1.9905493615976144e-05,
      "loss": 0.1393,
      "step": 269900
    },
    {
      "epoch": 0.9339942784202352,
      "grad_norm": 0.0015624769730493426,
      "learning_rate": 1.980171647392945e-05,
      "loss": 0.2367,
      "step": 270000
    },
    {
      "epoch": 0.9343402022270575,
      "grad_norm": 0.0005070731276646256,
      "learning_rate": 1.969793933188276e-05,
      "loss": 0.185,
      "step": 270100
    },
    {
      "epoch": 0.9346861260338798,
      "grad_norm": 0.6855233311653137,
      "learning_rate": 1.9594162189836064e-05,
      "loss": 0.0712,
      "step": 270200
    },
    {
      "epoch": 0.9350320498407021,
      "grad_norm": 0.001758191385306418,
      "learning_rate": 1.9490385047789373e-05,
      "loss": 0.1575,
      "step": 270300
    },
    {
      "epoch": 0.9353779736475244,
      "grad_norm": 16.05010986328125,
      "learning_rate": 1.938660790574268e-05,
      "loss": 0.0894,
      "step": 270400
    },
    {
      "epoch": 0.9357238974543467,
      "grad_norm": 0.001338761649094522,
      "learning_rate": 1.9282830763695984e-05,
      "loss": 0.324,
      "step": 270500
    },
    {
      "epoch": 0.936069821261169,
      "grad_norm": 112.19129180908203,
      "learning_rate": 1.9179053621649293e-05,
      "loss": 0.1556,
      "step": 270600
    },
    {
      "epoch": 0.9364157450679913,
      "grad_norm": 0.0015391447814181447,
      "learning_rate": 1.90752764796026e-05,
      "loss": 0.2317,
      "step": 270700
    },
    {
      "epoch": 0.9367616688748136,
      "grad_norm": 18.14146614074707,
      "learning_rate": 1.8971499337555908e-05,
      "loss": 0.2402,
      "step": 270800
    },
    {
      "epoch": 0.9371075926816359,
      "grad_norm": 0.02053004503250122,
      "learning_rate": 1.8867722195509213e-05,
      "loss": 0.2059,
      "step": 270900
    },
    {
      "epoch": 0.9374535164884582,
      "grad_norm": 0.0009323241538368165,
      "learning_rate": 1.8763945053462526e-05,
      "loss": 0.1009,
      "step": 271000
    },
    {
      "epoch": 0.9377994402952805,
      "grad_norm": 0.00021563995687756687,
      "learning_rate": 1.866016791141583e-05,
      "loss": 0.2239,
      "step": 271100
    },
    {
      "epoch": 0.9381453641021029,
      "grad_norm": 0.000647322682198137,
      "learning_rate": 1.8556390769369137e-05,
      "loss": 0.158,
      "step": 271200
    },
    {
      "epoch": 0.9384912879089252,
      "grad_norm": 6.626408139709383e-05,
      "learning_rate": 1.8452613627322446e-05,
      "loss": 0.1598,
      "step": 271300
    },
    {
      "epoch": 0.9388372117157475,
      "grad_norm": 0.0001676408137427643,
      "learning_rate": 1.834883648527575e-05,
      "loss": 0.198,
      "step": 271400
    },
    {
      "epoch": 0.9391831355225698,
      "grad_norm": 0.00019845824863296002,
      "learning_rate": 1.8245059343229057e-05,
      "loss": 0.0967,
      "step": 271500
    },
    {
      "epoch": 0.9395290593293921,
      "grad_norm": 0.00023427503765560687,
      "learning_rate": 1.8141282201182366e-05,
      "loss": 0.2647,
      "step": 271600
    },
    {
      "epoch": 0.9398749831362144,
      "grad_norm": 1.5581812858581543,
      "learning_rate": 1.803750505913567e-05,
      "loss": 0.1172,
      "step": 271700
    },
    {
      "epoch": 0.9402209069430367,
      "grad_norm": 0.0030270498245954514,
      "learning_rate": 1.793372791708898e-05,
      "loss": 0.0788,
      "step": 271800
    },
    {
      "epoch": 0.940566830749859,
      "grad_norm": 5.160566329956055,
      "learning_rate": 1.782995077504229e-05,
      "loss": 0.0972,
      "step": 271900
    },
    {
      "epoch": 0.9409127545566813,
      "grad_norm": 0.0007852342096157372,
      "learning_rate": 1.7726173632995595e-05,
      "loss": 0.1329,
      "step": 272000
    },
    {
      "epoch": 0.9412586783635036,
      "grad_norm": 0.0003285540733486414,
      "learning_rate": 1.7622396490948904e-05,
      "loss": 0.1703,
      "step": 272100
    },
    {
      "epoch": 0.9416046021703259,
      "grad_norm": 0.00038116276846267283,
      "learning_rate": 1.751861934890221e-05,
      "loss": 0.1928,
      "step": 272200
    },
    {
      "epoch": 0.9419505259771482,
      "grad_norm": 0.0008869408629834652,
      "learning_rate": 1.7414842206855515e-05,
      "loss": 0.1902,
      "step": 272300
    },
    {
      "epoch": 0.9422964497839705,
      "grad_norm": 0.03372595086693764,
      "learning_rate": 1.7311065064808824e-05,
      "loss": 0.0871,
      "step": 272400
    },
    {
      "epoch": 0.9426423735907928,
      "grad_norm": 0.001133349840529263,
      "learning_rate": 1.720728792276213e-05,
      "loss": 0.187,
      "step": 272500
    },
    {
      "epoch": 0.9429882973976152,
      "grad_norm": 0.00011935056681977585,
      "learning_rate": 1.7103510780715438e-05,
      "loss": 0.1211,
      "step": 272600
    },
    {
      "epoch": 0.9433342212044376,
      "grad_norm": 0.0022951439023017883,
      "learning_rate": 1.6999733638668747e-05,
      "loss": 0.0245,
      "step": 272700
    },
    {
      "epoch": 0.9436801450112599,
      "grad_norm": 11.323137283325195,
      "learning_rate": 1.6895956496622053e-05,
      "loss": 0.1405,
      "step": 272800
    },
    {
      "epoch": 0.9440260688180822,
      "grad_norm": 0.003452140837907791,
      "learning_rate": 1.6792179354575362e-05,
      "loss": 0.229,
      "step": 272900
    },
    {
      "epoch": 0.9443719926249045,
      "grad_norm": 0.0005125739262439311,
      "learning_rate": 1.6688402212528667e-05,
      "loss": 0.3445,
      "step": 273000
    },
    {
      "epoch": 0.9447179164317268,
      "grad_norm": 0.0004323673783801496,
      "learning_rate": 1.6584625070481973e-05,
      "loss": 0.0283,
      "step": 273100
    },
    {
      "epoch": 0.9450638402385491,
      "grad_norm": 18.422639846801758,
      "learning_rate": 1.6480847928435282e-05,
      "loss": 0.2148,
      "step": 273200
    },
    {
      "epoch": 0.9454097640453714,
      "grad_norm": 0.005950715392827988,
      "learning_rate": 1.6377070786388587e-05,
      "loss": 0.2614,
      "step": 273300
    },
    {
      "epoch": 0.9457556878521937,
      "grad_norm": 6.484456389443949e-05,
      "learning_rate": 1.6273293644341896e-05,
      "loss": 0.2601,
      "step": 273400
    },
    {
      "epoch": 0.946101611659016,
      "grad_norm": 0.00010918454063357785,
      "learning_rate": 1.6169516502295205e-05,
      "loss": 0.2186,
      "step": 273500
    },
    {
      "epoch": 0.9464475354658383,
      "grad_norm": 0.00025436648866161704,
      "learning_rate": 1.606573936024851e-05,
      "loss": 0.188,
      "step": 273600
    },
    {
      "epoch": 0.9467934592726606,
      "grad_norm": 6.0423320974223316e-05,
      "learning_rate": 1.5961962218201816e-05,
      "loss": 0.2891,
      "step": 273700
    },
    {
      "epoch": 0.9471393830794829,
      "grad_norm": 0.00362473470158875,
      "learning_rate": 1.5858185076155125e-05,
      "loss": 0.0985,
      "step": 273800
    },
    {
      "epoch": 0.9474853068863052,
      "grad_norm": 28.079086303710938,
      "learning_rate": 1.575440793410843e-05,
      "loss": 0.1465,
      "step": 273900
    },
    {
      "epoch": 0.9478312306931276,
      "grad_norm": 0.0003982969792559743,
      "learning_rate": 1.565063079206174e-05,
      "loss": 0.1819,
      "step": 274000
    },
    {
      "epoch": 0.9481771544999499,
      "grad_norm": 0.07776357978582382,
      "learning_rate": 1.5546853650015045e-05,
      "loss": 0.1556,
      "step": 274100
    },
    {
      "epoch": 0.9485230783067722,
      "grad_norm": 0.13833613693714142,
      "learning_rate": 1.5443076507968354e-05,
      "loss": 0.1615,
      "step": 274200
    },
    {
      "epoch": 0.9488690021135945,
      "grad_norm": 0.005206149071455002,
      "learning_rate": 1.5339299365921663e-05,
      "loss": 0.3342,
      "step": 274300
    },
    {
      "epoch": 0.9492149259204168,
      "grad_norm": 0.011554688215255737,
      "learning_rate": 1.5235522223874967e-05,
      "loss": 0.1654,
      "step": 274400
    },
    {
      "epoch": 0.9495608497272391,
      "grad_norm": 0.0005975172389298677,
      "learning_rate": 1.5131745081828274e-05,
      "loss": 0.0991,
      "step": 274500
    },
    {
      "epoch": 0.9499067735340614,
      "grad_norm": 0.0015724699478596449,
      "learning_rate": 1.5027967939781582e-05,
      "loss": 0.1291,
      "step": 274600
    },
    {
      "epoch": 0.9502526973408837,
      "grad_norm": 0.0014881648821756244,
      "learning_rate": 1.4924190797734889e-05,
      "loss": 0.156,
      "step": 274700
    },
    {
      "epoch": 0.950598621147706,
      "grad_norm": 0.7206903100013733,
      "learning_rate": 1.4820413655688198e-05,
      "loss": 0.3593,
      "step": 274800
    },
    {
      "epoch": 0.9509445449545283,
      "grad_norm": 0.0004464220837689936,
      "learning_rate": 1.4716636513641505e-05,
      "loss": 0.1906,
      "step": 274900
    },
    {
      "epoch": 0.9512904687613506,
      "grad_norm": 0.0027626133523881435,
      "learning_rate": 1.4612859371594812e-05,
      "loss": 0.2296,
      "step": 275000
    },
    {
      "epoch": 0.9516363925681729,
      "grad_norm": 0.00032926702988334,
      "learning_rate": 1.4509082229548118e-05,
      "loss": 0.2011,
      "step": 275100
    },
    {
      "epoch": 0.9519823163749952,
      "grad_norm": 0.000882286811247468,
      "learning_rate": 1.4405305087501425e-05,
      "loss": 0.1066,
      "step": 275200
    },
    {
      "epoch": 0.9523282401818175,
      "grad_norm": 0.0003816823591478169,
      "learning_rate": 1.4301527945454732e-05,
      "loss": 0.3165,
      "step": 275300
    },
    {
      "epoch": 0.9526741639886399,
      "grad_norm": 0.00227316259406507,
      "learning_rate": 1.419775080340804e-05,
      "loss": 0.248,
      "step": 275400
    },
    {
      "epoch": 0.9530200877954622,
      "grad_norm": 114.12724304199219,
      "learning_rate": 1.4093973661361347e-05,
      "loss": 0.2056,
      "step": 275500
    },
    {
      "epoch": 0.9533660116022845,
      "grad_norm": 0.000680803379509598,
      "learning_rate": 1.3990196519314656e-05,
      "loss": 0.2339,
      "step": 275600
    },
    {
      "epoch": 0.9537119354091068,
      "grad_norm": 5.343061638996005e-05,
      "learning_rate": 1.3886419377267963e-05,
      "loss": 0.1673,
      "step": 275700
    },
    {
      "epoch": 0.9540578592159291,
      "grad_norm": 0.00022951985010877252,
      "learning_rate": 1.3782642235221269e-05,
      "loss": 0.1422,
      "step": 275800
    },
    {
      "epoch": 0.9544037830227514,
      "grad_norm": 0.00035475537879392505,
      "learning_rate": 1.3678865093174576e-05,
      "loss": 0.1026,
      "step": 275900
    },
    {
      "epoch": 0.9547497068295737,
      "grad_norm": 0.3987214267253876,
      "learning_rate": 1.3575087951127883e-05,
      "loss": 0.1412,
      "step": 276000
    },
    {
      "epoch": 0.955095630636396,
      "grad_norm": 0.0005658420268446207,
      "learning_rate": 1.347131080908119e-05,
      "loss": 0.2127,
      "step": 276100
    },
    {
      "epoch": 0.9554415544432183,
      "grad_norm": 0.0018256286857649684,
      "learning_rate": 1.3367533667034498e-05,
      "loss": 0.1735,
      "step": 276200
    },
    {
      "epoch": 0.9557874782500406,
      "grad_norm": 0.0008837440400384367,
      "learning_rate": 1.3263756524987805e-05,
      "loss": 0.1657,
      "step": 276300
    },
    {
      "epoch": 0.9561334020568629,
      "grad_norm": 0.0001669104240136221,
      "learning_rate": 1.3159979382941114e-05,
      "loss": 0.2772,
      "step": 276400
    },
    {
      "epoch": 0.9564793258636852,
      "grad_norm": 0.01885562762618065,
      "learning_rate": 1.305620224089442e-05,
      "loss": 0.1516,
      "step": 276500
    },
    {
      "epoch": 0.9568252496705075,
      "grad_norm": 0.0002066808083327487,
      "learning_rate": 1.2952425098847727e-05,
      "loss": 0.1763,
      "step": 276600
    },
    {
      "epoch": 0.9571711734773298,
      "grad_norm": 0.0020255278795957565,
      "learning_rate": 1.2848647956801034e-05,
      "loss": 0.1496,
      "step": 276700
    },
    {
      "epoch": 0.9575170972841522,
      "grad_norm": 0.00040541612543165684,
      "learning_rate": 1.2744870814754341e-05,
      "loss": 0.1404,
      "step": 276800
    },
    {
      "epoch": 0.9578630210909745,
      "grad_norm": 0.18152369558811188,
      "learning_rate": 1.2641093672707648e-05,
      "loss": 0.1818,
      "step": 276900
    },
    {
      "epoch": 0.9582089448977968,
      "grad_norm": 0.7591705918312073,
      "learning_rate": 1.2537316530660956e-05,
      "loss": 0.2351,
      "step": 277000
    },
    {
      "epoch": 0.9585548687046191,
      "grad_norm": 0.0003717607178259641,
      "learning_rate": 1.2433539388614263e-05,
      "loss": 0.156,
      "step": 277100
    },
    {
      "epoch": 0.9589007925114414,
      "grad_norm": 0.17146359384059906,
      "learning_rate": 1.2329762246567569e-05,
      "loss": 0.2413,
      "step": 277200
    },
    {
      "epoch": 0.9592467163182637,
      "grad_norm": 0.0004164472338743508,
      "learning_rate": 1.2225985104520876e-05,
      "loss": 0.1978,
      "step": 277300
    },
    {
      "epoch": 0.959592640125086,
      "grad_norm": 0.0021749655716121197,
      "learning_rate": 1.2122207962474185e-05,
      "loss": 0.1586,
      "step": 277400
    },
    {
      "epoch": 0.9599385639319084,
      "grad_norm": 0.001306131831370294,
      "learning_rate": 1.2018430820427492e-05,
      "loss": 0.1064,
      "step": 277500
    },
    {
      "epoch": 0.9602844877387307,
      "grad_norm": 0.011665052734315395,
      "learning_rate": 1.19146536783808e-05,
      "loss": 0.0798,
      "step": 277600
    },
    {
      "epoch": 0.960630411545553,
      "grad_norm": 10.953960418701172,
      "learning_rate": 1.1810876536334107e-05,
      "loss": 0.1691,
      "step": 277700
    },
    {
      "epoch": 0.9609763353523753,
      "grad_norm": 0.0049968198873102665,
      "learning_rate": 1.1707099394287414e-05,
      "loss": 0.1513,
      "step": 277800
    },
    {
      "epoch": 0.9613222591591976,
      "grad_norm": 0.2277696132659912,
      "learning_rate": 1.160332225224072e-05,
      "loss": 0.2373,
      "step": 277900
    },
    {
      "epoch": 0.9616681829660199,
      "grad_norm": 71.73889923095703,
      "learning_rate": 1.1499545110194027e-05,
      "loss": 0.2217,
      "step": 278000
    },
    {
      "epoch": 0.9620141067728422,
      "grad_norm": 0.003950695041567087,
      "learning_rate": 1.1395767968147334e-05,
      "loss": 0.1927,
      "step": 278100
    },
    {
      "epoch": 0.9623600305796646,
      "grad_norm": 0.0017575954552739859,
      "learning_rate": 1.1291990826100643e-05,
      "loss": 0.1444,
      "step": 278200
    },
    {
      "epoch": 0.9627059543864869,
      "grad_norm": 0.000883624074049294,
      "learning_rate": 1.118821368405395e-05,
      "loss": 0.205,
      "step": 278300
    },
    {
      "epoch": 0.9630518781933092,
      "grad_norm": 0.00012121826875954866,
      "learning_rate": 1.1084436542007257e-05,
      "loss": 0.2406,
      "step": 278400
    },
    {
      "epoch": 0.9633978020001315,
      "grad_norm": 0.12691964209079742,
      "learning_rate": 1.0980659399960565e-05,
      "loss": 0.2326,
      "step": 278500
    },
    {
      "epoch": 0.9637437258069538,
      "grad_norm": 0.1831432580947876,
      "learning_rate": 1.087688225791387e-05,
      "loss": 0.2155,
      "step": 278600
    },
    {
      "epoch": 0.9640896496137761,
      "grad_norm": 0.002695513190701604,
      "learning_rate": 1.0773105115867177e-05,
      "loss": 0.1563,
      "step": 278700
    },
    {
      "epoch": 0.9644355734205984,
      "grad_norm": 0.00012871854414697737,
      "learning_rate": 1.0669327973820485e-05,
      "loss": 0.2447,
      "step": 278800
    },
    {
      "epoch": 0.9647814972274207,
      "grad_norm": 0.19877122342586517,
      "learning_rate": 1.0565550831773792e-05,
      "loss": 0.1755,
      "step": 278900
    },
    {
      "epoch": 0.965127421034243,
      "grad_norm": 0.003011179156601429,
      "learning_rate": 1.04617736897271e-05,
      "loss": 0.1883,
      "step": 279000
    },
    {
      "epoch": 0.9654733448410653,
      "grad_norm": 0.00402912637218833,
      "learning_rate": 1.0357996547680408e-05,
      "loss": 0.2163,
      "step": 279100
    },
    {
      "epoch": 0.9658192686478876,
      "grad_norm": 0.023293443024158478,
      "learning_rate": 1.0254219405633715e-05,
      "loss": 0.1141,
      "step": 279200
    },
    {
      "epoch": 0.9661651924547099,
      "grad_norm": 0.0023986459709703922,
      "learning_rate": 1.0150442263587021e-05,
      "loss": 0.3315,
      "step": 279300
    },
    {
      "epoch": 0.9665111162615322,
      "grad_norm": 0.00032405569800175726,
      "learning_rate": 1.0046665121540328e-05,
      "loss": 0.0375,
      "step": 279400
    },
    {
      "epoch": 0.9668570400683545,
      "grad_norm": 1.3742454051971436,
      "learning_rate": 9.942887979493635e-06,
      "loss": 0.1782,
      "step": 279500
    },
    {
      "epoch": 0.9672029638751769,
      "grad_norm": 0.0006406214088201523,
      "learning_rate": 9.839110837446943e-06,
      "loss": 0.2111,
      "step": 279600
    },
    {
      "epoch": 0.9675488876819992,
      "grad_norm": 0.0002528259065002203,
      "learning_rate": 9.73533369540025e-06,
      "loss": 0.3027,
      "step": 279700
    },
    {
      "epoch": 0.9678948114888215,
      "grad_norm": 0.0006789180333726108,
      "learning_rate": 9.631556553353559e-06,
      "loss": 0.392,
      "step": 279800
    },
    {
      "epoch": 0.9682407352956438,
      "grad_norm": 0.00013309998030308634,
      "learning_rate": 9.527779411306866e-06,
      "loss": 0.2885,
      "step": 279900
    },
    {
      "epoch": 0.9685866591024661,
      "grad_norm": 6.537261009216309,
      "learning_rate": 9.424002269260172e-06,
      "loss": 0.1725,
      "step": 280000
    },
    {
      "epoch": 0.9689325829092884,
      "grad_norm": 0.0012807825114578009,
      "learning_rate": 9.320225127213479e-06,
      "loss": 0.1868,
      "step": 280100
    },
    {
      "epoch": 0.9692785067161107,
      "grad_norm": 0.0015808341559022665,
      "learning_rate": 9.216447985166786e-06,
      "loss": 0.0693,
      "step": 280200
    },
    {
      "epoch": 0.969624430522933,
      "grad_norm": 0.00035733336699195206,
      "learning_rate": 9.112670843120093e-06,
      "loss": 0.3132,
      "step": 280300
    },
    {
      "epoch": 0.9699703543297553,
      "grad_norm": 0.0008088494651019573,
      "learning_rate": 9.0088937010734e-06,
      "loss": 0.2944,
      "step": 280400
    },
    {
      "epoch": 0.9703162781365776,
      "grad_norm": 5.616684913635254,
      "learning_rate": 8.905116559026708e-06,
      "loss": 0.1294,
      "step": 280500
    },
    {
      "epoch": 0.9706622019433999,
      "grad_norm": 0.00012649637938011438,
      "learning_rate": 8.801339416980015e-06,
      "loss": 0.2723,
      "step": 280600
    },
    {
      "epoch": 0.9710081257502222,
      "grad_norm": 0.0023946985602378845,
      "learning_rate": 8.697562274933322e-06,
      "loss": 0.2525,
      "step": 280700
    },
    {
      "epoch": 0.9713540495570445,
      "grad_norm": 0.0035294999834150076,
      "learning_rate": 8.59378513288663e-06,
      "loss": 0.1866,
      "step": 280800
    },
    {
      "epoch": 0.9716999733638668,
      "grad_norm": 21.159793853759766,
      "learning_rate": 8.490007990839937e-06,
      "loss": 0.0644,
      "step": 280900
    },
    {
      "epoch": 0.9720458971706892,
      "grad_norm": 0.0009286933345720172,
      "learning_rate": 8.386230848793244e-06,
      "loss": 0.142,
      "step": 281000
    },
    {
      "epoch": 0.9723918209775115,
      "grad_norm": 0.00023192031949292868,
      "learning_rate": 8.282453706746551e-06,
      "loss": 0.2585,
      "step": 281100
    },
    {
      "epoch": 0.9727377447843338,
      "grad_norm": 0.00017338217003270984,
      "learning_rate": 8.178676564699859e-06,
      "loss": 0.2115,
      "step": 281200
    },
    {
      "epoch": 0.9730836685911561,
      "grad_norm": 0.0066274721175432205,
      "learning_rate": 8.074899422653166e-06,
      "loss": 0.1689,
      "step": 281300
    },
    {
      "epoch": 0.9734295923979784,
      "grad_norm": 0.0013207033043727279,
      "learning_rate": 7.971122280606473e-06,
      "loss": 0.0804,
      "step": 281400
    },
    {
      "epoch": 0.9737755162048007,
      "grad_norm": 0.0007939511560834944,
      "learning_rate": 7.86734513855978e-06,
      "loss": 0.0864,
      "step": 281500
    },
    {
      "epoch": 0.974121440011623,
      "grad_norm": 0.00044813501881435513,
      "learning_rate": 7.763567996513088e-06,
      "loss": 0.1881,
      "step": 281600
    },
    {
      "epoch": 0.9744673638184453,
      "grad_norm": 21.271739959716797,
      "learning_rate": 7.659790854466395e-06,
      "loss": 0.0677,
      "step": 281700
    },
    {
      "epoch": 0.9748132876252676,
      "grad_norm": 0.003943456336855888,
      "learning_rate": 7.556013712419702e-06,
      "loss": 0.2256,
      "step": 281800
    },
    {
      "epoch": 0.97515921143209,
      "grad_norm": 0.0006128125824034214,
      "learning_rate": 7.452236570373009e-06,
      "loss": 0.208,
      "step": 281900
    },
    {
      "epoch": 0.9755051352389122,
      "grad_norm": 0.0006219949573278427,
      "learning_rate": 7.348459428326316e-06,
      "loss": 0.1353,
      "step": 282000
    },
    {
      "epoch": 0.9758510590457345,
      "grad_norm": 11.456669807434082,
      "learning_rate": 7.244682286279623e-06,
      "loss": 0.1526,
      "step": 282100
    },
    {
      "epoch": 0.9761969828525568,
      "grad_norm": 0.00014377057959791273,
      "learning_rate": 7.140905144232931e-06,
      "loss": 0.1912,
      "step": 282200
    },
    {
      "epoch": 0.9765429066593792,
      "grad_norm": 0.00026452154270373285,
      "learning_rate": 7.037128002186238e-06,
      "loss": 0.2219,
      "step": 282300
    },
    {
      "epoch": 0.9768888304662016,
      "grad_norm": 0.000996841350570321,
      "learning_rate": 6.933350860139545e-06,
      "loss": 0.104,
      "step": 282400
    },
    {
      "epoch": 0.9772347542730239,
      "grad_norm": 0.00012373628851491958,
      "learning_rate": 6.829573718092852e-06,
      "loss": 0.1951,
      "step": 282500
    },
    {
      "epoch": 0.9775806780798462,
      "grad_norm": 0.0006259474321268499,
      "learning_rate": 6.725796576046159e-06,
      "loss": 0.1919,
      "step": 282600
    },
    {
      "epoch": 0.9779266018866685,
      "grad_norm": 0.00445996318012476,
      "learning_rate": 6.622019433999467e-06,
      "loss": 0.1888,
      "step": 282700
    },
    {
      "epoch": 0.9782725256934908,
      "grad_norm": 0.0018549704691395164,
      "learning_rate": 6.518242291952774e-06,
      "loss": 0.329,
      "step": 282800
    },
    {
      "epoch": 0.9786184495003131,
      "grad_norm": 0.0004149480373598635,
      "learning_rate": 6.414465149906081e-06,
      "loss": 0.2683,
      "step": 282900
    },
    {
      "epoch": 0.9789643733071354,
      "grad_norm": 0.00018417455430608243,
      "learning_rate": 6.310688007859388e-06,
      "loss": 0.191,
      "step": 283000
    },
    {
      "epoch": 0.9793102971139577,
      "grad_norm": 0.0010056005557999015,
      "learning_rate": 6.206910865812696e-06,
      "loss": 0.2589,
      "step": 283100
    },
    {
      "epoch": 0.97965622092078,
      "grad_norm": 5.909771425649524e-05,
      "learning_rate": 6.103133723766003e-06,
      "loss": 0.2062,
      "step": 283200
    },
    {
      "epoch": 0.9800021447276023,
      "grad_norm": 0.0027008485049009323,
      "learning_rate": 5.999356581719309e-06,
      "loss": 0.1956,
      "step": 283300
    },
    {
      "epoch": 0.9803480685344246,
      "grad_norm": 0.0005932521889917552,
      "learning_rate": 5.8955794396726166e-06,
      "loss": 0.0975,
      "step": 283400
    },
    {
      "epoch": 0.9806939923412469,
      "grad_norm": 0.11299054324626923,
      "learning_rate": 5.791802297625925e-06,
      "loss": 0.1476,
      "step": 283500
    },
    {
      "epoch": 0.9810399161480692,
      "grad_norm": 0.00047219221596606076,
      "learning_rate": 5.688025155579232e-06,
      "loss": 0.0628,
      "step": 283600
    },
    {
      "epoch": 0.9813858399548915,
      "grad_norm": 11.433547973632812,
      "learning_rate": 5.584248013532538e-06,
      "loss": 0.1234,
      "step": 283700
    },
    {
      "epoch": 0.9817317637617139,
      "grad_norm": 0.03171977400779724,
      "learning_rate": 5.480470871485846e-06,
      "loss": 0.3247,
      "step": 283800
    },
    {
      "epoch": 0.9820776875685362,
      "grad_norm": 0.0004462819779291749,
      "learning_rate": 5.376693729439154e-06,
      "loss": 0.0737,
      "step": 283900
    },
    {
      "epoch": 0.9824236113753585,
      "grad_norm": 1.7945748567581177,
      "learning_rate": 5.27291658739246e-06,
      "loss": 0.1619,
      "step": 284000
    },
    {
      "epoch": 0.9827695351821808,
      "grad_norm": 0.0002275299048051238,
      "learning_rate": 5.169139445345767e-06,
      "loss": 0.3746,
      "step": 284100
    },
    {
      "epoch": 0.9831154589890031,
      "grad_norm": 0.0023487505968660116,
      "learning_rate": 5.065362303299075e-06,
      "loss": 0.067,
      "step": 284200
    },
    {
      "epoch": 0.9834613827958254,
      "grad_norm": 92.5155258178711,
      "learning_rate": 4.961585161252383e-06,
      "loss": 0.2171,
      "step": 284300
    },
    {
      "epoch": 0.9838073066026477,
      "grad_norm": 0.0008326130919158459,
      "learning_rate": 4.857808019205689e-06,
      "loss": 0.2367,
      "step": 284400
    },
    {
      "epoch": 0.98415323040947,
      "grad_norm": 0.00020178567501716316,
      "learning_rate": 4.754030877158996e-06,
      "loss": 0.1599,
      "step": 284500
    },
    {
      "epoch": 0.9844991542162923,
      "grad_norm": 0.0004390505491755903,
      "learning_rate": 4.650253735112304e-06,
      "loss": 0.2564,
      "step": 284600
    },
    {
      "epoch": 0.9848450780231146,
      "grad_norm": 0.006666568573564291,
      "learning_rate": 4.546476593065611e-06,
      "loss": 0.033,
      "step": 284700
    },
    {
      "epoch": 0.9851910018299369,
      "grad_norm": 0.0006020013825036585,
      "learning_rate": 4.442699451018918e-06,
      "loss": 0.0964,
      "step": 284800
    },
    {
      "epoch": 0.9855369256367592,
      "grad_norm": 0.006375639233738184,
      "learning_rate": 4.338922308972225e-06,
      "loss": 0.1694,
      "step": 284900
    },
    {
      "epoch": 0.9858828494435815,
      "grad_norm": 0.0003882167802657932,
      "learning_rate": 4.235145166925533e-06,
      "loss": 0.1439,
      "step": 285000
    },
    {
      "epoch": 0.9862287732504038,
      "grad_norm": 0.00022175135381985456,
      "learning_rate": 4.13136802487884e-06,
      "loss": 0.2289,
      "step": 285100
    },
    {
      "epoch": 0.9865746970572262,
      "grad_norm": 0.0004718452983070165,
      "learning_rate": 4.027590882832147e-06,
      "loss": 0.2565,
      "step": 285200
    },
    {
      "epoch": 0.9869206208640485,
      "grad_norm": 94.24354553222656,
      "learning_rate": 3.923813740785454e-06,
      "loss": 0.15,
      "step": 285300
    },
    {
      "epoch": 0.9872665446708708,
      "grad_norm": 0.0011269538663327694,
      "learning_rate": 3.820036598738762e-06,
      "loss": 0.1763,
      "step": 285400
    },
    {
      "epoch": 0.9876124684776931,
      "grad_norm": 0.0008312425925396383,
      "learning_rate": 3.7162594566920685e-06,
      "loss": 0.2315,
      "step": 285500
    },
    {
      "epoch": 0.9879583922845154,
      "grad_norm": 19.002525329589844,
      "learning_rate": 3.612482314645376e-06,
      "loss": 0.1568,
      "step": 285600
    },
    {
      "epoch": 0.9883043160913377,
      "grad_norm": 0.0009376744856126606,
      "learning_rate": 3.508705172598683e-06,
      "loss": 0.1192,
      "step": 285700
    },
    {
      "epoch": 0.98865023989816,
      "grad_norm": 2.105615749314893e-05,
      "learning_rate": 3.4049280305519907e-06,
      "loss": 0.1087,
      "step": 285800
    },
    {
      "epoch": 0.9889961637049823,
      "grad_norm": 0.0006147018284536898,
      "learning_rate": 3.3011508885052975e-06,
      "loss": 0.032,
      "step": 285900
    },
    {
      "epoch": 0.9893420875118046,
      "grad_norm": 0.3041434586048126,
      "learning_rate": 3.1973737464586047e-06,
      "loss": 0.105,
      "step": 286000
    },
    {
      "epoch": 0.9896880113186269,
      "grad_norm": 4.5983546442585066e-05,
      "learning_rate": 3.093596604411912e-06,
      "loss": 0.3061,
      "step": 286100
    },
    {
      "epoch": 0.9900339351254492,
      "grad_norm": 1.1506524085998535,
      "learning_rate": 2.989819462365219e-06,
      "loss": 0.2202,
      "step": 286200
    },
    {
      "epoch": 0.9903798589322715,
      "grad_norm": 41.94195556640625,
      "learning_rate": 2.8860423203185265e-06,
      "loss": 0.2659,
      "step": 286300
    },
    {
      "epoch": 0.9907257827390938,
      "grad_norm": 0.0002807876153383404,
      "learning_rate": 2.7822651782718333e-06,
      "loss": 0.203,
      "step": 286400
    },
    {
      "epoch": 0.9910717065459161,
      "grad_norm": 0.000551643199287355,
      "learning_rate": 2.678488036225141e-06,
      "loss": 0.1709,
      "step": 286500
    },
    {
      "epoch": 0.9914176303527386,
      "grad_norm": 0.25014784932136536,
      "learning_rate": 2.574710894178448e-06,
      "loss": 0.172,
      "step": 286600
    },
    {
      "epoch": 0.9917635541595609,
      "grad_norm": 0.010677655227482319,
      "learning_rate": 2.4709337521317555e-06,
      "loss": 0.2881,
      "step": 286700
    },
    {
      "epoch": 0.9921094779663832,
      "grad_norm": 0.001995290629565716,
      "learning_rate": 2.3671566100850623e-06,
      "loss": 0.2162,
      "step": 286800
    },
    {
      "epoch": 0.9924554017732055,
      "grad_norm": 0.00303830043412745,
      "learning_rate": 2.2633794680383696e-06,
      "loss": 0.1069,
      "step": 286900
    },
    {
      "epoch": 0.9928013255800278,
      "grad_norm": 0.0003490461967885494,
      "learning_rate": 2.159602325991677e-06,
      "loss": 0.1166,
      "step": 287000
    },
    {
      "epoch": 0.9931472493868501,
      "grad_norm": 0.00046455342089757323,
      "learning_rate": 2.055825183944984e-06,
      "loss": 0.1102,
      "step": 287100
    },
    {
      "epoch": 0.9934931731936724,
      "grad_norm": 0.001216721604578197,
      "learning_rate": 1.9520480418982914e-06,
      "loss": 0.146,
      "step": 287200
    },
    {
      "epoch": 0.9938390970004947,
      "grad_norm": 0.0003242857346776873,
      "learning_rate": 1.8482708998515984e-06,
      "loss": 0.2572,
      "step": 287300
    },
    {
      "epoch": 0.994185020807317,
      "grad_norm": 0.0018923834431916475,
      "learning_rate": 1.7444937578049057e-06,
      "loss": 0.1948,
      "step": 287400
    },
    {
      "epoch": 0.9945309446141393,
      "grad_norm": 0.0006484450423158705,
      "learning_rate": 1.640716615758213e-06,
      "loss": 0.1856,
      "step": 287500
    },
    {
      "epoch": 0.9948768684209616,
      "grad_norm": 0.0004551169404294342,
      "learning_rate": 1.5369394737115202e-06,
      "loss": 0.1448,
      "step": 287600
    },
    {
      "epoch": 0.9952227922277839,
      "grad_norm": 0.00010754872346296906,
      "learning_rate": 1.4331623316648274e-06,
      "loss": 0.3067,
      "step": 287700
    },
    {
      "epoch": 0.9955687160346062,
      "grad_norm": 0.005742148496210575,
      "learning_rate": 1.3293851896181347e-06,
      "loss": 0.1252,
      "step": 287800
    },
    {
      "epoch": 0.9959146398414285,
      "grad_norm": 0.0005422029062174261,
      "learning_rate": 1.225608047571442e-06,
      "loss": 0.2859,
      "step": 287900
    },
    {
      "epoch": 0.9962605636482509,
      "grad_norm": 0.0002878979139495641,
      "learning_rate": 1.121830905524749e-06,
      "loss": 0.1797,
      "step": 288000
    },
    {
      "epoch": 0.9966064874550732,
      "grad_norm": 0.00033963992609642446,
      "learning_rate": 1.0180537634780562e-06,
      "loss": 0.2261,
      "step": 288100
    },
    {
      "epoch": 0.9969524112618955,
      "grad_norm": 7.467076648026705e-05,
      "learning_rate": 9.142766214313634e-07,
      "loss": 0.0983,
      "step": 288200
    },
    {
      "epoch": 0.9972983350687178,
      "grad_norm": 0.24948710203170776,
      "learning_rate": 8.104994793846706e-07,
      "loss": 0.1978,
      "step": 288300
    },
    {
      "epoch": 0.9976442588755401,
      "grad_norm": 0.13849180936813354,
      "learning_rate": 7.067223373379779e-07,
      "loss": 0.185,
      "step": 288400
    },
    {
      "epoch": 0.9979901826823624,
      "grad_norm": 0.00024896199465729296,
      "learning_rate": 6.029451952912851e-07,
      "loss": 0.0888,
      "step": 288500
    },
    {
      "epoch": 0.9983361064891847,
      "grad_norm": 0.0007613972411490977,
      "learning_rate": 4.991680532445923e-07,
      "loss": 0.1357,
      "step": 288600
    },
    {
      "epoch": 0.998682030296007,
      "grad_norm": 0.002386606065556407,
      "learning_rate": 3.9539091119789947e-07,
      "loss": 0.0893,
      "step": 288700
    },
    {
      "epoch": 0.9990279541028293,
      "grad_norm": 0.0005744424997828901,
      "learning_rate": 2.916137691512067e-07,
      "loss": 0.2318,
      "step": 288800
    },
    {
      "epoch": 0.9993738779096516,
      "grad_norm": 0.00023767845414113253,
      "learning_rate": 1.8783662710451393e-07,
      "loss": 0.2644,
      "step": 288900
    },
    {
      "epoch": 0.9997198017164739,
      "grad_norm": 0.0012984367785975337,
      "learning_rate": 8.405948505782117e-08,
      "loss": 0.1057,
      "step": 289000
    }
  ],
  "logging_steps": 100,
  "max_steps": 289081,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1560587753892368e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
