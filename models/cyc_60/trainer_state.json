{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 313705,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0003187708197191629,
      "grad_norm": 3.7841312885284424,
      "learning_rate": 0.0002999043687540842,
      "loss": 3.6169,
      "step": 100
    },
    {
      "epoch": 0.0006375416394383258,
      "grad_norm": 3.8942079544067383,
      "learning_rate": 0.0002998087375081685,
      "loss": 3.6398,
      "step": 200
    },
    {
      "epoch": 0.0009563124591574887,
      "grad_norm": 2.759291648864746,
      "learning_rate": 0.00029971310626225275,
      "loss": 3.6571,
      "step": 300
    },
    {
      "epoch": 0.0012750832788766516,
      "grad_norm": 3.6858677864074707,
      "learning_rate": 0.000299617475016337,
      "loss": 3.6182,
      "step": 400
    },
    {
      "epoch": 0.0015938540985958145,
      "grad_norm": 3.3963100910186768,
      "learning_rate": 0.00029952184377042123,
      "loss": 3.6227,
      "step": 500
    },
    {
      "epoch": 0.0019126249183149774,
      "grad_norm": 3.6897919178009033,
      "learning_rate": 0.00029942621252450547,
      "loss": 3.6208,
      "step": 600
    },
    {
      "epoch": 0.0022313957380341405,
      "grad_norm": 3.0374433994293213,
      "learning_rate": 0.00029933058127858976,
      "loss": 3.6458,
      "step": 700
    },
    {
      "epoch": 0.002550166557753303,
      "grad_norm": 3.597485065460205,
      "learning_rate": 0.000299234950032674,
      "loss": 3.5639,
      "step": 800
    },
    {
      "epoch": 0.0028689373774724663,
      "grad_norm": 4.148677349090576,
      "learning_rate": 0.00029913931878675825,
      "loss": 3.4337,
      "step": 900
    },
    {
      "epoch": 0.003187708197191629,
      "grad_norm": 3.5693373680114746,
      "learning_rate": 0.0002990436875408425,
      "loss": 3.4255,
      "step": 1000
    },
    {
      "epoch": 0.003506479016910792,
      "grad_norm": 3.855780601501465,
      "learning_rate": 0.0002989480562949267,
      "loss": 3.368,
      "step": 1100
    },
    {
      "epoch": 0.0038252498366299547,
      "grad_norm": 4.754525661468506,
      "learning_rate": 0.000298852425049011,
      "loss": 3.3261,
      "step": 1200
    },
    {
      "epoch": 0.004144020656349117,
      "grad_norm": 6.893752574920654,
      "learning_rate": 0.00029875679380309526,
      "loss": 3.2061,
      "step": 1300
    },
    {
      "epoch": 0.004462791476068281,
      "grad_norm": 4.588566303253174,
      "learning_rate": 0.0002986611625571795,
      "loss": 3.2027,
      "step": 1400
    },
    {
      "epoch": 0.004781562295787444,
      "grad_norm": 7.735311508178711,
      "learning_rate": 0.00029856553131126374,
      "loss": 3.0854,
      "step": 1500
    },
    {
      "epoch": 0.005100333115506606,
      "grad_norm": 7.1080474853515625,
      "learning_rate": 0.000298469900065348,
      "loss": 2.9998,
      "step": 1600
    },
    {
      "epoch": 0.00541910393522577,
      "grad_norm": 5.460293292999268,
      "learning_rate": 0.0002983742688194322,
      "loss": 2.8572,
      "step": 1700
    },
    {
      "epoch": 0.0057378747549449325,
      "grad_norm": 8.622939109802246,
      "learning_rate": 0.0002982786375735165,
      "loss": 2.9952,
      "step": 1800
    },
    {
      "epoch": 0.006056645574664095,
      "grad_norm": 13.389545440673828,
      "learning_rate": 0.00029818300632760076,
      "loss": 2.9121,
      "step": 1900
    },
    {
      "epoch": 0.006375416394383258,
      "grad_norm": 8.423888206481934,
      "learning_rate": 0.000298087375081685,
      "loss": 3.0115,
      "step": 2000
    },
    {
      "epoch": 0.0066941872141024214,
      "grad_norm": 10.158764839172363,
      "learning_rate": 0.00029799174383576924,
      "loss": 2.7723,
      "step": 2100
    },
    {
      "epoch": 0.007012958033821584,
      "grad_norm": 11.494490623474121,
      "learning_rate": 0.0002978961125898535,
      "loss": 2.7403,
      "step": 2200
    },
    {
      "epoch": 0.007331728853540747,
      "grad_norm": 13.77318000793457,
      "learning_rate": 0.0002978004813439378,
      "loss": 2.6976,
      "step": 2300
    },
    {
      "epoch": 0.0076504996732599095,
      "grad_norm": 6.963923931121826,
      "learning_rate": 0.000297704850098022,
      "loss": 2.629,
      "step": 2400
    },
    {
      "epoch": 0.007969270492979073,
      "grad_norm": 13.236356735229492,
      "learning_rate": 0.00029760921885210625,
      "loss": 2.7259,
      "step": 2500
    },
    {
      "epoch": 0.008288041312698235,
      "grad_norm": 11.580574989318848,
      "learning_rate": 0.0002975135876061905,
      "loss": 2.7386,
      "step": 2600
    },
    {
      "epoch": 0.008606812132417398,
      "grad_norm": 10.595723152160645,
      "learning_rate": 0.00029741795636027473,
      "loss": 2.6774,
      "step": 2700
    },
    {
      "epoch": 0.008925582952136562,
      "grad_norm": 10.132521629333496,
      "learning_rate": 0.00029732232511435903,
      "loss": 2.5376,
      "step": 2800
    },
    {
      "epoch": 0.009244353771855724,
      "grad_norm": 8.858560562133789,
      "learning_rate": 0.00029722669386844327,
      "loss": 2.6718,
      "step": 2900
    },
    {
      "epoch": 0.009563124591574887,
      "grad_norm": 8.137725830078125,
      "learning_rate": 0.0002971310626225275,
      "loss": 2.4362,
      "step": 3000
    },
    {
      "epoch": 0.00988189541129405,
      "grad_norm": 9.153197288513184,
      "learning_rate": 0.00029703543137661175,
      "loss": 2.4797,
      "step": 3100
    },
    {
      "epoch": 0.010200666231013213,
      "grad_norm": 10.692465782165527,
      "learning_rate": 0.000296939800130696,
      "loss": 2.6085,
      "step": 3200
    },
    {
      "epoch": 0.010519437050732376,
      "grad_norm": 8.230053901672363,
      "learning_rate": 0.0002968441688847803,
      "loss": 2.4184,
      "step": 3300
    },
    {
      "epoch": 0.01083820787045154,
      "grad_norm": 6.9314985275268555,
      "learning_rate": 0.0002967485376388645,
      "loss": 2.2996,
      "step": 3400
    },
    {
      "epoch": 0.011156978690170702,
      "grad_norm": 10.501019477844238,
      "learning_rate": 0.00029665290639294877,
      "loss": 2.32,
      "step": 3500
    },
    {
      "epoch": 0.011475749509889865,
      "grad_norm": 5.459205627441406,
      "learning_rate": 0.000296557275147033,
      "loss": 2.3638,
      "step": 3600
    },
    {
      "epoch": 0.011794520329609027,
      "grad_norm": 9.20101547241211,
      "learning_rate": 0.00029646164390111725,
      "loss": 2.186,
      "step": 3700
    },
    {
      "epoch": 0.01211329114932819,
      "grad_norm": 25.77349281311035,
      "learning_rate": 0.0002963660126552015,
      "loss": 2.1507,
      "step": 3800
    },
    {
      "epoch": 0.012432061969047354,
      "grad_norm": 12.99041748046875,
      "learning_rate": 0.0002962703814092858,
      "loss": 2.3569,
      "step": 3900
    },
    {
      "epoch": 0.012750832788766516,
      "grad_norm": 10.052020072937012,
      "learning_rate": 0.00029617475016337,
      "loss": 2.0432,
      "step": 4000
    },
    {
      "epoch": 0.01306960360848568,
      "grad_norm": 9.68828296661377,
      "learning_rate": 0.00029607911891745426,
      "loss": 2.168,
      "step": 4100
    },
    {
      "epoch": 0.013388374428204843,
      "grad_norm": 14.065261840820312,
      "learning_rate": 0.0002959834876715385,
      "loss": 2.2317,
      "step": 4200
    },
    {
      "epoch": 0.013707145247924005,
      "grad_norm": 15.77820873260498,
      "learning_rate": 0.00029588785642562274,
      "loss": 2.0566,
      "step": 4300
    },
    {
      "epoch": 0.014025916067643168,
      "grad_norm": 11.938392639160156,
      "learning_rate": 0.00029579222517970704,
      "loss": 2.1694,
      "step": 4400
    },
    {
      "epoch": 0.01434468688736233,
      "grad_norm": 12.282574653625488,
      "learning_rate": 0.0002956965939337913,
      "loss": 2.1332,
      "step": 4500
    },
    {
      "epoch": 0.014663457707081494,
      "grad_norm": 4.617358207702637,
      "learning_rate": 0.0002956009626878755,
      "loss": 2.1387,
      "step": 4600
    },
    {
      "epoch": 0.014982228526800657,
      "grad_norm": 11.031489372253418,
      "learning_rate": 0.00029550533144195976,
      "loss": 2.0694,
      "step": 4700
    },
    {
      "epoch": 0.015300999346519819,
      "grad_norm": 12.378880500793457,
      "learning_rate": 0.000295409700196044,
      "loss": 2.093,
      "step": 4800
    },
    {
      "epoch": 0.015619770166238982,
      "grad_norm": 8.313124656677246,
      "learning_rate": 0.0002953140689501283,
      "loss": 1.9501,
      "step": 4900
    },
    {
      "epoch": 0.015938540985958146,
      "grad_norm": 7.410791397094727,
      "learning_rate": 0.00029521843770421253,
      "loss": 2.1861,
      "step": 5000
    },
    {
      "epoch": 0.01625731180567731,
      "grad_norm": 8.98111629486084,
      "learning_rate": 0.0002951228064582968,
      "loss": 1.7877,
      "step": 5100
    },
    {
      "epoch": 0.01657608262539647,
      "grad_norm": 24.248722076416016,
      "learning_rate": 0.000295027175212381,
      "loss": 1.9103,
      "step": 5200
    },
    {
      "epoch": 0.016894853445115633,
      "grad_norm": 22.122591018676758,
      "learning_rate": 0.00029493154396646525,
      "loss": 2.0895,
      "step": 5300
    },
    {
      "epoch": 0.017213624264834797,
      "grad_norm": 13.8676176071167,
      "learning_rate": 0.00029483591272054955,
      "loss": 1.5805,
      "step": 5400
    },
    {
      "epoch": 0.01753239508455396,
      "grad_norm": 13.92074966430664,
      "learning_rate": 0.0002947402814746338,
      "loss": 1.9782,
      "step": 5500
    },
    {
      "epoch": 0.017851165904273124,
      "grad_norm": 18.9875431060791,
      "learning_rate": 0.00029464465022871803,
      "loss": 1.8314,
      "step": 5600
    },
    {
      "epoch": 0.018169936723992287,
      "grad_norm": 2.5044445991516113,
      "learning_rate": 0.0002945490189828023,
      "loss": 1.9471,
      "step": 5700
    },
    {
      "epoch": 0.018488707543711447,
      "grad_norm": 14.513731956481934,
      "learning_rate": 0.0002944533877368865,
      "loss": 1.9185,
      "step": 5800
    },
    {
      "epoch": 0.01880747836343061,
      "grad_norm": 8.652247428894043,
      "learning_rate": 0.00029435775649097075,
      "loss": 1.7765,
      "step": 5900
    },
    {
      "epoch": 0.019126249183149775,
      "grad_norm": 16.546581268310547,
      "learning_rate": 0.00029426212524505505,
      "loss": 1.7912,
      "step": 6000
    },
    {
      "epoch": 0.019445020002868938,
      "grad_norm": 9.59492015838623,
      "learning_rate": 0.0002941664939991393,
      "loss": 1.897,
      "step": 6100
    },
    {
      "epoch": 0.0197637908225881,
      "grad_norm": 18.037670135498047,
      "learning_rate": 0.0002940708627532236,
      "loss": 1.7367,
      "step": 6200
    },
    {
      "epoch": 0.02008256164230726,
      "grad_norm": 8.208016395568848,
      "learning_rate": 0.0002939752315073078,
      "loss": 1.7655,
      "step": 6300
    },
    {
      "epoch": 0.020401332462026425,
      "grad_norm": 6.899148464202881,
      "learning_rate": 0.00029387960026139206,
      "loss": 1.6007,
      "step": 6400
    },
    {
      "epoch": 0.02072010328174559,
      "grad_norm": 30.02727508544922,
      "learning_rate": 0.0002937839690154763,
      "loss": 1.7841,
      "step": 6500
    },
    {
      "epoch": 0.021038874101464752,
      "grad_norm": 12.44609546661377,
      "learning_rate": 0.00029368833776956054,
      "loss": 1.8928,
      "step": 6600
    },
    {
      "epoch": 0.021357644921183916,
      "grad_norm": 12.265477180480957,
      "learning_rate": 0.00029359270652364484,
      "loss": 1.6955,
      "step": 6700
    },
    {
      "epoch": 0.02167641574090308,
      "grad_norm": 0.5906843543052673,
      "learning_rate": 0.0002934970752777291,
      "loss": 1.9133,
      "step": 6800
    },
    {
      "epoch": 0.02199518656062224,
      "grad_norm": 1.2387118339538574,
      "learning_rate": 0.0002934014440318133,
      "loss": 1.7851,
      "step": 6900
    },
    {
      "epoch": 0.022313957380341403,
      "grad_norm": 7.179101943969727,
      "learning_rate": 0.00029330581278589756,
      "loss": 1.5847,
      "step": 7000
    },
    {
      "epoch": 0.022632728200060567,
      "grad_norm": 15.095516204833984,
      "learning_rate": 0.0002932101815399818,
      "loss": 1.6459,
      "step": 7100
    },
    {
      "epoch": 0.02295149901977973,
      "grad_norm": 8.769259452819824,
      "learning_rate": 0.00029311455029406604,
      "loss": 1.4919,
      "step": 7200
    },
    {
      "epoch": 0.023270269839498894,
      "grad_norm": 23.633455276489258,
      "learning_rate": 0.00029301891904815033,
      "loss": 1.6877,
      "step": 7300
    },
    {
      "epoch": 0.023589040659218054,
      "grad_norm": 0.046713411808013916,
      "learning_rate": 0.0002929232878022346,
      "loss": 1.461,
      "step": 7400
    },
    {
      "epoch": 0.023907811478937217,
      "grad_norm": 13.496581077575684,
      "learning_rate": 0.0002928276565563188,
      "loss": 1.5356,
      "step": 7500
    },
    {
      "epoch": 0.02422658229865638,
      "grad_norm": 18.78156089782715,
      "learning_rate": 0.00029273202531040305,
      "loss": 1.3735,
      "step": 7600
    },
    {
      "epoch": 0.024545353118375544,
      "grad_norm": 11.574298858642578,
      "learning_rate": 0.0002926363940644873,
      "loss": 1.4731,
      "step": 7700
    },
    {
      "epoch": 0.024864123938094708,
      "grad_norm": 34.9834098815918,
      "learning_rate": 0.0002925407628185716,
      "loss": 1.6108,
      "step": 7800
    },
    {
      "epoch": 0.025182894757813868,
      "grad_norm": 1.6461681127548218,
      "learning_rate": 0.00029244513157265583,
      "loss": 1.2603,
      "step": 7900
    },
    {
      "epoch": 0.02550166557753303,
      "grad_norm": 5.979430675506592,
      "learning_rate": 0.00029234950032674007,
      "loss": 1.7348,
      "step": 8000
    },
    {
      "epoch": 0.025820436397252195,
      "grad_norm": 15.16672420501709,
      "learning_rate": 0.0002922538690808243,
      "loss": 1.3413,
      "step": 8100
    },
    {
      "epoch": 0.02613920721697136,
      "grad_norm": 12.380135536193848,
      "learning_rate": 0.00029215823783490855,
      "loss": 1.5189,
      "step": 8200
    },
    {
      "epoch": 0.026457978036690522,
      "grad_norm": 5.957970142364502,
      "learning_rate": 0.00029206260658899284,
      "loss": 1.2833,
      "step": 8300
    },
    {
      "epoch": 0.026776748856409686,
      "grad_norm": 30.443052291870117,
      "learning_rate": 0.0002919669753430771,
      "loss": 1.55,
      "step": 8400
    },
    {
      "epoch": 0.027095519676128846,
      "grad_norm": 0.7532463669776917,
      "learning_rate": 0.0002918713440971613,
      "loss": 1.5072,
      "step": 8500
    },
    {
      "epoch": 0.02741429049584801,
      "grad_norm": 17.689722061157227,
      "learning_rate": 0.00029177571285124557,
      "loss": 1.4385,
      "step": 8600
    },
    {
      "epoch": 0.027733061315567173,
      "grad_norm": 1.3039342164993286,
      "learning_rate": 0.0002916800816053298,
      "loss": 1.5217,
      "step": 8700
    },
    {
      "epoch": 0.028051832135286336,
      "grad_norm": 37.151615142822266,
      "learning_rate": 0.0002915844503594141,
      "loss": 1.9539,
      "step": 8800
    },
    {
      "epoch": 0.0283706029550055,
      "grad_norm": 9.637336730957031,
      "learning_rate": 0.00029148881911349834,
      "loss": 1.3755,
      "step": 8900
    },
    {
      "epoch": 0.02868937377472466,
      "grad_norm": 19.70574188232422,
      "learning_rate": 0.0002913931878675826,
      "loss": 1.5451,
      "step": 9000
    },
    {
      "epoch": 0.029008144594443824,
      "grad_norm": 6.601130962371826,
      "learning_rate": 0.0002912975566216668,
      "loss": 1.5558,
      "step": 9100
    },
    {
      "epoch": 0.029326915414162987,
      "grad_norm": 34.90167236328125,
      "learning_rate": 0.00029120192537575106,
      "loss": 1.3788,
      "step": 9200
    },
    {
      "epoch": 0.02964568623388215,
      "grad_norm": 16.85540771484375,
      "learning_rate": 0.0002911062941298353,
      "loss": 1.1979,
      "step": 9300
    },
    {
      "epoch": 0.029964457053601314,
      "grad_norm": 29.141176223754883,
      "learning_rate": 0.0002910106628839196,
      "loss": 1.5326,
      "step": 9400
    },
    {
      "epoch": 0.030283227873320478,
      "grad_norm": 11.140758514404297,
      "learning_rate": 0.00029091503163800384,
      "loss": 1.5895,
      "step": 9500
    },
    {
      "epoch": 0.030601998693039638,
      "grad_norm": 14.591060638427734,
      "learning_rate": 0.0002908194003920881,
      "loss": 1.3164,
      "step": 9600
    },
    {
      "epoch": 0.0309207695127588,
      "grad_norm": 1.563858151435852,
      "learning_rate": 0.0002907237691461723,
      "loss": 1.1269,
      "step": 9700
    },
    {
      "epoch": 0.031239540332477965,
      "grad_norm": 23.943227767944336,
      "learning_rate": 0.00029062813790025656,
      "loss": 1.5807,
      "step": 9800
    },
    {
      "epoch": 0.03155831115219713,
      "grad_norm": 9.639175415039062,
      "learning_rate": 0.00029053250665434085,
      "loss": 1.1261,
      "step": 9900
    },
    {
      "epoch": 0.03187708197191629,
      "grad_norm": 11.982278823852539,
      "learning_rate": 0.0002904368754084251,
      "loss": 1.1901,
      "step": 10000
    },
    {
      "epoch": 0.032195852791635456,
      "grad_norm": 42.404090881347656,
      "learning_rate": 0.00029034124416250933,
      "loss": 1.0149,
      "step": 10100
    },
    {
      "epoch": 0.03251462361135462,
      "grad_norm": 36.28913879394531,
      "learning_rate": 0.0002902456129165936,
      "loss": 1.4784,
      "step": 10200
    },
    {
      "epoch": 0.03283339443107378,
      "grad_norm": 11.277458190917969,
      "learning_rate": 0.0002901499816706778,
      "loss": 1.2829,
      "step": 10300
    },
    {
      "epoch": 0.03315216525079294,
      "grad_norm": 19.508516311645508,
      "learning_rate": 0.0002900543504247621,
      "loss": 1.4342,
      "step": 10400
    },
    {
      "epoch": 0.0334709360705121,
      "grad_norm": 18.813899993896484,
      "learning_rate": 0.00028995871917884635,
      "loss": 1.4936,
      "step": 10500
    },
    {
      "epoch": 0.033789706890231266,
      "grad_norm": 1.6222625970840454,
      "learning_rate": 0.0002898630879329306,
      "loss": 1.1425,
      "step": 10600
    },
    {
      "epoch": 0.03410847770995043,
      "grad_norm": 20.855743408203125,
      "learning_rate": 0.00028976745668701483,
      "loss": 1.2716,
      "step": 10700
    },
    {
      "epoch": 0.034427248529669593,
      "grad_norm": 0.7343500852584839,
      "learning_rate": 0.00028967182544109907,
      "loss": 1.2451,
      "step": 10800
    },
    {
      "epoch": 0.03474601934938876,
      "grad_norm": 4.380765914916992,
      "learning_rate": 0.00028957619419518336,
      "loss": 0.9146,
      "step": 10900
    },
    {
      "epoch": 0.03506479016910792,
      "grad_norm": 33.23251724243164,
      "learning_rate": 0.0002894805629492676,
      "loss": 1.4015,
      "step": 11000
    },
    {
      "epoch": 0.035383560988827084,
      "grad_norm": 26.363407135009766,
      "learning_rate": 0.00028938493170335185,
      "loss": 1.2874,
      "step": 11100
    },
    {
      "epoch": 0.03570233180854625,
      "grad_norm": 5.664946556091309,
      "learning_rate": 0.0002892893004574361,
      "loss": 1.5552,
      "step": 11200
    },
    {
      "epoch": 0.03602110262826541,
      "grad_norm": 0.33650967478752136,
      "learning_rate": 0.0002891936692115203,
      "loss": 1.7028,
      "step": 11300
    },
    {
      "epoch": 0.036339873447984575,
      "grad_norm": 1.6181360483169556,
      "learning_rate": 0.00028909803796560457,
      "loss": 1.6262,
      "step": 11400
    },
    {
      "epoch": 0.03665864426770373,
      "grad_norm": 11.746846199035645,
      "learning_rate": 0.00028900240671968886,
      "loss": 1.3295,
      "step": 11500
    },
    {
      "epoch": 0.036977415087422895,
      "grad_norm": 24.030820846557617,
      "learning_rate": 0.0002889067754737731,
      "loss": 1.3483,
      "step": 11600
    },
    {
      "epoch": 0.03729618590714206,
      "grad_norm": 4.488018989562988,
      "learning_rate": 0.0002888111442278574,
      "loss": 1.4612,
      "step": 11700
    },
    {
      "epoch": 0.03761495672686122,
      "grad_norm": 0.9135897755622864,
      "learning_rate": 0.00028871551298194164,
      "loss": 1.0508,
      "step": 11800
    },
    {
      "epoch": 0.037933727546580386,
      "grad_norm": 0.05470842868089676,
      "learning_rate": 0.0002886198817360258,
      "loss": 1.3807,
      "step": 11900
    },
    {
      "epoch": 0.03825249836629955,
      "grad_norm": 22.976491928100586,
      "learning_rate": 0.0002885242504901101,
      "loss": 1.1983,
      "step": 12000
    },
    {
      "epoch": 0.03857126918601871,
      "grad_norm": 21.282766342163086,
      "learning_rate": 0.00028842861924419436,
      "loss": 1.4658,
      "step": 12100
    },
    {
      "epoch": 0.038890040005737876,
      "grad_norm": 11.939050674438477,
      "learning_rate": 0.0002883329879982786,
      "loss": 1.134,
      "step": 12200
    },
    {
      "epoch": 0.03920881082545704,
      "grad_norm": 4.359952926635742,
      "learning_rate": 0.0002882373567523629,
      "loss": 1.0825,
      "step": 12300
    },
    {
      "epoch": 0.0395275816451762,
      "grad_norm": 0.5679219365119934,
      "learning_rate": 0.00028814172550644713,
      "loss": 1.208,
      "step": 12400
    },
    {
      "epoch": 0.03984635246489537,
      "grad_norm": 6.764280796051025,
      "learning_rate": 0.0002880460942605314,
      "loss": 1.3407,
      "step": 12500
    },
    {
      "epoch": 0.04016512328461452,
      "grad_norm": 10.105391502380371,
      "learning_rate": 0.0002879504630146156,
      "loss": 1.2917,
      "step": 12600
    },
    {
      "epoch": 0.04048389410433369,
      "grad_norm": 6.90084171295166,
      "learning_rate": 0.00028785483176869985,
      "loss": 1.4953,
      "step": 12700
    },
    {
      "epoch": 0.04080266492405285,
      "grad_norm": 18.932838439941406,
      "learning_rate": 0.00028775920052278415,
      "loss": 1.1516,
      "step": 12800
    },
    {
      "epoch": 0.041121435743772014,
      "grad_norm": 174.71107482910156,
      "learning_rate": 0.0002876635692768684,
      "loss": 1.2102,
      "step": 12900
    },
    {
      "epoch": 0.04144020656349118,
      "grad_norm": 0.1817770153284073,
      "learning_rate": 0.00028756793803095263,
      "loss": 1.2253,
      "step": 13000
    },
    {
      "epoch": 0.04175897738321034,
      "grad_norm": 0.007617425173521042,
      "learning_rate": 0.00028747230678503687,
      "loss": 1.2299,
      "step": 13100
    },
    {
      "epoch": 0.042077748202929505,
      "grad_norm": 27.406099319458008,
      "learning_rate": 0.0002873766755391211,
      "loss": 0.96,
      "step": 13200
    },
    {
      "epoch": 0.04239651902264867,
      "grad_norm": 35.03311538696289,
      "learning_rate": 0.0002872810442932054,
      "loss": 1.4396,
      "step": 13300
    },
    {
      "epoch": 0.04271528984236783,
      "grad_norm": 0.05297340080142021,
      "learning_rate": 0.00028718541304728964,
      "loss": 1.2098,
      "step": 13400
    },
    {
      "epoch": 0.043034060662086995,
      "grad_norm": 6.606644153594971,
      "learning_rate": 0.0002870897818013739,
      "loss": 1.1272,
      "step": 13500
    },
    {
      "epoch": 0.04335283148180616,
      "grad_norm": 0.042629312723875046,
      "learning_rate": 0.0002869941505554581,
      "loss": 1.1022,
      "step": 13600
    },
    {
      "epoch": 0.043671602301525315,
      "grad_norm": 13.627386093139648,
      "learning_rate": 0.00028689851930954237,
      "loss": 1.3051,
      "step": 13700
    },
    {
      "epoch": 0.04399037312124448,
      "grad_norm": 80.9852294921875,
      "learning_rate": 0.00028680288806362666,
      "loss": 1.2359,
      "step": 13800
    },
    {
      "epoch": 0.04430914394096364,
      "grad_norm": 15.78607177734375,
      "learning_rate": 0.0002867072568177109,
      "loss": 1.0378,
      "step": 13900
    },
    {
      "epoch": 0.044627914760682806,
      "grad_norm": 55.0542106628418,
      "learning_rate": 0.00028661162557179514,
      "loss": 1.3418,
      "step": 14000
    },
    {
      "epoch": 0.04494668558040197,
      "grad_norm": 0.006226507015526295,
      "learning_rate": 0.0002865159943258794,
      "loss": 0.8722,
      "step": 14100
    },
    {
      "epoch": 0.04526545640012113,
      "grad_norm": 0.9830866456031799,
      "learning_rate": 0.0002864203630799636,
      "loss": 1.4005,
      "step": 14200
    },
    {
      "epoch": 0.0455842272198403,
      "grad_norm": 18.423770904541016,
      "learning_rate": 0.00028632473183404786,
      "loss": 1.4296,
      "step": 14300
    },
    {
      "epoch": 0.04590299803955946,
      "grad_norm": 36.406982421875,
      "learning_rate": 0.00028622910058813216,
      "loss": 1.2361,
      "step": 14400
    },
    {
      "epoch": 0.046221768859278624,
      "grad_norm": 17.74161720275879,
      "learning_rate": 0.0002861334693422164,
      "loss": 1.1682,
      "step": 14500
    },
    {
      "epoch": 0.04654053967899779,
      "grad_norm": 2.845334053039551,
      "learning_rate": 0.00028603783809630064,
      "loss": 1.1595,
      "step": 14600
    },
    {
      "epoch": 0.046859310498716944,
      "grad_norm": 8.253265380859375,
      "learning_rate": 0.0002859422068503849,
      "loss": 1.0829,
      "step": 14700
    },
    {
      "epoch": 0.04717808131843611,
      "grad_norm": 79.8324203491211,
      "learning_rate": 0.0002858465756044691,
      "loss": 1.1491,
      "step": 14800
    },
    {
      "epoch": 0.04749685213815527,
      "grad_norm": 36.003082275390625,
      "learning_rate": 0.0002857509443585534,
      "loss": 1.1362,
      "step": 14900
    },
    {
      "epoch": 0.047815622957874435,
      "grad_norm": 19.76540184020996,
      "learning_rate": 0.00028565531311263765,
      "loss": 1.1475,
      "step": 15000
    },
    {
      "epoch": 0.0481343937775936,
      "grad_norm": 1.6449824571609497,
      "learning_rate": 0.0002855596818667219,
      "loss": 1.4265,
      "step": 15100
    },
    {
      "epoch": 0.04845316459731276,
      "grad_norm": 16.39402961730957,
      "learning_rate": 0.00028546405062080613,
      "loss": 0.7912,
      "step": 15200
    },
    {
      "epoch": 0.048771935417031925,
      "grad_norm": 10.618372917175293,
      "learning_rate": 0.0002853684193748904,
      "loss": 0.8763,
      "step": 15300
    },
    {
      "epoch": 0.04909070623675109,
      "grad_norm": 0.24749663472175598,
      "learning_rate": 0.00028527278812897467,
      "loss": 1.1204,
      "step": 15400
    },
    {
      "epoch": 0.04940947705647025,
      "grad_norm": 21.19567108154297,
      "learning_rate": 0.0002851771568830589,
      "loss": 0.8805,
      "step": 15500
    },
    {
      "epoch": 0.049728247876189416,
      "grad_norm": 56.8543815612793,
      "learning_rate": 0.00028508152563714315,
      "loss": 1.2274,
      "step": 15600
    },
    {
      "epoch": 0.05004701869590858,
      "grad_norm": 0.5577722191810608,
      "learning_rate": 0.0002849858943912274,
      "loss": 1.047,
      "step": 15700
    },
    {
      "epoch": 0.050365789515627736,
      "grad_norm": 27.503103256225586,
      "learning_rate": 0.00028489026314531163,
      "loss": 1.2241,
      "step": 15800
    },
    {
      "epoch": 0.0506845603353469,
      "grad_norm": 66.08430480957031,
      "learning_rate": 0.0002847946318993959,
      "loss": 1.2049,
      "step": 15900
    },
    {
      "epoch": 0.05100333115506606,
      "grad_norm": 1.1496855020523071,
      "learning_rate": 0.00028469900065348017,
      "loss": 1.3431,
      "step": 16000
    },
    {
      "epoch": 0.05132210197478523,
      "grad_norm": 16.86458969116211,
      "learning_rate": 0.0002846033694075644,
      "loss": 1.036,
      "step": 16100
    },
    {
      "epoch": 0.05164087279450439,
      "grad_norm": 2.3273394107818604,
      "learning_rate": 0.00028450773816164865,
      "loss": 1.0558,
      "step": 16200
    },
    {
      "epoch": 0.051959643614223554,
      "grad_norm": 106.52767181396484,
      "learning_rate": 0.0002844121069157329,
      "loss": 1.2668,
      "step": 16300
    },
    {
      "epoch": 0.05227841443394272,
      "grad_norm": 23.64142608642578,
      "learning_rate": 0.0002843164756698171,
      "loss": 0.8864,
      "step": 16400
    },
    {
      "epoch": 0.05259718525366188,
      "grad_norm": 22.503984451293945,
      "learning_rate": 0.0002842208444239014,
      "loss": 1.0675,
      "step": 16500
    },
    {
      "epoch": 0.052915956073381044,
      "grad_norm": 21.277746200561523,
      "learning_rate": 0.00028412521317798566,
      "loss": 1.0773,
      "step": 16600
    },
    {
      "epoch": 0.05323472689310021,
      "grad_norm": 39.592247009277344,
      "learning_rate": 0.0002840295819320699,
      "loss": 1.3931,
      "step": 16700
    },
    {
      "epoch": 0.05355349771281937,
      "grad_norm": 58.98653030395508,
      "learning_rate": 0.00028393395068615414,
      "loss": 1.0676,
      "step": 16800
    },
    {
      "epoch": 0.05387226853253853,
      "grad_norm": 39.46066665649414,
      "learning_rate": 0.0002838383194402384,
      "loss": 1.0425,
      "step": 16900
    },
    {
      "epoch": 0.05419103935225769,
      "grad_norm": 16.83552360534668,
      "learning_rate": 0.0002837426881943227,
      "loss": 1.1805,
      "step": 17000
    },
    {
      "epoch": 0.054509810171976855,
      "grad_norm": 11.438231468200684,
      "learning_rate": 0.0002836470569484069,
      "loss": 1.1715,
      "step": 17100
    },
    {
      "epoch": 0.05482858099169602,
      "grad_norm": 1.2635409832000732,
      "learning_rate": 0.00028355142570249116,
      "loss": 0.9963,
      "step": 17200
    },
    {
      "epoch": 0.05514735181141518,
      "grad_norm": 86.45429229736328,
      "learning_rate": 0.0002834557944565754,
      "loss": 0.9403,
      "step": 17300
    },
    {
      "epoch": 0.055466122631134346,
      "grad_norm": 7.4418864250183105,
      "learning_rate": 0.00028336016321065964,
      "loss": 1.1214,
      "step": 17400
    },
    {
      "epoch": 0.05578489345085351,
      "grad_norm": 0.09194516390562057,
      "learning_rate": 0.00028326453196474393,
      "loss": 1.1472,
      "step": 17500
    },
    {
      "epoch": 0.05610366427057267,
      "grad_norm": 13.311997413635254,
      "learning_rate": 0.0002831689007188282,
      "loss": 0.8798,
      "step": 17600
    },
    {
      "epoch": 0.056422435090291836,
      "grad_norm": 9.119521141052246,
      "learning_rate": 0.0002830732694729124,
      "loss": 1.0357,
      "step": 17700
    },
    {
      "epoch": 0.056741205910011,
      "grad_norm": 11.676985740661621,
      "learning_rate": 0.0002829776382269967,
      "loss": 1.0136,
      "step": 17800
    },
    {
      "epoch": 0.057059976729730164,
      "grad_norm": 1.408814549446106,
      "learning_rate": 0.00028288200698108095,
      "loss": 0.7323,
      "step": 17900
    },
    {
      "epoch": 0.05737874754944932,
      "grad_norm": 0.00984205026179552,
      "learning_rate": 0.0002827863757351652,
      "loss": 0.8635,
      "step": 18000
    },
    {
      "epoch": 0.057697518369168484,
      "grad_norm": 44.56109619140625,
      "learning_rate": 0.00028269074448924943,
      "loss": 0.9991,
      "step": 18100
    },
    {
      "epoch": 0.05801628918888765,
      "grad_norm": 0.1476350575685501,
      "learning_rate": 0.00028259511324333367,
      "loss": 1.1557,
      "step": 18200
    },
    {
      "epoch": 0.05833506000860681,
      "grad_norm": 8.022204399108887,
      "learning_rate": 0.00028249948199741796,
      "loss": 1.2463,
      "step": 18300
    },
    {
      "epoch": 0.058653830828325974,
      "grad_norm": 27.527097702026367,
      "learning_rate": 0.0002824038507515022,
      "loss": 1.1346,
      "step": 18400
    },
    {
      "epoch": 0.05897260164804514,
      "grad_norm": 57.762306213378906,
      "learning_rate": 0.00028230821950558645,
      "loss": 1.153,
      "step": 18500
    },
    {
      "epoch": 0.0592913724677643,
      "grad_norm": 2.077165126800537,
      "learning_rate": 0.0002822125882596707,
      "loss": 1.1804,
      "step": 18600
    },
    {
      "epoch": 0.059610143287483465,
      "grad_norm": 12.893097877502441,
      "learning_rate": 0.0002821169570137549,
      "loss": 0.9634,
      "step": 18700
    },
    {
      "epoch": 0.05992891410720263,
      "grad_norm": 11.914868354797363,
      "learning_rate": 0.0002820213257678392,
      "loss": 1.0143,
      "step": 18800
    },
    {
      "epoch": 0.06024768492692179,
      "grad_norm": 0.18776513636112213,
      "learning_rate": 0.00028192569452192346,
      "loss": 1.2727,
      "step": 18900
    },
    {
      "epoch": 0.060566455746640956,
      "grad_norm": 40.418338775634766,
      "learning_rate": 0.0002818300632760077,
      "loss": 1.3288,
      "step": 19000
    },
    {
      "epoch": 0.06088522656636011,
      "grad_norm": 10.403048515319824,
      "learning_rate": 0.00028173443203009194,
      "loss": 0.9832,
      "step": 19100
    },
    {
      "epoch": 0.061203997386079276,
      "grad_norm": 12.366683006286621,
      "learning_rate": 0.0002816388007841762,
      "loss": 0.8575,
      "step": 19200
    },
    {
      "epoch": 0.06152276820579844,
      "grad_norm": 12.251880645751953,
      "learning_rate": 0.0002815431695382604,
      "loss": 1.2591,
      "step": 19300
    },
    {
      "epoch": 0.0618415390255176,
      "grad_norm": 23.67656135559082,
      "learning_rate": 0.0002814475382923447,
      "loss": 0.7609,
      "step": 19400
    },
    {
      "epoch": 0.062160309845236766,
      "grad_norm": 0.12356653809547424,
      "learning_rate": 0.00028135190704642896,
      "loss": 0.9713,
      "step": 19500
    },
    {
      "epoch": 0.06247908066495593,
      "grad_norm": 16.27498435974121,
      "learning_rate": 0.0002812562758005132,
      "loss": 0.9111,
      "step": 19600
    },
    {
      "epoch": 0.0627978514846751,
      "grad_norm": 42.62971115112305,
      "learning_rate": 0.00028116064455459744,
      "loss": 1.4932,
      "step": 19700
    },
    {
      "epoch": 0.06311662230439426,
      "grad_norm": 28.52406120300293,
      "learning_rate": 0.0002810650133086817,
      "loss": 1.144,
      "step": 19800
    },
    {
      "epoch": 0.06343539312411342,
      "grad_norm": 51.32414245605469,
      "learning_rate": 0.00028096938206276597,
      "loss": 0.8048,
      "step": 19900
    },
    {
      "epoch": 0.06375416394383258,
      "grad_norm": 0.2015993744134903,
      "learning_rate": 0.0002808737508168502,
      "loss": 0.9178,
      "step": 20000
    },
    {
      "epoch": 0.06407293476355175,
      "grad_norm": 0.43938276171684265,
      "learning_rate": 0.00028077811957093445,
      "loss": 0.8533,
      "step": 20100
    },
    {
      "epoch": 0.06439170558327091,
      "grad_norm": 85.64292907714844,
      "learning_rate": 0.0002806824883250187,
      "loss": 0.8792,
      "step": 20200
    },
    {
      "epoch": 0.06471047640299007,
      "grad_norm": 0.03635210543870926,
      "learning_rate": 0.00028058685707910293,
      "loss": 1.0966,
      "step": 20300
    },
    {
      "epoch": 0.06502924722270924,
      "grad_norm": 28.472808837890625,
      "learning_rate": 0.00028049122583318723,
      "loss": 1.1087,
      "step": 20400
    },
    {
      "epoch": 0.0653480180424284,
      "grad_norm": 151.64166259765625,
      "learning_rate": 0.00028039559458727147,
      "loss": 1.1642,
      "step": 20500
    },
    {
      "epoch": 0.06566678886214757,
      "grad_norm": 41.26844024658203,
      "learning_rate": 0.0002802999633413557,
      "loss": 0.8843,
      "step": 20600
    },
    {
      "epoch": 0.06598555968186672,
      "grad_norm": 0.00917855091392994,
      "learning_rate": 0.00028020433209543995,
      "loss": 1.23,
      "step": 20700
    },
    {
      "epoch": 0.06630433050158588,
      "grad_norm": 23.518856048583984,
      "learning_rate": 0.0002801087008495242,
      "loss": 0.9747,
      "step": 20800
    },
    {
      "epoch": 0.06662310132130504,
      "grad_norm": 74.58646392822266,
      "learning_rate": 0.0002800130696036085,
      "loss": 1.0807,
      "step": 20900
    },
    {
      "epoch": 0.0669418721410242,
      "grad_norm": 0.015925798565149307,
      "learning_rate": 0.0002799174383576927,
      "loss": 1.0087,
      "step": 21000
    },
    {
      "epoch": 0.06726064296074337,
      "grad_norm": 42.6551628112793,
      "learning_rate": 0.00027982180711177697,
      "loss": 0.8824,
      "step": 21100
    },
    {
      "epoch": 0.06757941378046253,
      "grad_norm": 1.4797145128250122,
      "learning_rate": 0.0002797261758658612,
      "loss": 0.9796,
      "step": 21200
    },
    {
      "epoch": 0.0678981846001817,
      "grad_norm": 17.440004348754883,
      "learning_rate": 0.00027963054461994545,
      "loss": 0.9545,
      "step": 21300
    },
    {
      "epoch": 0.06821695541990086,
      "grad_norm": 0.02657587081193924,
      "learning_rate": 0.0002795349133740297,
      "loss": 0.7769,
      "step": 21400
    },
    {
      "epoch": 0.06853572623962002,
      "grad_norm": 17.6214599609375,
      "learning_rate": 0.000279439282128114,
      "loss": 0.9867,
      "step": 21500
    },
    {
      "epoch": 0.06885449705933919,
      "grad_norm": 0.20224560797214508,
      "learning_rate": 0.0002793436508821982,
      "loss": 0.9065,
      "step": 21600
    },
    {
      "epoch": 0.06917326787905835,
      "grad_norm": 39.437171936035156,
      "learning_rate": 0.00027924801963628246,
      "loss": 1.0765,
      "step": 21700
    },
    {
      "epoch": 0.06949203869877751,
      "grad_norm": 6.480484962463379,
      "learning_rate": 0.0002791523883903667,
      "loss": 0.676,
      "step": 21800
    },
    {
      "epoch": 0.06981080951849668,
      "grad_norm": 16.889211654663086,
      "learning_rate": 0.00027905675714445094,
      "loss": 0.9156,
      "step": 21900
    },
    {
      "epoch": 0.07012958033821584,
      "grad_norm": 24.695148468017578,
      "learning_rate": 0.00027896112589853524,
      "loss": 0.9621,
      "step": 22000
    },
    {
      "epoch": 0.070448351157935,
      "grad_norm": 23.022319793701172,
      "learning_rate": 0.0002788654946526195,
      "loss": 1.287,
      "step": 22100
    },
    {
      "epoch": 0.07076712197765417,
      "grad_norm": 22.510251998901367,
      "learning_rate": 0.0002787698634067037,
      "loss": 1.0687,
      "step": 22200
    },
    {
      "epoch": 0.07108589279737333,
      "grad_norm": 0.8340399861335754,
      "learning_rate": 0.00027867423216078796,
      "loss": 0.9308,
      "step": 22300
    },
    {
      "epoch": 0.0714046636170925,
      "grad_norm": 32.08052444458008,
      "learning_rate": 0.0002785786009148722,
      "loss": 1.1912,
      "step": 22400
    },
    {
      "epoch": 0.07172343443681166,
      "grad_norm": 0.21352370083332062,
      "learning_rate": 0.0002784829696689565,
      "loss": 0.8284,
      "step": 22500
    },
    {
      "epoch": 0.07204220525653082,
      "grad_norm": 22.424392700195312,
      "learning_rate": 0.00027838733842304073,
      "loss": 0.828,
      "step": 22600
    },
    {
      "epoch": 0.07236097607624999,
      "grad_norm": 40.0490608215332,
      "learning_rate": 0.000278291707177125,
      "loss": 0.7393,
      "step": 22700
    },
    {
      "epoch": 0.07267974689596915,
      "grad_norm": 15.574034690856934,
      "learning_rate": 0.0002781960759312092,
      "loss": 0.9584,
      "step": 22800
    },
    {
      "epoch": 0.0729985177156883,
      "grad_norm": 0.03085688129067421,
      "learning_rate": 0.00027810044468529345,
      "loss": 1.0565,
      "step": 22900
    },
    {
      "epoch": 0.07331728853540746,
      "grad_norm": 17.445165634155273,
      "learning_rate": 0.00027800481343937775,
      "loss": 0.8689,
      "step": 23000
    },
    {
      "epoch": 0.07363605935512663,
      "grad_norm": 11.722577095031738,
      "learning_rate": 0.000277909182193462,
      "loss": 1.009,
      "step": 23100
    },
    {
      "epoch": 0.07395483017484579,
      "grad_norm": 0.07804759591817856,
      "learning_rate": 0.00027781355094754623,
      "loss": 0.8106,
      "step": 23200
    },
    {
      "epoch": 0.07427360099456495,
      "grad_norm": 18.075590133666992,
      "learning_rate": 0.0002777179197016305,
      "loss": 1.0305,
      "step": 23300
    },
    {
      "epoch": 0.07459237181428412,
      "grad_norm": 6.964670181274414,
      "learning_rate": 0.0002776222884557147,
      "loss": 1.1866,
      "step": 23400
    },
    {
      "epoch": 0.07491114263400328,
      "grad_norm": 7.6445441246032715,
      "learning_rate": 0.00027752665720979895,
      "loss": 0.9264,
      "step": 23500
    },
    {
      "epoch": 0.07522991345372244,
      "grad_norm": 7.490091323852539,
      "learning_rate": 0.00027743102596388325,
      "loss": 0.9977,
      "step": 23600
    },
    {
      "epoch": 0.07554868427344161,
      "grad_norm": 0.041677411645650864,
      "learning_rate": 0.0002773353947179675,
      "loss": 1.0281,
      "step": 23700
    },
    {
      "epoch": 0.07586745509316077,
      "grad_norm": 19.634492874145508,
      "learning_rate": 0.0002772397634720518,
      "loss": 0.891,
      "step": 23800
    },
    {
      "epoch": 0.07618622591287993,
      "grad_norm": 75.07364654541016,
      "learning_rate": 0.000277144132226136,
      "loss": 0.8081,
      "step": 23900
    },
    {
      "epoch": 0.0765049967325991,
      "grad_norm": 0.304982990026474,
      "learning_rate": 0.0002770485009802202,
      "loss": 0.7201,
      "step": 24000
    },
    {
      "epoch": 0.07682376755231826,
      "grad_norm": 0.35789358615875244,
      "learning_rate": 0.0002769528697343045,
      "loss": 0.8459,
      "step": 24100
    },
    {
      "epoch": 0.07714253837203743,
      "grad_norm": 68.71206665039062,
      "learning_rate": 0.00027685723848838874,
      "loss": 0.807,
      "step": 24200
    },
    {
      "epoch": 0.07746130919175659,
      "grad_norm": 3.208174467086792,
      "learning_rate": 0.00027676160724247304,
      "loss": 1.0478,
      "step": 24300
    },
    {
      "epoch": 0.07778008001147575,
      "grad_norm": 0.4807998538017273,
      "learning_rate": 0.0002766659759965573,
      "loss": 0.6904,
      "step": 24400
    },
    {
      "epoch": 0.07809885083119492,
      "grad_norm": 0.7172457575798035,
      "learning_rate": 0.0002765703447506415,
      "loss": 1.0136,
      "step": 24500
    },
    {
      "epoch": 0.07841762165091408,
      "grad_norm": 79.09828186035156,
      "learning_rate": 0.00027647471350472576,
      "loss": 0.846,
      "step": 24600
    },
    {
      "epoch": 0.07873639247063324,
      "grad_norm": 0.13173355162143707,
      "learning_rate": 0.00027637908225881,
      "loss": 1.0837,
      "step": 24700
    },
    {
      "epoch": 0.0790551632903524,
      "grad_norm": 25.446807861328125,
      "learning_rate": 0.00027628345101289424,
      "loss": 1.0748,
      "step": 24800
    },
    {
      "epoch": 0.07937393411007157,
      "grad_norm": 25.6200008392334,
      "learning_rate": 0.00027618781976697853,
      "loss": 0.7706,
      "step": 24900
    },
    {
      "epoch": 0.07969270492979073,
      "grad_norm": 59.592857360839844,
      "learning_rate": 0.00027609218852106277,
      "loss": 0.8331,
      "step": 25000
    },
    {
      "epoch": 0.08001147574950988,
      "grad_norm": 0.2659927010536194,
      "learning_rate": 0.000275996557275147,
      "loss": 0.8056,
      "step": 25100
    },
    {
      "epoch": 0.08033024656922905,
      "grad_norm": 0.08159591257572174,
      "learning_rate": 0.00027590092602923125,
      "loss": 0.9775,
      "step": 25200
    },
    {
      "epoch": 0.08064901738894821,
      "grad_norm": 42.92259216308594,
      "learning_rate": 0.0002758052947833155,
      "loss": 0.7939,
      "step": 25300
    },
    {
      "epoch": 0.08096778820866737,
      "grad_norm": 40.3507080078125,
      "learning_rate": 0.0002757096635373998,
      "loss": 1.0066,
      "step": 25400
    },
    {
      "epoch": 0.08128655902838654,
      "grad_norm": 0.002320538042113185,
      "learning_rate": 0.00027561403229148403,
      "loss": 0.9683,
      "step": 25500
    },
    {
      "epoch": 0.0816053298481057,
      "grad_norm": 0.1921892911195755,
      "learning_rate": 0.00027551840104556827,
      "loss": 1.193,
      "step": 25600
    },
    {
      "epoch": 0.08192410066782486,
      "grad_norm": 10.371918678283691,
      "learning_rate": 0.0002754227697996525,
      "loss": 0.9553,
      "step": 25700
    },
    {
      "epoch": 0.08224287148754403,
      "grad_norm": 0.01198585145175457,
      "learning_rate": 0.00027532713855373675,
      "loss": 0.8342,
      "step": 25800
    },
    {
      "epoch": 0.08256164230726319,
      "grad_norm": 18.201515197753906,
      "learning_rate": 0.00027523150730782104,
      "loss": 0.9692,
      "step": 25900
    },
    {
      "epoch": 0.08288041312698236,
      "grad_norm": 71.8530502319336,
      "learning_rate": 0.0002751358760619053,
      "loss": 1.0534,
      "step": 26000
    },
    {
      "epoch": 0.08319918394670152,
      "grad_norm": 45.414466857910156,
      "learning_rate": 0.0002750402448159895,
      "loss": 1.1311,
      "step": 26100
    },
    {
      "epoch": 0.08351795476642068,
      "grad_norm": 5.262435436248779,
      "learning_rate": 0.00027494461357007377,
      "loss": 0.9191,
      "step": 26200
    },
    {
      "epoch": 0.08383672558613985,
      "grad_norm": 0.0733645111322403,
      "learning_rate": 0.000274848982324158,
      "loss": 0.7426,
      "step": 26300
    },
    {
      "epoch": 0.08415549640585901,
      "grad_norm": 0.12660688161849976,
      "learning_rate": 0.0002747533510782423,
      "loss": 0.7951,
      "step": 26400
    },
    {
      "epoch": 0.08447426722557817,
      "grad_norm": 2.02189564704895,
      "learning_rate": 0.00027465771983232654,
      "loss": 0.9122,
      "step": 26500
    },
    {
      "epoch": 0.08479303804529734,
      "grad_norm": 23.353351593017578,
      "learning_rate": 0.0002745620885864108,
      "loss": 0.8905,
      "step": 26600
    },
    {
      "epoch": 0.0851118088650165,
      "grad_norm": 0.0063906703144311905,
      "learning_rate": 0.000274466457340495,
      "loss": 1.0763,
      "step": 26700
    },
    {
      "epoch": 0.08543057968473566,
      "grad_norm": 231.30477905273438,
      "learning_rate": 0.00027437082609457926,
      "loss": 0.8117,
      "step": 26800
    },
    {
      "epoch": 0.08574935050445483,
      "grad_norm": 0.0030420890543609858,
      "learning_rate": 0.0002742751948486635,
      "loss": 0.8719,
      "step": 26900
    },
    {
      "epoch": 0.08606812132417399,
      "grad_norm": 1.5612491369247437,
      "learning_rate": 0.0002741795636027478,
      "loss": 1.0511,
      "step": 27000
    },
    {
      "epoch": 0.08638689214389315,
      "grad_norm": 65.7627182006836,
      "learning_rate": 0.00027408393235683204,
      "loss": 0.9103,
      "step": 27100
    },
    {
      "epoch": 0.08670566296361232,
      "grad_norm": 57.623905181884766,
      "learning_rate": 0.0002739883011109163,
      "loss": 0.9538,
      "step": 27200
    },
    {
      "epoch": 0.08702443378333147,
      "grad_norm": 36.02698516845703,
      "learning_rate": 0.0002738926698650005,
      "loss": 0.883,
      "step": 27300
    },
    {
      "epoch": 0.08734320460305063,
      "grad_norm": 0.010769245214760303,
      "learning_rate": 0.00027379703861908476,
      "loss": 1.0684,
      "step": 27400
    },
    {
      "epoch": 0.0876619754227698,
      "grad_norm": 26.333415985107422,
      "learning_rate": 0.00027370140737316905,
      "loss": 0.9729,
      "step": 27500
    },
    {
      "epoch": 0.08798074624248896,
      "grad_norm": 0.6059714555740356,
      "learning_rate": 0.0002736057761272533,
      "loss": 0.8216,
      "step": 27600
    },
    {
      "epoch": 0.08829951706220812,
      "grad_norm": 0.08023636788129807,
      "learning_rate": 0.00027351014488133753,
      "loss": 0.6462,
      "step": 27700
    },
    {
      "epoch": 0.08861828788192729,
      "grad_norm": 29.388349533081055,
      "learning_rate": 0.0002734145136354218,
      "loss": 1.1465,
      "step": 27800
    },
    {
      "epoch": 0.08893705870164645,
      "grad_norm": 14.946920394897461,
      "learning_rate": 0.000273318882389506,
      "loss": 1.1354,
      "step": 27900
    },
    {
      "epoch": 0.08925582952136561,
      "grad_norm": 0.0002663978957571089,
      "learning_rate": 0.0002732232511435903,
      "loss": 0.7312,
      "step": 28000
    },
    {
      "epoch": 0.08957460034108478,
      "grad_norm": 0.7197738289833069,
      "learning_rate": 0.00027312761989767455,
      "loss": 0.9326,
      "step": 28100
    },
    {
      "epoch": 0.08989337116080394,
      "grad_norm": 0.010708306916058064,
      "learning_rate": 0.0002730319886517588,
      "loss": 1.0276,
      "step": 28200
    },
    {
      "epoch": 0.0902121419805231,
      "grad_norm": 2.7850770950317383,
      "learning_rate": 0.00027293635740584303,
      "loss": 0.6044,
      "step": 28300
    },
    {
      "epoch": 0.09053091280024227,
      "grad_norm": 8.5219087600708,
      "learning_rate": 0.00027284072615992727,
      "loss": 0.8245,
      "step": 28400
    },
    {
      "epoch": 0.09084968361996143,
      "grad_norm": 0.06975113600492477,
      "learning_rate": 0.00027274509491401156,
      "loss": 1.0941,
      "step": 28500
    },
    {
      "epoch": 0.0911684544396806,
      "grad_norm": 26.758094787597656,
      "learning_rate": 0.0002726494636680958,
      "loss": 0.6722,
      "step": 28600
    },
    {
      "epoch": 0.09148722525939976,
      "grad_norm": 4.423669338226318,
      "learning_rate": 0.00027255383242218005,
      "loss": 1.0437,
      "step": 28700
    },
    {
      "epoch": 0.09180599607911892,
      "grad_norm": 0.004794781561940908,
      "learning_rate": 0.0002724582011762643,
      "loss": 1.0141,
      "step": 28800
    },
    {
      "epoch": 0.09212476689883808,
      "grad_norm": 26.352088928222656,
      "learning_rate": 0.0002723625699303485,
      "loss": 1.3174,
      "step": 28900
    },
    {
      "epoch": 0.09244353771855725,
      "grad_norm": 1.3543511629104614,
      "learning_rate": 0.00027226693868443277,
      "loss": 1.0775,
      "step": 29000
    },
    {
      "epoch": 0.09276230853827641,
      "grad_norm": 4.109668254852295,
      "learning_rate": 0.00027217130743851706,
      "loss": 0.8994,
      "step": 29100
    },
    {
      "epoch": 0.09308107935799557,
      "grad_norm": 27.078588485717773,
      "learning_rate": 0.0002720756761926013,
      "loss": 0.6757,
      "step": 29200
    },
    {
      "epoch": 0.09339985017771474,
      "grad_norm": 4.167779445648193,
      "learning_rate": 0.0002719800449466856,
      "loss": 0.7107,
      "step": 29300
    },
    {
      "epoch": 0.09371862099743389,
      "grad_norm": 0.055261168628931046,
      "learning_rate": 0.00027188441370076984,
      "loss": 0.9367,
      "step": 29400
    },
    {
      "epoch": 0.09403739181715305,
      "grad_norm": 0.44038552045822144,
      "learning_rate": 0.000271788782454854,
      "loss": 1.1365,
      "step": 29500
    },
    {
      "epoch": 0.09435616263687222,
      "grad_norm": 0.803245484828949,
      "learning_rate": 0.0002716931512089383,
      "loss": 0.6651,
      "step": 29600
    },
    {
      "epoch": 0.09467493345659138,
      "grad_norm": 27.575769424438477,
      "learning_rate": 0.00027159751996302256,
      "loss": 0.7546,
      "step": 29700
    },
    {
      "epoch": 0.09499370427631054,
      "grad_norm": 6.977118492126465,
      "learning_rate": 0.0002715018887171068,
      "loss": 1.1302,
      "step": 29800
    },
    {
      "epoch": 0.0953124750960297,
      "grad_norm": 13.199296951293945,
      "learning_rate": 0.0002714062574711911,
      "loss": 0.9684,
      "step": 29900
    },
    {
      "epoch": 0.09563124591574887,
      "grad_norm": 9.415790557861328,
      "learning_rate": 0.00027131062622527533,
      "loss": 0.6069,
      "step": 30000
    },
    {
      "epoch": 0.09595001673546803,
      "grad_norm": 0.010034624487161636,
      "learning_rate": 0.0002712149949793596,
      "loss": 1.1294,
      "step": 30100
    },
    {
      "epoch": 0.0962687875551872,
      "grad_norm": 13.289576530456543,
      "learning_rate": 0.0002711193637334438,
      "loss": 0.7245,
      "step": 30200
    },
    {
      "epoch": 0.09658755837490636,
      "grad_norm": 33.65536117553711,
      "learning_rate": 0.00027102373248752805,
      "loss": 0.8542,
      "step": 30300
    },
    {
      "epoch": 0.09690632919462552,
      "grad_norm": 0.008075537160038948,
      "learning_rate": 0.00027092810124161235,
      "loss": 0.8271,
      "step": 30400
    },
    {
      "epoch": 0.09722510001434469,
      "grad_norm": 21.66472625732422,
      "learning_rate": 0.0002708324699956966,
      "loss": 0.7151,
      "step": 30500
    },
    {
      "epoch": 0.09754387083406385,
      "grad_norm": 33.396671295166016,
      "learning_rate": 0.00027073683874978083,
      "loss": 0.7392,
      "step": 30600
    },
    {
      "epoch": 0.09786264165378301,
      "grad_norm": 16.97966957092285,
      "learning_rate": 0.00027064120750386507,
      "loss": 1.0179,
      "step": 30700
    },
    {
      "epoch": 0.09818141247350218,
      "grad_norm": 38.75822067260742,
      "learning_rate": 0.0002705455762579493,
      "loss": 1.0346,
      "step": 30800
    },
    {
      "epoch": 0.09850018329322134,
      "grad_norm": 3.764920234680176,
      "learning_rate": 0.0002704499450120336,
      "loss": 0.7875,
      "step": 30900
    },
    {
      "epoch": 0.0988189541129405,
      "grad_norm": 0.03669806569814682,
      "learning_rate": 0.00027035431376611784,
      "loss": 0.7101,
      "step": 31000
    },
    {
      "epoch": 0.09913772493265967,
      "grad_norm": 12.633737564086914,
      "learning_rate": 0.0002702586825202021,
      "loss": 0.7936,
      "step": 31100
    },
    {
      "epoch": 0.09945649575237883,
      "grad_norm": 0.0977872833609581,
      "learning_rate": 0.0002701630512742863,
      "loss": 0.6297,
      "step": 31200
    },
    {
      "epoch": 0.099775266572098,
      "grad_norm": 0.18971800804138184,
      "learning_rate": 0.00027006742002837057,
      "loss": 0.8754,
      "step": 31300
    },
    {
      "epoch": 0.10009403739181716,
      "grad_norm": 2.162721633911133,
      "learning_rate": 0.00026997178878245486,
      "loss": 0.9405,
      "step": 31400
    },
    {
      "epoch": 0.10041280821153632,
      "grad_norm": 30.47330093383789,
      "learning_rate": 0.0002698761575365391,
      "loss": 0.6989,
      "step": 31500
    },
    {
      "epoch": 0.10073157903125547,
      "grad_norm": 37.127986907958984,
      "learning_rate": 0.00026978052629062334,
      "loss": 0.8956,
      "step": 31600
    },
    {
      "epoch": 0.10105034985097464,
      "grad_norm": 0.013111764565110207,
      "learning_rate": 0.0002696848950447076,
      "loss": 0.8156,
      "step": 31700
    },
    {
      "epoch": 0.1013691206706938,
      "grad_norm": 1.7244752645492554,
      "learning_rate": 0.0002695892637987918,
      "loss": 0.7657,
      "step": 31800
    },
    {
      "epoch": 0.10168789149041296,
      "grad_norm": 19.02088737487793,
      "learning_rate": 0.00026949363255287606,
      "loss": 0.7,
      "step": 31900
    },
    {
      "epoch": 0.10200666231013213,
      "grad_norm": 61.00727081298828,
      "learning_rate": 0.00026939800130696036,
      "loss": 0.8384,
      "step": 32000
    },
    {
      "epoch": 0.10232543312985129,
      "grad_norm": 27.527206420898438,
      "learning_rate": 0.0002693023700610446,
      "loss": 0.896,
      "step": 32100
    },
    {
      "epoch": 0.10264420394957045,
      "grad_norm": 9.364212036132812,
      "learning_rate": 0.00026920673881512884,
      "loss": 0.7976,
      "step": 32200
    },
    {
      "epoch": 0.10296297476928962,
      "grad_norm": 0.0031015854328870773,
      "learning_rate": 0.0002691111075692131,
      "loss": 0.7749,
      "step": 32300
    },
    {
      "epoch": 0.10328174558900878,
      "grad_norm": 0.7728925347328186,
      "learning_rate": 0.0002690154763232973,
      "loss": 0.73,
      "step": 32400
    },
    {
      "epoch": 0.10360051640872794,
      "grad_norm": 0.6512666344642639,
      "learning_rate": 0.0002689198450773816,
      "loss": 0.6594,
      "step": 32500
    },
    {
      "epoch": 0.10391928722844711,
      "grad_norm": 0.06384875625371933,
      "learning_rate": 0.00026882421383146585,
      "loss": 0.6857,
      "step": 32600
    },
    {
      "epoch": 0.10423805804816627,
      "grad_norm": 0.36005839705467224,
      "learning_rate": 0.0002687285825855501,
      "loss": 1.0394,
      "step": 32700
    },
    {
      "epoch": 0.10455682886788543,
      "grad_norm": 0.020464133471250534,
      "learning_rate": 0.00026863295133963433,
      "loss": 0.8423,
      "step": 32800
    },
    {
      "epoch": 0.1048755996876046,
      "grad_norm": 0.01639832742512226,
      "learning_rate": 0.0002685373200937186,
      "loss": 0.8043,
      "step": 32900
    },
    {
      "epoch": 0.10519437050732376,
      "grad_norm": 48.32734680175781,
      "learning_rate": 0.00026844168884780287,
      "loss": 0.6954,
      "step": 33000
    },
    {
      "epoch": 0.10551314132704293,
      "grad_norm": 0.22889916598796844,
      "learning_rate": 0.0002683460576018871,
      "loss": 0.7209,
      "step": 33100
    },
    {
      "epoch": 0.10583191214676209,
      "grad_norm": 0.0030476958490908146,
      "learning_rate": 0.00026825042635597135,
      "loss": 0.7313,
      "step": 33200
    },
    {
      "epoch": 0.10615068296648125,
      "grad_norm": 0.09175419807434082,
      "learning_rate": 0.0002681547951100556,
      "loss": 0.9989,
      "step": 33300
    },
    {
      "epoch": 0.10646945378620042,
      "grad_norm": 0.023846568539738655,
      "learning_rate": 0.00026805916386413983,
      "loss": 0.9021,
      "step": 33400
    },
    {
      "epoch": 0.10678822460591958,
      "grad_norm": 5.039190292358398,
      "learning_rate": 0.0002679635326182241,
      "loss": 0.6167,
      "step": 33500
    },
    {
      "epoch": 0.10710699542563874,
      "grad_norm": 0.22593913972377777,
      "learning_rate": 0.00026786790137230836,
      "loss": 1.0338,
      "step": 33600
    },
    {
      "epoch": 0.1074257662453579,
      "grad_norm": 15.037595748901367,
      "learning_rate": 0.0002677722701263926,
      "loss": 0.6432,
      "step": 33700
    },
    {
      "epoch": 0.10774453706507706,
      "grad_norm": 94.65717315673828,
      "learning_rate": 0.00026767663888047685,
      "loss": 0.8591,
      "step": 33800
    },
    {
      "epoch": 0.10806330788479622,
      "grad_norm": 25.408275604248047,
      "learning_rate": 0.0002675810076345611,
      "loss": 0.8887,
      "step": 33900
    },
    {
      "epoch": 0.10838207870451538,
      "grad_norm": 0.13276562094688416,
      "learning_rate": 0.0002674853763886453,
      "loss": 0.9823,
      "step": 34000
    },
    {
      "epoch": 0.10870084952423455,
      "grad_norm": 0.38663384318351746,
      "learning_rate": 0.0002673897451427296,
      "loss": 0.6568,
      "step": 34100
    },
    {
      "epoch": 0.10901962034395371,
      "grad_norm": 28.927459716796875,
      "learning_rate": 0.00026729411389681386,
      "loss": 0.8116,
      "step": 34200
    },
    {
      "epoch": 0.10933839116367287,
      "grad_norm": 0.4256832003593445,
      "learning_rate": 0.0002671984826508981,
      "loss": 0.6733,
      "step": 34300
    },
    {
      "epoch": 0.10965716198339204,
      "grad_norm": 59.11311340332031,
      "learning_rate": 0.00026710285140498234,
      "loss": 0.5364,
      "step": 34400
    },
    {
      "epoch": 0.1099759328031112,
      "grad_norm": 42.30532455444336,
      "learning_rate": 0.0002670072201590666,
      "loss": 1.0247,
      "step": 34500
    },
    {
      "epoch": 0.11029470362283036,
      "grad_norm": 0.02205071970820427,
      "learning_rate": 0.0002669115889131509,
      "loss": 0.6816,
      "step": 34600
    },
    {
      "epoch": 0.11061347444254953,
      "grad_norm": 0.0017279062885791063,
      "learning_rate": 0.0002668159576672351,
      "loss": 0.9362,
      "step": 34700
    },
    {
      "epoch": 0.11093224526226869,
      "grad_norm": 44.701942443847656,
      "learning_rate": 0.00026672032642131936,
      "loss": 0.8949,
      "step": 34800
    },
    {
      "epoch": 0.11125101608198786,
      "grad_norm": 66.50836181640625,
      "learning_rate": 0.0002666246951754036,
      "loss": 0.7244,
      "step": 34900
    },
    {
      "epoch": 0.11156978690170702,
      "grad_norm": 80.5391845703125,
      "learning_rate": 0.00026652906392948784,
      "loss": 0.8019,
      "step": 35000
    },
    {
      "epoch": 0.11188855772142618,
      "grad_norm": 0.6146169304847717,
      "learning_rate": 0.00026643343268357213,
      "loss": 0.4663,
      "step": 35100
    },
    {
      "epoch": 0.11220732854114535,
      "grad_norm": 12.653748512268066,
      "learning_rate": 0.0002663378014376564,
      "loss": 0.5015,
      "step": 35200
    },
    {
      "epoch": 0.11252609936086451,
      "grad_norm": 0.4386695623397827,
      "learning_rate": 0.0002662421701917406,
      "loss": 0.8015,
      "step": 35300
    },
    {
      "epoch": 0.11284487018058367,
      "grad_norm": 4.764272689819336,
      "learning_rate": 0.0002661465389458249,
      "loss": 0.6844,
      "step": 35400
    },
    {
      "epoch": 0.11316364100030284,
      "grad_norm": 32.48883819580078,
      "learning_rate": 0.00026605090769990915,
      "loss": 0.9284,
      "step": 35500
    },
    {
      "epoch": 0.113482411820022,
      "grad_norm": 33.61322784423828,
      "learning_rate": 0.0002659552764539934,
      "loss": 0.6541,
      "step": 35600
    },
    {
      "epoch": 0.11380118263974116,
      "grad_norm": 0.053491439670324326,
      "learning_rate": 0.00026585964520807763,
      "loss": 0.6145,
      "step": 35700
    },
    {
      "epoch": 0.11411995345946033,
      "grad_norm": 44.10401153564453,
      "learning_rate": 0.00026576401396216187,
      "loss": 0.6912,
      "step": 35800
    },
    {
      "epoch": 0.11443872427917949,
      "grad_norm": 0.013645727187395096,
      "learning_rate": 0.00026566838271624616,
      "loss": 0.5293,
      "step": 35900
    },
    {
      "epoch": 0.11475749509889864,
      "grad_norm": 54.77745819091797,
      "learning_rate": 0.0002655727514703304,
      "loss": 0.7287,
      "step": 36000
    },
    {
      "epoch": 0.1150762659186178,
      "grad_norm": 9.215317726135254,
      "learning_rate": 0.00026547712022441464,
      "loss": 0.8699,
      "step": 36100
    },
    {
      "epoch": 0.11539503673833697,
      "grad_norm": 70.23393249511719,
      "learning_rate": 0.0002653814889784989,
      "loss": 0.7501,
      "step": 36200
    },
    {
      "epoch": 0.11571380755805613,
      "grad_norm": 11.310879707336426,
      "learning_rate": 0.0002652858577325831,
      "loss": 0.6332,
      "step": 36300
    },
    {
      "epoch": 0.1160325783777753,
      "grad_norm": 0.13209110498428345,
      "learning_rate": 0.0002651902264866674,
      "loss": 0.8762,
      "step": 36400
    },
    {
      "epoch": 0.11635134919749446,
      "grad_norm": 112.1089096069336,
      "learning_rate": 0.00026509459524075166,
      "loss": 0.8853,
      "step": 36500
    },
    {
      "epoch": 0.11667012001721362,
      "grad_norm": 0.0014156694523990154,
      "learning_rate": 0.0002649989639948359,
      "loss": 0.8605,
      "step": 36600
    },
    {
      "epoch": 0.11698889083693279,
      "grad_norm": 4.598029136657715,
      "learning_rate": 0.00026490333274892014,
      "loss": 0.7203,
      "step": 36700
    },
    {
      "epoch": 0.11730766165665195,
      "grad_norm": 0.0034004771150648594,
      "learning_rate": 0.0002648077015030044,
      "loss": 0.7515,
      "step": 36800
    },
    {
      "epoch": 0.11762643247637111,
      "grad_norm": 0.0030590386595577,
      "learning_rate": 0.0002647120702570886,
      "loss": 0.8351,
      "step": 36900
    },
    {
      "epoch": 0.11794520329609028,
      "grad_norm": 17.116668701171875,
      "learning_rate": 0.0002646164390111729,
      "loss": 1.0016,
      "step": 37000
    },
    {
      "epoch": 0.11826397411580944,
      "grad_norm": 1.7833021879196167,
      "learning_rate": 0.00026452080776525716,
      "loss": 0.8095,
      "step": 37100
    },
    {
      "epoch": 0.1185827449355286,
      "grad_norm": 0.007644379511475563,
      "learning_rate": 0.0002644251765193414,
      "loss": 0.6559,
      "step": 37200
    },
    {
      "epoch": 0.11890151575524777,
      "grad_norm": 52.451744079589844,
      "learning_rate": 0.00026432954527342564,
      "loss": 0.7053,
      "step": 37300
    },
    {
      "epoch": 0.11922028657496693,
      "grad_norm": 0.016564127057790756,
      "learning_rate": 0.0002642339140275099,
      "loss": 0.9184,
      "step": 37400
    },
    {
      "epoch": 0.1195390573946861,
      "grad_norm": 0.005504585802555084,
      "learning_rate": 0.00026413828278159417,
      "loss": 1.0204,
      "step": 37500
    },
    {
      "epoch": 0.11985782821440526,
      "grad_norm": 9.276415824890137,
      "learning_rate": 0.0002640426515356784,
      "loss": 1.077,
      "step": 37600
    },
    {
      "epoch": 0.12017659903412442,
      "grad_norm": 32.664608001708984,
      "learning_rate": 0.00026394702028976265,
      "loss": 0.8281,
      "step": 37700
    },
    {
      "epoch": 0.12049536985384358,
      "grad_norm": 21.81192398071289,
      "learning_rate": 0.0002638513890438469,
      "loss": 0.6336,
      "step": 37800
    },
    {
      "epoch": 0.12081414067356275,
      "grad_norm": 43.91317367553711,
      "learning_rate": 0.00026375575779793113,
      "loss": 0.7818,
      "step": 37900
    },
    {
      "epoch": 0.12113291149328191,
      "grad_norm": 3.774200916290283,
      "learning_rate": 0.00026366012655201543,
      "loss": 0.6257,
      "step": 38000
    },
    {
      "epoch": 0.12145168231300107,
      "grad_norm": 118.01194763183594,
      "learning_rate": 0.00026356449530609967,
      "loss": 0.7596,
      "step": 38100
    },
    {
      "epoch": 0.12177045313272022,
      "grad_norm": 0.005810422357171774,
      "learning_rate": 0.0002634688640601839,
      "loss": 0.7917,
      "step": 38200
    },
    {
      "epoch": 0.12208922395243939,
      "grad_norm": 54.895877838134766,
      "learning_rate": 0.00026337323281426815,
      "loss": 0.8521,
      "step": 38300
    },
    {
      "epoch": 0.12240799477215855,
      "grad_norm": 8.266692161560059,
      "learning_rate": 0.0002632776015683524,
      "loss": 0.54,
      "step": 38400
    },
    {
      "epoch": 0.12272676559187772,
      "grad_norm": 1.0529730319976807,
      "learning_rate": 0.0002631819703224367,
      "loss": 0.8074,
      "step": 38500
    },
    {
      "epoch": 0.12304553641159688,
      "grad_norm": 69.75054168701172,
      "learning_rate": 0.0002630863390765209,
      "loss": 0.6053,
      "step": 38600
    },
    {
      "epoch": 0.12336430723131604,
      "grad_norm": 0.02169043757021427,
      "learning_rate": 0.00026299070783060517,
      "loss": 0.9024,
      "step": 38700
    },
    {
      "epoch": 0.1236830780510352,
      "grad_norm": 0.6254860162734985,
      "learning_rate": 0.0002628950765846894,
      "loss": 0.6026,
      "step": 38800
    },
    {
      "epoch": 0.12400184887075437,
      "grad_norm": 0.7047790288925171,
      "learning_rate": 0.00026279944533877365,
      "loss": 0.6226,
      "step": 38900
    },
    {
      "epoch": 0.12432061969047353,
      "grad_norm": 1.1189169883728027,
      "learning_rate": 0.0002627038140928579,
      "loss": 0.7624,
      "step": 39000
    },
    {
      "epoch": 0.1246393905101927,
      "grad_norm": 0.01725970394909382,
      "learning_rate": 0.0002626081828469422,
      "loss": 0.7205,
      "step": 39100
    },
    {
      "epoch": 0.12495816132991186,
      "grad_norm": 0.0065383403562009335,
      "learning_rate": 0.0002625125516010264,
      "loss": 0.6156,
      "step": 39200
    },
    {
      "epoch": 0.125276932149631,
      "grad_norm": 0.0704607367515564,
      "learning_rate": 0.00026241692035511066,
      "loss": 0.5397,
      "step": 39300
    },
    {
      "epoch": 0.1255957029693502,
      "grad_norm": 52.1117057800293,
      "learning_rate": 0.0002623212891091949,
      "loss": 0.5921,
      "step": 39400
    },
    {
      "epoch": 0.12591447378906934,
      "grad_norm": 0.030223693698644638,
      "learning_rate": 0.00026222565786327914,
      "loss": 0.8481,
      "step": 39500
    },
    {
      "epoch": 0.12623324460878851,
      "grad_norm": 0.017619455233216286,
      "learning_rate": 0.00026213002661736344,
      "loss": 0.9403,
      "step": 39600
    },
    {
      "epoch": 0.12655201542850766,
      "grad_norm": 24.07187271118164,
      "learning_rate": 0.0002620343953714477,
      "loss": 0.7825,
      "step": 39700
    },
    {
      "epoch": 0.12687078624822684,
      "grad_norm": 31.358985900878906,
      "learning_rate": 0.0002619387641255319,
      "loss": 0.8266,
      "step": 39800
    },
    {
      "epoch": 0.127189557067946,
      "grad_norm": 0.017780805006623268,
      "learning_rate": 0.00026184313287961616,
      "loss": 0.492,
      "step": 39900
    },
    {
      "epoch": 0.12750832788766517,
      "grad_norm": 0.05937027186155319,
      "learning_rate": 0.0002617475016337004,
      "loss": 0.7491,
      "step": 40000
    },
    {
      "epoch": 0.12782709870738432,
      "grad_norm": 0.007108777761459351,
      "learning_rate": 0.0002616518703877847,
      "loss": 0.9277,
      "step": 40100
    },
    {
      "epoch": 0.1281458695271035,
      "grad_norm": 2.6366803646087646,
      "learning_rate": 0.00026155623914186893,
      "loss": 0.8341,
      "step": 40200
    },
    {
      "epoch": 0.12846464034682264,
      "grad_norm": 0.21051272749900818,
      "learning_rate": 0.0002614606078959532,
      "loss": 0.6261,
      "step": 40300
    },
    {
      "epoch": 0.12878341116654182,
      "grad_norm": 0.36818838119506836,
      "learning_rate": 0.0002613649766500374,
      "loss": 0.8819,
      "step": 40400
    },
    {
      "epoch": 0.12910218198626097,
      "grad_norm": 0.0034952028654515743,
      "learning_rate": 0.00026126934540412165,
      "loss": 0.5063,
      "step": 40500
    },
    {
      "epoch": 0.12942095280598015,
      "grad_norm": 0.00531455734744668,
      "learning_rate": 0.00026117371415820595,
      "loss": 0.7681,
      "step": 40600
    },
    {
      "epoch": 0.1297397236256993,
      "grad_norm": 0.35853683948516846,
      "learning_rate": 0.0002610780829122902,
      "loss": 0.5409,
      "step": 40700
    },
    {
      "epoch": 0.13005849444541848,
      "grad_norm": 0.019478464499115944,
      "learning_rate": 0.00026098245166637443,
      "loss": 0.9155,
      "step": 40800
    },
    {
      "epoch": 0.13037726526513763,
      "grad_norm": 0.7066096067428589,
      "learning_rate": 0.0002608868204204587,
      "loss": 0.7214,
      "step": 40900
    },
    {
      "epoch": 0.1306960360848568,
      "grad_norm": 0.037650059908628464,
      "learning_rate": 0.0002607911891745429,
      "loss": 0.8435,
      "step": 41000
    },
    {
      "epoch": 0.13101480690457595,
      "grad_norm": 0.7539331912994385,
      "learning_rate": 0.00026069555792862715,
      "loss": 0.4976,
      "step": 41100
    },
    {
      "epoch": 0.13133357772429513,
      "grad_norm": 10.639071464538574,
      "learning_rate": 0.00026059992668271145,
      "loss": 0.7731,
      "step": 41200
    },
    {
      "epoch": 0.13165234854401428,
      "grad_norm": 0.0035587463062256575,
      "learning_rate": 0.0002605042954367957,
      "loss": 0.806,
      "step": 41300
    },
    {
      "epoch": 0.13197111936373343,
      "grad_norm": 49.35920333862305,
      "learning_rate": 0.00026040866419088,
      "loss": 0.7278,
      "step": 41400
    },
    {
      "epoch": 0.1322898901834526,
      "grad_norm": 0.3999769985675812,
      "learning_rate": 0.0002603130329449642,
      "loss": 0.4477,
      "step": 41500
    },
    {
      "epoch": 0.13260866100317176,
      "grad_norm": 18.386560440063477,
      "learning_rate": 0.0002602174016990484,
      "loss": 0.8851,
      "step": 41600
    },
    {
      "epoch": 0.13292743182289093,
      "grad_norm": 27.530179977416992,
      "learning_rate": 0.0002601217704531327,
      "loss": 1.052,
      "step": 41700
    },
    {
      "epoch": 0.13324620264261008,
      "grad_norm": 2.233342170715332,
      "learning_rate": 0.00026002613920721694,
      "loss": 0.8581,
      "step": 41800
    },
    {
      "epoch": 0.13356497346232926,
      "grad_norm": 0.035247962921857834,
      "learning_rate": 0.00025993050796130124,
      "loss": 0.9594,
      "step": 41900
    },
    {
      "epoch": 0.1338837442820484,
      "grad_norm": 0.02633599564433098,
      "learning_rate": 0.0002598348767153855,
      "loss": 0.7836,
      "step": 42000
    },
    {
      "epoch": 0.1342025151017676,
      "grad_norm": 5.195586204528809,
      "learning_rate": 0.0002597392454694697,
      "loss": 0.6331,
      "step": 42100
    },
    {
      "epoch": 0.13452128592148674,
      "grad_norm": 0.10146794468164444,
      "learning_rate": 0.00025964361422355396,
      "loss": 0.9499,
      "step": 42200
    },
    {
      "epoch": 0.13484005674120592,
      "grad_norm": 0.006402403581887484,
      "learning_rate": 0.0002595479829776382,
      "loss": 0.5462,
      "step": 42300
    },
    {
      "epoch": 0.13515882756092507,
      "grad_norm": 0.018235918134450912,
      "learning_rate": 0.00025945235173172244,
      "loss": 0.8718,
      "step": 42400
    },
    {
      "epoch": 0.13547759838064424,
      "grad_norm": 0.004785292316228151,
      "learning_rate": 0.00025935672048580673,
      "loss": 0.5362,
      "step": 42500
    },
    {
      "epoch": 0.1357963692003634,
      "grad_norm": 64.17142486572266,
      "learning_rate": 0.00025926108923989097,
      "loss": 0.6962,
      "step": 42600
    },
    {
      "epoch": 0.13611514002008257,
      "grad_norm": 57.164546966552734,
      "learning_rate": 0.0002591654579939752,
      "loss": 0.6964,
      "step": 42700
    },
    {
      "epoch": 0.13643391083980172,
      "grad_norm": 3.6295182704925537,
      "learning_rate": 0.00025906982674805945,
      "loss": 0.3929,
      "step": 42800
    },
    {
      "epoch": 0.1367526816595209,
      "grad_norm": 0.0481281578540802,
      "learning_rate": 0.0002589741955021437,
      "loss": 0.6674,
      "step": 42900
    },
    {
      "epoch": 0.13707145247924005,
      "grad_norm": 212.624267578125,
      "learning_rate": 0.000258878564256228,
      "loss": 0.4991,
      "step": 43000
    },
    {
      "epoch": 0.13739022329895922,
      "grad_norm": 1.4372996091842651,
      "learning_rate": 0.00025878293301031223,
      "loss": 0.6156,
      "step": 43100
    },
    {
      "epoch": 0.13770899411867837,
      "grad_norm": 0.006305934861302376,
      "learning_rate": 0.00025868730176439647,
      "loss": 0.9939,
      "step": 43200
    },
    {
      "epoch": 0.13802776493839755,
      "grad_norm": 47.8460578918457,
      "learning_rate": 0.0002585916705184807,
      "loss": 0.6563,
      "step": 43300
    },
    {
      "epoch": 0.1383465357581167,
      "grad_norm": 36.387359619140625,
      "learning_rate": 0.00025849603927256495,
      "loss": 0.8074,
      "step": 43400
    },
    {
      "epoch": 0.13866530657783588,
      "grad_norm": 18.450544357299805,
      "learning_rate": 0.00025840040802664924,
      "loss": 0.5704,
      "step": 43500
    },
    {
      "epoch": 0.13898407739755503,
      "grad_norm": 0.08512012660503387,
      "learning_rate": 0.0002583047767807335,
      "loss": 0.9004,
      "step": 43600
    },
    {
      "epoch": 0.13930284821727418,
      "grad_norm": 8.795586585998535,
      "learning_rate": 0.0002582091455348177,
      "loss": 0.5968,
      "step": 43700
    },
    {
      "epoch": 0.13962161903699336,
      "grad_norm": 23.178674697875977,
      "learning_rate": 0.00025811351428890197,
      "loss": 0.6334,
      "step": 43800
    },
    {
      "epoch": 0.1399403898567125,
      "grad_norm": 0.0617077499628067,
      "learning_rate": 0.0002580178830429862,
      "loss": 0.554,
      "step": 43900
    },
    {
      "epoch": 0.14025916067643168,
      "grad_norm": 0.012270476669073105,
      "learning_rate": 0.0002579222517970705,
      "loss": 0.8434,
      "step": 44000
    },
    {
      "epoch": 0.14057793149615083,
      "grad_norm": 0.03388199955224991,
      "learning_rate": 0.00025782662055115474,
      "loss": 0.9593,
      "step": 44100
    },
    {
      "epoch": 0.14089670231587,
      "grad_norm": 0.004995906259864569,
      "learning_rate": 0.000257730989305239,
      "loss": 0.7585,
      "step": 44200
    },
    {
      "epoch": 0.14121547313558916,
      "grad_norm": 11.305585861206055,
      "learning_rate": 0.0002576353580593232,
      "loss": 0.7348,
      "step": 44300
    },
    {
      "epoch": 0.14153424395530834,
      "grad_norm": 90.49750518798828,
      "learning_rate": 0.00025753972681340746,
      "loss": 0.744,
      "step": 44400
    },
    {
      "epoch": 0.1418530147750275,
      "grad_norm": 0.01835639402270317,
      "learning_rate": 0.0002574440955674917,
      "loss": 0.4303,
      "step": 44500
    },
    {
      "epoch": 0.14217178559474666,
      "grad_norm": 13.050702095031738,
      "learning_rate": 0.000257348464321576,
      "loss": 0.8597,
      "step": 44600
    },
    {
      "epoch": 0.1424905564144658,
      "grad_norm": 4.310907363891602,
      "learning_rate": 0.00025725283307566024,
      "loss": 0.8793,
      "step": 44700
    },
    {
      "epoch": 0.142809327234185,
      "grad_norm": 0.20128493010997772,
      "learning_rate": 0.0002571572018297445,
      "loss": 0.6963,
      "step": 44800
    },
    {
      "epoch": 0.14312809805390414,
      "grad_norm": 22.452247619628906,
      "learning_rate": 0.0002570615705838287,
      "loss": 0.7391,
      "step": 44900
    },
    {
      "epoch": 0.14344686887362332,
      "grad_norm": 0.8587485551834106,
      "learning_rate": 0.00025696593933791296,
      "loss": 0.8187,
      "step": 45000
    },
    {
      "epoch": 0.14376563969334247,
      "grad_norm": 0.6873751878738403,
      "learning_rate": 0.00025687030809199725,
      "loss": 0.7252,
      "step": 45100
    },
    {
      "epoch": 0.14408441051306164,
      "grad_norm": 36.69794464111328,
      "learning_rate": 0.0002567746768460815,
      "loss": 0.5168,
      "step": 45200
    },
    {
      "epoch": 0.1444031813327808,
      "grad_norm": 46.465850830078125,
      "learning_rate": 0.00025667904560016573,
      "loss": 0.6738,
      "step": 45300
    },
    {
      "epoch": 0.14472195215249997,
      "grad_norm": 0.026366086676716805,
      "learning_rate": 0.00025658341435425,
      "loss": 0.7222,
      "step": 45400
    },
    {
      "epoch": 0.14504072297221912,
      "grad_norm": 80.74571228027344,
      "learning_rate": 0.0002564877831083342,
      "loss": 0.5281,
      "step": 45500
    },
    {
      "epoch": 0.1453594937919383,
      "grad_norm": 0.2418309897184372,
      "learning_rate": 0.0002563921518624185,
      "loss": 0.7848,
      "step": 45600
    },
    {
      "epoch": 0.14567826461165745,
      "grad_norm": 0.0059774648398160934,
      "learning_rate": 0.00025629652061650275,
      "loss": 1.0119,
      "step": 45700
    },
    {
      "epoch": 0.1459970354313766,
      "grad_norm": 0.18265627324581146,
      "learning_rate": 0.000256200889370587,
      "loss": 0.682,
      "step": 45800
    },
    {
      "epoch": 0.14631580625109578,
      "grad_norm": 0.0017183213494718075,
      "learning_rate": 0.00025610525812467123,
      "loss": 0.9604,
      "step": 45900
    },
    {
      "epoch": 0.14663457707081493,
      "grad_norm": 0.010352990590035915,
      "learning_rate": 0.00025600962687875547,
      "loss": 0.7407,
      "step": 46000
    },
    {
      "epoch": 0.1469533478905341,
      "grad_norm": 43.21519470214844,
      "learning_rate": 0.00025591399563283976,
      "loss": 0.9381,
      "step": 46100
    },
    {
      "epoch": 0.14727211871025325,
      "grad_norm": 0.004777205176651478,
      "learning_rate": 0.000255818364386924,
      "loss": 0.71,
      "step": 46200
    },
    {
      "epoch": 0.14759088952997243,
      "grad_norm": 10.246039390563965,
      "learning_rate": 0.00025572273314100825,
      "loss": 0.7842,
      "step": 46300
    },
    {
      "epoch": 0.14790966034969158,
      "grad_norm": 0.0018607189413160086,
      "learning_rate": 0.0002556271018950925,
      "loss": 0.8389,
      "step": 46400
    },
    {
      "epoch": 0.14822843116941076,
      "grad_norm": 0.0030887655448168516,
      "learning_rate": 0.0002555314706491767,
      "loss": 0.5996,
      "step": 46500
    },
    {
      "epoch": 0.1485472019891299,
      "grad_norm": 15.103476524353027,
      "learning_rate": 0.00025543583940326097,
      "loss": 0.7338,
      "step": 46600
    },
    {
      "epoch": 0.14886597280884908,
      "grad_norm": 0.7629439830780029,
      "learning_rate": 0.00025534020815734526,
      "loss": 0.6923,
      "step": 46700
    },
    {
      "epoch": 0.14918474362856823,
      "grad_norm": 0.01772112213075161,
      "learning_rate": 0.0002552445769114295,
      "loss": 0.518,
      "step": 46800
    },
    {
      "epoch": 0.1495035144482874,
      "grad_norm": 0.029699955135583878,
      "learning_rate": 0.0002551489456655138,
      "loss": 0.9683,
      "step": 46900
    },
    {
      "epoch": 0.14982228526800656,
      "grad_norm": 43.13624954223633,
      "learning_rate": 0.00025505331441959804,
      "loss": 0.9728,
      "step": 47000
    },
    {
      "epoch": 0.15014105608772574,
      "grad_norm": 0.016189830377697945,
      "learning_rate": 0.0002549576831736822,
      "loss": 0.5046,
      "step": 47100
    },
    {
      "epoch": 0.1504598269074449,
      "grad_norm": 12.37502670288086,
      "learning_rate": 0.0002548620519277665,
      "loss": 0.6793,
      "step": 47200
    },
    {
      "epoch": 0.15077859772716407,
      "grad_norm": 0.06889814138412476,
      "learning_rate": 0.00025476642068185076,
      "loss": 0.7131,
      "step": 47300
    },
    {
      "epoch": 0.15109736854688322,
      "grad_norm": 0.06614867597818375,
      "learning_rate": 0.000254670789435935,
      "loss": 0.7214,
      "step": 47400
    },
    {
      "epoch": 0.1514161393666024,
      "grad_norm": 0.09211952239274979,
      "learning_rate": 0.0002545751581900193,
      "loss": 0.6907,
      "step": 47500
    },
    {
      "epoch": 0.15173491018632154,
      "grad_norm": 0.041872452944517136,
      "learning_rate": 0.00025447952694410353,
      "loss": 0.5878,
      "step": 47600
    },
    {
      "epoch": 0.15205368100604072,
      "grad_norm": 33.87409210205078,
      "learning_rate": 0.00025438389569818777,
      "loss": 0.6407,
      "step": 47700
    },
    {
      "epoch": 0.15237245182575987,
      "grad_norm": 0.3060319125652313,
      "learning_rate": 0.000254288264452272,
      "loss": 0.7262,
      "step": 47800
    },
    {
      "epoch": 0.15269122264547905,
      "grad_norm": 55.38869857788086,
      "learning_rate": 0.00025419263320635625,
      "loss": 0.9712,
      "step": 47900
    },
    {
      "epoch": 0.1530099934651982,
      "grad_norm": 0.16019649803638458,
      "learning_rate": 0.00025409700196044055,
      "loss": 0.6712,
      "step": 48000
    },
    {
      "epoch": 0.15332876428491735,
      "grad_norm": 0.13944517076015472,
      "learning_rate": 0.0002540013707145248,
      "loss": 0.8715,
      "step": 48100
    },
    {
      "epoch": 0.15364753510463652,
      "grad_norm": 39.42578125,
      "learning_rate": 0.00025390573946860903,
      "loss": 0.909,
      "step": 48200
    },
    {
      "epoch": 0.15396630592435567,
      "grad_norm": 0.002901744795963168,
      "learning_rate": 0.00025381010822269327,
      "loss": 0.5488,
      "step": 48300
    },
    {
      "epoch": 0.15428507674407485,
      "grad_norm": 5.364471912384033,
      "learning_rate": 0.0002537144769767775,
      "loss": 0.6521,
      "step": 48400
    },
    {
      "epoch": 0.154603847563794,
      "grad_norm": 6.948624610900879,
      "learning_rate": 0.0002536188457308618,
      "loss": 0.6014,
      "step": 48500
    },
    {
      "epoch": 0.15492261838351318,
      "grad_norm": 0.031138572841882706,
      "learning_rate": 0.00025352321448494604,
      "loss": 0.5838,
      "step": 48600
    },
    {
      "epoch": 0.15524138920323233,
      "grad_norm": 0.0032253791578114033,
      "learning_rate": 0.0002534275832390303,
      "loss": 0.6429,
      "step": 48700
    },
    {
      "epoch": 0.1555601600229515,
      "grad_norm": 77.05927276611328,
      "learning_rate": 0.0002533319519931145,
      "loss": 0.7626,
      "step": 48800
    },
    {
      "epoch": 0.15587893084267065,
      "grad_norm": 21.753278732299805,
      "learning_rate": 0.00025323632074719877,
      "loss": 0.63,
      "step": 48900
    },
    {
      "epoch": 0.15619770166238983,
      "grad_norm": 0.00463744904845953,
      "learning_rate": 0.00025314068950128306,
      "loss": 0.7864,
      "step": 49000
    },
    {
      "epoch": 0.15651647248210898,
      "grad_norm": 13.798235893249512,
      "learning_rate": 0.0002530450582553673,
      "loss": 0.4287,
      "step": 49100
    },
    {
      "epoch": 0.15683524330182816,
      "grad_norm": 4.267522811889648,
      "learning_rate": 0.00025294942700945154,
      "loss": 0.5085,
      "step": 49200
    },
    {
      "epoch": 0.1571540141215473,
      "grad_norm": 42.653865814208984,
      "learning_rate": 0.0002528537957635358,
      "loss": 0.6445,
      "step": 49300
    },
    {
      "epoch": 0.15747278494126649,
      "grad_norm": 95.21366882324219,
      "learning_rate": 0.00025275816451762,
      "loss": 0.7327,
      "step": 49400
    },
    {
      "epoch": 0.15779155576098564,
      "grad_norm": 17.154272079467773,
      "learning_rate": 0.00025266253327170426,
      "loss": 0.4386,
      "step": 49500
    },
    {
      "epoch": 0.1581103265807048,
      "grad_norm": 5.471948146820068,
      "learning_rate": 0.00025256690202578856,
      "loss": 0.2952,
      "step": 49600
    },
    {
      "epoch": 0.15842909740042396,
      "grad_norm": 0.08278928697109222,
      "learning_rate": 0.0002524712707798728,
      "loss": 0.8008,
      "step": 49700
    },
    {
      "epoch": 0.15874786822014314,
      "grad_norm": 42.81609344482422,
      "learning_rate": 0.00025237563953395704,
      "loss": 0.4797,
      "step": 49800
    },
    {
      "epoch": 0.1590666390398623,
      "grad_norm": 0.7927757501602173,
      "learning_rate": 0.0002522800082880413,
      "loss": 0.8395,
      "step": 49900
    },
    {
      "epoch": 0.15938540985958147,
      "grad_norm": 53.15052795410156,
      "learning_rate": 0.0002521843770421255,
      "loss": 0.7325,
      "step": 50000
    },
    {
      "epoch": 0.15970418067930062,
      "grad_norm": 41.307289123535156,
      "learning_rate": 0.0002520887457962098,
      "loss": 0.5522,
      "step": 50100
    },
    {
      "epoch": 0.16002295149901977,
      "grad_norm": 29.31371307373047,
      "learning_rate": 0.00025199311455029405,
      "loss": 0.4662,
      "step": 50200
    },
    {
      "epoch": 0.16034172231873894,
      "grad_norm": 0.4951360821723938,
      "learning_rate": 0.0002518974833043783,
      "loss": 0.6599,
      "step": 50300
    },
    {
      "epoch": 0.1606604931384581,
      "grad_norm": 0.00819956511259079,
      "learning_rate": 0.00025180185205846253,
      "loss": 0.7687,
      "step": 50400
    },
    {
      "epoch": 0.16097926395817727,
      "grad_norm": 21.90972328186035,
      "learning_rate": 0.0002517062208125468,
      "loss": 0.7589,
      "step": 50500
    },
    {
      "epoch": 0.16129803477789642,
      "grad_norm": 12.025655746459961,
      "learning_rate": 0.00025161058956663107,
      "loss": 0.7828,
      "step": 50600
    },
    {
      "epoch": 0.1616168055976156,
      "grad_norm": 0.4041576087474823,
      "learning_rate": 0.0002515149583207153,
      "loss": 0.5984,
      "step": 50700
    },
    {
      "epoch": 0.16193557641733475,
      "grad_norm": 0.002900961320847273,
      "learning_rate": 0.00025141932707479955,
      "loss": 0.503,
      "step": 50800
    },
    {
      "epoch": 0.16225434723705393,
      "grad_norm": 30.374858856201172,
      "learning_rate": 0.0002513236958288838,
      "loss": 0.5492,
      "step": 50900
    },
    {
      "epoch": 0.16257311805677307,
      "grad_norm": 22.696508407592773,
      "learning_rate": 0.00025122806458296803,
      "loss": 0.7312,
      "step": 51000
    },
    {
      "epoch": 0.16289188887649225,
      "grad_norm": 0.10344374924898148,
      "learning_rate": 0.0002511324333370523,
      "loss": 0.7702,
      "step": 51100
    },
    {
      "epoch": 0.1632106596962114,
      "grad_norm": 9.203460693359375,
      "learning_rate": 0.00025103680209113656,
      "loss": 0.6752,
      "step": 51200
    },
    {
      "epoch": 0.16352943051593058,
      "grad_norm": 35.8379020690918,
      "learning_rate": 0.0002509411708452208,
      "loss": 0.7449,
      "step": 51300
    },
    {
      "epoch": 0.16384820133564973,
      "grad_norm": 1.2852789163589478,
      "learning_rate": 0.00025084553959930505,
      "loss": 0.6263,
      "step": 51400
    },
    {
      "epoch": 0.1641669721553689,
      "grad_norm": 3.8515098094940186,
      "learning_rate": 0.0002507499083533893,
      "loss": 0.72,
      "step": 51500
    },
    {
      "epoch": 0.16448574297508806,
      "grad_norm": 0.002257994143292308,
      "learning_rate": 0.0002506542771074735,
      "loss": 0.5423,
      "step": 51600
    },
    {
      "epoch": 0.16480451379480723,
      "grad_norm": 1.0281063318252563,
      "learning_rate": 0.0002505586458615578,
      "loss": 0.4933,
      "step": 51700
    },
    {
      "epoch": 0.16512328461452638,
      "grad_norm": 34.91572952270508,
      "learning_rate": 0.00025046301461564206,
      "loss": 0.6789,
      "step": 51800
    },
    {
      "epoch": 0.16544205543424556,
      "grad_norm": 42.19369125366211,
      "learning_rate": 0.0002503673833697263,
      "loss": 0.6635,
      "step": 51900
    },
    {
      "epoch": 0.1657608262539647,
      "grad_norm": 0.0044871484860777855,
      "learning_rate": 0.00025027175212381054,
      "loss": 0.6036,
      "step": 52000
    },
    {
      "epoch": 0.1660795970736839,
      "grad_norm": 0.06495276838541031,
      "learning_rate": 0.0002501761208778948,
      "loss": 0.536,
      "step": 52100
    },
    {
      "epoch": 0.16639836789340304,
      "grad_norm": 0.0018066390184685588,
      "learning_rate": 0.0002500804896319791,
      "loss": 0.4346,
      "step": 52200
    },
    {
      "epoch": 0.1667171387131222,
      "grad_norm": 0.000729260325897485,
      "learning_rate": 0.0002499848583860633,
      "loss": 0.7534,
      "step": 52300
    },
    {
      "epoch": 0.16703590953284136,
      "grad_norm": 0.0550587996840477,
      "learning_rate": 0.0002498892271401476,
      "loss": 0.6549,
      "step": 52400
    },
    {
      "epoch": 0.16735468035256051,
      "grad_norm": 0.5688314437866211,
      "learning_rate": 0.0002497935958942318,
      "loss": 0.9349,
      "step": 52500
    },
    {
      "epoch": 0.1676734511722797,
      "grad_norm": 0.05708634480834007,
      "learning_rate": 0.00024969796464831604,
      "loss": 0.4949,
      "step": 52600
    },
    {
      "epoch": 0.16799222199199884,
      "grad_norm": 28.31810188293457,
      "learning_rate": 0.00024960233340240033,
      "loss": 0.6952,
      "step": 52700
    },
    {
      "epoch": 0.16831099281171802,
      "grad_norm": 46.99721145629883,
      "learning_rate": 0.0002495067021564846,
      "loss": 0.529,
      "step": 52800
    },
    {
      "epoch": 0.16862976363143717,
      "grad_norm": 21.26101303100586,
      "learning_rate": 0.0002494110709105688,
      "loss": 0.5573,
      "step": 52900
    },
    {
      "epoch": 0.16894853445115635,
      "grad_norm": 54.58336639404297,
      "learning_rate": 0.0002493154396646531,
      "loss": 0.8392,
      "step": 53000
    },
    {
      "epoch": 0.1692673052708755,
      "grad_norm": 0.08237215876579285,
      "learning_rate": 0.0002492198084187373,
      "loss": 0.6566,
      "step": 53100
    },
    {
      "epoch": 0.16958607609059467,
      "grad_norm": 36.24849319458008,
      "learning_rate": 0.0002491241771728216,
      "loss": 0.5899,
      "step": 53200
    },
    {
      "epoch": 0.16990484691031382,
      "grad_norm": 1.6131058931350708,
      "learning_rate": 0.00024902854592690583,
      "loss": 0.7245,
      "step": 53300
    },
    {
      "epoch": 0.170223617730033,
      "grad_norm": 0.07272686064243317,
      "learning_rate": 0.00024893291468099007,
      "loss": 0.6279,
      "step": 53400
    },
    {
      "epoch": 0.17054238854975215,
      "grad_norm": 0.0005066837766207755,
      "learning_rate": 0.00024883728343507436,
      "loss": 0.9355,
      "step": 53500
    },
    {
      "epoch": 0.17086115936947133,
      "grad_norm": 0.0019272296922281384,
      "learning_rate": 0.0002487416521891586,
      "loss": 0.4187,
      "step": 53600
    },
    {
      "epoch": 0.17117993018919048,
      "grad_norm": 0.007921886630356312,
      "learning_rate": 0.00024864602094324284,
      "loss": 0.5827,
      "step": 53700
    },
    {
      "epoch": 0.17149870100890965,
      "grad_norm": 27.643287658691406,
      "learning_rate": 0.0002485503896973271,
      "loss": 0.9039,
      "step": 53800
    },
    {
      "epoch": 0.1718174718286288,
      "grad_norm": 0.020702065899968147,
      "learning_rate": 0.0002484547584514113,
      "loss": 0.7327,
      "step": 53900
    },
    {
      "epoch": 0.17213624264834798,
      "grad_norm": 89.88591003417969,
      "learning_rate": 0.0002483591272054956,
      "loss": 0.7582,
      "step": 54000
    },
    {
      "epoch": 0.17245501346806713,
      "grad_norm": 6.452044486999512,
      "learning_rate": 0.00024826349595957986,
      "loss": 0.504,
      "step": 54100
    },
    {
      "epoch": 0.1727737842877863,
      "grad_norm": 30.388608932495117,
      "learning_rate": 0.0002481678647136641,
      "loss": 0.7805,
      "step": 54200
    },
    {
      "epoch": 0.17309255510750546,
      "grad_norm": 0.0009662733064033091,
      "learning_rate": 0.00024807223346774834,
      "loss": 0.7772,
      "step": 54300
    },
    {
      "epoch": 0.17341132592722464,
      "grad_norm": 0.22316789627075195,
      "learning_rate": 0.0002479766022218326,
      "loss": 0.6848,
      "step": 54400
    },
    {
      "epoch": 0.17373009674694379,
      "grad_norm": 0.1539343297481537,
      "learning_rate": 0.0002478809709759169,
      "loss": 0.7104,
      "step": 54500
    },
    {
      "epoch": 0.17404886756666293,
      "grad_norm": 3.622006416320801,
      "learning_rate": 0.0002477853397300011,
      "loss": 0.47,
      "step": 54600
    },
    {
      "epoch": 0.1743676383863821,
      "grad_norm": 0.7646722793579102,
      "learning_rate": 0.00024768970848408536,
      "loss": 0.4733,
      "step": 54700
    },
    {
      "epoch": 0.17468640920610126,
      "grad_norm": 0.5908974409103394,
      "learning_rate": 0.0002475940772381696,
      "loss": 0.6249,
      "step": 54800
    },
    {
      "epoch": 0.17500518002582044,
      "grad_norm": 0.11540873348712921,
      "learning_rate": 0.00024749844599225384,
      "loss": 0.6661,
      "step": 54900
    },
    {
      "epoch": 0.1753239508455396,
      "grad_norm": 64.11109924316406,
      "learning_rate": 0.0002474028147463381,
      "loss": 0.537,
      "step": 55000
    },
    {
      "epoch": 0.17564272166525877,
      "grad_norm": 82.8966064453125,
      "learning_rate": 0.00024730718350042237,
      "loss": 0.5776,
      "step": 55100
    },
    {
      "epoch": 0.17596149248497792,
      "grad_norm": 62.4371223449707,
      "learning_rate": 0.0002472115522545066,
      "loss": 0.5912,
      "step": 55200
    },
    {
      "epoch": 0.1762802633046971,
      "grad_norm": 0.00011482443369459361,
      "learning_rate": 0.00024711592100859085,
      "loss": 0.5896,
      "step": 55300
    },
    {
      "epoch": 0.17659903412441624,
      "grad_norm": 24.265579223632812,
      "learning_rate": 0.0002470202897626751,
      "loss": 0.5526,
      "step": 55400
    },
    {
      "epoch": 0.17691780494413542,
      "grad_norm": 58.95161819458008,
      "learning_rate": 0.00024692465851675933,
      "loss": 0.8375,
      "step": 55500
    },
    {
      "epoch": 0.17723657576385457,
      "grad_norm": 0.6548629403114319,
      "learning_rate": 0.00024682902727084363,
      "loss": 0.6501,
      "step": 55600
    },
    {
      "epoch": 0.17755534658357375,
      "grad_norm": 34.638668060302734,
      "learning_rate": 0.00024673339602492787,
      "loss": 0.6771,
      "step": 55700
    },
    {
      "epoch": 0.1778741174032929,
      "grad_norm": 16.198076248168945,
      "learning_rate": 0.0002466377647790121,
      "loss": 0.8045,
      "step": 55800
    },
    {
      "epoch": 0.17819288822301207,
      "grad_norm": 0.0015542773762717843,
      "learning_rate": 0.00024654213353309635,
      "loss": 0.374,
      "step": 55900
    },
    {
      "epoch": 0.17851165904273122,
      "grad_norm": 12.563241004943848,
      "learning_rate": 0.0002464465022871806,
      "loss": 0.5884,
      "step": 56000
    },
    {
      "epoch": 0.1788304298624504,
      "grad_norm": 56.05903244018555,
      "learning_rate": 0.0002463508710412649,
      "loss": 0.4253,
      "step": 56100
    },
    {
      "epoch": 0.17914920068216955,
      "grad_norm": 0.00359943020157516,
      "learning_rate": 0.0002462552397953491,
      "loss": 0.8072,
      "step": 56200
    },
    {
      "epoch": 0.17946797150188873,
      "grad_norm": 0.00060944480355829,
      "learning_rate": 0.00024615960854943336,
      "loss": 0.7239,
      "step": 56300
    },
    {
      "epoch": 0.17978674232160788,
      "grad_norm": 13.489920616149902,
      "learning_rate": 0.0002460639773035176,
      "loss": 0.4768,
      "step": 56400
    },
    {
      "epoch": 0.18010551314132706,
      "grad_norm": 0.0013644590508192778,
      "learning_rate": 0.00024596834605760185,
      "loss": 0.7621,
      "step": 56500
    },
    {
      "epoch": 0.1804242839610462,
      "grad_norm": 0.7126476764678955,
      "learning_rate": 0.0002458727148116861,
      "loss": 0.6452,
      "step": 56600
    },
    {
      "epoch": 0.18074305478076536,
      "grad_norm": 0.0010574869811534882,
      "learning_rate": 0.0002457770835657704,
      "loss": 0.4642,
      "step": 56700
    },
    {
      "epoch": 0.18106182560048453,
      "grad_norm": 0.02384907566010952,
      "learning_rate": 0.0002456814523198546,
      "loss": 0.629,
      "step": 56800
    },
    {
      "epoch": 0.18138059642020368,
      "grad_norm": 92.68798828125,
      "learning_rate": 0.00024558582107393886,
      "loss": 0.5549,
      "step": 56900
    },
    {
      "epoch": 0.18169936723992286,
      "grad_norm": 0.20140483975410461,
      "learning_rate": 0.0002454901898280231,
      "loss": 0.7704,
      "step": 57000
    },
    {
      "epoch": 0.182018138059642,
      "grad_norm": 0.23004375398159027,
      "learning_rate": 0.00024539455858210734,
      "loss": 0.5798,
      "step": 57100
    },
    {
      "epoch": 0.1823369088793612,
      "grad_norm": 0.8161366581916809,
      "learning_rate": 0.00024529892733619164,
      "loss": 0.684,
      "step": 57200
    },
    {
      "epoch": 0.18265567969908034,
      "grad_norm": 0.0007343128672800958,
      "learning_rate": 0.0002452032960902759,
      "loss": 0.6996,
      "step": 57300
    },
    {
      "epoch": 0.18297445051879951,
      "grad_norm": 8.967988967895508,
      "learning_rate": 0.0002451076648443601,
      "loss": 0.4827,
      "step": 57400
    },
    {
      "epoch": 0.18329322133851866,
      "grad_norm": 33.4660758972168,
      "learning_rate": 0.00024501203359844436,
      "loss": 0.6124,
      "step": 57500
    },
    {
      "epoch": 0.18361199215823784,
      "grad_norm": 4.905120372772217,
      "learning_rate": 0.0002449164023525286,
      "loss": 0.7202,
      "step": 57600
    },
    {
      "epoch": 0.183930762977957,
      "grad_norm": 32.2919807434082,
      "learning_rate": 0.0002448207711066129,
      "loss": 0.5988,
      "step": 57700
    },
    {
      "epoch": 0.18424953379767617,
      "grad_norm": 38.76881408691406,
      "learning_rate": 0.00024472513986069713,
      "loss": 0.5175,
      "step": 57800
    },
    {
      "epoch": 0.18456830461739532,
      "grad_norm": 0.002483111573383212,
      "learning_rate": 0.0002446295086147814,
      "loss": 0.5053,
      "step": 57900
    },
    {
      "epoch": 0.1848870754371145,
      "grad_norm": 0.002928593195974827,
      "learning_rate": 0.0002445338773688656,
      "loss": 0.3861,
      "step": 58000
    },
    {
      "epoch": 0.18520584625683364,
      "grad_norm": 0.074173204600811,
      "learning_rate": 0.00024443824612294985,
      "loss": 0.635,
      "step": 58100
    },
    {
      "epoch": 0.18552461707655282,
      "grad_norm": 67.68861389160156,
      "learning_rate": 0.00024434261487703415,
      "loss": 0.6969,
      "step": 58200
    },
    {
      "epoch": 0.18584338789627197,
      "grad_norm": 22.113279342651367,
      "learning_rate": 0.0002442469836311184,
      "loss": 0.525,
      "step": 58300
    },
    {
      "epoch": 0.18616215871599115,
      "grad_norm": 44.190372467041016,
      "learning_rate": 0.00024415135238520263,
      "loss": 0.577,
      "step": 58400
    },
    {
      "epoch": 0.1864809295357103,
      "grad_norm": 41.00539779663086,
      "learning_rate": 0.0002440557211392869,
      "loss": 0.4623,
      "step": 58500
    },
    {
      "epoch": 0.18679970035542948,
      "grad_norm": 0.09366890043020248,
      "learning_rate": 0.00024396008989337114,
      "loss": 0.7543,
      "step": 58600
    },
    {
      "epoch": 0.18711847117514863,
      "grad_norm": 72.41608428955078,
      "learning_rate": 0.00024386445864745538,
      "loss": 0.6596,
      "step": 58700
    },
    {
      "epoch": 0.18743724199486778,
      "grad_norm": 27.48334503173828,
      "learning_rate": 0.00024376882740153964,
      "loss": 0.4094,
      "step": 58800
    },
    {
      "epoch": 0.18775601281458695,
      "grad_norm": 0.1781865954399109,
      "learning_rate": 0.00024367319615562389,
      "loss": 0.6063,
      "step": 58900
    },
    {
      "epoch": 0.1880747836343061,
      "grad_norm": 0.4850441813468933,
      "learning_rate": 0.00024357756490970815,
      "loss": 0.5906,
      "step": 59000
    },
    {
      "epoch": 0.18839355445402528,
      "grad_norm": 0.15701846778392792,
      "learning_rate": 0.0002434819336637924,
      "loss": 0.5154,
      "step": 59100
    },
    {
      "epoch": 0.18871232527374443,
      "grad_norm": 0.2678128182888031,
      "learning_rate": 0.00024338630241787663,
      "loss": 0.7808,
      "step": 59200
    },
    {
      "epoch": 0.1890310960934636,
      "grad_norm": 0.38213279843330383,
      "learning_rate": 0.0002432906711719609,
      "loss": 0.8486,
      "step": 59300
    },
    {
      "epoch": 0.18934986691318276,
      "grad_norm": 40.60723876953125,
      "learning_rate": 0.00024319503992604514,
      "loss": 0.8342,
      "step": 59400
    },
    {
      "epoch": 0.18966863773290193,
      "grad_norm": 0.020575199276208878,
      "learning_rate": 0.0002430994086801294,
      "loss": 0.6336,
      "step": 59500
    },
    {
      "epoch": 0.18998740855262108,
      "grad_norm": 65.83396911621094,
      "learning_rate": 0.00024300377743421365,
      "loss": 0.7411,
      "step": 59600
    },
    {
      "epoch": 0.19030617937234026,
      "grad_norm": 9.192880630493164,
      "learning_rate": 0.0002429081461882979,
      "loss": 0.9063,
      "step": 59700
    },
    {
      "epoch": 0.1906249501920594,
      "grad_norm": 41.59715270996094,
      "learning_rate": 0.00024281251494238216,
      "loss": 0.4827,
      "step": 59800
    },
    {
      "epoch": 0.1909437210117786,
      "grad_norm": 1.2262156009674072,
      "learning_rate": 0.0002427168836964664,
      "loss": 0.5363,
      "step": 59900
    },
    {
      "epoch": 0.19126249183149774,
      "grad_norm": 0.0018620212795212865,
      "learning_rate": 0.00024262125245055064,
      "loss": 0.7244,
      "step": 60000
    },
    {
      "epoch": 0.19158126265121692,
      "grad_norm": 82.51451873779297,
      "learning_rate": 0.0002425256212046349,
      "loss": 0.6047,
      "step": 60100
    },
    {
      "epoch": 0.19190003347093607,
      "grad_norm": 0.3876490592956543,
      "learning_rate": 0.00024242998995871915,
      "loss": 0.6568,
      "step": 60200
    },
    {
      "epoch": 0.19221880429065524,
      "grad_norm": 0.4195976257324219,
      "learning_rate": 0.0002423343587128034,
      "loss": 0.5221,
      "step": 60300
    },
    {
      "epoch": 0.1925375751103744,
      "grad_norm": 0.39122286438941956,
      "learning_rate": 0.00024223872746688765,
      "loss": 0.7032,
      "step": 60400
    },
    {
      "epoch": 0.19285634593009357,
      "grad_norm": 0.0013415259309113026,
      "learning_rate": 0.0002421430962209719,
      "loss": 0.4367,
      "step": 60500
    },
    {
      "epoch": 0.19317511674981272,
      "grad_norm": 3.4190406799316406,
      "learning_rate": 0.00024204746497505616,
      "loss": 0.4734,
      "step": 60600
    },
    {
      "epoch": 0.1934938875695319,
      "grad_norm": 0.014587722718715668,
      "learning_rate": 0.0002419518337291404,
      "loss": 0.3766,
      "step": 60700
    },
    {
      "epoch": 0.19381265838925105,
      "grad_norm": 14.594953536987305,
      "learning_rate": 0.00024185620248322464,
      "loss": 0.6096,
      "step": 60800
    },
    {
      "epoch": 0.19413142920897022,
      "grad_norm": 0.0023153263609856367,
      "learning_rate": 0.0002417605712373089,
      "loss": 0.4998,
      "step": 60900
    },
    {
      "epoch": 0.19445020002868937,
      "grad_norm": 18.8299560546875,
      "learning_rate": 0.00024166493999139315,
      "loss": 0.5693,
      "step": 61000
    },
    {
      "epoch": 0.19476897084840852,
      "grad_norm": 22.646556854248047,
      "learning_rate": 0.00024156930874547742,
      "loss": 0.5451,
      "step": 61100
    },
    {
      "epoch": 0.1950877416681277,
      "grad_norm": 0.08144661039113998,
      "learning_rate": 0.00024147367749956166,
      "loss": 0.5575,
      "step": 61200
    },
    {
      "epoch": 0.19540651248784685,
      "grad_norm": 0.16586938500404358,
      "learning_rate": 0.0002413780462536459,
      "loss": 0.6338,
      "step": 61300
    },
    {
      "epoch": 0.19572528330756603,
      "grad_norm": 0.003992709796875715,
      "learning_rate": 0.0002412824150077302,
      "loss": 0.6192,
      "step": 61400
    },
    {
      "epoch": 0.19604405412728518,
      "grad_norm": 0.0008055915241129696,
      "learning_rate": 0.0002411867837618144,
      "loss": 0.6233,
      "step": 61500
    },
    {
      "epoch": 0.19636282494700436,
      "grad_norm": 0.00848423782736063,
      "learning_rate": 0.0002410911525158987,
      "loss": 0.5652,
      "step": 61600
    },
    {
      "epoch": 0.1966815957667235,
      "grad_norm": 0.005454246420413256,
      "learning_rate": 0.00024099552126998294,
      "loss": 0.5471,
      "step": 61700
    },
    {
      "epoch": 0.19700036658644268,
      "grad_norm": 28.56107521057129,
      "learning_rate": 0.00024089989002406715,
      "loss": 0.7913,
      "step": 61800
    },
    {
      "epoch": 0.19731913740616183,
      "grad_norm": 0.18529780209064484,
      "learning_rate": 0.00024080425877815145,
      "loss": 0.3256,
      "step": 61900
    },
    {
      "epoch": 0.197637908225881,
      "grad_norm": 0.0012195780873298645,
      "learning_rate": 0.0002407086275322357,
      "loss": 0.5067,
      "step": 62000
    },
    {
      "epoch": 0.19795667904560016,
      "grad_norm": 0.004096740856766701,
      "learning_rate": 0.0002406129962863199,
      "loss": 0.4567,
      "step": 62100
    },
    {
      "epoch": 0.19827544986531934,
      "grad_norm": 0.11704526096582413,
      "learning_rate": 0.0002405173650404042,
      "loss": 0.6591,
      "step": 62200
    },
    {
      "epoch": 0.1985942206850385,
      "grad_norm": 0.05319301038980484,
      "learning_rate": 0.00024042173379448844,
      "loss": 0.4232,
      "step": 62300
    },
    {
      "epoch": 0.19891299150475766,
      "grad_norm": 0.40833139419555664,
      "learning_rate": 0.0002403261025485727,
      "loss": 0.4504,
      "step": 62400
    },
    {
      "epoch": 0.1992317623244768,
      "grad_norm": 38.73416519165039,
      "learning_rate": 0.00024023047130265694,
      "loss": 0.4835,
      "step": 62500
    },
    {
      "epoch": 0.199550533144196,
      "grad_norm": 210.8044891357422,
      "learning_rate": 0.00024013484005674118,
      "loss": 0.6828,
      "step": 62600
    },
    {
      "epoch": 0.19986930396391514,
      "grad_norm": 0.014248753897845745,
      "learning_rate": 0.00024003920881082545,
      "loss": 0.3717,
      "step": 62700
    },
    {
      "epoch": 0.20018807478363432,
      "grad_norm": 0.010257778689265251,
      "learning_rate": 0.0002399435775649097,
      "loss": 0.564,
      "step": 62800
    },
    {
      "epoch": 0.20050684560335347,
      "grad_norm": 0.5753825902938843,
      "learning_rate": 0.00023984794631899393,
      "loss": 0.6758,
      "step": 62900
    },
    {
      "epoch": 0.20082561642307264,
      "grad_norm": 0.002643026178702712,
      "learning_rate": 0.0002397523150730782,
      "loss": 0.7518,
      "step": 63000
    },
    {
      "epoch": 0.2011443872427918,
      "grad_norm": 0.0016500396886840463,
      "learning_rate": 0.00023965668382716244,
      "loss": 0.6492,
      "step": 63100
    },
    {
      "epoch": 0.20146315806251094,
      "grad_norm": 7.067897796630859,
      "learning_rate": 0.0002395610525812467,
      "loss": 0.6604,
      "step": 63200
    },
    {
      "epoch": 0.20178192888223012,
      "grad_norm": 0.6466590166091919,
      "learning_rate": 0.00023946542133533095,
      "loss": 0.4836,
      "step": 63300
    },
    {
      "epoch": 0.20210069970194927,
      "grad_norm": 0.004998276475816965,
      "learning_rate": 0.0002393697900894152,
      "loss": 0.4367,
      "step": 63400
    },
    {
      "epoch": 0.20241947052166845,
      "grad_norm": 0.021371612325310707,
      "learning_rate": 0.00023927415884349946,
      "loss": 0.7369,
      "step": 63500
    },
    {
      "epoch": 0.2027382413413876,
      "grad_norm": 16.531980514526367,
      "learning_rate": 0.0002391785275975837,
      "loss": 0.8315,
      "step": 63600
    },
    {
      "epoch": 0.20305701216110678,
      "grad_norm": 23.915542602539062,
      "learning_rate": 0.00023908289635166796,
      "loss": 0.7485,
      "step": 63700
    },
    {
      "epoch": 0.20337578298082593,
      "grad_norm": 7.814794540405273,
      "learning_rate": 0.0002389872651057522,
      "loss": 0.6046,
      "step": 63800
    },
    {
      "epoch": 0.2036945538005451,
      "grad_norm": 0.3820965886116028,
      "learning_rate": 0.00023889163385983645,
      "loss": 0.8713,
      "step": 63900
    },
    {
      "epoch": 0.20401332462026425,
      "grad_norm": 0.00026122567942366004,
      "learning_rate": 0.0002387960026139207,
      "loss": 0.4252,
      "step": 64000
    },
    {
      "epoch": 0.20433209543998343,
      "grad_norm": 0.016640322282910347,
      "learning_rate": 0.00023870037136800495,
      "loss": 0.6025,
      "step": 64100
    },
    {
      "epoch": 0.20465086625970258,
      "grad_norm": 0.9184589982032776,
      "learning_rate": 0.0002386047401220892,
      "loss": 0.5951,
      "step": 64200
    },
    {
      "epoch": 0.20496963707942176,
      "grad_norm": 33.04819107055664,
      "learning_rate": 0.00023850910887617346,
      "loss": 0.6044,
      "step": 64300
    },
    {
      "epoch": 0.2052884078991409,
      "grad_norm": 0.16251209378242493,
      "learning_rate": 0.0002384134776302577,
      "loss": 0.4535,
      "step": 64400
    },
    {
      "epoch": 0.20560717871886008,
      "grad_norm": 0.035512570291757584,
      "learning_rate": 0.00023831784638434197,
      "loss": 0.647,
      "step": 64500
    },
    {
      "epoch": 0.20592594953857923,
      "grad_norm": 68.57184600830078,
      "learning_rate": 0.0002382222151384262,
      "loss": 0.5879,
      "step": 64600
    },
    {
      "epoch": 0.2062447203582984,
      "grad_norm": 0.008244924247264862,
      "learning_rate": 0.00023812658389251045,
      "loss": 0.5326,
      "step": 64700
    },
    {
      "epoch": 0.20656349117801756,
      "grad_norm": 0.3287942707538605,
      "learning_rate": 0.00023803095264659472,
      "loss": 0.5706,
      "step": 64800
    },
    {
      "epoch": 0.20688226199773674,
      "grad_norm": 0.010871188715100288,
      "learning_rate": 0.00023793532140067896,
      "loss": 0.3793,
      "step": 64900
    },
    {
      "epoch": 0.2072010328174559,
      "grad_norm": 0.2691264748573303,
      "learning_rate": 0.0002378396901547632,
      "loss": 0.6688,
      "step": 65000
    },
    {
      "epoch": 0.20751980363717507,
      "grad_norm": 0.04102107882499695,
      "learning_rate": 0.00023774405890884746,
      "loss": 0.7439,
      "step": 65100
    },
    {
      "epoch": 0.20783857445689422,
      "grad_norm": 35.52317810058594,
      "learning_rate": 0.0002376484276629317,
      "loss": 0.6409,
      "step": 65200
    },
    {
      "epoch": 0.2081573452766134,
      "grad_norm": 4.027495861053467,
      "learning_rate": 0.00023755279641701597,
      "loss": 0.7384,
      "step": 65300
    },
    {
      "epoch": 0.20847611609633254,
      "grad_norm": 1.2732166051864624,
      "learning_rate": 0.0002374571651711002,
      "loss": 0.7467,
      "step": 65400
    },
    {
      "epoch": 0.2087948869160517,
      "grad_norm": 8.97910213470459,
      "learning_rate": 0.00023736153392518445,
      "loss": 0.584,
      "step": 65500
    },
    {
      "epoch": 0.20911365773577087,
      "grad_norm": 0.03254865109920502,
      "learning_rate": 0.00023726590267926872,
      "loss": 0.4985,
      "step": 65600
    },
    {
      "epoch": 0.20943242855549002,
      "grad_norm": 27.567180633544922,
      "learning_rate": 0.00023717027143335296,
      "loss": 0.4025,
      "step": 65700
    },
    {
      "epoch": 0.2097511993752092,
      "grad_norm": 0.005667430814355612,
      "learning_rate": 0.00023707464018743723,
      "loss": 0.5408,
      "step": 65800
    },
    {
      "epoch": 0.21006997019492835,
      "grad_norm": 0.030955487862229347,
      "learning_rate": 0.00023697900894152147,
      "loss": 0.7704,
      "step": 65900
    },
    {
      "epoch": 0.21038874101464752,
      "grad_norm": 60.09634017944336,
      "learning_rate": 0.0002368833776956057,
      "loss": 0.6379,
      "step": 66000
    },
    {
      "epoch": 0.21070751183436667,
      "grad_norm": 0.010104003362357616,
      "learning_rate": 0.00023678774644968998,
      "loss": 0.3843,
      "step": 66100
    },
    {
      "epoch": 0.21102628265408585,
      "grad_norm": 0.0018590011168271303,
      "learning_rate": 0.00023669211520377422,
      "loss": 0.5961,
      "step": 66200
    },
    {
      "epoch": 0.211345053473805,
      "grad_norm": 0.007653535809367895,
      "learning_rate": 0.00023659648395785846,
      "loss": 0.4993,
      "step": 66300
    },
    {
      "epoch": 0.21166382429352418,
      "grad_norm": 50.666866302490234,
      "learning_rate": 0.00023650085271194272,
      "loss": 0.4766,
      "step": 66400
    },
    {
      "epoch": 0.21198259511324333,
      "grad_norm": 0.0012216352624818683,
      "learning_rate": 0.00023640522146602697,
      "loss": 0.5305,
      "step": 66500
    },
    {
      "epoch": 0.2123013659329625,
      "grad_norm": 0.12223227322101593,
      "learning_rate": 0.00023630959022011123,
      "loss": 0.6303,
      "step": 66600
    },
    {
      "epoch": 0.21262013675268165,
      "grad_norm": 83.91912078857422,
      "learning_rate": 0.00023621395897419547,
      "loss": 0.5096,
      "step": 66700
    },
    {
      "epoch": 0.21293890757240083,
      "grad_norm": 2.9527950286865234,
      "learning_rate": 0.0002361183277282797,
      "loss": 0.5152,
      "step": 66800
    },
    {
      "epoch": 0.21325767839211998,
      "grad_norm": 0.05853741988539696,
      "learning_rate": 0.00023602269648236398,
      "loss": 0.2912,
      "step": 66900
    },
    {
      "epoch": 0.21357644921183916,
      "grad_norm": 0.006041462067514658,
      "learning_rate": 0.00023592706523644822,
      "loss": 0.4801,
      "step": 67000
    },
    {
      "epoch": 0.2138952200315583,
      "grad_norm": 0.0017056702636182308,
      "learning_rate": 0.00023583143399053246,
      "loss": 0.5829,
      "step": 67100
    },
    {
      "epoch": 0.21421399085127749,
      "grad_norm": 0.002953555900603533,
      "learning_rate": 0.00023573580274461673,
      "loss": 0.5568,
      "step": 67200
    },
    {
      "epoch": 0.21453276167099664,
      "grad_norm": 0.01553148590028286,
      "learning_rate": 0.00023564017149870097,
      "loss": 0.6225,
      "step": 67300
    },
    {
      "epoch": 0.2148515324907158,
      "grad_norm": 0.005028490908443928,
      "learning_rate": 0.00023554454025278526,
      "loss": 0.5376,
      "step": 67400
    },
    {
      "epoch": 0.21517030331043496,
      "grad_norm": 17.710132598876953,
      "learning_rate": 0.00023544890900686948,
      "loss": 0.6016,
      "step": 67500
    },
    {
      "epoch": 0.2154890741301541,
      "grad_norm": 0.0031125762034207582,
      "learning_rate": 0.00023535327776095372,
      "loss": 0.5187,
      "step": 67600
    },
    {
      "epoch": 0.2158078449498733,
      "grad_norm": 21.024274826049805,
      "learning_rate": 0.000235257646515038,
      "loss": 0.4968,
      "step": 67700
    },
    {
      "epoch": 0.21612661576959244,
      "grad_norm": 11.371508598327637,
      "learning_rate": 0.00023516201526912225,
      "loss": 0.5139,
      "step": 67800
    },
    {
      "epoch": 0.21644538658931162,
      "grad_norm": 4.09591817855835,
      "learning_rate": 0.00023506638402320652,
      "loss": 0.7123,
      "step": 67900
    },
    {
      "epoch": 0.21676415740903077,
      "grad_norm": 12.666179656982422,
      "learning_rate": 0.00023497075277729076,
      "loss": 0.7465,
      "step": 68000
    },
    {
      "epoch": 0.21708292822874994,
      "grad_norm": 0.025894880294799805,
      "learning_rate": 0.000234875121531375,
      "loss": 0.4503,
      "step": 68100
    },
    {
      "epoch": 0.2174016990484691,
      "grad_norm": 0.7214265465736389,
      "learning_rate": 0.00023477949028545927,
      "loss": 0.6168,
      "step": 68200
    },
    {
      "epoch": 0.21772046986818827,
      "grad_norm": 0.09890671819448471,
      "learning_rate": 0.0002346838590395435,
      "loss": 0.5193,
      "step": 68300
    },
    {
      "epoch": 0.21803924068790742,
      "grad_norm": 0.11589627712965012,
      "learning_rate": 0.00023458822779362775,
      "loss": 0.5554,
      "step": 68400
    },
    {
      "epoch": 0.2183580115076266,
      "grad_norm": 23.4317626953125,
      "learning_rate": 0.00023449259654771202,
      "loss": 0.5345,
      "step": 68500
    },
    {
      "epoch": 0.21867678232734575,
      "grad_norm": 0.05166337639093399,
      "learning_rate": 0.00023439696530179626,
      "loss": 0.8936,
      "step": 68600
    },
    {
      "epoch": 0.21899555314706493,
      "grad_norm": 22.11107635498047,
      "learning_rate": 0.00023430133405588052,
      "loss": 0.604,
      "step": 68700
    },
    {
      "epoch": 0.21931432396678407,
      "grad_norm": 0.31958526372909546,
      "learning_rate": 0.00023420570280996476,
      "loss": 0.5236,
      "step": 68800
    },
    {
      "epoch": 0.21963309478650325,
      "grad_norm": 0.008778015151619911,
      "learning_rate": 0.000234110071564049,
      "loss": 0.4635,
      "step": 68900
    },
    {
      "epoch": 0.2199518656062224,
      "grad_norm": 6.0151448249816895,
      "learning_rate": 0.00023401444031813327,
      "loss": 0.4483,
      "step": 69000
    },
    {
      "epoch": 0.22027063642594158,
      "grad_norm": 0.004439841024577618,
      "learning_rate": 0.0002339188090722175,
      "loss": 0.5268,
      "step": 69100
    },
    {
      "epoch": 0.22058940724566073,
      "grad_norm": 0.07787404954433441,
      "learning_rate": 0.00023382317782630175,
      "loss": 0.5028,
      "step": 69200
    },
    {
      "epoch": 0.2209081780653799,
      "grad_norm": 5.004149436950684,
      "learning_rate": 0.00023372754658038602,
      "loss": 0.7031,
      "step": 69300
    },
    {
      "epoch": 0.22122694888509906,
      "grad_norm": 0.0010071953292936087,
      "learning_rate": 0.00023363191533447026,
      "loss": 0.5224,
      "step": 69400
    },
    {
      "epoch": 0.22154571970481823,
      "grad_norm": 0.001957827480509877,
      "learning_rate": 0.00023353628408855453,
      "loss": 0.3323,
      "step": 69500
    },
    {
      "epoch": 0.22186449052453738,
      "grad_norm": 0.0006869052303954959,
      "learning_rate": 0.00023344065284263877,
      "loss": 0.5844,
      "step": 69600
    },
    {
      "epoch": 0.22218326134425653,
      "grad_norm": 0.004251804202795029,
      "learning_rate": 0.000233345021596723,
      "loss": 0.8167,
      "step": 69700
    },
    {
      "epoch": 0.2225020321639757,
      "grad_norm": 87.25788116455078,
      "learning_rate": 0.00023324939035080728,
      "loss": 0.7583,
      "step": 69800
    },
    {
      "epoch": 0.22282080298369486,
      "grad_norm": 0.11104721575975418,
      "learning_rate": 0.00023315375910489152,
      "loss": 0.6932,
      "step": 69900
    },
    {
      "epoch": 0.22313957380341404,
      "grad_norm": 0.013207387179136276,
      "learning_rate": 0.00023305812785897578,
      "loss": 0.4731,
      "step": 70000
    },
    {
      "epoch": 0.2234583446231332,
      "grad_norm": 0.0014068522723391652,
      "learning_rate": 0.00023296249661306002,
      "loss": 0.5204,
      "step": 70100
    },
    {
      "epoch": 0.22377711544285236,
      "grad_norm": 82.89295196533203,
      "learning_rate": 0.00023286686536714427,
      "loss": 0.3734,
      "step": 70200
    },
    {
      "epoch": 0.22409588626257151,
      "grad_norm": 0.2696760296821594,
      "learning_rate": 0.00023277123412122853,
      "loss": 0.4776,
      "step": 70300
    },
    {
      "epoch": 0.2244146570822907,
      "grad_norm": 0.03330584987998009,
      "learning_rate": 0.00023267560287531277,
      "loss": 0.5345,
      "step": 70400
    },
    {
      "epoch": 0.22473342790200984,
      "grad_norm": 0.007584406062960625,
      "learning_rate": 0.000232579971629397,
      "loss": 0.2768,
      "step": 70500
    },
    {
      "epoch": 0.22505219872172902,
      "grad_norm": 0.06474794447422028,
      "learning_rate": 0.00023248434038348128,
      "loss": 0.5029,
      "step": 70600
    },
    {
      "epoch": 0.22537096954144817,
      "grad_norm": 0.001857162220403552,
      "learning_rate": 0.00023238870913756552,
      "loss": 0.5303,
      "step": 70700
    },
    {
      "epoch": 0.22568974036116735,
      "grad_norm": 0.0008399668731726706,
      "learning_rate": 0.0002322930778916498,
      "loss": 0.7222,
      "step": 70800
    },
    {
      "epoch": 0.2260085111808865,
      "grad_norm": 0.001877699396573007,
      "learning_rate": 0.00023219744664573403,
      "loss": 0.3796,
      "step": 70900
    },
    {
      "epoch": 0.22632728200060567,
      "grad_norm": 45.83506774902344,
      "learning_rate": 0.00023210181539981827,
      "loss": 0.6,
      "step": 71000
    },
    {
      "epoch": 0.22664605282032482,
      "grad_norm": 0.009147732518613338,
      "learning_rate": 0.00023200618415390254,
      "loss": 0.4876,
      "step": 71100
    },
    {
      "epoch": 0.226964823640044,
      "grad_norm": 0.011171001009643078,
      "learning_rate": 0.00023191055290798678,
      "loss": 0.6414,
      "step": 71200
    },
    {
      "epoch": 0.22728359445976315,
      "grad_norm": 0.0036877854727208614,
      "learning_rate": 0.00023181492166207102,
      "loss": 0.6184,
      "step": 71300
    },
    {
      "epoch": 0.22760236527948233,
      "grad_norm": 8.706734657287598,
      "learning_rate": 0.00023171929041615528,
      "loss": 0.5852,
      "step": 71400
    },
    {
      "epoch": 0.22792113609920148,
      "grad_norm": 0.000478185509564355,
      "learning_rate": 0.00023162365917023953,
      "loss": 0.572,
      "step": 71500
    },
    {
      "epoch": 0.22823990691892065,
      "grad_norm": 0.0025199383962899446,
      "learning_rate": 0.0002315280279243238,
      "loss": 0.5846,
      "step": 71600
    },
    {
      "epoch": 0.2285586777386398,
      "grad_norm": 41.98094940185547,
      "learning_rate": 0.00023143239667840803,
      "loss": 0.5195,
      "step": 71700
    },
    {
      "epoch": 0.22887744855835898,
      "grad_norm": 0.0037532709538936615,
      "learning_rate": 0.00023133676543249227,
      "loss": 0.7812,
      "step": 71800
    },
    {
      "epoch": 0.22919621937807813,
      "grad_norm": 0.026707759127020836,
      "learning_rate": 0.00023124113418657654,
      "loss": 0.7006,
      "step": 71900
    },
    {
      "epoch": 0.22951499019779728,
      "grad_norm": 0.3360534608364105,
      "learning_rate": 0.00023114550294066078,
      "loss": 0.5835,
      "step": 72000
    },
    {
      "epoch": 0.22983376101751646,
      "grad_norm": 72.78900146484375,
      "learning_rate": 0.00023104987169474505,
      "loss": 0.4347,
      "step": 72100
    },
    {
      "epoch": 0.2301525318372356,
      "grad_norm": 27.682823181152344,
      "learning_rate": 0.0002309542404488293,
      "loss": 0.4566,
      "step": 72200
    },
    {
      "epoch": 0.23047130265695479,
      "grad_norm": 0.45854106545448303,
      "learning_rate": 0.00023085860920291353,
      "loss": 0.8853,
      "step": 72300
    },
    {
      "epoch": 0.23079007347667393,
      "grad_norm": 29.30876922607422,
      "learning_rate": 0.0002307629779569978,
      "loss": 0.5892,
      "step": 72400
    },
    {
      "epoch": 0.2311088442963931,
      "grad_norm": 0.0034982007928192616,
      "learning_rate": 0.00023066734671108204,
      "loss": 0.5774,
      "step": 72500
    },
    {
      "epoch": 0.23142761511611226,
      "grad_norm": 0.0022498408798128366,
      "learning_rate": 0.00023057171546516628,
      "loss": 0.3842,
      "step": 72600
    },
    {
      "epoch": 0.23174638593583144,
      "grad_norm": 33.60634231567383,
      "learning_rate": 0.00023047608421925054,
      "loss": 0.7015,
      "step": 72700
    },
    {
      "epoch": 0.2320651567555506,
      "grad_norm": 0.001705468399450183,
      "learning_rate": 0.00023038045297333479,
      "loss": 0.603,
      "step": 72800
    },
    {
      "epoch": 0.23238392757526977,
      "grad_norm": 0.007901182398200035,
      "learning_rate": 0.00023028482172741908,
      "loss": 0.4737,
      "step": 72900
    },
    {
      "epoch": 0.23270269839498892,
      "grad_norm": 0.0005276307347230613,
      "learning_rate": 0.0002301891904815033,
      "loss": 0.4325,
      "step": 73000
    },
    {
      "epoch": 0.2330214692147081,
      "grad_norm": 0.0013616522774100304,
      "learning_rate": 0.00023009355923558753,
      "loss": 0.6161,
      "step": 73100
    },
    {
      "epoch": 0.23334024003442724,
      "grad_norm": 69.61189270019531,
      "learning_rate": 0.00022999792798967183,
      "loss": 0.6278,
      "step": 73200
    },
    {
      "epoch": 0.23365901085414642,
      "grad_norm": 56.62105941772461,
      "learning_rate": 0.00022990229674375604,
      "loss": 0.9484,
      "step": 73300
    },
    {
      "epoch": 0.23397778167386557,
      "grad_norm": 31.763690948486328,
      "learning_rate": 0.00022980666549784028,
      "loss": 0.6177,
      "step": 73400
    },
    {
      "epoch": 0.23429655249358475,
      "grad_norm": 0.16083842515945435,
      "learning_rate": 0.00022971103425192458,
      "loss": 0.4415,
      "step": 73500
    },
    {
      "epoch": 0.2346153233133039,
      "grad_norm": 0.00455610454082489,
      "learning_rate": 0.0002296154030060088,
      "loss": 0.5998,
      "step": 73600
    },
    {
      "epoch": 0.23493409413302307,
      "grad_norm": 0.09214897453784943,
      "learning_rate": 0.00022951977176009308,
      "loss": 0.4593,
      "step": 73700
    },
    {
      "epoch": 0.23525286495274222,
      "grad_norm": 91.51705169677734,
      "learning_rate": 0.00022942414051417732,
      "loss": 0.5825,
      "step": 73800
    },
    {
      "epoch": 0.2355716357724614,
      "grad_norm": 0.029255259782075882,
      "learning_rate": 0.00022932850926826154,
      "loss": 0.4827,
      "step": 73900
    },
    {
      "epoch": 0.23589040659218055,
      "grad_norm": 0.45939359068870544,
      "learning_rate": 0.00022923287802234583,
      "loss": 0.7192,
      "step": 74000
    },
    {
      "epoch": 0.2362091774118997,
      "grad_norm": 0.6840077638626099,
      "learning_rate": 0.00022913724677643007,
      "loss": 0.4117,
      "step": 74100
    },
    {
      "epoch": 0.23652794823161888,
      "grad_norm": 58.9881706237793,
      "learning_rate": 0.00022904161553051434,
      "loss": 0.5095,
      "step": 74200
    },
    {
      "epoch": 0.23684671905133803,
      "grad_norm": 0.024844450876116753,
      "learning_rate": 0.00022894598428459858,
      "loss": 0.5379,
      "step": 74300
    },
    {
      "epoch": 0.2371654898710572,
      "grad_norm": 0.020975159481167793,
      "learning_rate": 0.00022885035303868282,
      "loss": 0.5957,
      "step": 74400
    },
    {
      "epoch": 0.23748426069077636,
      "grad_norm": 0.0017722679767757654,
      "learning_rate": 0.0002287547217927671,
      "loss": 0.6274,
      "step": 74500
    },
    {
      "epoch": 0.23780303151049553,
      "grad_norm": 31.638856887817383,
      "learning_rate": 0.00022865909054685133,
      "loss": 0.6902,
      "step": 74600
    },
    {
      "epoch": 0.23812180233021468,
      "grad_norm": 0.0741388276219368,
      "learning_rate": 0.00022856345930093557,
      "loss": 0.5013,
      "step": 74700
    },
    {
      "epoch": 0.23844057314993386,
      "grad_norm": 0.0013028585817664862,
      "learning_rate": 0.00022846782805501984,
      "loss": 0.4238,
      "step": 74800
    },
    {
      "epoch": 0.238759343969653,
      "grad_norm": 67.36315155029297,
      "learning_rate": 0.00022837219680910408,
      "loss": 0.5808,
      "step": 74900
    },
    {
      "epoch": 0.2390781147893722,
      "grad_norm": 70.83342742919922,
      "learning_rate": 0.00022827656556318834,
      "loss": 0.6766,
      "step": 75000
    },
    {
      "epoch": 0.23939688560909134,
      "grad_norm": 0.9774414300918579,
      "learning_rate": 0.00022818093431727258,
      "loss": 0.734,
      "step": 75100
    },
    {
      "epoch": 0.23971565642881051,
      "grad_norm": 0.06952499598264694,
      "learning_rate": 0.00022808530307135682,
      "loss": 0.5168,
      "step": 75200
    },
    {
      "epoch": 0.24003442724852966,
      "grad_norm": 0.16226455569267273,
      "learning_rate": 0.0002279896718254411,
      "loss": 0.6449,
      "step": 75300
    },
    {
      "epoch": 0.24035319806824884,
      "grad_norm": 0.032566990703344345,
      "learning_rate": 0.00022789404057952533,
      "loss": 0.5093,
      "step": 75400
    },
    {
      "epoch": 0.240671968887968,
      "grad_norm": 56.667137145996094,
      "learning_rate": 0.00022779840933360957,
      "loss": 0.8795,
      "step": 75500
    },
    {
      "epoch": 0.24099073970768717,
      "grad_norm": 0.05605805665254593,
      "learning_rate": 0.00022770277808769384,
      "loss": 0.4406,
      "step": 75600
    },
    {
      "epoch": 0.24130951052740632,
      "grad_norm": 0.05325894430279732,
      "learning_rate": 0.00022760714684177808,
      "loss": 0.4377,
      "step": 75700
    },
    {
      "epoch": 0.2416282813471255,
      "grad_norm": 54.62519073486328,
      "learning_rate": 0.00022751151559586235,
      "loss": 0.4658,
      "step": 75800
    },
    {
      "epoch": 0.24194705216684464,
      "grad_norm": 32.59591293334961,
      "learning_rate": 0.0002274158843499466,
      "loss": 0.5826,
      "step": 75900
    },
    {
      "epoch": 0.24226582298656382,
      "grad_norm": 0.2096114456653595,
      "learning_rate": 0.00022732025310403083,
      "loss": 0.47,
      "step": 76000
    },
    {
      "epoch": 0.24258459380628297,
      "grad_norm": 0.040231816470623016,
      "learning_rate": 0.0002272246218581151,
      "loss": 0.5075,
      "step": 76100
    },
    {
      "epoch": 0.24290336462600215,
      "grad_norm": 9.484373092651367,
      "learning_rate": 0.00022712899061219934,
      "loss": 0.5837,
      "step": 76200
    },
    {
      "epoch": 0.2432221354457213,
      "grad_norm": 0.00017438680515624583,
      "learning_rate": 0.0002270333593662836,
      "loss": 0.4547,
      "step": 76300
    },
    {
      "epoch": 0.24354090626544045,
      "grad_norm": 52.11327362060547,
      "learning_rate": 0.00022693772812036784,
      "loss": 0.632,
      "step": 76400
    },
    {
      "epoch": 0.24385967708515963,
      "grad_norm": 0.0006946583744138479,
      "learning_rate": 0.00022684209687445208,
      "loss": 0.4507,
      "step": 76500
    },
    {
      "epoch": 0.24417844790487878,
      "grad_norm": 0.13719317317008972,
      "learning_rate": 0.00022674646562853635,
      "loss": 0.592,
      "step": 76600
    },
    {
      "epoch": 0.24449721872459795,
      "grad_norm": 0.5084612369537354,
      "learning_rate": 0.0002266508343826206,
      "loss": 0.6936,
      "step": 76700
    },
    {
      "epoch": 0.2448159895443171,
      "grad_norm": 53.181514739990234,
      "learning_rate": 0.00022655520313670483,
      "loss": 0.5652,
      "step": 76800
    },
    {
      "epoch": 0.24513476036403628,
      "grad_norm": 16.23546028137207,
      "learning_rate": 0.0002264595718907891,
      "loss": 0.4438,
      "step": 76900
    },
    {
      "epoch": 0.24545353118375543,
      "grad_norm": 0.020776966586709023,
      "learning_rate": 0.00022636394064487334,
      "loss": 0.6101,
      "step": 77000
    },
    {
      "epoch": 0.2457723020034746,
      "grad_norm": 49.56684875488281,
      "learning_rate": 0.0002262683093989576,
      "loss": 0.5915,
      "step": 77100
    },
    {
      "epoch": 0.24609107282319376,
      "grad_norm": 0.03878898173570633,
      "learning_rate": 0.00022617267815304185,
      "loss": 0.4026,
      "step": 77200
    },
    {
      "epoch": 0.24640984364291293,
      "grad_norm": 0.09374586492776871,
      "learning_rate": 0.0002260770469071261,
      "loss": 0.4754,
      "step": 77300
    },
    {
      "epoch": 0.24672861446263208,
      "grad_norm": 0.019087230786681175,
      "learning_rate": 0.00022598141566121036,
      "loss": 0.3108,
      "step": 77400
    },
    {
      "epoch": 0.24704738528235126,
      "grad_norm": 0.014094062149524689,
      "learning_rate": 0.0002258857844152946,
      "loss": 0.3687,
      "step": 77500
    },
    {
      "epoch": 0.2473661561020704,
      "grad_norm": 0.002702129539102316,
      "learning_rate": 0.00022579015316937884,
      "loss": 0.5861,
      "step": 77600
    },
    {
      "epoch": 0.2476849269217896,
      "grad_norm": 0.008339125663042068,
      "learning_rate": 0.0002256945219234631,
      "loss": 0.2937,
      "step": 77700
    },
    {
      "epoch": 0.24800369774150874,
      "grad_norm": 0.02664070576429367,
      "learning_rate": 0.00022559889067754735,
      "loss": 0.6817,
      "step": 77800
    },
    {
      "epoch": 0.24832246856122792,
      "grad_norm": 70.38018035888672,
      "learning_rate": 0.0002255032594316316,
      "loss": 0.3465,
      "step": 77900
    },
    {
      "epoch": 0.24864123938094707,
      "grad_norm": 46.57314682006836,
      "learning_rate": 0.00022540762818571585,
      "loss": 0.6122,
      "step": 78000
    },
    {
      "epoch": 0.24896001020066624,
      "grad_norm": 45.0856819152832,
      "learning_rate": 0.0002253119969398001,
      "loss": 0.6928,
      "step": 78100
    },
    {
      "epoch": 0.2492787810203854,
      "grad_norm": 52.1390266418457,
      "learning_rate": 0.00022521636569388436,
      "loss": 0.5536,
      "step": 78200
    },
    {
      "epoch": 0.24959755184010457,
      "grad_norm": 0.010230493731796741,
      "learning_rate": 0.0002251207344479686,
      "loss": 0.3492,
      "step": 78300
    },
    {
      "epoch": 0.24991632265982372,
      "grad_norm": 23.96942901611328,
      "learning_rate": 0.00022502510320205287,
      "loss": 0.5506,
      "step": 78400
    },
    {
      "epoch": 0.25023509347954287,
      "grad_norm": 0.005544096697121859,
      "learning_rate": 0.0002249294719561371,
      "loss": 0.6192,
      "step": 78500
    },
    {
      "epoch": 0.250553864299262,
      "grad_norm": 0.00031998148187994957,
      "learning_rate": 0.00022483384071022135,
      "loss": 0.5593,
      "step": 78600
    },
    {
      "epoch": 0.2508726351189812,
      "grad_norm": 0.4284568727016449,
      "learning_rate": 0.00022473820946430562,
      "loss": 0.566,
      "step": 78700
    },
    {
      "epoch": 0.2511914059387004,
      "grad_norm": 0.00855699647217989,
      "learning_rate": 0.00022464257821838986,
      "loss": 0.4881,
      "step": 78800
    },
    {
      "epoch": 0.2515101767584195,
      "grad_norm": 0.017728818580508232,
      "learning_rate": 0.0002245469469724741,
      "loss": 0.339,
      "step": 78900
    },
    {
      "epoch": 0.2518289475781387,
      "grad_norm": 77.61815643310547,
      "learning_rate": 0.00022445131572655836,
      "loss": 0.6074,
      "step": 79000
    },
    {
      "epoch": 0.2521477183978579,
      "grad_norm": 0.0033797400537878275,
      "learning_rate": 0.0002243556844806426,
      "loss": 0.7331,
      "step": 79100
    },
    {
      "epoch": 0.25246648921757703,
      "grad_norm": 0.9705147743225098,
      "learning_rate": 0.0002242600532347269,
      "loss": 0.4674,
      "step": 79200
    },
    {
      "epoch": 0.2527852600372962,
      "grad_norm": 0.0030183461494743824,
      "learning_rate": 0.00022416442198881114,
      "loss": 0.6795,
      "step": 79300
    },
    {
      "epoch": 0.2531040308570153,
      "grad_norm": 0.015124903991818428,
      "learning_rate": 0.00022406879074289535,
      "loss": 0.7061,
      "step": 79400
    },
    {
      "epoch": 0.25342280167673453,
      "grad_norm": 0.008188423700630665,
      "learning_rate": 0.00022397315949697965,
      "loss": 0.4969,
      "step": 79500
    },
    {
      "epoch": 0.2537415724964537,
      "grad_norm": 0.012971232645213604,
      "learning_rate": 0.0002238775282510639,
      "loss": 0.5857,
      "step": 79600
    },
    {
      "epoch": 0.25406034331617283,
      "grad_norm": 0.3137364685535431,
      "learning_rate": 0.0002237818970051481,
      "loss": 0.4052,
      "step": 79700
    },
    {
      "epoch": 0.254379114135892,
      "grad_norm": 0.05181281641125679,
      "learning_rate": 0.0002236862657592324,
      "loss": 0.5767,
      "step": 79800
    },
    {
      "epoch": 0.2546978849556112,
      "grad_norm": 27.588150024414062,
      "learning_rate": 0.00022359063451331664,
      "loss": 0.4571,
      "step": 79900
    },
    {
      "epoch": 0.25501665577533034,
      "grad_norm": 29.210947036743164,
      "learning_rate": 0.0002234950032674009,
      "loss": 0.582,
      "step": 80000
    },
    {
      "epoch": 0.2553354265950495,
      "grad_norm": 0.004600744228810072,
      "learning_rate": 0.00022339937202148514,
      "loss": 0.7338,
      "step": 80100
    },
    {
      "epoch": 0.25565419741476864,
      "grad_norm": 47.53258514404297,
      "learning_rate": 0.00022330374077556938,
      "loss": 0.6904,
      "step": 80200
    },
    {
      "epoch": 0.25597296823448784,
      "grad_norm": 0.004519636742770672,
      "learning_rate": 0.00022320810952965365,
      "loss": 0.4794,
      "step": 80300
    },
    {
      "epoch": 0.256291739054207,
      "grad_norm": 0.057053036987781525,
      "learning_rate": 0.0002231124782837379,
      "loss": 0.3738,
      "step": 80400
    },
    {
      "epoch": 0.25661050987392614,
      "grad_norm": 0.0003096661239396781,
      "learning_rate": 0.00022301684703782213,
      "loss": 0.6169,
      "step": 80500
    },
    {
      "epoch": 0.2569292806936453,
      "grad_norm": 0.031173333525657654,
      "learning_rate": 0.0002229212157919064,
      "loss": 0.5436,
      "step": 80600
    },
    {
      "epoch": 0.25724805151336444,
      "grad_norm": 12.98164176940918,
      "learning_rate": 0.00022282558454599064,
      "loss": 0.6054,
      "step": 80700
    },
    {
      "epoch": 0.25756682233308364,
      "grad_norm": 56.44243240356445,
      "learning_rate": 0.0002227299533000749,
      "loss": 0.4354,
      "step": 80800
    },
    {
      "epoch": 0.2578855931528028,
      "grad_norm": 20.366846084594727,
      "learning_rate": 0.00022263432205415915,
      "loss": 0.4144,
      "step": 80900
    },
    {
      "epoch": 0.25820436397252194,
      "grad_norm": 0.39741000533103943,
      "learning_rate": 0.0002225386908082434,
      "loss": 0.5094,
      "step": 81000
    },
    {
      "epoch": 0.2585231347922411,
      "grad_norm": 61.24143981933594,
      "learning_rate": 0.00022244305956232766,
      "loss": 0.6617,
      "step": 81100
    },
    {
      "epoch": 0.2588419056119603,
      "grad_norm": 38.73939514160156,
      "learning_rate": 0.0002223474283164119,
      "loss": 0.6087,
      "step": 81200
    },
    {
      "epoch": 0.25916067643167945,
      "grad_norm": 0.18869927525520325,
      "learning_rate": 0.00022225179707049616,
      "loss": 0.455,
      "step": 81300
    },
    {
      "epoch": 0.2594794472513986,
      "grad_norm": 39.0504150390625,
      "learning_rate": 0.0002221561658245804,
      "loss": 0.6082,
      "step": 81400
    },
    {
      "epoch": 0.25979821807111775,
      "grad_norm": 0.16724686324596405,
      "learning_rate": 0.00022206053457866464,
      "loss": 0.2849,
      "step": 81500
    },
    {
      "epoch": 0.26011698889083695,
      "grad_norm": 0.0031531290151178837,
      "learning_rate": 0.0002219649033327489,
      "loss": 0.6345,
      "step": 81600
    },
    {
      "epoch": 0.2604357597105561,
      "grad_norm": 0.8811171054840088,
      "learning_rate": 0.00022186927208683315,
      "loss": 0.4584,
      "step": 81700
    },
    {
      "epoch": 0.26075453053027525,
      "grad_norm": 0.17682519555091858,
      "learning_rate": 0.0002217736408409174,
      "loss": 0.5067,
      "step": 81800
    },
    {
      "epoch": 0.2610733013499944,
      "grad_norm": 0.9066930413246155,
      "learning_rate": 0.00022167800959500166,
      "loss": 0.4348,
      "step": 81900
    },
    {
      "epoch": 0.2613920721697136,
      "grad_norm": 30.11783218383789,
      "learning_rate": 0.0002215823783490859,
      "loss": 0.3235,
      "step": 82000
    },
    {
      "epoch": 0.26171084298943276,
      "grad_norm": 0.027781808748841286,
      "learning_rate": 0.00022148674710317017,
      "loss": 0.3792,
      "step": 82100
    },
    {
      "epoch": 0.2620296138091519,
      "grad_norm": 0.012302093207836151,
      "learning_rate": 0.0002213911158572544,
      "loss": 0.561,
      "step": 82200
    },
    {
      "epoch": 0.26234838462887106,
      "grad_norm": 45.180233001708984,
      "learning_rate": 0.00022129548461133865,
      "loss": 0.4619,
      "step": 82300
    },
    {
      "epoch": 0.26266715544859026,
      "grad_norm": 1.056269645690918,
      "learning_rate": 0.00022119985336542292,
      "loss": 0.4997,
      "step": 82400
    },
    {
      "epoch": 0.2629859262683094,
      "grad_norm": 10.75877571105957,
      "learning_rate": 0.00022110422211950716,
      "loss": 0.5505,
      "step": 82500
    },
    {
      "epoch": 0.26330469708802856,
      "grad_norm": 149.86936950683594,
      "learning_rate": 0.0002210085908735914,
      "loss": 0.683,
      "step": 82600
    },
    {
      "epoch": 0.2636234679077477,
      "grad_norm": 18.52376937866211,
      "learning_rate": 0.00022091295962767566,
      "loss": 0.3829,
      "step": 82700
    },
    {
      "epoch": 0.26394223872746686,
      "grad_norm": 71.57024383544922,
      "learning_rate": 0.0002208173283817599,
      "loss": 0.604,
      "step": 82800
    },
    {
      "epoch": 0.26426100954718607,
      "grad_norm": 85.6969223022461,
      "learning_rate": 0.00022072169713584417,
      "loss": 0.4205,
      "step": 82900
    },
    {
      "epoch": 0.2645797803669052,
      "grad_norm": 0.2611527442932129,
      "learning_rate": 0.0002206260658899284,
      "loss": 0.5312,
      "step": 83000
    },
    {
      "epoch": 0.26489855118662436,
      "grad_norm": 26.128515243530273,
      "learning_rate": 0.00022053043464401265,
      "loss": 0.4612,
      "step": 83100
    },
    {
      "epoch": 0.2652173220063435,
      "grad_norm": 56.2304801940918,
      "learning_rate": 0.00022043480339809692,
      "loss": 0.5014,
      "step": 83200
    },
    {
      "epoch": 0.2655360928260627,
      "grad_norm": 2.4150643348693848,
      "learning_rate": 0.00022033917215218116,
      "loss": 0.2756,
      "step": 83300
    },
    {
      "epoch": 0.26585486364578187,
      "grad_norm": 1.223035216331482,
      "learning_rate": 0.00022024354090626543,
      "loss": 0.4651,
      "step": 83400
    },
    {
      "epoch": 0.266173634465501,
      "grad_norm": 0.14216718077659607,
      "learning_rate": 0.00022014790966034967,
      "loss": 0.4494,
      "step": 83500
    },
    {
      "epoch": 0.26649240528522017,
      "grad_norm": 123.23885345458984,
      "learning_rate": 0.0002200522784144339,
      "loss": 0.4978,
      "step": 83600
    },
    {
      "epoch": 0.2668111761049394,
      "grad_norm": 26.640037536621094,
      "learning_rate": 0.00021995664716851818,
      "loss": 0.5693,
      "step": 83700
    },
    {
      "epoch": 0.2671299469246585,
      "grad_norm": 1.3919543027877808,
      "learning_rate": 0.00021986101592260242,
      "loss": 0.5408,
      "step": 83800
    },
    {
      "epoch": 0.2674487177443777,
      "grad_norm": 0.0017182278679683805,
      "learning_rate": 0.00021976538467668666,
      "loss": 0.5318,
      "step": 83900
    },
    {
      "epoch": 0.2677674885640968,
      "grad_norm": 0.0035756640136241913,
      "learning_rate": 0.00021966975343077092,
      "loss": 0.6693,
      "step": 84000
    },
    {
      "epoch": 0.26808625938381603,
      "grad_norm": 0.005137247499078512,
      "learning_rate": 0.00021957412218485517,
      "loss": 0.6286,
      "step": 84100
    },
    {
      "epoch": 0.2684050302035352,
      "grad_norm": 0.001794166979379952,
      "learning_rate": 0.00021947849093893943,
      "loss": 0.3085,
      "step": 84200
    },
    {
      "epoch": 0.2687238010232543,
      "grad_norm": 32.09030532836914,
      "learning_rate": 0.00021938285969302367,
      "loss": 0.4815,
      "step": 84300
    },
    {
      "epoch": 0.2690425718429735,
      "grad_norm": 0.010809025727212429,
      "learning_rate": 0.0002192872284471079,
      "loss": 0.637,
      "step": 84400
    },
    {
      "epoch": 0.2693613426626927,
      "grad_norm": 0.0008790926658548415,
      "learning_rate": 0.00021919159720119218,
      "loss": 0.2492,
      "step": 84500
    },
    {
      "epoch": 0.26968011348241183,
      "grad_norm": 51.606597900390625,
      "learning_rate": 0.00021909596595527642,
      "loss": 0.4482,
      "step": 84600
    },
    {
      "epoch": 0.269998884302131,
      "grad_norm": 0.0002766130492091179,
      "learning_rate": 0.00021900033470936066,
      "loss": 0.5721,
      "step": 84700
    },
    {
      "epoch": 0.27031765512185013,
      "grad_norm": 14.834982872009277,
      "learning_rate": 0.00021890470346344493,
      "loss": 0.522,
      "step": 84800
    },
    {
      "epoch": 0.27063642594156934,
      "grad_norm": 0.013951716013252735,
      "learning_rate": 0.00021880907221752917,
      "loss": 0.3804,
      "step": 84900
    },
    {
      "epoch": 0.2709551967612885,
      "grad_norm": 0.01656716875731945,
      "learning_rate": 0.00021871344097161346,
      "loss": 0.3855,
      "step": 85000
    },
    {
      "epoch": 0.27127396758100764,
      "grad_norm": 0.017773978412151337,
      "learning_rate": 0.00021861780972569768,
      "loss": 0.3994,
      "step": 85100
    },
    {
      "epoch": 0.2715927384007268,
      "grad_norm": 0.1482248306274414,
      "learning_rate": 0.00021852217847978192,
      "loss": 0.6126,
      "step": 85200
    },
    {
      "epoch": 0.27191150922044593,
      "grad_norm": 0.03025188110768795,
      "learning_rate": 0.0002184265472338662,
      "loss": 0.5075,
      "step": 85300
    },
    {
      "epoch": 0.27223028004016514,
      "grad_norm": 0.00028328955522738397,
      "learning_rate": 0.00021833091598795045,
      "loss": 0.5259,
      "step": 85400
    },
    {
      "epoch": 0.2725490508598843,
      "grad_norm": 0.006493053399026394,
      "learning_rate": 0.00021823528474203472,
      "loss": 0.4327,
      "step": 85500
    },
    {
      "epoch": 0.27286782167960344,
      "grad_norm": 0.0002625489723868668,
      "learning_rate": 0.00021813965349611896,
      "loss": 0.4278,
      "step": 85600
    },
    {
      "epoch": 0.2731865924993226,
      "grad_norm": 0.013984628953039646,
      "learning_rate": 0.0002180440222502032,
      "loss": 0.6244,
      "step": 85700
    },
    {
      "epoch": 0.2735053633190418,
      "grad_norm": 0.02863278053700924,
      "learning_rate": 0.00021794839100428747,
      "loss": 0.5016,
      "step": 85800
    },
    {
      "epoch": 0.27382413413876094,
      "grad_norm": 0.07629380375146866,
      "learning_rate": 0.0002178527597583717,
      "loss": 0.3985,
      "step": 85900
    },
    {
      "epoch": 0.2741429049584801,
      "grad_norm": 23.167421340942383,
      "learning_rate": 0.00021775712851245595,
      "loss": 0.609,
      "step": 86000
    },
    {
      "epoch": 0.27446167577819924,
      "grad_norm": 28.670055389404297,
      "learning_rate": 0.00021766149726654022,
      "loss": 0.6556,
      "step": 86100
    },
    {
      "epoch": 0.27478044659791845,
      "grad_norm": 49.217857360839844,
      "learning_rate": 0.00021756586602062446,
      "loss": 0.8042,
      "step": 86200
    },
    {
      "epoch": 0.2750992174176376,
      "grad_norm": 0.1898309588432312,
      "learning_rate": 0.00021747023477470872,
      "loss": 0.4217,
      "step": 86300
    },
    {
      "epoch": 0.27541798823735675,
      "grad_norm": 0.011169089935719967,
      "learning_rate": 0.00021737460352879296,
      "loss": 0.5902,
      "step": 86400
    },
    {
      "epoch": 0.2757367590570759,
      "grad_norm": 0.004893805366009474,
      "learning_rate": 0.0002172789722828772,
      "loss": 0.4044,
      "step": 86500
    },
    {
      "epoch": 0.2760555298767951,
      "grad_norm": 0.0019440838368609548,
      "learning_rate": 0.00021718334103696147,
      "loss": 0.5058,
      "step": 86600
    },
    {
      "epoch": 0.27637430069651425,
      "grad_norm": 0.0013977495254948735,
      "learning_rate": 0.0002170877097910457,
      "loss": 0.5297,
      "step": 86700
    },
    {
      "epoch": 0.2766930715162334,
      "grad_norm": 0.014427735470235348,
      "learning_rate": 0.00021699207854512995,
      "loss": 0.5133,
      "step": 86800
    },
    {
      "epoch": 0.27701184233595255,
      "grad_norm": 0.0029164429288357496,
      "learning_rate": 0.00021689644729921422,
      "loss": 0.4851,
      "step": 86900
    },
    {
      "epoch": 0.27733061315567176,
      "grad_norm": 0.011521630920469761,
      "learning_rate": 0.00021680081605329846,
      "loss": 0.5647,
      "step": 87000
    },
    {
      "epoch": 0.2776493839753909,
      "grad_norm": 0.0005442491383291781,
      "learning_rate": 0.00021670518480738273,
      "loss": 0.4257,
      "step": 87100
    },
    {
      "epoch": 0.27796815479511006,
      "grad_norm": 0.001272187801077962,
      "learning_rate": 0.00021660955356146697,
      "loss": 0.4619,
      "step": 87200
    },
    {
      "epoch": 0.2782869256148292,
      "grad_norm": 0.005900127813220024,
      "learning_rate": 0.0002165139223155512,
      "loss": 0.3209,
      "step": 87300
    },
    {
      "epoch": 0.27860569643454836,
      "grad_norm": 2.4467267990112305,
      "learning_rate": 0.00021641829106963548,
      "loss": 0.3035,
      "step": 87400
    },
    {
      "epoch": 0.27892446725426756,
      "grad_norm": 0.04925382882356644,
      "learning_rate": 0.00021632265982371972,
      "loss": 0.292,
      "step": 87500
    },
    {
      "epoch": 0.2792432380739867,
      "grad_norm": 0.00012989713286515325,
      "learning_rate": 0.00021622702857780398,
      "loss": 0.5393,
      "step": 87600
    },
    {
      "epoch": 0.27956200889370586,
      "grad_norm": 2.4024598598480225,
      "learning_rate": 0.00021613139733188822,
      "loss": 0.5823,
      "step": 87700
    },
    {
      "epoch": 0.279880779713425,
      "grad_norm": 23.031551361083984,
      "learning_rate": 0.00021603576608597246,
      "loss": 0.4491,
      "step": 87800
    },
    {
      "epoch": 0.2801995505331442,
      "grad_norm": 0.007684530224651098,
      "learning_rate": 0.00021594013484005673,
      "loss": 0.558,
      "step": 87900
    },
    {
      "epoch": 0.28051832135286336,
      "grad_norm": 0.00045097951078787446,
      "learning_rate": 0.00021584450359414097,
      "loss": 0.6812,
      "step": 88000
    },
    {
      "epoch": 0.2808370921725825,
      "grad_norm": 0.00037143679219298065,
      "learning_rate": 0.0002157488723482252,
      "loss": 0.4463,
      "step": 88100
    },
    {
      "epoch": 0.28115586299230166,
      "grad_norm": 0.047758329659700394,
      "learning_rate": 0.00021565324110230948,
      "loss": 0.6115,
      "step": 88200
    },
    {
      "epoch": 0.28147463381202087,
      "grad_norm": 23.85761070251465,
      "learning_rate": 0.00021555760985639372,
      "loss": 0.5509,
      "step": 88300
    },
    {
      "epoch": 0.28179340463174,
      "grad_norm": 0.3541424572467804,
      "learning_rate": 0.000215461978610478,
      "loss": 0.4515,
      "step": 88400
    },
    {
      "epoch": 0.28211217545145917,
      "grad_norm": 0.012652342207729816,
      "learning_rate": 0.00021536634736456223,
      "loss": 0.4224,
      "step": 88500
    },
    {
      "epoch": 0.2824309462711783,
      "grad_norm": 34.707088470458984,
      "learning_rate": 0.00021527071611864647,
      "loss": 0.3729,
      "step": 88600
    },
    {
      "epoch": 0.2827497170908975,
      "grad_norm": 0.00010981228842865676,
      "learning_rate": 0.00021517508487273074,
      "loss": 0.2606,
      "step": 88700
    },
    {
      "epoch": 0.2830684879106167,
      "grad_norm": 58.76902389526367,
      "learning_rate": 0.00021507945362681498,
      "loss": 0.5855,
      "step": 88800
    },
    {
      "epoch": 0.2833872587303358,
      "grad_norm": 0.005764089524745941,
      "learning_rate": 0.00021498382238089922,
      "loss": 0.614,
      "step": 88900
    },
    {
      "epoch": 0.283706029550055,
      "grad_norm": 0.0034285872243344784,
      "learning_rate": 0.00021488819113498348,
      "loss": 0.4718,
      "step": 89000
    },
    {
      "epoch": 0.2840248003697742,
      "grad_norm": 0.23738063871860504,
      "learning_rate": 0.00021479255988906772,
      "loss": 0.4593,
      "step": 89100
    },
    {
      "epoch": 0.2843435711894933,
      "grad_norm": 0.02577589638531208,
      "learning_rate": 0.000214696928643152,
      "loss": 0.5098,
      "step": 89200
    },
    {
      "epoch": 0.2846623420092125,
      "grad_norm": 0.0004325468325987458,
      "learning_rate": 0.00021460129739723623,
      "loss": 0.3514,
      "step": 89300
    },
    {
      "epoch": 0.2849811128289316,
      "grad_norm": 0.0016255165683105588,
      "learning_rate": 0.00021450566615132047,
      "loss": 0.3676,
      "step": 89400
    },
    {
      "epoch": 0.2852998836486508,
      "grad_norm": 0.0005431444733403623,
      "learning_rate": 0.00021441003490540474,
      "loss": 0.4577,
      "step": 89500
    },
    {
      "epoch": 0.28561865446837,
      "grad_norm": 0.009988418780267239,
      "learning_rate": 0.00021431440365948898,
      "loss": 0.6571,
      "step": 89600
    },
    {
      "epoch": 0.28593742528808913,
      "grad_norm": 0.04777955636382103,
      "learning_rate": 0.00021421877241357325,
      "loss": 0.4347,
      "step": 89700
    },
    {
      "epoch": 0.2862561961078083,
      "grad_norm": 0.007241417188197374,
      "learning_rate": 0.0002141231411676575,
      "loss": 0.5566,
      "step": 89800
    },
    {
      "epoch": 0.28657496692752743,
      "grad_norm": 0.0010661811102181673,
      "learning_rate": 0.00021402750992174173,
      "loss": 0.5377,
      "step": 89900
    },
    {
      "epoch": 0.28689373774724664,
      "grad_norm": 0.5637102127075195,
      "learning_rate": 0.000213931878675826,
      "loss": 0.5412,
      "step": 90000
    },
    {
      "epoch": 0.2872125085669658,
      "grad_norm": 0.2187943011522293,
      "learning_rate": 0.00021383624742991024,
      "loss": 0.5581,
      "step": 90100
    },
    {
      "epoch": 0.28753127938668493,
      "grad_norm": 0.0038152066990733147,
      "learning_rate": 0.00021374061618399448,
      "loss": 0.5611,
      "step": 90200
    },
    {
      "epoch": 0.2878500502064041,
      "grad_norm": 0.09277977049350739,
      "learning_rate": 0.00021364498493807874,
      "loss": 0.1323,
      "step": 90300
    },
    {
      "epoch": 0.2881688210261233,
      "grad_norm": 0.0003574281872715801,
      "learning_rate": 0.00021354935369216299,
      "loss": 0.2459,
      "step": 90400
    },
    {
      "epoch": 0.28848759184584244,
      "grad_norm": 19.72042465209961,
      "learning_rate": 0.00021345372244624728,
      "loss": 0.5473,
      "step": 90500
    },
    {
      "epoch": 0.2888063626655616,
      "grad_norm": 0.0010180800454691052,
      "learning_rate": 0.0002133580912003315,
      "loss": 0.4328,
      "step": 90600
    },
    {
      "epoch": 0.28912513348528074,
      "grad_norm": 0.0031292035710066557,
      "learning_rate": 0.00021326245995441573,
      "loss": 0.3613,
      "step": 90700
    },
    {
      "epoch": 0.28944390430499994,
      "grad_norm": 1.311432123184204,
      "learning_rate": 0.00021316682870850003,
      "loss": 0.3241,
      "step": 90800
    },
    {
      "epoch": 0.2897626751247191,
      "grad_norm": 0.006924709305167198,
      "learning_rate": 0.00021307119746258424,
      "loss": 0.6384,
      "step": 90900
    },
    {
      "epoch": 0.29008144594443824,
      "grad_norm": 0.0024722192902117968,
      "learning_rate": 0.00021297556621666848,
      "loss": 0.5509,
      "step": 91000
    },
    {
      "epoch": 0.2904002167641574,
      "grad_norm": 54.977622985839844,
      "learning_rate": 0.00021287993497075278,
      "loss": 0.4371,
      "step": 91100
    },
    {
      "epoch": 0.2907189875838766,
      "grad_norm": 38.76726531982422,
      "learning_rate": 0.000212784303724837,
      "loss": 0.5634,
      "step": 91200
    },
    {
      "epoch": 0.29103775840359575,
      "grad_norm": 0.0012866125907748938,
      "learning_rate": 0.00021268867247892128,
      "loss": 0.4427,
      "step": 91300
    },
    {
      "epoch": 0.2913565292233149,
      "grad_norm": 0.034120965749025345,
      "learning_rate": 0.00021259304123300552,
      "loss": 0.3102,
      "step": 91400
    },
    {
      "epoch": 0.29167530004303405,
      "grad_norm": 59.897186279296875,
      "learning_rate": 0.00021249740998708974,
      "loss": 0.6173,
      "step": 91500
    },
    {
      "epoch": 0.2919940708627532,
      "grad_norm": 55.67202377319336,
      "learning_rate": 0.00021240177874117403,
      "loss": 0.3734,
      "step": 91600
    },
    {
      "epoch": 0.2923128416824724,
      "grad_norm": 0.1374250203371048,
      "learning_rate": 0.00021230614749525827,
      "loss": 0.4997,
      "step": 91700
    },
    {
      "epoch": 0.29263161250219155,
      "grad_norm": 94.5177230834961,
      "learning_rate": 0.00021221051624934254,
      "loss": 0.4984,
      "step": 91800
    },
    {
      "epoch": 0.2929503833219107,
      "grad_norm": 0.00020720678730867803,
      "learning_rate": 0.00021211488500342678,
      "loss": 0.5042,
      "step": 91900
    },
    {
      "epoch": 0.29326915414162985,
      "grad_norm": 0.10474000126123428,
      "learning_rate": 0.00021201925375751102,
      "loss": 0.3351,
      "step": 92000
    },
    {
      "epoch": 0.29358792496134906,
      "grad_norm": 0.13284392654895782,
      "learning_rate": 0.0002119236225115953,
      "loss": 0.4935,
      "step": 92100
    },
    {
      "epoch": 0.2939066957810682,
      "grad_norm": 0.03605632111430168,
      "learning_rate": 0.00021182799126567953,
      "loss": 0.5492,
      "step": 92200
    },
    {
      "epoch": 0.29422546660078736,
      "grad_norm": 0.002312879543751478,
      "learning_rate": 0.00021173236001976377,
      "loss": 0.3223,
      "step": 92300
    },
    {
      "epoch": 0.2945442374205065,
      "grad_norm": 0.0031047267839312553,
      "learning_rate": 0.00021163672877384804,
      "loss": 0.5584,
      "step": 92400
    },
    {
      "epoch": 0.2948630082402257,
      "grad_norm": 97.2959213256836,
      "learning_rate": 0.00021154109752793228,
      "loss": 0.3709,
      "step": 92500
    },
    {
      "epoch": 0.29518177905994486,
      "grad_norm": 0.002606921596452594,
      "learning_rate": 0.00021144546628201654,
      "loss": 0.4459,
      "step": 92600
    },
    {
      "epoch": 0.295500549879664,
      "grad_norm": 0.024301700294017792,
      "learning_rate": 0.00021134983503610078,
      "loss": 0.8056,
      "step": 92700
    },
    {
      "epoch": 0.29581932069938316,
      "grad_norm": 0.014903577044606209,
      "learning_rate": 0.00021125420379018502,
      "loss": 0.5886,
      "step": 92800
    },
    {
      "epoch": 0.29613809151910236,
      "grad_norm": 62.906681060791016,
      "learning_rate": 0.0002111585725442693,
      "loss": 0.5302,
      "step": 92900
    },
    {
      "epoch": 0.2964568623388215,
      "grad_norm": 0.000824492599349469,
      "learning_rate": 0.00021106294129835353,
      "loss": 0.4534,
      "step": 93000
    },
    {
      "epoch": 0.29677563315854066,
      "grad_norm": 0.0018688990967348218,
      "learning_rate": 0.00021096731005243777,
      "loss": 0.2338,
      "step": 93100
    },
    {
      "epoch": 0.2970944039782598,
      "grad_norm": 2.3073530197143555,
      "learning_rate": 0.00021087167880652204,
      "loss": 0.4759,
      "step": 93200
    },
    {
      "epoch": 0.297413174797979,
      "grad_norm": 1.4601318836212158,
      "learning_rate": 0.00021077604756060628,
      "loss": 0.4784,
      "step": 93300
    },
    {
      "epoch": 0.29773194561769817,
      "grad_norm": 0.0006671220762655139,
      "learning_rate": 0.00021068041631469055,
      "loss": 0.3877,
      "step": 93400
    },
    {
      "epoch": 0.2980507164374173,
      "grad_norm": 12.865523338317871,
      "learning_rate": 0.0002105847850687748,
      "loss": 0.5216,
      "step": 93500
    },
    {
      "epoch": 0.29836948725713647,
      "grad_norm": 82.74324798583984,
      "learning_rate": 0.00021048915382285903,
      "loss": 0.7092,
      "step": 93600
    },
    {
      "epoch": 0.2986882580768556,
      "grad_norm": 3.83831787109375,
      "learning_rate": 0.0002103935225769433,
      "loss": 0.4068,
      "step": 93700
    },
    {
      "epoch": 0.2990070288965748,
      "grad_norm": 0.00021943618776276708,
      "learning_rate": 0.00021029789133102754,
      "loss": 0.5279,
      "step": 93800
    },
    {
      "epoch": 0.29932579971629397,
      "grad_norm": 0.001967189135029912,
      "learning_rate": 0.0002102022600851118,
      "loss": 0.3981,
      "step": 93900
    },
    {
      "epoch": 0.2996445705360131,
      "grad_norm": 15.294356346130371,
      "learning_rate": 0.00021010662883919604,
      "loss": 0.3893,
      "step": 94000
    },
    {
      "epoch": 0.29996334135573227,
      "grad_norm": 15.5675687789917,
      "learning_rate": 0.00021001099759328028,
      "loss": 0.3256,
      "step": 94100
    },
    {
      "epoch": 0.3002821121754515,
      "grad_norm": 32.493370056152344,
      "learning_rate": 0.00020991536634736455,
      "loss": 0.3482,
      "step": 94200
    },
    {
      "epoch": 0.3006008829951706,
      "grad_norm": 78.52513122558594,
      "learning_rate": 0.0002098197351014488,
      "loss": 0.3837,
      "step": 94300
    },
    {
      "epoch": 0.3009196538148898,
      "grad_norm": 9.089094161987305,
      "learning_rate": 0.00020972410385553303,
      "loss": 0.2148,
      "step": 94400
    },
    {
      "epoch": 0.3012384246346089,
      "grad_norm": 0.0014239838346838951,
      "learning_rate": 0.0002096284726096173,
      "loss": 0.5647,
      "step": 94500
    },
    {
      "epoch": 0.30155719545432813,
      "grad_norm": 0.02790326625108719,
      "learning_rate": 0.00020953284136370154,
      "loss": 0.4825,
      "step": 94600
    },
    {
      "epoch": 0.3018759662740473,
      "grad_norm": 0.00033131090458482504,
      "learning_rate": 0.0002094372101177858,
      "loss": 0.599,
      "step": 94700
    },
    {
      "epoch": 0.30219473709376643,
      "grad_norm": 0.009246311150491238,
      "learning_rate": 0.00020934157887187005,
      "loss": 0.3616,
      "step": 94800
    },
    {
      "epoch": 0.3025135079134856,
      "grad_norm": 0.34576112031936646,
      "learning_rate": 0.0002092459476259543,
      "loss": 0.7517,
      "step": 94900
    },
    {
      "epoch": 0.3028322787332048,
      "grad_norm": 0.019523868337273598,
      "learning_rate": 0.00020915031638003856,
      "loss": 0.5947,
      "step": 95000
    },
    {
      "epoch": 0.30315104955292393,
      "grad_norm": 15.476637840270996,
      "learning_rate": 0.0002090546851341228,
      "loss": 0.2565,
      "step": 95100
    },
    {
      "epoch": 0.3034698203726431,
      "grad_norm": 0.10220427066087723,
      "learning_rate": 0.00020895905388820704,
      "loss": 0.5062,
      "step": 95200
    },
    {
      "epoch": 0.30378859119236223,
      "grad_norm": 0.011906388215720654,
      "learning_rate": 0.0002088634226422913,
      "loss": 0.3083,
      "step": 95300
    },
    {
      "epoch": 0.30410736201208144,
      "grad_norm": 2.1751041412353516,
      "learning_rate": 0.00020876779139637554,
      "loss": 0.4921,
      "step": 95400
    },
    {
      "epoch": 0.3044261328318006,
      "grad_norm": 0.00035974738420918584,
      "learning_rate": 0.0002086721601504598,
      "loss": 0.3688,
      "step": 95500
    },
    {
      "epoch": 0.30474490365151974,
      "grad_norm": 0.00011632914538495243,
      "learning_rate": 0.00020857652890454405,
      "loss": 0.427,
      "step": 95600
    },
    {
      "epoch": 0.3050636744712389,
      "grad_norm": 0.0018843880388885736,
      "learning_rate": 0.0002084808976586283,
      "loss": 0.4128,
      "step": 95700
    },
    {
      "epoch": 0.3053824452909581,
      "grad_norm": 0.03543877974152565,
      "learning_rate": 0.00020838526641271256,
      "loss": 0.4281,
      "step": 95800
    },
    {
      "epoch": 0.30570121611067724,
      "grad_norm": 0.02152326889336109,
      "learning_rate": 0.0002082896351667968,
      "loss": 0.4315,
      "step": 95900
    },
    {
      "epoch": 0.3060199869303964,
      "grad_norm": 0.002980264835059643,
      "learning_rate": 0.00020819400392088107,
      "loss": 0.32,
      "step": 96000
    },
    {
      "epoch": 0.30633875775011554,
      "grad_norm": 0.00022416292631532997,
      "learning_rate": 0.0002080983726749653,
      "loss": 0.3467,
      "step": 96100
    },
    {
      "epoch": 0.3066575285698347,
      "grad_norm": 0.5905153155326843,
      "learning_rate": 0.00020800274142904955,
      "loss": 0.4423,
      "step": 96200
    },
    {
      "epoch": 0.3069762993895539,
      "grad_norm": 0.7940438985824585,
      "learning_rate": 0.00020790711018313382,
      "loss": 0.6273,
      "step": 96300
    },
    {
      "epoch": 0.30729507020927305,
      "grad_norm": 27.481033325195312,
      "learning_rate": 0.00020781147893721806,
      "loss": 0.5482,
      "step": 96400
    },
    {
      "epoch": 0.3076138410289922,
      "grad_norm": 0.3233177959918976,
      "learning_rate": 0.0002077158476913023,
      "loss": 0.3526,
      "step": 96500
    },
    {
      "epoch": 0.30793261184871135,
      "grad_norm": 37.0679931640625,
      "learning_rate": 0.00020762021644538656,
      "loss": 0.5195,
      "step": 96600
    },
    {
      "epoch": 0.30825138266843055,
      "grad_norm": 0.06966422498226166,
      "learning_rate": 0.0002075245851994708,
      "loss": 0.4872,
      "step": 96700
    },
    {
      "epoch": 0.3085701534881497,
      "grad_norm": 131.2924346923828,
      "learning_rate": 0.0002074289539535551,
      "loss": 0.6313,
      "step": 96800
    },
    {
      "epoch": 0.30888892430786885,
      "grad_norm": 0.08887293934822083,
      "learning_rate": 0.00020733332270763934,
      "loss": 0.5088,
      "step": 96900
    },
    {
      "epoch": 0.309207695127588,
      "grad_norm": 0.00014622417802456766,
      "learning_rate": 0.00020723769146172355,
      "loss": 0.4445,
      "step": 97000
    },
    {
      "epoch": 0.3095264659473072,
      "grad_norm": 26.515605926513672,
      "learning_rate": 0.00020714206021580785,
      "loss": 0.4213,
      "step": 97100
    },
    {
      "epoch": 0.30984523676702636,
      "grad_norm": 0.0491853766143322,
      "learning_rate": 0.0002070464289698921,
      "loss": 0.3118,
      "step": 97200
    },
    {
      "epoch": 0.3101640075867455,
      "grad_norm": 0.19904081523418427,
      "learning_rate": 0.0002069507977239763,
      "loss": 0.4337,
      "step": 97300
    },
    {
      "epoch": 0.31048277840646465,
      "grad_norm": 0.005070019513368607,
      "learning_rate": 0.0002068551664780606,
      "loss": 0.7231,
      "step": 97400
    },
    {
      "epoch": 0.31080154922618386,
      "grad_norm": 0.10429030656814575,
      "learning_rate": 0.00020675953523214484,
      "loss": 0.6027,
      "step": 97500
    },
    {
      "epoch": 0.311120320045903,
      "grad_norm": 0.00030796087230555713,
      "learning_rate": 0.0002066639039862291,
      "loss": 0.4845,
      "step": 97600
    },
    {
      "epoch": 0.31143909086562216,
      "grad_norm": 8.882585825631395e-05,
      "learning_rate": 0.00020656827274031334,
      "loss": 0.2923,
      "step": 97700
    },
    {
      "epoch": 0.3117578616853413,
      "grad_norm": 0.0003212022711522877,
      "learning_rate": 0.00020647264149439758,
      "loss": 0.4367,
      "step": 97800
    },
    {
      "epoch": 0.3120766325050605,
      "grad_norm": 103.40782165527344,
      "learning_rate": 0.00020637701024848185,
      "loss": 0.4303,
      "step": 97900
    },
    {
      "epoch": 0.31239540332477966,
      "grad_norm": 0.0033692705910652876,
      "learning_rate": 0.0002062813790025661,
      "loss": 0.2552,
      "step": 98000
    },
    {
      "epoch": 0.3127141741444988,
      "grad_norm": 0.0026427407283335924,
      "learning_rate": 0.00020618574775665036,
      "loss": 0.5284,
      "step": 98100
    },
    {
      "epoch": 0.31303294496421796,
      "grad_norm": 0.0005346090765669942,
      "learning_rate": 0.0002060901165107346,
      "loss": 0.4975,
      "step": 98200
    },
    {
      "epoch": 0.3133517157839371,
      "grad_norm": 0.0459020659327507,
      "learning_rate": 0.00020599448526481884,
      "loss": 0.5261,
      "step": 98300
    },
    {
      "epoch": 0.3136704866036563,
      "grad_norm": 98.64604949951172,
      "learning_rate": 0.0002058988540189031,
      "loss": 0.5375,
      "step": 98400
    },
    {
      "epoch": 0.31398925742337547,
      "grad_norm": 0.009618349373340607,
      "learning_rate": 0.00020580322277298735,
      "loss": 0.5249,
      "step": 98500
    },
    {
      "epoch": 0.3143080282430946,
      "grad_norm": 1.859383463859558,
      "learning_rate": 0.0002057075915270716,
      "loss": 0.3455,
      "step": 98600
    },
    {
      "epoch": 0.31462679906281377,
      "grad_norm": 0.0019604575354605913,
      "learning_rate": 0.00020561196028115586,
      "loss": 0.6125,
      "step": 98700
    },
    {
      "epoch": 0.31494556988253297,
      "grad_norm": 0.0012189012486487627,
      "learning_rate": 0.0002055163290352401,
      "loss": 0.5899,
      "step": 98800
    },
    {
      "epoch": 0.3152643407022521,
      "grad_norm": 54.66252517700195,
      "learning_rate": 0.00020542069778932436,
      "loss": 0.4687,
      "step": 98900
    },
    {
      "epoch": 0.31558311152197127,
      "grad_norm": 1.5575780868530273,
      "learning_rate": 0.0002053250665434086,
      "loss": 0.3617,
      "step": 99000
    },
    {
      "epoch": 0.3159018823416904,
      "grad_norm": 41.00727462768555,
      "learning_rate": 0.00020522943529749284,
      "loss": 0.4981,
      "step": 99100
    },
    {
      "epoch": 0.3162206531614096,
      "grad_norm": 29.604028701782227,
      "learning_rate": 0.0002051338040515771,
      "loss": 0.5437,
      "step": 99200
    },
    {
      "epoch": 0.3165394239811288,
      "grad_norm": 10.251509666442871,
      "learning_rate": 0.00020503817280566135,
      "loss": 0.3582,
      "step": 99300
    },
    {
      "epoch": 0.3168581948008479,
      "grad_norm": 95.22880554199219,
      "learning_rate": 0.0002049425415597456,
      "loss": 0.4545,
      "step": 99400
    },
    {
      "epoch": 0.3171769656205671,
      "grad_norm": 0.01009935699403286,
      "learning_rate": 0.00020484691031382986,
      "loss": 0.4573,
      "step": 99500
    },
    {
      "epoch": 0.3174957364402863,
      "grad_norm": 0.007115859538316727,
      "learning_rate": 0.0002047512790679141,
      "loss": 0.6321,
      "step": 99600
    },
    {
      "epoch": 0.31781450726000543,
      "grad_norm": 0.01966922916471958,
      "learning_rate": 0.00020465564782199837,
      "loss": 0.4226,
      "step": 99700
    },
    {
      "epoch": 0.3181332780797246,
      "grad_norm": 67.61743927001953,
      "learning_rate": 0.0002045600165760826,
      "loss": 0.2869,
      "step": 99800
    },
    {
      "epoch": 0.31845204889944373,
      "grad_norm": 0.017674749717116356,
      "learning_rate": 0.00020446438533016685,
      "loss": 0.3831,
      "step": 99900
    },
    {
      "epoch": 0.31877081971916293,
      "grad_norm": 1.1265406608581543,
      "learning_rate": 0.00020436875408425112,
      "loss": 0.5122,
      "step": 100000
    },
    {
      "epoch": 0.3190895905388821,
      "grad_norm": 16.007360458374023,
      "learning_rate": 0.00020427312283833536,
      "loss": 0.3887,
      "step": 100100
    },
    {
      "epoch": 0.31940836135860123,
      "grad_norm": 6.0784454345703125,
      "learning_rate": 0.00020417749159241962,
      "loss": 0.2784,
      "step": 100200
    },
    {
      "epoch": 0.3197271321783204,
      "grad_norm": 0.12595492601394653,
      "learning_rate": 0.00020408186034650386,
      "loss": 0.5761,
      "step": 100300
    },
    {
      "epoch": 0.32004590299803953,
      "grad_norm": 20.09254264831543,
      "learning_rate": 0.0002039862291005881,
      "loss": 0.529,
      "step": 100400
    },
    {
      "epoch": 0.32036467381775874,
      "grad_norm": 0.00011601727601373568,
      "learning_rate": 0.00020389059785467237,
      "loss": 0.6355,
      "step": 100500
    },
    {
      "epoch": 0.3206834446374779,
      "grad_norm": 2.918858289718628,
      "learning_rate": 0.0002037949666087566,
      "loss": 0.3163,
      "step": 100600
    },
    {
      "epoch": 0.32100221545719704,
      "grad_norm": 0.01571268029510975,
      "learning_rate": 0.00020369933536284085,
      "loss": 0.4331,
      "step": 100700
    },
    {
      "epoch": 0.3213209862769162,
      "grad_norm": 0.0047474694438278675,
      "learning_rate": 0.00020360370411692512,
      "loss": 0.2365,
      "step": 100800
    },
    {
      "epoch": 0.3216397570966354,
      "grad_norm": 0.008851634338498116,
      "learning_rate": 0.00020350807287100936,
      "loss": 0.492,
      "step": 100900
    },
    {
      "epoch": 0.32195852791635454,
      "grad_norm": 0.0002913581265602261,
      "learning_rate": 0.00020341244162509363,
      "loss": 0.429,
      "step": 101000
    },
    {
      "epoch": 0.3222772987360737,
      "grad_norm": 0.00011598785931710154,
      "learning_rate": 0.00020331681037917787,
      "loss": 0.4357,
      "step": 101100
    },
    {
      "epoch": 0.32259606955579284,
      "grad_norm": 0.02403493970632553,
      "learning_rate": 0.0002032211791332621,
      "loss": 0.4796,
      "step": 101200
    },
    {
      "epoch": 0.32291484037551205,
      "grad_norm": 0.022036531940102577,
      "learning_rate": 0.00020312554788734638,
      "loss": 0.3838,
      "step": 101300
    },
    {
      "epoch": 0.3232336111952312,
      "grad_norm": 0.0035156728699803352,
      "learning_rate": 0.00020302991664143062,
      "loss": 0.41,
      "step": 101400
    },
    {
      "epoch": 0.32355238201495035,
      "grad_norm": 0.002025719964876771,
      "learning_rate": 0.00020293428539551486,
      "loss": 0.6171,
      "step": 101500
    },
    {
      "epoch": 0.3238711528346695,
      "grad_norm": 0.000572682823985815,
      "learning_rate": 0.00020283865414959912,
      "loss": 0.5166,
      "step": 101600
    },
    {
      "epoch": 0.3241899236543887,
      "grad_norm": 0.0003208052658010274,
      "learning_rate": 0.00020274302290368336,
      "loss": 0.5403,
      "step": 101700
    },
    {
      "epoch": 0.32450869447410785,
      "grad_norm": 0.26762446761131287,
      "learning_rate": 0.00020264739165776763,
      "loss": 0.3448,
      "step": 101800
    },
    {
      "epoch": 0.324827465293827,
      "grad_norm": 0.0006698024808429182,
      "learning_rate": 0.00020255176041185187,
      "loss": 0.468,
      "step": 101900
    },
    {
      "epoch": 0.32514623611354615,
      "grad_norm": 0.10990916192531586,
      "learning_rate": 0.0002024561291659361,
      "loss": 0.2619,
      "step": 102000
    },
    {
      "epoch": 0.32546500693326536,
      "grad_norm": 20.965866088867188,
      "learning_rate": 0.00020236049792002038,
      "loss": 0.4051,
      "step": 102100
    },
    {
      "epoch": 0.3257837777529845,
      "grad_norm": 0.019732272252440453,
      "learning_rate": 0.00020226486667410462,
      "loss": 0.3651,
      "step": 102200
    },
    {
      "epoch": 0.32610254857270365,
      "grad_norm": 0.3680015802383423,
      "learning_rate": 0.00020216923542818886,
      "loss": 0.5831,
      "step": 102300
    },
    {
      "epoch": 0.3264213193924228,
      "grad_norm": 0.00024941141600720584,
      "learning_rate": 0.00020207360418227313,
      "loss": 0.386,
      "step": 102400
    },
    {
      "epoch": 0.32674009021214195,
      "grad_norm": 0.00021679396741092205,
      "learning_rate": 0.00020197797293635737,
      "loss": 0.355,
      "step": 102500
    },
    {
      "epoch": 0.32705886103186116,
      "grad_norm": 0.00041108246659860015,
      "learning_rate": 0.00020188234169044166,
      "loss": 0.5283,
      "step": 102600
    },
    {
      "epoch": 0.3273776318515803,
      "grad_norm": 14.27956771850586,
      "learning_rate": 0.00020178671044452588,
      "loss": 0.4868,
      "step": 102700
    },
    {
      "epoch": 0.32769640267129946,
      "grad_norm": 0.00833088532090187,
      "learning_rate": 0.00020169107919861012,
      "loss": 0.5668,
      "step": 102800
    },
    {
      "epoch": 0.3280151734910186,
      "grad_norm": 33.31673049926758,
      "learning_rate": 0.0002015954479526944,
      "loss": 0.3235,
      "step": 102900
    },
    {
      "epoch": 0.3283339443107378,
      "grad_norm": 109.94058990478516,
      "learning_rate": 0.00020149981670677862,
      "loss": 0.3353,
      "step": 103000
    },
    {
      "epoch": 0.32865271513045696,
      "grad_norm": 17.109745025634766,
      "learning_rate": 0.00020140418546086292,
      "loss": 0.4407,
      "step": 103100
    },
    {
      "epoch": 0.3289714859501761,
      "grad_norm": 24.339292526245117,
      "learning_rate": 0.00020130855421494716,
      "loss": 0.4532,
      "step": 103200
    },
    {
      "epoch": 0.32929025676989526,
      "grad_norm": 0.04178984463214874,
      "learning_rate": 0.0002012129229690314,
      "loss": 0.5323,
      "step": 103300
    },
    {
      "epoch": 0.32960902758961447,
      "grad_norm": 6.092009544372559,
      "learning_rate": 0.00020111729172311567,
      "loss": 0.4506,
      "step": 103400
    },
    {
      "epoch": 0.3299277984093336,
      "grad_norm": 0.00012369645992293954,
      "learning_rate": 0.0002010216604771999,
      "loss": 0.4693,
      "step": 103500
    },
    {
      "epoch": 0.33024656922905277,
      "grad_norm": 0.01017996110022068,
      "learning_rate": 0.00020092602923128415,
      "loss": 0.3963,
      "step": 103600
    },
    {
      "epoch": 0.3305653400487719,
      "grad_norm": 0.010172901675105095,
      "learning_rate": 0.00020083039798536842,
      "loss": 0.4409,
      "step": 103700
    },
    {
      "epoch": 0.3308841108684911,
      "grad_norm": 0.018727095797657967,
      "learning_rate": 0.00020073476673945266,
      "loss": 0.3813,
      "step": 103800
    },
    {
      "epoch": 0.33120288168821027,
      "grad_norm": 0.01635202020406723,
      "learning_rate": 0.00020063913549353692,
      "loss": 0.2341,
      "step": 103900
    },
    {
      "epoch": 0.3315216525079294,
      "grad_norm": 0.1931879222393036,
      "learning_rate": 0.00020054350424762116,
      "loss": 0.7549,
      "step": 104000
    },
    {
      "epoch": 0.33184042332764857,
      "grad_norm": 0.004460031166672707,
      "learning_rate": 0.0002004478730017054,
      "loss": 0.4986,
      "step": 104100
    },
    {
      "epoch": 0.3321591941473678,
      "grad_norm": 0.000247828516876325,
      "learning_rate": 0.00020035224175578967,
      "loss": 0.5753,
      "step": 104200
    },
    {
      "epoch": 0.3324779649670869,
      "grad_norm": 0.0025730098132044077,
      "learning_rate": 0.0002002566105098739,
      "loss": 0.5291,
      "step": 104300
    },
    {
      "epoch": 0.3327967357868061,
      "grad_norm": 0.003024038625881076,
      "learning_rate": 0.00020016097926395815,
      "loss": 0.2398,
      "step": 104400
    },
    {
      "epoch": 0.3331155066065252,
      "grad_norm": 0.0007115078624337912,
      "learning_rate": 0.00020006534801804242,
      "loss": 0.2543,
      "step": 104500
    },
    {
      "epoch": 0.3334342774262444,
      "grad_norm": 0.0008706842781975865,
      "learning_rate": 0.00019996971677212666,
      "loss": 0.4085,
      "step": 104600
    },
    {
      "epoch": 0.3337530482459636,
      "grad_norm": 0.0008076729718595743,
      "learning_rate": 0.00019987408552621093,
      "loss": 0.589,
      "step": 104700
    },
    {
      "epoch": 0.33407181906568273,
      "grad_norm": 0.11442876607179642,
      "learning_rate": 0.00019977845428029517,
      "loss": 0.4236,
      "step": 104800
    },
    {
      "epoch": 0.3343905898854019,
      "grad_norm": 0.00018759504018817097,
      "learning_rate": 0.0001996828230343794,
      "loss": 0.3664,
      "step": 104900
    },
    {
      "epoch": 0.33470936070512103,
      "grad_norm": 6.565152645111084,
      "learning_rate": 0.00019958719178846368,
      "loss": 0.2735,
      "step": 105000
    },
    {
      "epoch": 0.33502813152484023,
      "grad_norm": 52.66645431518555,
      "learning_rate": 0.00019949156054254792,
      "loss": 0.4608,
      "step": 105100
    },
    {
      "epoch": 0.3353469023445594,
      "grad_norm": 0.0006791391060687602,
      "learning_rate": 0.00019939592929663218,
      "loss": 0.2746,
      "step": 105200
    },
    {
      "epoch": 0.33566567316427853,
      "grad_norm": 53.00074005126953,
      "learning_rate": 0.00019930029805071642,
      "loss": 0.5607,
      "step": 105300
    },
    {
      "epoch": 0.3359844439839977,
      "grad_norm": 71.19749450683594,
      "learning_rate": 0.00019920466680480066,
      "loss": 0.4649,
      "step": 105400
    },
    {
      "epoch": 0.3363032148037169,
      "grad_norm": 0.0017107176827266812,
      "learning_rate": 0.00019910903555888493,
      "loss": 0.3566,
      "step": 105500
    },
    {
      "epoch": 0.33662198562343604,
      "grad_norm": 0.006904497276991606,
      "learning_rate": 0.00019901340431296917,
      "loss": 0.465,
      "step": 105600
    },
    {
      "epoch": 0.3369407564431552,
      "grad_norm": 0.1328548789024353,
      "learning_rate": 0.0001989177730670534,
      "loss": 0.1579,
      "step": 105700
    },
    {
      "epoch": 0.33725952726287434,
      "grad_norm": 0.000434613146353513,
      "learning_rate": 0.00019882214182113768,
      "loss": 0.5655,
      "step": 105800
    },
    {
      "epoch": 0.33757829808259354,
      "grad_norm": 0.006616656668484211,
      "learning_rate": 0.00019872651057522192,
      "loss": 0.3612,
      "step": 105900
    },
    {
      "epoch": 0.3378970689023127,
      "grad_norm": 0.00023181881988421082,
      "learning_rate": 0.0001986308793293062,
      "loss": 0.3934,
      "step": 106000
    },
    {
      "epoch": 0.33821583972203184,
      "grad_norm": 77.24748992919922,
      "learning_rate": 0.00019853524808339043,
      "loss": 0.6657,
      "step": 106100
    },
    {
      "epoch": 0.338534610541751,
      "grad_norm": 0.007260405924171209,
      "learning_rate": 0.00019843961683747467,
      "loss": 0.5944,
      "step": 106200
    },
    {
      "epoch": 0.3388533813614702,
      "grad_norm": 16.20659637451172,
      "learning_rate": 0.00019834398559155894,
      "loss": 0.3199,
      "step": 106300
    },
    {
      "epoch": 0.33917215218118935,
      "grad_norm": 0.00047129089944064617,
      "learning_rate": 0.00019824835434564318,
      "loss": 0.4587,
      "step": 106400
    },
    {
      "epoch": 0.3394909230009085,
      "grad_norm": 0.07028021663427353,
      "learning_rate": 0.00019815272309972742,
      "loss": 0.3802,
      "step": 106500
    },
    {
      "epoch": 0.33980969382062765,
      "grad_norm": 93.95122528076172,
      "learning_rate": 0.00019805709185381168,
      "loss": 0.6187,
      "step": 106600
    },
    {
      "epoch": 0.34012846464034685,
      "grad_norm": 0.0001946262491401285,
      "learning_rate": 0.00019796146060789592,
      "loss": 0.461,
      "step": 106700
    },
    {
      "epoch": 0.340447235460066,
      "grad_norm": 0.004429748747497797,
      "learning_rate": 0.0001978658293619802,
      "loss": 0.531,
      "step": 106800
    },
    {
      "epoch": 0.34076600627978515,
      "grad_norm": 0.0003346033627167344,
      "learning_rate": 0.00019777019811606443,
      "loss": 0.3794,
      "step": 106900
    },
    {
      "epoch": 0.3410847770995043,
      "grad_norm": 0.025608191266655922,
      "learning_rate": 0.00019767456687014867,
      "loss": 0.2407,
      "step": 107000
    },
    {
      "epoch": 0.34140354791922345,
      "grad_norm": 62.1741828918457,
      "learning_rate": 0.00019757893562423294,
      "loss": 0.4213,
      "step": 107100
    },
    {
      "epoch": 0.34172231873894265,
      "grad_norm": 44.62942123413086,
      "learning_rate": 0.00019748330437831718,
      "loss": 0.8166,
      "step": 107200
    },
    {
      "epoch": 0.3420410895586618,
      "grad_norm": 0.0017710697138682008,
      "learning_rate": 0.00019738767313240145,
      "loss": 0.3844,
      "step": 107300
    },
    {
      "epoch": 0.34235986037838095,
      "grad_norm": 0.0011478554224595428,
      "learning_rate": 0.0001972920418864857,
      "loss": 0.324,
      "step": 107400
    },
    {
      "epoch": 0.3426786311981001,
      "grad_norm": 0.0001744946785038337,
      "learning_rate": 0.00019719641064056993,
      "loss": 0.3581,
      "step": 107500
    },
    {
      "epoch": 0.3429974020178193,
      "grad_norm": 0.5294016599655151,
      "learning_rate": 0.0001971007793946542,
      "loss": 0.4514,
      "step": 107600
    },
    {
      "epoch": 0.34331617283753846,
      "grad_norm": 0.0005547089967876673,
      "learning_rate": 0.00019700514814873844,
      "loss": 0.2542,
      "step": 107700
    },
    {
      "epoch": 0.3436349436572576,
      "grad_norm": 0.028978882357478142,
      "learning_rate": 0.00019690951690282268,
      "loss": 0.486,
      "step": 107800
    },
    {
      "epoch": 0.34395371447697676,
      "grad_norm": 0.0003529847599565983,
      "learning_rate": 0.00019681388565690694,
      "loss": 0.3445,
      "step": 107900
    },
    {
      "epoch": 0.34427248529669596,
      "grad_norm": 7.084103107452393,
      "learning_rate": 0.00019671825441099118,
      "loss": 0.486,
      "step": 108000
    },
    {
      "epoch": 0.3445912561164151,
      "grad_norm": 0.004761343356221914,
      "learning_rate": 0.00019662262316507545,
      "loss": 0.6039,
      "step": 108100
    },
    {
      "epoch": 0.34491002693613426,
      "grad_norm": 0.007194550707936287,
      "learning_rate": 0.0001965269919191597,
      "loss": 0.3386,
      "step": 108200
    },
    {
      "epoch": 0.3452287977558534,
      "grad_norm": 0.008274823427200317,
      "learning_rate": 0.00019643136067324393,
      "loss": 0.3199,
      "step": 108300
    },
    {
      "epoch": 0.3455475685755726,
      "grad_norm": 0.01668562926352024,
      "learning_rate": 0.00019633572942732823,
      "loss": 0.469,
      "step": 108400
    },
    {
      "epoch": 0.34586633939529177,
      "grad_norm": 0.0047724503092467785,
      "learning_rate": 0.00019624009818141244,
      "loss": 0.6058,
      "step": 108500
    },
    {
      "epoch": 0.3461851102150109,
      "grad_norm": 0.015386578626930714,
      "learning_rate": 0.00019614446693549668,
      "loss": 0.6683,
      "step": 108600
    },
    {
      "epoch": 0.34650388103473007,
      "grad_norm": 2.5007448196411133,
      "learning_rate": 0.00019604883568958098,
      "loss": 0.5117,
      "step": 108700
    },
    {
      "epoch": 0.34682265185444927,
      "grad_norm": 0.0011989998165518045,
      "learning_rate": 0.0001959532044436652,
      "loss": 0.2994,
      "step": 108800
    },
    {
      "epoch": 0.3471414226741684,
      "grad_norm": 0.0013922115322202444,
      "learning_rate": 0.00019585757319774948,
      "loss": 0.4805,
      "step": 108900
    },
    {
      "epoch": 0.34746019349388757,
      "grad_norm": 0.006946545094251633,
      "learning_rate": 0.00019576194195183372,
      "loss": 0.4537,
      "step": 109000
    },
    {
      "epoch": 0.3477789643136067,
      "grad_norm": 1.0267301797866821,
      "learning_rate": 0.00019566631070591794,
      "loss": 0.4,
      "step": 109100
    },
    {
      "epoch": 0.34809773513332587,
      "grad_norm": 0.0002701904159039259,
      "learning_rate": 0.00019557067946000223,
      "loss": 0.4193,
      "step": 109200
    },
    {
      "epoch": 0.3484165059530451,
      "grad_norm": 6.186350345611572,
      "learning_rate": 0.00019547504821408647,
      "loss": 0.2652,
      "step": 109300
    },
    {
      "epoch": 0.3487352767727642,
      "grad_norm": 0.010353432036936283,
      "learning_rate": 0.00019537941696817074,
      "loss": 0.462,
      "step": 109400
    },
    {
      "epoch": 0.3490540475924834,
      "grad_norm": 0.04841937869787216,
      "learning_rate": 0.00019528378572225498,
      "loss": 0.4586,
      "step": 109500
    },
    {
      "epoch": 0.3493728184122025,
      "grad_norm": 41.72587203979492,
      "learning_rate": 0.00019518815447633922,
      "loss": 0.4685,
      "step": 109600
    },
    {
      "epoch": 0.34969158923192173,
      "grad_norm": 3.6960136640118435e-05,
      "learning_rate": 0.0001950925232304235,
      "loss": 0.3381,
      "step": 109700
    },
    {
      "epoch": 0.3500103600516409,
      "grad_norm": 64.66075134277344,
      "learning_rate": 0.00019499689198450773,
      "loss": 0.402,
      "step": 109800
    },
    {
      "epoch": 0.35032913087136003,
      "grad_norm": 0.001465292414650321,
      "learning_rate": 0.00019490126073859197,
      "loss": 0.3457,
      "step": 109900
    },
    {
      "epoch": 0.3506479016910792,
      "grad_norm": 0.011507770977914333,
      "learning_rate": 0.00019480562949267624,
      "loss": 0.3427,
      "step": 110000
    },
    {
      "epoch": 0.3509666725107984,
      "grad_norm": 22.542055130004883,
      "learning_rate": 0.00019470999824676048,
      "loss": 0.352,
      "step": 110100
    },
    {
      "epoch": 0.35128544333051753,
      "grad_norm": 2.0166633129119873,
      "learning_rate": 0.00019461436700084474,
      "loss": 0.4156,
      "step": 110200
    },
    {
      "epoch": 0.3516042141502367,
      "grad_norm": 154.09376525878906,
      "learning_rate": 0.00019451873575492898,
      "loss": 0.3175,
      "step": 110300
    },
    {
      "epoch": 0.35192298496995583,
      "grad_norm": 0.0002470684121362865,
      "learning_rate": 0.00019442310450901322,
      "loss": 0.5022,
      "step": 110400
    },
    {
      "epoch": 0.35224175578967504,
      "grad_norm": 12.959192276000977,
      "learning_rate": 0.0001943274732630975,
      "loss": 0.5609,
      "step": 110500
    },
    {
      "epoch": 0.3525605266093942,
      "grad_norm": 0.006700874771922827,
      "learning_rate": 0.00019423184201718173,
      "loss": 0.3805,
      "step": 110600
    },
    {
      "epoch": 0.35287929742911334,
      "grad_norm": 0.7214827537536621,
      "learning_rate": 0.00019413621077126597,
      "loss": 0.448,
      "step": 110700
    },
    {
      "epoch": 0.3531980682488325,
      "grad_norm": 0.0006308924639597535,
      "learning_rate": 0.00019404057952535024,
      "loss": 0.4436,
      "step": 110800
    },
    {
      "epoch": 0.3535168390685517,
      "grad_norm": 0.00219931174069643,
      "learning_rate": 0.00019394494827943448,
      "loss": 0.4395,
      "step": 110900
    },
    {
      "epoch": 0.35383560988827084,
      "grad_norm": 0.00011682533659040928,
      "learning_rate": 0.00019384931703351875,
      "loss": 0.3347,
      "step": 111000
    },
    {
      "epoch": 0.35415438070799,
      "grad_norm": 0.10486973822116852,
      "learning_rate": 0.000193753685787603,
      "loss": 0.4685,
      "step": 111100
    },
    {
      "epoch": 0.35447315152770914,
      "grad_norm": 0.001732368255034089,
      "learning_rate": 0.00019365805454168723,
      "loss": 0.4986,
      "step": 111200
    },
    {
      "epoch": 0.3547919223474283,
      "grad_norm": 0.001798816374503076,
      "learning_rate": 0.0001935624232957715,
      "loss": 0.3349,
      "step": 111300
    },
    {
      "epoch": 0.3551106931671475,
      "grad_norm": 0.0009404902230016887,
      "learning_rate": 0.00019346679204985574,
      "loss": 0.4377,
      "step": 111400
    },
    {
      "epoch": 0.35542946398686664,
      "grad_norm": 0.9129233360290527,
      "learning_rate": 0.00019337116080394,
      "loss": 0.4098,
      "step": 111500
    },
    {
      "epoch": 0.3557482348065858,
      "grad_norm": 0.0008529733750037849,
      "learning_rate": 0.00019327552955802424,
      "loss": 0.4071,
      "step": 111600
    },
    {
      "epoch": 0.35606700562630494,
      "grad_norm": 0.001825754763558507,
      "learning_rate": 0.00019317989831210848,
      "loss": 0.476,
      "step": 111700
    },
    {
      "epoch": 0.35638577644602415,
      "grad_norm": 0.0035752917174249887,
      "learning_rate": 0.00019308426706619275,
      "loss": 0.4475,
      "step": 111800
    },
    {
      "epoch": 0.3567045472657433,
      "grad_norm": 0.0002222567709395662,
      "learning_rate": 0.000192988635820277,
      "loss": 0.4239,
      "step": 111900
    },
    {
      "epoch": 0.35702331808546245,
      "grad_norm": 4.3624725341796875,
      "learning_rate": 0.00019289300457436123,
      "loss": 0.4936,
      "step": 112000
    },
    {
      "epoch": 0.3573420889051816,
      "grad_norm": 0.0009616613388061523,
      "learning_rate": 0.0001927973733284455,
      "loss": 0.2369,
      "step": 112100
    },
    {
      "epoch": 0.3576608597249008,
      "grad_norm": 0.004017149563878775,
      "learning_rate": 0.00019270174208252974,
      "loss": 0.4313,
      "step": 112200
    },
    {
      "epoch": 0.35797963054461995,
      "grad_norm": 0.017584368586540222,
      "learning_rate": 0.000192606110836614,
      "loss": 0.4038,
      "step": 112300
    },
    {
      "epoch": 0.3582984013643391,
      "grad_norm": 3.686037540435791,
      "learning_rate": 0.00019251047959069825,
      "loss": 0.3286,
      "step": 112400
    },
    {
      "epoch": 0.35861717218405825,
      "grad_norm": 0.011429975740611553,
      "learning_rate": 0.0001924148483447825,
      "loss": 0.5473,
      "step": 112500
    },
    {
      "epoch": 0.35893594300377746,
      "grad_norm": 0.007900857366621494,
      "learning_rate": 0.00019231921709886676,
      "loss": 0.4256,
      "step": 112600
    },
    {
      "epoch": 0.3592547138234966,
      "grad_norm": 0.03856418654322624,
      "learning_rate": 0.000192223585852951,
      "loss": 0.5209,
      "step": 112700
    },
    {
      "epoch": 0.35957348464321576,
      "grad_norm": 80.9841537475586,
      "learning_rate": 0.00019212795460703524,
      "loss": 0.4739,
      "step": 112800
    },
    {
      "epoch": 0.3598922554629349,
      "grad_norm": 0.02351728267967701,
      "learning_rate": 0.0001920323233611195,
      "loss": 0.4473,
      "step": 112900
    },
    {
      "epoch": 0.3602110262826541,
      "grad_norm": 0.004876044578850269,
      "learning_rate": 0.00019193669211520374,
      "loss": 0.2261,
      "step": 113000
    },
    {
      "epoch": 0.36052979710237326,
      "grad_norm": 0.008495451882481575,
      "learning_rate": 0.000191841060869288,
      "loss": 0.418,
      "step": 113100
    },
    {
      "epoch": 0.3608485679220924,
      "grad_norm": 0.02049362100660801,
      "learning_rate": 0.00019174542962337225,
      "loss": 0.4026,
      "step": 113200
    },
    {
      "epoch": 0.36116733874181156,
      "grad_norm": 0.05026973783969879,
      "learning_rate": 0.0001916497983774565,
      "loss": 0.4406,
      "step": 113300
    },
    {
      "epoch": 0.3614861095615307,
      "grad_norm": 0.6391115188598633,
      "learning_rate": 0.00019155416713154076,
      "loss": 0.683,
      "step": 113400
    },
    {
      "epoch": 0.3618048803812499,
      "grad_norm": 6.441114902496338,
      "learning_rate": 0.000191458535885625,
      "loss": 0.5401,
      "step": 113500
    },
    {
      "epoch": 0.36212365120096907,
      "grad_norm": 0.003997810184955597,
      "learning_rate": 0.00019136290463970927,
      "loss": 0.401,
      "step": 113600
    },
    {
      "epoch": 0.3624424220206882,
      "grad_norm": 0.052014369517564774,
      "learning_rate": 0.0001912672733937935,
      "loss": 0.6425,
      "step": 113700
    },
    {
      "epoch": 0.36276119284040736,
      "grad_norm": 7.905956268310547,
      "learning_rate": 0.00019117164214787775,
      "loss": 0.2506,
      "step": 113800
    },
    {
      "epoch": 0.36307996366012657,
      "grad_norm": 0.00013767374912276864,
      "learning_rate": 0.00019107601090196202,
      "loss": 0.394,
      "step": 113900
    },
    {
      "epoch": 0.3633987344798457,
      "grad_norm": 59.703880310058594,
      "learning_rate": 0.00019098037965604626,
      "loss": 0.4555,
      "step": 114000
    },
    {
      "epoch": 0.36371750529956487,
      "grad_norm": 0.006193623412400484,
      "learning_rate": 0.0001908847484101305,
      "loss": 0.5398,
      "step": 114100
    },
    {
      "epoch": 0.364036276119284,
      "grad_norm": 0.4501926004886627,
      "learning_rate": 0.00019078911716421476,
      "loss": 0.4947,
      "step": 114200
    },
    {
      "epoch": 0.3643550469390032,
      "grad_norm": 0.0018235697643831372,
      "learning_rate": 0.000190693485918299,
      "loss": 0.4836,
      "step": 114300
    },
    {
      "epoch": 0.3646738177587224,
      "grad_norm": 0.007916340604424477,
      "learning_rate": 0.0001905978546723833,
      "loss": 0.3139,
      "step": 114400
    },
    {
      "epoch": 0.3649925885784415,
      "grad_norm": 0.0025261249393224716,
      "learning_rate": 0.00019050222342646754,
      "loss": 0.2629,
      "step": 114500
    },
    {
      "epoch": 0.3653113593981607,
      "grad_norm": 0.1862693428993225,
      "learning_rate": 0.00019040659218055175,
      "loss": 0.3013,
      "step": 114600
    },
    {
      "epoch": 0.3656301302178799,
      "grad_norm": 0.0020860922522842884,
      "learning_rate": 0.00019031096093463605,
      "loss": 0.3693,
      "step": 114700
    },
    {
      "epoch": 0.36594890103759903,
      "grad_norm": 0.0005953584332019091,
      "learning_rate": 0.0001902153296887203,
      "loss": 0.3352,
      "step": 114800
    },
    {
      "epoch": 0.3662676718573182,
      "grad_norm": 2.7658168619382195e-05,
      "learning_rate": 0.0001901196984428045,
      "loss": 0.5264,
      "step": 114900
    },
    {
      "epoch": 0.3665864426770373,
      "grad_norm": 0.000309764058329165,
      "learning_rate": 0.0001900240671968888,
      "loss": 0.4466,
      "step": 115000
    },
    {
      "epoch": 0.36690521349675653,
      "grad_norm": 0.0018102905014529824,
      "learning_rate": 0.00018992843595097304,
      "loss": 0.4097,
      "step": 115100
    },
    {
      "epoch": 0.3672239843164757,
      "grad_norm": 0.0002478876558598131,
      "learning_rate": 0.0001898328047050573,
      "loss": 0.354,
      "step": 115200
    },
    {
      "epoch": 0.36754275513619483,
      "grad_norm": 0.7843716740608215,
      "learning_rate": 0.00018973717345914154,
      "loss": 0.4447,
      "step": 115300
    },
    {
      "epoch": 0.367861525955914,
      "grad_norm": 0.010553495958447456,
      "learning_rate": 0.00018964154221322578,
      "loss": 0.3006,
      "step": 115400
    },
    {
      "epoch": 0.36818029677563313,
      "grad_norm": 0.09113702923059464,
      "learning_rate": 0.00018954591096731005,
      "loss": 0.3682,
      "step": 115500
    },
    {
      "epoch": 0.36849906759535234,
      "grad_norm": 0.009514261968433857,
      "learning_rate": 0.0001894502797213943,
      "loss": 0.3881,
      "step": 115600
    },
    {
      "epoch": 0.3688178384150715,
      "grad_norm": 1.1086636781692505,
      "learning_rate": 0.00018935464847547856,
      "loss": 0.4248,
      "step": 115700
    },
    {
      "epoch": 0.36913660923479064,
      "grad_norm": 0.3252594769001007,
      "learning_rate": 0.0001892590172295628,
      "loss": 0.4806,
      "step": 115800
    },
    {
      "epoch": 0.3694553800545098,
      "grad_norm": 90.45477294921875,
      "learning_rate": 0.00018916338598364704,
      "loss": 0.3818,
      "step": 115900
    },
    {
      "epoch": 0.369774150874229,
      "grad_norm": 0.0033936433028429747,
      "learning_rate": 0.0001890677547377313,
      "loss": 0.647,
      "step": 116000
    },
    {
      "epoch": 0.37009292169394814,
      "grad_norm": 0.0255756676197052,
      "learning_rate": 0.00018897212349181555,
      "loss": 0.3298,
      "step": 116100
    },
    {
      "epoch": 0.3704116925136673,
      "grad_norm": 0.0486559122800827,
      "learning_rate": 0.0001888764922458998,
      "loss": 0.3373,
      "step": 116200
    },
    {
      "epoch": 0.37073046333338644,
      "grad_norm": 64.82361602783203,
      "learning_rate": 0.00018878086099998406,
      "loss": 0.4828,
      "step": 116300
    },
    {
      "epoch": 0.37104923415310564,
      "grad_norm": 18.17388153076172,
      "learning_rate": 0.0001886852297540683,
      "loss": 0.2916,
      "step": 116400
    },
    {
      "epoch": 0.3713680049728248,
      "grad_norm": 74.94640350341797,
      "learning_rate": 0.00018858959850815256,
      "loss": 0.3755,
      "step": 116500
    },
    {
      "epoch": 0.37168677579254394,
      "grad_norm": 0.045165419578552246,
      "learning_rate": 0.0001884939672622368,
      "loss": 0.6046,
      "step": 116600
    },
    {
      "epoch": 0.3720055466122631,
      "grad_norm": 0.04394003748893738,
      "learning_rate": 0.00018839833601632104,
      "loss": 0.5167,
      "step": 116700
    },
    {
      "epoch": 0.3723243174319823,
      "grad_norm": 19.242555618286133,
      "learning_rate": 0.0001883027047704053,
      "loss": 0.4042,
      "step": 116800
    },
    {
      "epoch": 0.37264308825170145,
      "grad_norm": 0.6776509881019592,
      "learning_rate": 0.00018820707352448955,
      "loss": 0.3716,
      "step": 116900
    },
    {
      "epoch": 0.3729618590714206,
      "grad_norm": 71.50715637207031,
      "learning_rate": 0.0001881114422785738,
      "loss": 0.4116,
      "step": 117000
    },
    {
      "epoch": 0.37328062989113975,
      "grad_norm": 0.023839283734560013,
      "learning_rate": 0.00018801581103265806,
      "loss": 0.5057,
      "step": 117100
    },
    {
      "epoch": 0.37359940071085895,
      "grad_norm": 79.72604370117188,
      "learning_rate": 0.0001879201797867423,
      "loss": 0.3984,
      "step": 117200
    },
    {
      "epoch": 0.3739181715305781,
      "grad_norm": 10.853580474853516,
      "learning_rate": 0.00018782454854082657,
      "loss": 0.4238,
      "step": 117300
    },
    {
      "epoch": 0.37423694235029725,
      "grad_norm": 0.008277526125311852,
      "learning_rate": 0.0001877289172949108,
      "loss": 0.4216,
      "step": 117400
    },
    {
      "epoch": 0.3745557131700164,
      "grad_norm": 0.0013132337480783463,
      "learning_rate": 0.00018763328604899505,
      "loss": 0.4938,
      "step": 117500
    },
    {
      "epoch": 0.37487448398973555,
      "grad_norm": 0.14847485721111298,
      "learning_rate": 0.00018753765480307932,
      "loss": 0.3493,
      "step": 117600
    },
    {
      "epoch": 0.37519325480945476,
      "grad_norm": 0.00018086725322064012,
      "learning_rate": 0.00018744202355716356,
      "loss": 0.6606,
      "step": 117700
    },
    {
      "epoch": 0.3755120256291739,
      "grad_norm": 0.01740453951060772,
      "learning_rate": 0.00018734639231124782,
      "loss": 0.5006,
      "step": 117800
    },
    {
      "epoch": 0.37583079644889306,
      "grad_norm": 0.00015893417003098875,
      "learning_rate": 0.00018725076106533206,
      "loss": 0.3729,
      "step": 117900
    },
    {
      "epoch": 0.3761495672686122,
      "grad_norm": 0.015575588680803776,
      "learning_rate": 0.0001871551298194163,
      "loss": 0.3771,
      "step": 118000
    },
    {
      "epoch": 0.3764683380883314,
      "grad_norm": 0.0033174820709973574,
      "learning_rate": 0.00018705949857350057,
      "loss": 0.2863,
      "step": 118100
    },
    {
      "epoch": 0.37678710890805056,
      "grad_norm": 0.0026854269672185183,
      "learning_rate": 0.0001869638673275848,
      "loss": 0.2536,
      "step": 118200
    },
    {
      "epoch": 0.3771058797277697,
      "grad_norm": 0.016825569793581963,
      "learning_rate": 0.00018686823608166905,
      "loss": 0.5156,
      "step": 118300
    },
    {
      "epoch": 0.37742465054748886,
      "grad_norm": 0.010325041599571705,
      "learning_rate": 0.00018677260483575332,
      "loss": 0.43,
      "step": 118400
    },
    {
      "epoch": 0.37774342136720807,
      "grad_norm": 0.1506882905960083,
      "learning_rate": 0.00018667697358983756,
      "loss": 0.2205,
      "step": 118500
    },
    {
      "epoch": 0.3780621921869272,
      "grad_norm": 2.7498972485773265e-05,
      "learning_rate": 0.00018658134234392183,
      "loss": 0.465,
      "step": 118600
    },
    {
      "epoch": 0.37838096300664636,
      "grad_norm": 0.07508876919746399,
      "learning_rate": 0.00018648571109800607,
      "loss": 0.459,
      "step": 118700
    },
    {
      "epoch": 0.3786997338263655,
      "grad_norm": 0.0007244648295454681,
      "learning_rate": 0.0001863900798520903,
      "loss": 0.4413,
      "step": 118800
    },
    {
      "epoch": 0.3790185046460847,
      "grad_norm": 0.031283725053071976,
      "learning_rate": 0.00018629444860617458,
      "loss": 0.5333,
      "step": 118900
    },
    {
      "epoch": 0.37933727546580387,
      "grad_norm": 40.72169494628906,
      "learning_rate": 0.00018619881736025882,
      "loss": 0.2336,
      "step": 119000
    },
    {
      "epoch": 0.379656046285523,
      "grad_norm": 0.001832431647926569,
      "learning_rate": 0.00018610318611434306,
      "loss": 0.5186,
      "step": 119100
    },
    {
      "epoch": 0.37997481710524217,
      "grad_norm": 0.004480501636862755,
      "learning_rate": 0.00018600755486842732,
      "loss": 0.3214,
      "step": 119200
    },
    {
      "epoch": 0.3802935879249614,
      "grad_norm": 0.23922903835773468,
      "learning_rate": 0.00018591192362251156,
      "loss": 0.2205,
      "step": 119300
    },
    {
      "epoch": 0.3806123587446805,
      "grad_norm": 0.025746889412403107,
      "learning_rate": 0.00018581629237659583,
      "loss": 0.2436,
      "step": 119400
    },
    {
      "epoch": 0.3809311295643997,
      "grad_norm": 0.0020331363193690777,
      "learning_rate": 0.00018572066113068007,
      "loss": 0.5076,
      "step": 119500
    },
    {
      "epoch": 0.3812499003841188,
      "grad_norm": 0.0019697058014571667,
      "learning_rate": 0.0001856250298847643,
      "loss": 0.3417,
      "step": 119600
    },
    {
      "epoch": 0.38156867120383803,
      "grad_norm": 0.0001588228769833222,
      "learning_rate": 0.00018552939863884858,
      "loss": 0.489,
      "step": 119700
    },
    {
      "epoch": 0.3818874420235572,
      "grad_norm": 0.0009328340529464185,
      "learning_rate": 0.00018543376739293282,
      "loss": 0.6315,
      "step": 119800
    },
    {
      "epoch": 0.3822062128432763,
      "grad_norm": 4.629355430603027,
      "learning_rate": 0.00018533813614701712,
      "loss": 0.4371,
      "step": 119900
    },
    {
      "epoch": 0.3825249836629955,
      "grad_norm": 0.0068344734609127045,
      "learning_rate": 0.00018524250490110133,
      "loss": 0.4189,
      "step": 120000
    },
    {
      "epoch": 0.3828437544827146,
      "grad_norm": 55.37727737426758,
      "learning_rate": 0.00018514687365518557,
      "loss": 0.5793,
      "step": 120100
    },
    {
      "epoch": 0.38316252530243383,
      "grad_norm": 46.262733459472656,
      "learning_rate": 0.00018505124240926986,
      "loss": 0.2711,
      "step": 120200
    },
    {
      "epoch": 0.383481296122153,
      "grad_norm": 0.0008374670287594199,
      "learning_rate": 0.00018495561116335408,
      "loss": 0.5043,
      "step": 120300
    },
    {
      "epoch": 0.38380006694187213,
      "grad_norm": 0.002797208959236741,
      "learning_rate": 0.00018485997991743832,
      "loss": 0.3513,
      "step": 120400
    },
    {
      "epoch": 0.3841188377615913,
      "grad_norm": 0.2865144908428192,
      "learning_rate": 0.0001847643486715226,
      "loss": 0.3489,
      "step": 120500
    },
    {
      "epoch": 0.3844376085813105,
      "grad_norm": 0.1519584059715271,
      "learning_rate": 0.00018466871742560682,
      "loss": 0.3356,
      "step": 120600
    },
    {
      "epoch": 0.38475637940102964,
      "grad_norm": 56.67271423339844,
      "learning_rate": 0.00018457308617969112,
      "loss": 0.6208,
      "step": 120700
    },
    {
      "epoch": 0.3850751502207488,
      "grad_norm": 0.0003055545093957335,
      "learning_rate": 0.00018447745493377536,
      "loss": 0.4355,
      "step": 120800
    },
    {
      "epoch": 0.38539392104046793,
      "grad_norm": 71.43305206298828,
      "learning_rate": 0.0001843818236878596,
      "loss": 0.2954,
      "step": 120900
    },
    {
      "epoch": 0.38571269186018714,
      "grad_norm": 0.0018727709539234638,
      "learning_rate": 0.00018428619244194387,
      "loss": 0.2142,
      "step": 121000
    },
    {
      "epoch": 0.3860314626799063,
      "grad_norm": 11.017290115356445,
      "learning_rate": 0.0001841905611960281,
      "loss": 0.409,
      "step": 121100
    },
    {
      "epoch": 0.38635023349962544,
      "grad_norm": 0.02779899723827839,
      "learning_rate": 0.00018409492995011235,
      "loss": 0.3371,
      "step": 121200
    },
    {
      "epoch": 0.3866690043193446,
      "grad_norm": 8.841663360595703,
      "learning_rate": 0.00018399929870419662,
      "loss": 0.5624,
      "step": 121300
    },
    {
      "epoch": 0.3869877751390638,
      "grad_norm": 0.005523171741515398,
      "learning_rate": 0.00018390366745828086,
      "loss": 0.3948,
      "step": 121400
    },
    {
      "epoch": 0.38730654595878294,
      "grad_norm": 0.01312867272645235,
      "learning_rate": 0.00018380803621236512,
      "loss": 0.2552,
      "step": 121500
    },
    {
      "epoch": 0.3876253167785021,
      "grad_norm": 4.398002056404948e-05,
      "learning_rate": 0.00018371240496644936,
      "loss": 0.3342,
      "step": 121600
    },
    {
      "epoch": 0.38794408759822124,
      "grad_norm": 0.9220010638237,
      "learning_rate": 0.0001836167737205336,
      "loss": 0.4794,
      "step": 121700
    },
    {
      "epoch": 0.38826285841794045,
      "grad_norm": 0.00574966985732317,
      "learning_rate": 0.00018352114247461787,
      "loss": 0.4096,
      "step": 121800
    },
    {
      "epoch": 0.3885816292376596,
      "grad_norm": 0.3197656273841858,
      "learning_rate": 0.0001834255112287021,
      "loss": 0.1975,
      "step": 121900
    },
    {
      "epoch": 0.38890040005737875,
      "grad_norm": 0.00011578582780202851,
      "learning_rate": 0.00018332987998278638,
      "loss": 0.2689,
      "step": 122000
    },
    {
      "epoch": 0.3892191708770979,
      "grad_norm": 0.00018653525330591947,
      "learning_rate": 0.00018323424873687062,
      "loss": 0.4743,
      "step": 122100
    },
    {
      "epoch": 0.38953794169681705,
      "grad_norm": 141.02520751953125,
      "learning_rate": 0.00018313861749095486,
      "loss": 0.3479,
      "step": 122200
    },
    {
      "epoch": 0.38985671251653625,
      "grad_norm": 0.203144371509552,
      "learning_rate": 0.00018304298624503913,
      "loss": 0.3656,
      "step": 122300
    },
    {
      "epoch": 0.3901754833362554,
      "grad_norm": 2.01198410987854,
      "learning_rate": 0.00018294735499912337,
      "loss": 0.4308,
      "step": 122400
    },
    {
      "epoch": 0.39049425415597455,
      "grad_norm": 0.002805958269163966,
      "learning_rate": 0.0001828517237532076,
      "loss": 0.5946,
      "step": 122500
    },
    {
      "epoch": 0.3908130249756937,
      "grad_norm": 0.0020676087588071823,
      "learning_rate": 0.00018275609250729188,
      "loss": 0.305,
      "step": 122600
    },
    {
      "epoch": 0.3911317957954129,
      "grad_norm": 29.527652740478516,
      "learning_rate": 0.00018266046126137612,
      "loss": 0.3532,
      "step": 122700
    },
    {
      "epoch": 0.39145056661513206,
      "grad_norm": 0.000794456631410867,
      "learning_rate": 0.00018256483001546038,
      "loss": 0.6204,
      "step": 122800
    },
    {
      "epoch": 0.3917693374348512,
      "grad_norm": 22.725719451904297,
      "learning_rate": 0.00018246919876954462,
      "loss": 0.2798,
      "step": 122900
    },
    {
      "epoch": 0.39208810825457036,
      "grad_norm": 23.34838104248047,
      "learning_rate": 0.00018237356752362886,
      "loss": 0.2886,
      "step": 123000
    },
    {
      "epoch": 0.39240687907428956,
      "grad_norm": 30.91170310974121,
      "learning_rate": 0.00018227793627771313,
      "loss": 0.4697,
      "step": 123100
    },
    {
      "epoch": 0.3927256498940087,
      "grad_norm": 0.248707115650177,
      "learning_rate": 0.00018218230503179737,
      "loss": 0.6574,
      "step": 123200
    },
    {
      "epoch": 0.39304442071372786,
      "grad_norm": 0.001353871077299118,
      "learning_rate": 0.0001820866737858816,
      "loss": 0.3795,
      "step": 123300
    },
    {
      "epoch": 0.393363191533447,
      "grad_norm": 0.00480792997404933,
      "learning_rate": 0.00018199104253996588,
      "loss": 0.4765,
      "step": 123400
    },
    {
      "epoch": 0.3936819623531662,
      "grad_norm": 0.018675975501537323,
      "learning_rate": 0.00018189541129405012,
      "loss": 0.3483,
      "step": 123500
    },
    {
      "epoch": 0.39400073317288536,
      "grad_norm": 3.444901943206787,
      "learning_rate": 0.0001817997800481344,
      "loss": 0.3434,
      "step": 123600
    },
    {
      "epoch": 0.3943195039926045,
      "grad_norm": 0.0007677217945456505,
      "learning_rate": 0.00018170414880221863,
      "loss": 0.2593,
      "step": 123700
    },
    {
      "epoch": 0.39463827481232366,
      "grad_norm": 0.018584026023745537,
      "learning_rate": 0.00018160851755630287,
      "loss": 0.4168,
      "step": 123800
    },
    {
      "epoch": 0.39495704563204287,
      "grad_norm": 0.021589066833257675,
      "learning_rate": 0.00018151288631038714,
      "loss": 0.3984,
      "step": 123900
    },
    {
      "epoch": 0.395275816451762,
      "grad_norm": 0.0003569882537703961,
      "learning_rate": 0.00018141725506447138,
      "loss": 0.3288,
      "step": 124000
    },
    {
      "epoch": 0.39559458727148117,
      "grad_norm": 42.311973571777344,
      "learning_rate": 0.00018132162381855562,
      "loss": 0.4403,
      "step": 124100
    },
    {
      "epoch": 0.3959133580912003,
      "grad_norm": 32.225894927978516,
      "learning_rate": 0.00018122599257263988,
      "loss": 0.3458,
      "step": 124200
    },
    {
      "epoch": 0.39623212891091947,
      "grad_norm": 0.002239230554550886,
      "learning_rate": 0.00018113036132672412,
      "loss": 0.4239,
      "step": 124300
    },
    {
      "epoch": 0.3965508997306387,
      "grad_norm": 0.0004315692640375346,
      "learning_rate": 0.0001810347300808084,
      "loss": 0.3947,
      "step": 124400
    },
    {
      "epoch": 0.3968696705503578,
      "grad_norm": 0.004569885786622763,
      "learning_rate": 0.00018093909883489263,
      "loss": 0.2989,
      "step": 124500
    },
    {
      "epoch": 0.397188441370077,
      "grad_norm": 0.08480221778154373,
      "learning_rate": 0.00018084346758897687,
      "loss": 0.3315,
      "step": 124600
    },
    {
      "epoch": 0.3975072121897961,
      "grad_norm": 94.23230743408203,
      "learning_rate": 0.00018074783634306114,
      "loss": 0.3435,
      "step": 124700
    },
    {
      "epoch": 0.3978259830095153,
      "grad_norm": 0.02275817096233368,
      "learning_rate": 0.00018065220509714538,
      "loss": 0.3637,
      "step": 124800
    },
    {
      "epoch": 0.3981447538292345,
      "grad_norm": 0.21468980610370636,
      "learning_rate": 0.00018055657385122965,
      "loss": 0.4121,
      "step": 124900
    },
    {
      "epoch": 0.3984635246489536,
      "grad_norm": 53.527217864990234,
      "learning_rate": 0.0001804609426053139,
      "loss": 0.3955,
      "step": 125000
    },
    {
      "epoch": 0.3987822954686728,
      "grad_norm": 0.00013375158596318215,
      "learning_rate": 0.00018036531135939813,
      "loss": 0.3442,
      "step": 125100
    },
    {
      "epoch": 0.399101066288392,
      "grad_norm": 0.008231248706579208,
      "learning_rate": 0.0001802696801134824,
      "loss": 0.2252,
      "step": 125200
    },
    {
      "epoch": 0.39941983710811113,
      "grad_norm": 0.0002641272731125355,
      "learning_rate": 0.00018017404886756664,
      "loss": 0.2641,
      "step": 125300
    },
    {
      "epoch": 0.3997386079278303,
      "grad_norm": 0.0001139656305895187,
      "learning_rate": 0.00018007841762165088,
      "loss": 0.4346,
      "step": 125400
    },
    {
      "epoch": 0.40005737874754943,
      "grad_norm": 2.6289069652557373,
      "learning_rate": 0.00017998278637573514,
      "loss": 0.3089,
      "step": 125500
    },
    {
      "epoch": 0.40037614956726864,
      "grad_norm": 0.0006188040133565664,
      "learning_rate": 0.00017988715512981938,
      "loss": 0.283,
      "step": 125600
    },
    {
      "epoch": 0.4006949203869878,
      "grad_norm": 0.9431567192077637,
      "learning_rate": 0.00017979152388390365,
      "loss": 0.4475,
      "step": 125700
    },
    {
      "epoch": 0.40101369120670693,
      "grad_norm": 0.5588090419769287,
      "learning_rate": 0.0001796958926379879,
      "loss": 0.2146,
      "step": 125800
    },
    {
      "epoch": 0.4013324620264261,
      "grad_norm": 0.005532098468393087,
      "learning_rate": 0.00017960026139207213,
      "loss": 0.5995,
      "step": 125900
    },
    {
      "epoch": 0.4016512328461453,
      "grad_norm": 0.0002898516249842942,
      "learning_rate": 0.00017950463014615643,
      "loss": 0.1232,
      "step": 126000
    },
    {
      "epoch": 0.40197000366586444,
      "grad_norm": 0.014959597028791904,
      "learning_rate": 0.00017940899890024064,
      "loss": 0.3229,
      "step": 126100
    },
    {
      "epoch": 0.4022887744855836,
      "grad_norm": 0.00019862169574480504,
      "learning_rate": 0.00017931336765432488,
      "loss": 0.4693,
      "step": 126200
    },
    {
      "epoch": 0.40260754530530274,
      "grad_norm": 0.0028062262572348118,
      "learning_rate": 0.00017921773640840918,
      "loss": 0.2732,
      "step": 126300
    },
    {
      "epoch": 0.4029263161250219,
      "grad_norm": 0.22904935479164124,
      "learning_rate": 0.0001791221051624934,
      "loss": 0.3926,
      "step": 126400
    },
    {
      "epoch": 0.4032450869447411,
      "grad_norm": 0.005939082242548466,
      "learning_rate": 0.00017902647391657768,
      "loss": 0.197,
      "step": 126500
    },
    {
      "epoch": 0.40356385776446024,
      "grad_norm": 0.009868942201137543,
      "learning_rate": 0.00017893084267066192,
      "loss": 0.3965,
      "step": 126600
    },
    {
      "epoch": 0.4038826285841794,
      "grad_norm": 0.00013027619570493698,
      "learning_rate": 0.00017883521142474614,
      "loss": 0.3709,
      "step": 126700
    },
    {
      "epoch": 0.40420139940389854,
      "grad_norm": 0.05472255498170853,
      "learning_rate": 0.00017873958017883043,
      "loss": 0.1396,
      "step": 126800
    },
    {
      "epoch": 0.40452017022361775,
      "grad_norm": 29.22572135925293,
      "learning_rate": 0.00017864394893291467,
      "loss": 0.2599,
      "step": 126900
    },
    {
      "epoch": 0.4048389410433369,
      "grad_norm": 6.948638439178467,
      "learning_rate": 0.00017854831768699894,
      "loss": 0.5817,
      "step": 127000
    },
    {
      "epoch": 0.40515771186305605,
      "grad_norm": 0.0007905818056315184,
      "learning_rate": 0.00017845268644108318,
      "loss": 0.4489,
      "step": 127100
    },
    {
      "epoch": 0.4054764826827752,
      "grad_norm": 0.00016661178960930556,
      "learning_rate": 0.00017835705519516742,
      "loss": 0.3484,
      "step": 127200
    },
    {
      "epoch": 0.4057952535024944,
      "grad_norm": 18.784029006958008,
      "learning_rate": 0.0001782614239492517,
      "loss": 0.3622,
      "step": 127300
    },
    {
      "epoch": 0.40611402432221355,
      "grad_norm": 0.05317780002951622,
      "learning_rate": 0.00017816579270333593,
      "loss": 0.3373,
      "step": 127400
    },
    {
      "epoch": 0.4064327951419327,
      "grad_norm": 20.758520126342773,
      "learning_rate": 0.00017807016145742017,
      "loss": 0.2648,
      "step": 127500
    },
    {
      "epoch": 0.40675156596165185,
      "grad_norm": 0.000886421708855778,
      "learning_rate": 0.00017797453021150444,
      "loss": 0.385,
      "step": 127600
    },
    {
      "epoch": 0.40707033678137106,
      "grad_norm": 0.007652051281183958,
      "learning_rate": 0.00017787889896558868,
      "loss": 0.4334,
      "step": 127700
    },
    {
      "epoch": 0.4073891076010902,
      "grad_norm": 0.0026430028956383467,
      "learning_rate": 0.00017778326771967294,
      "loss": 0.4579,
      "step": 127800
    },
    {
      "epoch": 0.40770787842080936,
      "grad_norm": 0.0016459018224850297,
      "learning_rate": 0.00017768763647375718,
      "loss": 0.4022,
      "step": 127900
    },
    {
      "epoch": 0.4080266492405285,
      "grad_norm": 0.022719891741871834,
      "learning_rate": 0.00017759200522784142,
      "loss": 0.5553,
      "step": 128000
    },
    {
      "epoch": 0.4083454200602477,
      "grad_norm": 0.00011982645810348913,
      "learning_rate": 0.0001774963739819257,
      "loss": 0.4714,
      "step": 128100
    },
    {
      "epoch": 0.40866419087996686,
      "grad_norm": 0.003977153915911913,
      "learning_rate": 0.00017740074273600993,
      "loss": 0.2811,
      "step": 128200
    },
    {
      "epoch": 0.408982961699686,
      "grad_norm": 2.2474780082702637,
      "learning_rate": 0.00017730511149009417,
      "loss": 0.5759,
      "step": 128300
    },
    {
      "epoch": 0.40930173251940516,
      "grad_norm": 69.37479400634766,
      "learning_rate": 0.00017720948024417844,
      "loss": 0.2715,
      "step": 128400
    },
    {
      "epoch": 0.4096205033391243,
      "grad_norm": 0.00014052330516278744,
      "learning_rate": 0.00017711384899826268,
      "loss": 0.49,
      "step": 128500
    },
    {
      "epoch": 0.4099392741588435,
      "grad_norm": 0.0057188537903130054,
      "learning_rate": 0.00017701821775234695,
      "loss": 0.3085,
      "step": 128600
    },
    {
      "epoch": 0.41025804497856266,
      "grad_norm": 4.531941890716553,
      "learning_rate": 0.0001769225865064312,
      "loss": 0.5341,
      "step": 128700
    },
    {
      "epoch": 0.4105768157982818,
      "grad_norm": 0.3239952027797699,
      "learning_rate": 0.00017682695526051543,
      "loss": 0.3207,
      "step": 128800
    },
    {
      "epoch": 0.41089558661800096,
      "grad_norm": 3.393298538867384e-05,
      "learning_rate": 0.0001767313240145997,
      "loss": 0.3991,
      "step": 128900
    },
    {
      "epoch": 0.41121435743772017,
      "grad_norm": 42.51802062988281,
      "learning_rate": 0.00017663569276868394,
      "loss": 0.4084,
      "step": 129000
    },
    {
      "epoch": 0.4115331282574393,
      "grad_norm": 3.981122426921502e-05,
      "learning_rate": 0.0001765400615227682,
      "loss": 0.2501,
      "step": 129100
    },
    {
      "epoch": 0.41185189907715847,
      "grad_norm": 0.029846137389540672,
      "learning_rate": 0.00017644443027685244,
      "loss": 0.5131,
      "step": 129200
    },
    {
      "epoch": 0.4121706698968776,
      "grad_norm": 0.02756827510893345,
      "learning_rate": 0.00017634879903093668,
      "loss": 0.4111,
      "step": 129300
    },
    {
      "epoch": 0.4124894407165968,
      "grad_norm": 0.0001081384252756834,
      "learning_rate": 0.00017625316778502095,
      "loss": 0.2576,
      "step": 129400
    },
    {
      "epoch": 0.41280821153631597,
      "grad_norm": 0.0007253204239532351,
      "learning_rate": 0.0001761575365391052,
      "loss": 0.3509,
      "step": 129500
    },
    {
      "epoch": 0.4131269823560351,
      "grad_norm": 0.5197455286979675,
      "learning_rate": 0.00017606190529318943,
      "loss": 0.3848,
      "step": 129600
    },
    {
      "epoch": 0.41344575317575427,
      "grad_norm": 32.683067321777344,
      "learning_rate": 0.0001759662740472737,
      "loss": 0.3074,
      "step": 129700
    },
    {
      "epoch": 0.4137645239954735,
      "grad_norm": 0.002928084461018443,
      "learning_rate": 0.00017587064280135794,
      "loss": 0.3186,
      "step": 129800
    },
    {
      "epoch": 0.4140832948151926,
      "grad_norm": 0.1541971117258072,
      "learning_rate": 0.0001757750115554422,
      "loss": 0.5612,
      "step": 129900
    },
    {
      "epoch": 0.4144020656349118,
      "grad_norm": 0.0016362819587811828,
      "learning_rate": 0.00017567938030952645,
      "loss": 0.2653,
      "step": 130000
    },
    {
      "epoch": 0.4147208364546309,
      "grad_norm": 0.006728681270033121,
      "learning_rate": 0.0001755837490636107,
      "loss": 0.4105,
      "step": 130100
    },
    {
      "epoch": 0.41503960727435013,
      "grad_norm": 0.000814383034594357,
      "learning_rate": 0.00017548811781769496,
      "loss": 0.6355,
      "step": 130200
    },
    {
      "epoch": 0.4153583780940693,
      "grad_norm": 0.0006379805854521692,
      "learning_rate": 0.0001753924865717792,
      "loss": 0.3914,
      "step": 130300
    },
    {
      "epoch": 0.41567714891378843,
      "grad_norm": 0.0006638635531999171,
      "learning_rate": 0.00017529685532586344,
      "loss": 0.3893,
      "step": 130400
    },
    {
      "epoch": 0.4159959197335076,
      "grad_norm": 0.00858580507338047,
      "learning_rate": 0.0001752012240799477,
      "loss": 0.3454,
      "step": 130500
    },
    {
      "epoch": 0.4163146905532268,
      "grad_norm": 0.0027123219333589077,
      "learning_rate": 0.00017510559283403194,
      "loss": 0.4249,
      "step": 130600
    },
    {
      "epoch": 0.41663346137294593,
      "grad_norm": 34.01747131347656,
      "learning_rate": 0.0001750099615881162,
      "loss": 0.4578,
      "step": 130700
    },
    {
      "epoch": 0.4169522321926651,
      "grad_norm": 0.0024846906308084726,
      "learning_rate": 0.00017491433034220045,
      "loss": 0.3929,
      "step": 130800
    },
    {
      "epoch": 0.41727100301238423,
      "grad_norm": 0.44829463958740234,
      "learning_rate": 0.0001748186990962847,
      "loss": 0.2936,
      "step": 130900
    },
    {
      "epoch": 0.4175897738321034,
      "grad_norm": 44.86738967895508,
      "learning_rate": 0.00017472306785036896,
      "loss": 0.2336,
      "step": 131000
    },
    {
      "epoch": 0.4179085446518226,
      "grad_norm": 17.213293075561523,
      "learning_rate": 0.0001746274366044532,
      "loss": 0.3699,
      "step": 131100
    },
    {
      "epoch": 0.41822731547154174,
      "grad_norm": 0.0015976015711203218,
      "learning_rate": 0.00017453180535853747,
      "loss": 0.4778,
      "step": 131200
    },
    {
      "epoch": 0.4185460862912609,
      "grad_norm": 5.896491050720215,
      "learning_rate": 0.0001744361741126217,
      "loss": 0.2961,
      "step": 131300
    },
    {
      "epoch": 0.41886485711098004,
      "grad_norm": 0.0006487462669610977,
      "learning_rate": 0.00017434054286670595,
      "loss": 0.379,
      "step": 131400
    },
    {
      "epoch": 0.41918362793069924,
      "grad_norm": 0.6867986917495728,
      "learning_rate": 0.00017424491162079022,
      "loss": 0.4916,
      "step": 131500
    },
    {
      "epoch": 0.4195023987504184,
      "grad_norm": 0.7987284660339355,
      "learning_rate": 0.00017414928037487446,
      "loss": 0.3407,
      "step": 131600
    },
    {
      "epoch": 0.41982116957013754,
      "grad_norm": 4.750200271606445,
      "learning_rate": 0.0001740536491289587,
      "loss": 0.3576,
      "step": 131700
    },
    {
      "epoch": 0.4201399403898567,
      "grad_norm": 0.0019335179822519422,
      "learning_rate": 0.00017395801788304296,
      "loss": 0.4696,
      "step": 131800
    },
    {
      "epoch": 0.4204587112095759,
      "grad_norm": 109.70124816894531,
      "learning_rate": 0.0001738623866371272,
      "loss": 0.3357,
      "step": 131900
    },
    {
      "epoch": 0.42077748202929505,
      "grad_norm": 0.0031749182380735874,
      "learning_rate": 0.0001737667553912115,
      "loss": 0.3354,
      "step": 132000
    },
    {
      "epoch": 0.4210962528490142,
      "grad_norm": 0.000404618214815855,
      "learning_rate": 0.0001736711241452957,
      "loss": 0.5101,
      "step": 132100
    },
    {
      "epoch": 0.42141502366873335,
      "grad_norm": 0.0012790120672434568,
      "learning_rate": 0.00017357549289937995,
      "loss": 0.3064,
      "step": 132200
    },
    {
      "epoch": 0.42173379448845255,
      "grad_norm": 39.000858306884766,
      "learning_rate": 0.00017347986165346425,
      "loss": 0.3097,
      "step": 132300
    },
    {
      "epoch": 0.4220525653081717,
      "grad_norm": 0.1534779816865921,
      "learning_rate": 0.0001733842304075485,
      "loss": 0.4648,
      "step": 132400
    },
    {
      "epoch": 0.42237133612789085,
      "grad_norm": 51.56775665283203,
      "learning_rate": 0.0001732885991616327,
      "loss": 0.5763,
      "step": 132500
    },
    {
      "epoch": 0.42269010694761,
      "grad_norm": 22.34882354736328,
      "learning_rate": 0.000173192967915717,
      "loss": 0.4046,
      "step": 132600
    },
    {
      "epoch": 0.4230088777673292,
      "grad_norm": 0.004938632249832153,
      "learning_rate": 0.00017309733666980124,
      "loss": 0.3485,
      "step": 132700
    },
    {
      "epoch": 0.42332764858704836,
      "grad_norm": 0.0009047873900271952,
      "learning_rate": 0.0001730017054238855,
      "loss": 0.2954,
      "step": 132800
    },
    {
      "epoch": 0.4236464194067675,
      "grad_norm": 0.00030295716715045273,
      "learning_rate": 0.00017290607417796974,
      "loss": 0.1832,
      "step": 132900
    },
    {
      "epoch": 0.42396519022648665,
      "grad_norm": 0.0012270063161849976,
      "learning_rate": 0.00017281044293205398,
      "loss": 0.2894,
      "step": 133000
    },
    {
      "epoch": 0.4242839610462058,
      "grad_norm": 0.000797214568592608,
      "learning_rate": 0.00017271481168613825,
      "loss": 0.3292,
      "step": 133100
    },
    {
      "epoch": 0.424602731865925,
      "grad_norm": 0.0013083403464406729,
      "learning_rate": 0.0001726191804402225,
      "loss": 0.3123,
      "step": 133200
    },
    {
      "epoch": 0.42492150268564416,
      "grad_norm": 10.000625610351562,
      "learning_rate": 0.00017252354919430676,
      "loss": 0.3521,
      "step": 133300
    },
    {
      "epoch": 0.4252402735053633,
      "grad_norm": 0.0693146139383316,
      "learning_rate": 0.000172427917948391,
      "loss": 0.3313,
      "step": 133400
    },
    {
      "epoch": 0.42555904432508246,
      "grad_norm": 0.20261764526367188,
      "learning_rate": 0.00017233228670247524,
      "loss": 0.757,
      "step": 133500
    },
    {
      "epoch": 0.42587781514480166,
      "grad_norm": 24.03373908996582,
      "learning_rate": 0.0001722366554565595,
      "loss": 0.3558,
      "step": 133600
    },
    {
      "epoch": 0.4261965859645208,
      "grad_norm": 0.009353646077215672,
      "learning_rate": 0.00017214102421064375,
      "loss": 0.2653,
      "step": 133700
    },
    {
      "epoch": 0.42651535678423996,
      "grad_norm": 48.40485382080078,
      "learning_rate": 0.000172045392964728,
      "loss": 0.5355,
      "step": 133800
    },
    {
      "epoch": 0.4268341276039591,
      "grad_norm": 8.768005500314757e-06,
      "learning_rate": 0.00017194976171881226,
      "loss": 0.3167,
      "step": 133900
    },
    {
      "epoch": 0.4271528984236783,
      "grad_norm": 0.03361266106367111,
      "learning_rate": 0.0001718541304728965,
      "loss": 0.579,
      "step": 134000
    },
    {
      "epoch": 0.42747166924339747,
      "grad_norm": 1.9326152801513672,
      "learning_rate": 0.00017175849922698076,
      "loss": 0.3407,
      "step": 134100
    },
    {
      "epoch": 0.4277904400631166,
      "grad_norm": 30.598674774169922,
      "learning_rate": 0.000171662867981065,
      "loss": 0.2471,
      "step": 134200
    },
    {
      "epoch": 0.42810921088283577,
      "grad_norm": 0.028708573430776596,
      "learning_rate": 0.00017156723673514924,
      "loss": 0.3693,
      "step": 134300
    },
    {
      "epoch": 0.42842798170255497,
      "grad_norm": 0.009430385194718838,
      "learning_rate": 0.0001714716054892335,
      "loss": 0.3362,
      "step": 134400
    },
    {
      "epoch": 0.4287467525222741,
      "grad_norm": 0.011699867434799671,
      "learning_rate": 0.00017137597424331775,
      "loss": 0.3563,
      "step": 134500
    },
    {
      "epoch": 0.42906552334199327,
      "grad_norm": 5.6600354582769796e-05,
      "learning_rate": 0.000171280342997402,
      "loss": 0.3801,
      "step": 134600
    },
    {
      "epoch": 0.4293842941617124,
      "grad_norm": 0.07459764927625656,
      "learning_rate": 0.00017118471175148626,
      "loss": 0.4054,
      "step": 134700
    },
    {
      "epoch": 0.4297030649814316,
      "grad_norm": 28.944135665893555,
      "learning_rate": 0.0001710890805055705,
      "loss": 0.3102,
      "step": 134800
    },
    {
      "epoch": 0.4300218358011508,
      "grad_norm": 0.0025223230477422476,
      "learning_rate": 0.00017099344925965477,
      "loss": 0.1953,
      "step": 134900
    },
    {
      "epoch": 0.4303406066208699,
      "grad_norm": 0.0024308054707944393,
      "learning_rate": 0.000170897818013739,
      "loss": 0.2845,
      "step": 135000
    },
    {
      "epoch": 0.4306593774405891,
      "grad_norm": 7.452362478943542e-05,
      "learning_rate": 0.00017080218676782325,
      "loss": 0.4667,
      "step": 135100
    },
    {
      "epoch": 0.4309781482603082,
      "grad_norm": 0.0059584672562778,
      "learning_rate": 0.00017070655552190752,
      "loss": 0.4234,
      "step": 135200
    },
    {
      "epoch": 0.43129691908002743,
      "grad_norm": 131.75721740722656,
      "learning_rate": 0.00017061092427599176,
      "loss": 0.4353,
      "step": 135300
    },
    {
      "epoch": 0.4316156898997466,
      "grad_norm": 0.0395069494843483,
      "learning_rate": 0.00017051529303007602,
      "loss": 0.4211,
      "step": 135400
    },
    {
      "epoch": 0.43193446071946573,
      "grad_norm": 35.265018463134766,
      "learning_rate": 0.00017041966178416026,
      "loss": 0.6125,
      "step": 135500
    },
    {
      "epoch": 0.4322532315391849,
      "grad_norm": 18.988569259643555,
      "learning_rate": 0.0001703240305382445,
      "loss": 0.5072,
      "step": 135600
    },
    {
      "epoch": 0.4325720023589041,
      "grad_norm": 0.5955209732055664,
      "learning_rate": 0.00017022839929232877,
      "loss": 0.5339,
      "step": 135700
    },
    {
      "epoch": 0.43289077317862323,
      "grad_norm": 0.3839466869831085,
      "learning_rate": 0.000170132768046413,
      "loss": 0.3047,
      "step": 135800
    },
    {
      "epoch": 0.4332095439983424,
      "grad_norm": 0.013614659197628498,
      "learning_rate": 0.00017003713680049725,
      "loss": 0.405,
      "step": 135900
    },
    {
      "epoch": 0.43352831481806153,
      "grad_norm": 17.083784103393555,
      "learning_rate": 0.00016994150555458152,
      "loss": 0.3136,
      "step": 136000
    },
    {
      "epoch": 0.43384708563778074,
      "grad_norm": 3.466819543973543e-05,
      "learning_rate": 0.00016984587430866576,
      "loss": 0.4373,
      "step": 136100
    },
    {
      "epoch": 0.4341658564574999,
      "grad_norm": 40.45732116699219,
      "learning_rate": 0.00016975024306275003,
      "loss": 0.3625,
      "step": 136200
    },
    {
      "epoch": 0.43448462727721904,
      "grad_norm": 0.013477248139679432,
      "learning_rate": 0.00016965461181683427,
      "loss": 0.2429,
      "step": 136300
    },
    {
      "epoch": 0.4348033980969382,
      "grad_norm": 0.026341013610363007,
      "learning_rate": 0.0001695589805709185,
      "loss": 0.29,
      "step": 136400
    },
    {
      "epoch": 0.4351221689166574,
      "grad_norm": 0.0953734964132309,
      "learning_rate": 0.00016946334932500278,
      "loss": 0.2757,
      "step": 136500
    },
    {
      "epoch": 0.43544093973637654,
      "grad_norm": 0.00017446809215471148,
      "learning_rate": 0.00016936771807908702,
      "loss": 0.295,
      "step": 136600
    },
    {
      "epoch": 0.4357597105560957,
      "grad_norm": 0.0004510782891884446,
      "learning_rate": 0.00016927208683317126,
      "loss": 0.355,
      "step": 136700
    },
    {
      "epoch": 0.43607848137581484,
      "grad_norm": 0.07825270295143127,
      "learning_rate": 0.00016917645558725552,
      "loss": 0.3714,
      "step": 136800
    },
    {
      "epoch": 0.43639725219553405,
      "grad_norm": 0.00023834644525777549,
      "learning_rate": 0.00016908082434133976,
      "loss": 0.3283,
      "step": 136900
    },
    {
      "epoch": 0.4367160230152532,
      "grad_norm": 73.4450912475586,
      "learning_rate": 0.00016898519309542403,
      "loss": 0.641,
      "step": 137000
    },
    {
      "epoch": 0.43703479383497235,
      "grad_norm": 0.0247100368142128,
      "learning_rate": 0.00016888956184950827,
      "loss": 0.4339,
      "step": 137100
    },
    {
      "epoch": 0.4373535646546915,
      "grad_norm": 0.0001352130639133975,
      "learning_rate": 0.0001687939306035925,
      "loss": 0.2903,
      "step": 137200
    },
    {
      "epoch": 0.43767233547441065,
      "grad_norm": 6.440046787261963,
      "learning_rate": 0.00016869829935767678,
      "loss": 0.3618,
      "step": 137300
    },
    {
      "epoch": 0.43799110629412985,
      "grad_norm": 21.423845291137695,
      "learning_rate": 0.00016860266811176102,
      "loss": 0.3734,
      "step": 137400
    },
    {
      "epoch": 0.438309877113849,
      "grad_norm": 0.00010400934843346477,
      "learning_rate": 0.00016850703686584531,
      "loss": 0.2546,
      "step": 137500
    },
    {
      "epoch": 0.43862864793356815,
      "grad_norm": 0.013126988895237446,
      "learning_rate": 0.00016841140561992953,
      "loss": 0.3195,
      "step": 137600
    },
    {
      "epoch": 0.4389474187532873,
      "grad_norm": 0.7777475118637085,
      "learning_rate": 0.00016831577437401377,
      "loss": 0.4291,
      "step": 137700
    },
    {
      "epoch": 0.4392661895730065,
      "grad_norm": 56.75590133666992,
      "learning_rate": 0.00016822014312809806,
      "loss": 0.2605,
      "step": 137800
    },
    {
      "epoch": 0.43958496039272565,
      "grad_norm": 97.48070526123047,
      "learning_rate": 0.00016812451188218228,
      "loss": 0.349,
      "step": 137900
    },
    {
      "epoch": 0.4399037312124448,
      "grad_norm": 0.017304010689258575,
      "learning_rate": 0.00016802888063626652,
      "loss": 0.4981,
      "step": 138000
    },
    {
      "epoch": 0.44022250203216395,
      "grad_norm": 6.594624042510986,
      "learning_rate": 0.0001679332493903508,
      "loss": 0.3935,
      "step": 138100
    },
    {
      "epoch": 0.44054127285188316,
      "grad_norm": 0.00199578027240932,
      "learning_rate": 0.00016783761814443502,
      "loss": 0.4261,
      "step": 138200
    },
    {
      "epoch": 0.4408600436716023,
      "grad_norm": 0.008778315968811512,
      "learning_rate": 0.00016774198689851932,
      "loss": 0.4181,
      "step": 138300
    },
    {
      "epoch": 0.44117881449132146,
      "grad_norm": 3.161195755004883,
      "learning_rate": 0.00016764635565260356,
      "loss": 0.4681,
      "step": 138400
    },
    {
      "epoch": 0.4414975853110406,
      "grad_norm": 0.001735163852572441,
      "learning_rate": 0.0001675507244066878,
      "loss": 0.4048,
      "step": 138500
    },
    {
      "epoch": 0.4418163561307598,
      "grad_norm": 32.177581787109375,
      "learning_rate": 0.00016745509316077207,
      "loss": 0.3616,
      "step": 138600
    },
    {
      "epoch": 0.44213512695047896,
      "grad_norm": 10.893537521362305,
      "learning_rate": 0.0001673594619148563,
      "loss": 0.4948,
      "step": 138700
    },
    {
      "epoch": 0.4424538977701981,
      "grad_norm": 0.002735396148636937,
      "learning_rate": 0.00016726383066894055,
      "loss": 0.2059,
      "step": 138800
    },
    {
      "epoch": 0.44277266858991726,
      "grad_norm": 0.00027962028980255127,
      "learning_rate": 0.00016716819942302482,
      "loss": 0.1878,
      "step": 138900
    },
    {
      "epoch": 0.44309143940963647,
      "grad_norm": 0.0003549513057805598,
      "learning_rate": 0.00016707256817710906,
      "loss": 0.3604,
      "step": 139000
    },
    {
      "epoch": 0.4434102102293556,
      "grad_norm": 0.005941964220255613,
      "learning_rate": 0.00016697693693119332,
      "loss": 0.4861,
      "step": 139100
    },
    {
      "epoch": 0.44372898104907477,
      "grad_norm": 35.762115478515625,
      "learning_rate": 0.00016688130568527756,
      "loss": 0.2851,
      "step": 139200
    },
    {
      "epoch": 0.4440477518687939,
      "grad_norm": 0.00029030561563558877,
      "learning_rate": 0.0001667856744393618,
      "loss": 0.3778,
      "step": 139300
    },
    {
      "epoch": 0.44436652268851307,
      "grad_norm": 100.33261108398438,
      "learning_rate": 0.00016669004319344607,
      "loss": 0.4043,
      "step": 139400
    },
    {
      "epoch": 0.44468529350823227,
      "grad_norm": 24.962295532226562,
      "learning_rate": 0.0001665944119475303,
      "loss": 0.2946,
      "step": 139500
    },
    {
      "epoch": 0.4450040643279514,
      "grad_norm": 303.1590270996094,
      "learning_rate": 0.00016649878070161458,
      "loss": 0.3826,
      "step": 139600
    },
    {
      "epoch": 0.44532283514767057,
      "grad_norm": 57.766082763671875,
      "learning_rate": 0.00016640314945569882,
      "loss": 0.4307,
      "step": 139700
    },
    {
      "epoch": 0.4456416059673897,
      "grad_norm": 0.0029067371506243944,
      "learning_rate": 0.00016630751820978306,
      "loss": 0.3036,
      "step": 139800
    },
    {
      "epoch": 0.4459603767871089,
      "grad_norm": 14.920245170593262,
      "learning_rate": 0.00016621188696386733,
      "loss": 0.4337,
      "step": 139900
    },
    {
      "epoch": 0.4462791476068281,
      "grad_norm": 0.0007315041730180383,
      "learning_rate": 0.00016611625571795157,
      "loss": 0.3919,
      "step": 140000
    },
    {
      "epoch": 0.4465979184265472,
      "grad_norm": 45.625274658203125,
      "learning_rate": 0.0001660206244720358,
      "loss": 0.4316,
      "step": 140100
    },
    {
      "epoch": 0.4469166892462664,
      "grad_norm": 0.00030803418485447764,
      "learning_rate": 0.00016592499322612008,
      "loss": 0.4195,
      "step": 140200
    },
    {
      "epoch": 0.4472354600659856,
      "grad_norm": 0.025181636214256287,
      "learning_rate": 0.00016582936198020432,
      "loss": 0.486,
      "step": 140300
    },
    {
      "epoch": 0.44755423088570473,
      "grad_norm": 0.010193215683102608,
      "learning_rate": 0.00016573373073428858,
      "loss": 0.2243,
      "step": 140400
    },
    {
      "epoch": 0.4478730017054239,
      "grad_norm": 0.0007542280945926905,
      "learning_rate": 0.00016563809948837282,
      "loss": 0.3176,
      "step": 140500
    },
    {
      "epoch": 0.44819177252514303,
      "grad_norm": 0.0040650335140526295,
      "learning_rate": 0.00016554246824245706,
      "loss": 0.3101,
      "step": 140600
    },
    {
      "epoch": 0.44851054334486223,
      "grad_norm": 33.53394317626953,
      "learning_rate": 0.00016544683699654133,
      "loss": 0.3815,
      "step": 140700
    },
    {
      "epoch": 0.4488293141645814,
      "grad_norm": 0.0019772169180214405,
      "learning_rate": 0.00016535120575062557,
      "loss": 0.2779,
      "step": 140800
    },
    {
      "epoch": 0.44914808498430053,
      "grad_norm": 0.17096443474292755,
      "learning_rate": 0.0001652555745047098,
      "loss": 0.2776,
      "step": 140900
    },
    {
      "epoch": 0.4494668558040197,
      "grad_norm": 0.16568389534950256,
      "learning_rate": 0.00016515994325879408,
      "loss": 0.3694,
      "step": 141000
    },
    {
      "epoch": 0.4497856266237389,
      "grad_norm": 45.64981460571289,
      "learning_rate": 0.00016506431201287832,
      "loss": 0.2564,
      "step": 141100
    },
    {
      "epoch": 0.45010439744345804,
      "grad_norm": 0.015940207988023758,
      "learning_rate": 0.0001649686807669626,
      "loss": 0.5809,
      "step": 141200
    },
    {
      "epoch": 0.4504231682631772,
      "grad_norm": 0.019455840811133385,
      "learning_rate": 0.00016487304952104683,
      "loss": 0.2593,
      "step": 141300
    },
    {
      "epoch": 0.45074193908289634,
      "grad_norm": 0.0004335700359661132,
      "learning_rate": 0.00016477741827513107,
      "loss": 0.3291,
      "step": 141400
    },
    {
      "epoch": 0.45106070990261554,
      "grad_norm": 0.000269873533397913,
      "learning_rate": 0.00016468178702921534,
      "loss": 0.3268,
      "step": 141500
    },
    {
      "epoch": 0.4513794807223347,
      "grad_norm": 0.006085612345486879,
      "learning_rate": 0.00016458615578329958,
      "loss": 0.463,
      "step": 141600
    },
    {
      "epoch": 0.45169825154205384,
      "grad_norm": 0.02549494244158268,
      "learning_rate": 0.00016449052453738384,
      "loss": 0.4118,
      "step": 141700
    },
    {
      "epoch": 0.452017022361773,
      "grad_norm": 0.000552079058252275,
      "learning_rate": 0.00016439489329146808,
      "loss": 0.2546,
      "step": 141800
    },
    {
      "epoch": 0.45233579318149214,
      "grad_norm": 0.00015132423141039908,
      "learning_rate": 0.00016429926204555232,
      "loss": 0.2006,
      "step": 141900
    },
    {
      "epoch": 0.45265456400121135,
      "grad_norm": 2.012294054031372,
      "learning_rate": 0.0001642036307996366,
      "loss": 0.1923,
      "step": 142000
    },
    {
      "epoch": 0.4529733348209305,
      "grad_norm": 0.0004259774286765605,
      "learning_rate": 0.00016410799955372083,
      "loss": 0.222,
      "step": 142100
    },
    {
      "epoch": 0.45329210564064965,
      "grad_norm": 0.01610407419502735,
      "learning_rate": 0.00016401236830780507,
      "loss": 0.4415,
      "step": 142200
    },
    {
      "epoch": 0.4536108764603688,
      "grad_norm": 2.4964842796325684,
      "learning_rate": 0.00016391673706188934,
      "loss": 0.2544,
      "step": 142300
    },
    {
      "epoch": 0.453929647280088,
      "grad_norm": 4.2031988414237276e-05,
      "learning_rate": 0.00016382110581597358,
      "loss": 0.4799,
      "step": 142400
    },
    {
      "epoch": 0.45424841809980715,
      "grad_norm": 46.58952331542969,
      "learning_rate": 0.00016372547457005785,
      "loss": 0.473,
      "step": 142500
    },
    {
      "epoch": 0.4545671889195263,
      "grad_norm": 0.00038291129749268293,
      "learning_rate": 0.0001636298433241421,
      "loss": 0.4999,
      "step": 142600
    },
    {
      "epoch": 0.45488595973924545,
      "grad_norm": 35.84535217285156,
      "learning_rate": 0.00016353421207822633,
      "loss": 0.2376,
      "step": 142700
    },
    {
      "epoch": 0.45520473055896465,
      "grad_norm": 38.80058670043945,
      "learning_rate": 0.0001634385808323106,
      "loss": 0.1855,
      "step": 142800
    },
    {
      "epoch": 0.4555235013786838,
      "grad_norm": 18.93067169189453,
      "learning_rate": 0.00016334294958639484,
      "loss": 0.3689,
      "step": 142900
    },
    {
      "epoch": 0.45584227219840295,
      "grad_norm": 68.89445495605469,
      "learning_rate": 0.00016324731834047908,
      "loss": 0.4693,
      "step": 143000
    },
    {
      "epoch": 0.4561610430181221,
      "grad_norm": 0.0009225711110047996,
      "learning_rate": 0.00016315168709456334,
      "loss": 0.3038,
      "step": 143100
    },
    {
      "epoch": 0.4564798138378413,
      "grad_norm": 0.00019648036686703563,
      "learning_rate": 0.00016305605584864758,
      "loss": 0.3305,
      "step": 143200
    },
    {
      "epoch": 0.45679858465756046,
      "grad_norm": 2.520601987838745,
      "learning_rate": 0.00016296042460273185,
      "loss": 0.462,
      "step": 143300
    },
    {
      "epoch": 0.4571173554772796,
      "grad_norm": 0.0004207419988233596,
      "learning_rate": 0.0001628647933568161,
      "loss": 0.4341,
      "step": 143400
    },
    {
      "epoch": 0.45743612629699876,
      "grad_norm": 0.0007687348988838494,
      "learning_rate": 0.00016276916211090033,
      "loss": 0.3747,
      "step": 143500
    },
    {
      "epoch": 0.45775489711671796,
      "grad_norm": 0.0015779092209413648,
      "learning_rate": 0.00016267353086498463,
      "loss": 0.5188,
      "step": 143600
    },
    {
      "epoch": 0.4580736679364371,
      "grad_norm": 0.00024130668316502124,
      "learning_rate": 0.00016257789961906884,
      "loss": 0.453,
      "step": 143700
    },
    {
      "epoch": 0.45839243875615626,
      "grad_norm": 0.0016984022222459316,
      "learning_rate": 0.00016248226837315313,
      "loss": 0.3987,
      "step": 143800
    },
    {
      "epoch": 0.4587112095758754,
      "grad_norm": 0.00036811057361774147,
      "learning_rate": 0.00016238663712723738,
      "loss": 0.3646,
      "step": 143900
    },
    {
      "epoch": 0.45902998039559456,
      "grad_norm": 0.011503646150231361,
      "learning_rate": 0.0001622910058813216,
      "loss": 0.3107,
      "step": 144000
    },
    {
      "epoch": 0.45934875121531377,
      "grad_norm": 0.0007412430131807923,
      "learning_rate": 0.00016219537463540588,
      "loss": 0.4049,
      "step": 144100
    },
    {
      "epoch": 0.4596675220350329,
      "grad_norm": 0.8953084945678711,
      "learning_rate": 0.00016209974338949012,
      "loss": 0.1563,
      "step": 144200
    },
    {
      "epoch": 0.45998629285475207,
      "grad_norm": 44.758975982666016,
      "learning_rate": 0.00016200411214357434,
      "loss": 0.4062,
      "step": 144300
    },
    {
      "epoch": 0.4603050636744712,
      "grad_norm": 0.019827842712402344,
      "learning_rate": 0.00016190848089765863,
      "loss": 0.3359,
      "step": 144400
    },
    {
      "epoch": 0.4606238344941904,
      "grad_norm": 0.0006628161645494401,
      "learning_rate": 0.00016181284965174287,
      "loss": 0.2681,
      "step": 144500
    },
    {
      "epoch": 0.46094260531390957,
      "grad_norm": 0.00016413137200288475,
      "learning_rate": 0.00016171721840582714,
      "loss": 0.299,
      "step": 144600
    },
    {
      "epoch": 0.4612613761336287,
      "grad_norm": 0.00018307728169020265,
      "learning_rate": 0.00016162158715991138,
      "loss": 0.4623,
      "step": 144700
    },
    {
      "epoch": 0.46158014695334787,
      "grad_norm": 17.09593391418457,
      "learning_rate": 0.00016152595591399562,
      "loss": 0.5897,
      "step": 144800
    },
    {
      "epoch": 0.4618989177730671,
      "grad_norm": 0.00024937058333307505,
      "learning_rate": 0.0001614303246680799,
      "loss": 0.473,
      "step": 144900
    },
    {
      "epoch": 0.4622176885927862,
      "grad_norm": 9.117258741753176e-05,
      "learning_rate": 0.00016133469342216413,
      "loss": 0.4882,
      "step": 145000
    },
    {
      "epoch": 0.4625364594125054,
      "grad_norm": 0.0021311920136213303,
      "learning_rate": 0.00016123906217624837,
      "loss": 0.3528,
      "step": 145100
    },
    {
      "epoch": 0.4628552302322245,
      "grad_norm": 23.32130241394043,
      "learning_rate": 0.00016114343093033264,
      "loss": 0.4326,
      "step": 145200
    },
    {
      "epoch": 0.46317400105194373,
      "grad_norm": 0.00041198113467544317,
      "learning_rate": 0.00016104779968441688,
      "loss": 0.3875,
      "step": 145300
    },
    {
      "epoch": 0.4634927718716629,
      "grad_norm": 0.03000127524137497,
      "learning_rate": 0.00016095216843850114,
      "loss": 0.3055,
      "step": 145400
    },
    {
      "epoch": 0.46381154269138203,
      "grad_norm": 0.06547779589891434,
      "learning_rate": 0.00016085653719258538,
      "loss": 0.3111,
      "step": 145500
    },
    {
      "epoch": 0.4641303135111012,
      "grad_norm": 0.8699524998664856,
      "learning_rate": 0.00016076090594666962,
      "loss": 0.314,
      "step": 145600
    },
    {
      "epoch": 0.4644490843308204,
      "grad_norm": 0.0002274600265081972,
      "learning_rate": 0.0001606652747007539,
      "loss": 0.4844,
      "step": 145700
    },
    {
      "epoch": 0.46476785515053953,
      "grad_norm": 0.0002506435848772526,
      "learning_rate": 0.00016056964345483813,
      "loss": 0.1699,
      "step": 145800
    },
    {
      "epoch": 0.4650866259702587,
      "grad_norm": 60.26005172729492,
      "learning_rate": 0.00016047401220892237,
      "loss": 0.4197,
      "step": 145900
    },
    {
      "epoch": 0.46540539678997783,
      "grad_norm": 0.1427605003118515,
      "learning_rate": 0.00016037838096300664,
      "loss": 0.2653,
      "step": 146000
    },
    {
      "epoch": 0.465724167609697,
      "grad_norm": 0.003201050916686654,
      "learning_rate": 0.00016028274971709088,
      "loss": 0.1213,
      "step": 146100
    },
    {
      "epoch": 0.4660429384294162,
      "grad_norm": 0.0015110501553863287,
      "learning_rate": 0.00016018711847117515,
      "loss": 0.6361,
      "step": 146200
    },
    {
      "epoch": 0.46636170924913534,
      "grad_norm": 0.042669545859098434,
      "learning_rate": 0.0001600914872252594,
      "loss": 0.454,
      "step": 146300
    },
    {
      "epoch": 0.4666804800688545,
      "grad_norm": 2.5928523540496826,
      "learning_rate": 0.00015999585597934363,
      "loss": 0.3078,
      "step": 146400
    },
    {
      "epoch": 0.46699925088857364,
      "grad_norm": 0.0015408409526571631,
      "learning_rate": 0.0001599002247334279,
      "loss": 0.3405,
      "step": 146500
    },
    {
      "epoch": 0.46731802170829284,
      "grad_norm": 0.0006530489772558212,
      "learning_rate": 0.00015980459348751214,
      "loss": 0.2418,
      "step": 146600
    },
    {
      "epoch": 0.467636792528012,
      "grad_norm": 21.03451919555664,
      "learning_rate": 0.0001597089622415964,
      "loss": 0.4634,
      "step": 146700
    },
    {
      "epoch": 0.46795556334773114,
      "grad_norm": 1.687905192375183,
      "learning_rate": 0.00015961333099568064,
      "loss": 0.3818,
      "step": 146800
    },
    {
      "epoch": 0.4682743341674503,
      "grad_norm": 0.5345309972763062,
      "learning_rate": 0.00015951769974976488,
      "loss": 0.3185,
      "step": 146900
    },
    {
      "epoch": 0.4685931049871695,
      "grad_norm": 0.1357191503047943,
      "learning_rate": 0.00015942206850384915,
      "loss": 0.1808,
      "step": 147000
    },
    {
      "epoch": 0.46891187580688865,
      "grad_norm": 0.12112881243228912,
      "learning_rate": 0.0001593264372579334,
      "loss": 0.2286,
      "step": 147100
    },
    {
      "epoch": 0.4692306466266078,
      "grad_norm": 0.0001382613554596901,
      "learning_rate": 0.00015923080601201763,
      "loss": 0.5398,
      "step": 147200
    },
    {
      "epoch": 0.46954941744632694,
      "grad_norm": 0.00014559457486029714,
      "learning_rate": 0.0001591351747661019,
      "loss": 0.1584,
      "step": 147300
    },
    {
      "epoch": 0.46986818826604615,
      "grad_norm": 0.0002815107873175293,
      "learning_rate": 0.00015903954352018614,
      "loss": 0.2584,
      "step": 147400
    },
    {
      "epoch": 0.4701869590857653,
      "grad_norm": 0.11927729099988937,
      "learning_rate": 0.0001589439122742704,
      "loss": 0.4406,
      "step": 147500
    },
    {
      "epoch": 0.47050572990548445,
      "grad_norm": 2.7308003902435303,
      "learning_rate": 0.00015884828102835465,
      "loss": 0.283,
      "step": 147600
    },
    {
      "epoch": 0.4708245007252036,
      "grad_norm": 4.74450716865249e-05,
      "learning_rate": 0.0001587526497824389,
      "loss": 0.4468,
      "step": 147700
    },
    {
      "epoch": 0.4711432715449228,
      "grad_norm": 0.0021526936907321215,
      "learning_rate": 0.00015865701853652316,
      "loss": 0.2186,
      "step": 147800
    },
    {
      "epoch": 0.47146204236464195,
      "grad_norm": 0.025443734601140022,
      "learning_rate": 0.0001585613872906074,
      "loss": 0.3875,
      "step": 147900
    },
    {
      "epoch": 0.4717808131843611,
      "grad_norm": 0.0015437767142429948,
      "learning_rate": 0.00015846575604469164,
      "loss": 0.3853,
      "step": 148000
    },
    {
      "epoch": 0.47209958400408025,
      "grad_norm": 0.008109474554657936,
      "learning_rate": 0.0001583701247987759,
      "loss": 0.3958,
      "step": 148100
    },
    {
      "epoch": 0.4724183548237994,
      "grad_norm": 0.004117488395422697,
      "learning_rate": 0.00015827449355286014,
      "loss": 0.3392,
      "step": 148200
    },
    {
      "epoch": 0.4727371256435186,
      "grad_norm": 0.0016174366464838386,
      "learning_rate": 0.0001581788623069444,
      "loss": 0.207,
      "step": 148300
    },
    {
      "epoch": 0.47305589646323776,
      "grad_norm": 0.06143844872713089,
      "learning_rate": 0.00015808323106102865,
      "loss": 0.2643,
      "step": 148400
    },
    {
      "epoch": 0.4733746672829569,
      "grad_norm": 0.0007097999332472682,
      "learning_rate": 0.0001579875998151129,
      "loss": 0.3315,
      "step": 148500
    },
    {
      "epoch": 0.47369343810267606,
      "grad_norm": 0.028965050354599953,
      "learning_rate": 0.00015789196856919716,
      "loss": 0.406,
      "step": 148600
    },
    {
      "epoch": 0.47401220892239526,
      "grad_norm": 10.677310943603516,
      "learning_rate": 0.0001577963373232814,
      "loss": 0.4648,
      "step": 148700
    },
    {
      "epoch": 0.4743309797421144,
      "grad_norm": 6.051884651184082,
      "learning_rate": 0.00015770070607736567,
      "loss": 0.286,
      "step": 148800
    },
    {
      "epoch": 0.47464975056183356,
      "grad_norm": 0.07183268666267395,
      "learning_rate": 0.0001576050748314499,
      "loss": 0.3063,
      "step": 148900
    },
    {
      "epoch": 0.4749685213815527,
      "grad_norm": 0.17235715687274933,
      "learning_rate": 0.00015750944358553415,
      "loss": 0.441,
      "step": 149000
    },
    {
      "epoch": 0.4752872922012719,
      "grad_norm": 0.00037268639425747097,
      "learning_rate": 0.00015741381233961842,
      "loss": 0.162,
      "step": 149100
    },
    {
      "epoch": 0.47560606302099107,
      "grad_norm": 0.009589247405529022,
      "learning_rate": 0.00015731818109370266,
      "loss": 0.2327,
      "step": 149200
    },
    {
      "epoch": 0.4759248338407102,
      "grad_norm": 0.000589597737416625,
      "learning_rate": 0.0001572225498477869,
      "loss": 0.4566,
      "step": 149300
    },
    {
      "epoch": 0.47624360466042936,
      "grad_norm": 4.31887674331665,
      "learning_rate": 0.00015712691860187116,
      "loss": 0.4131,
      "step": 149400
    },
    {
      "epoch": 0.47656237548014857,
      "grad_norm": 5.9461668570293114e-05,
      "learning_rate": 0.0001570312873559554,
      "loss": 0.1771,
      "step": 149500
    },
    {
      "epoch": 0.4768811462998677,
      "grad_norm": 0.0008282196358777583,
      "learning_rate": 0.0001569356561100397,
      "loss": 0.3203,
      "step": 149600
    },
    {
      "epoch": 0.47719991711958687,
      "grad_norm": 8.755606651306152,
      "learning_rate": 0.0001568400248641239,
      "loss": 0.1424,
      "step": 149700
    },
    {
      "epoch": 0.477518687939306,
      "grad_norm": 0.005229952745139599,
      "learning_rate": 0.00015674439361820815,
      "loss": 0.416,
      "step": 149800
    },
    {
      "epoch": 0.4778374587590252,
      "grad_norm": 0.09938942641019821,
      "learning_rate": 0.00015664876237229245,
      "loss": 0.5236,
      "step": 149900
    },
    {
      "epoch": 0.4781562295787444,
      "grad_norm": 0.028973381966352463,
      "learning_rate": 0.0001565531311263767,
      "loss": 0.4179,
      "step": 150000
    },
    {
      "epoch": 0.4784750003984635,
      "grad_norm": 0.00471753953024745,
      "learning_rate": 0.0001564574998804609,
      "loss": 0.4178,
      "step": 150100
    },
    {
      "epoch": 0.4787937712181827,
      "grad_norm": 1.5128708582778927e-05,
      "learning_rate": 0.0001563618686345452,
      "loss": 0.2941,
      "step": 150200
    },
    {
      "epoch": 0.4791125420379018,
      "grad_norm": 3.712030957103707e-05,
      "learning_rate": 0.00015626623738862944,
      "loss": 0.2856,
      "step": 150300
    },
    {
      "epoch": 0.47943131285762103,
      "grad_norm": 0.0021434957161545753,
      "learning_rate": 0.0001561706061427137,
      "loss": 0.3049,
      "step": 150400
    },
    {
      "epoch": 0.4797500836773402,
      "grad_norm": 0.00045303578372113407,
      "learning_rate": 0.00015607497489679794,
      "loss": 0.3995,
      "step": 150500
    },
    {
      "epoch": 0.4800688544970593,
      "grad_norm": 0.0009635343449190259,
      "learning_rate": 0.00015597934365088218,
      "loss": 0.2631,
      "step": 150600
    },
    {
      "epoch": 0.4803876253167785,
      "grad_norm": 1.1616300344467163,
      "learning_rate": 0.00015588371240496645,
      "loss": 0.3892,
      "step": 150700
    },
    {
      "epoch": 0.4807063961364977,
      "grad_norm": 6.809609476476908e-05,
      "learning_rate": 0.0001557880811590507,
      "loss": 0.388,
      "step": 150800
    },
    {
      "epoch": 0.48102516695621683,
      "grad_norm": 0.08750084042549133,
      "learning_rate": 0.00015569244991313496,
      "loss": 0.2177,
      "step": 150900
    },
    {
      "epoch": 0.481343937775936,
      "grad_norm": 0.00046859742724336684,
      "learning_rate": 0.0001555968186672192,
      "loss": 0.3629,
      "step": 151000
    },
    {
      "epoch": 0.48166270859565513,
      "grad_norm": 3.8955562114715576,
      "learning_rate": 0.00015550118742130344,
      "loss": 0.2187,
      "step": 151100
    },
    {
      "epoch": 0.48198147941537434,
      "grad_norm": 0.0028675058856606483,
      "learning_rate": 0.0001554055561753877,
      "loss": 0.4246,
      "step": 151200
    },
    {
      "epoch": 0.4823002502350935,
      "grad_norm": 4.6121625900268555,
      "learning_rate": 0.00015530992492947195,
      "loss": 0.1796,
      "step": 151300
    },
    {
      "epoch": 0.48261902105481264,
      "grad_norm": 0.0027657803148031235,
      "learning_rate": 0.0001552142936835562,
      "loss": 0.3681,
      "step": 151400
    },
    {
      "epoch": 0.4829377918745318,
      "grad_norm": 0.0068052345886826515,
      "learning_rate": 0.00015511866243764046,
      "loss": 0.2267,
      "step": 151500
    },
    {
      "epoch": 0.483256562694251,
      "grad_norm": 39.09248733520508,
      "learning_rate": 0.0001550230311917247,
      "loss": 0.291,
      "step": 151600
    },
    {
      "epoch": 0.48357533351397014,
      "grad_norm": 0.022667307406663895,
      "learning_rate": 0.00015492739994580896,
      "loss": 0.3659,
      "step": 151700
    },
    {
      "epoch": 0.4838941043336893,
      "grad_norm": 0.04949011653661728,
      "learning_rate": 0.0001548317686998932,
      "loss": 0.4049,
      "step": 151800
    },
    {
      "epoch": 0.48421287515340844,
      "grad_norm": 0.002605748362839222,
      "learning_rate": 0.00015473613745397744,
      "loss": 0.2604,
      "step": 151900
    },
    {
      "epoch": 0.48453164597312764,
      "grad_norm": 0.0428110733628273,
      "learning_rate": 0.0001546405062080617,
      "loss": 0.4308,
      "step": 152000
    },
    {
      "epoch": 0.4848504167928468,
      "grad_norm": 0.474162220954895,
      "learning_rate": 0.00015454487496214595,
      "loss": 0.5135,
      "step": 152100
    },
    {
      "epoch": 0.48516918761256594,
      "grad_norm": 2.5247247219085693,
      "learning_rate": 0.0001544492437162302,
      "loss": 0.3862,
      "step": 152200
    },
    {
      "epoch": 0.4854879584322851,
      "grad_norm": 0.007230983581393957,
      "learning_rate": 0.00015435361247031446,
      "loss": 0.2809,
      "step": 152300
    },
    {
      "epoch": 0.4858067292520043,
      "grad_norm": 0.000690936460159719,
      "learning_rate": 0.0001542579812243987,
      "loss": 0.4179,
      "step": 152400
    },
    {
      "epoch": 0.48612550007172345,
      "grad_norm": 0.0004974618786945939,
      "learning_rate": 0.00015416234997848297,
      "loss": 0.1743,
      "step": 152500
    },
    {
      "epoch": 0.4864442708914426,
      "grad_norm": 0.0016067844117060304,
      "learning_rate": 0.0001540667187325672,
      "loss": 0.2212,
      "step": 152600
    },
    {
      "epoch": 0.48676304171116175,
      "grad_norm": 0.0015293668257072568,
      "learning_rate": 0.00015397108748665145,
      "loss": 0.3919,
      "step": 152700
    },
    {
      "epoch": 0.4870818125308809,
      "grad_norm": 0.0007660711416974664,
      "learning_rate": 0.00015387545624073572,
      "loss": 0.332,
      "step": 152800
    },
    {
      "epoch": 0.4874005833506001,
      "grad_norm": 0.0014976940583437681,
      "learning_rate": 0.00015377982499481996,
      "loss": 0.3802,
      "step": 152900
    },
    {
      "epoch": 0.48771935417031925,
      "grad_norm": 0.8317384123802185,
      "learning_rate": 0.00015368419374890422,
      "loss": 0.5381,
      "step": 153000
    },
    {
      "epoch": 0.4880381249900384,
      "grad_norm": 45.76398849487305,
      "learning_rate": 0.00015358856250298846,
      "loss": 0.3867,
      "step": 153100
    },
    {
      "epoch": 0.48835689580975755,
      "grad_norm": 5.969611811451614e-05,
      "learning_rate": 0.0001534929312570727,
      "loss": 0.1486,
      "step": 153200
    },
    {
      "epoch": 0.48867566662947676,
      "grad_norm": 0.0074442969635128975,
      "learning_rate": 0.00015339730001115697,
      "loss": 0.2191,
      "step": 153300
    },
    {
      "epoch": 0.4889944374491959,
      "grad_norm": 0.003092730650678277,
      "learning_rate": 0.0001533016687652412,
      "loss": 0.1362,
      "step": 153400
    },
    {
      "epoch": 0.48931320826891506,
      "grad_norm": 0.0016167083522304893,
      "learning_rate": 0.00015320603751932545,
      "loss": 0.1953,
      "step": 153500
    },
    {
      "epoch": 0.4896319790886342,
      "grad_norm": 0.0010192404733970761,
      "learning_rate": 0.00015311040627340972,
      "loss": 0.2195,
      "step": 153600
    },
    {
      "epoch": 0.4899507499083534,
      "grad_norm": 0.0036501288414001465,
      "learning_rate": 0.00015301477502749396,
      "loss": 0.1579,
      "step": 153700
    },
    {
      "epoch": 0.49026952072807256,
      "grad_norm": 0.0007456399034708738,
      "learning_rate": 0.00015291914378157823,
      "loss": 0.1975,
      "step": 153800
    },
    {
      "epoch": 0.4905882915477917,
      "grad_norm": 0.02152954787015915,
      "learning_rate": 0.00015282351253566247,
      "loss": 0.4484,
      "step": 153900
    },
    {
      "epoch": 0.49090706236751086,
      "grad_norm": 0.00014878685760777444,
      "learning_rate": 0.0001527278812897467,
      "loss": 0.1468,
      "step": 154000
    },
    {
      "epoch": 0.49122583318723007,
      "grad_norm": 0.000153315719217062,
      "learning_rate": 0.00015263225004383098,
      "loss": 0.2713,
      "step": 154100
    },
    {
      "epoch": 0.4915446040069492,
      "grad_norm": 0.010934615507721901,
      "learning_rate": 0.00015253661879791522,
      "loss": 0.3602,
      "step": 154200
    },
    {
      "epoch": 0.49186337482666836,
      "grad_norm": 0.0013774660183116794,
      "learning_rate": 0.00015244098755199946,
      "loss": 0.3577,
      "step": 154300
    },
    {
      "epoch": 0.4921821456463875,
      "grad_norm": 0.00016580597730353475,
      "learning_rate": 0.00015234535630608372,
      "loss": 0.2969,
      "step": 154400
    },
    {
      "epoch": 0.4925009164661067,
      "grad_norm": 0.0016180076636373997,
      "learning_rate": 0.00015224972506016796,
      "loss": 0.2793,
      "step": 154500
    },
    {
      "epoch": 0.49281968728582587,
      "grad_norm": 1.46861384564545e-05,
      "learning_rate": 0.00015215409381425223,
      "loss": 0.5325,
      "step": 154600
    },
    {
      "epoch": 0.493138458105545,
      "grad_norm": 108.83148193359375,
      "learning_rate": 0.00015205846256833647,
      "loss": 0.268,
      "step": 154700
    },
    {
      "epoch": 0.49345722892526417,
      "grad_norm": 0.0010909921256825328,
      "learning_rate": 0.0001519628313224207,
      "loss": 0.217,
      "step": 154800
    },
    {
      "epoch": 0.4937759997449833,
      "grad_norm": 0.001496935379691422,
      "learning_rate": 0.00015186720007650498,
      "loss": 0.3598,
      "step": 154900
    },
    {
      "epoch": 0.4940947705647025,
      "grad_norm": 0.0010293566156178713,
      "learning_rate": 0.00015177156883058922,
      "loss": 0.1537,
      "step": 155000
    },
    {
      "epoch": 0.4944135413844217,
      "grad_norm": 82.19666290283203,
      "learning_rate": 0.00015167593758467351,
      "loss": 0.314,
      "step": 155100
    },
    {
      "epoch": 0.4947323122041408,
      "grad_norm": 0.00012019598216284066,
      "learning_rate": 0.00015158030633875773,
      "loss": 0.5008,
      "step": 155200
    },
    {
      "epoch": 0.49505108302386,
      "grad_norm": 6.200554370880127,
      "learning_rate": 0.00015148467509284197,
      "loss": 0.2289,
      "step": 155300
    },
    {
      "epoch": 0.4953698538435792,
      "grad_norm": 4.076464392710477e-05,
      "learning_rate": 0.00015138904384692626,
      "loss": 0.3739,
      "step": 155400
    },
    {
      "epoch": 0.4956886246632983,
      "grad_norm": 1.6308025806210935e-05,
      "learning_rate": 0.00015129341260101048,
      "loss": 0.3502,
      "step": 155500
    },
    {
      "epoch": 0.4960073954830175,
      "grad_norm": 0.0012526653008535504,
      "learning_rate": 0.00015119778135509472,
      "loss": 0.453,
      "step": 155600
    },
    {
      "epoch": 0.4963261663027366,
      "grad_norm": 0.0009221866494044662,
      "learning_rate": 0.000151102150109179,
      "loss": 0.2784,
      "step": 155700
    },
    {
      "epoch": 0.49664493712245583,
      "grad_norm": 0.10019973665475845,
      "learning_rate": 0.00015100651886326322,
      "loss": 0.2762,
      "step": 155800
    },
    {
      "epoch": 0.496963707942175,
      "grad_norm": 0.012078406289219856,
      "learning_rate": 0.00015091088761734752,
      "loss": 0.2602,
      "step": 155900
    },
    {
      "epoch": 0.49728247876189413,
      "grad_norm": 0.03610220551490784,
      "learning_rate": 0.00015081525637143176,
      "loss": 0.3801,
      "step": 156000
    },
    {
      "epoch": 0.4976012495816133,
      "grad_norm": 0.2547263503074646,
      "learning_rate": 0.00015071962512551597,
      "loss": 0.4023,
      "step": 156100
    },
    {
      "epoch": 0.4979200204013325,
      "grad_norm": 7.22741024219431e-05,
      "learning_rate": 0.00015062399387960027,
      "loss": 0.2721,
      "step": 156200
    },
    {
      "epoch": 0.49823879122105164,
      "grad_norm": 0.00016086496179923415,
      "learning_rate": 0.0001505283626336845,
      "loss": 0.4201,
      "step": 156300
    },
    {
      "epoch": 0.4985575620407708,
      "grad_norm": 0.0027258917689323425,
      "learning_rate": 0.00015043273138776875,
      "loss": 0.504,
      "step": 156400
    },
    {
      "epoch": 0.49887633286048993,
      "grad_norm": 0.0666295737028122,
      "learning_rate": 0.00015033710014185302,
      "loss": 0.2405,
      "step": 156500
    },
    {
      "epoch": 0.49919510368020914,
      "grad_norm": 4.229126930236816,
      "learning_rate": 0.00015024146889593726,
      "loss": 0.5711,
      "step": 156600
    },
    {
      "epoch": 0.4995138744999283,
      "grad_norm": 0.0004839949542656541,
      "learning_rate": 0.00015014583765002152,
      "loss": 0.1909,
      "step": 156700
    },
    {
      "epoch": 0.49983264531964744,
      "grad_norm": 88.856201171875,
      "learning_rate": 0.00015005020640410576,
      "loss": 0.5259,
      "step": 156800
    },
    {
      "epoch": 0.5001514161393666,
      "grad_norm": 0.000957991520408541,
      "learning_rate": 0.00014995457515819,
      "loss": 0.4872,
      "step": 156900
    },
    {
      "epoch": 0.5004701869590857,
      "grad_norm": 1.6977578401565552,
      "learning_rate": 0.00014985894391227427,
      "loss": 0.5637,
      "step": 157000
    },
    {
      "epoch": 0.5007889577788049,
      "grad_norm": 0.0005286760278977454,
      "learning_rate": 0.0001497633126663585,
      "loss": 0.4744,
      "step": 157100
    },
    {
      "epoch": 0.501107728598524,
      "grad_norm": 0.0003667915298137814,
      "learning_rate": 0.00014966768142044275,
      "loss": 0.4595,
      "step": 157200
    },
    {
      "epoch": 0.5014264994182432,
      "grad_norm": 0.037599463015794754,
      "learning_rate": 0.00014957205017452702,
      "loss": 0.2482,
      "step": 157300
    },
    {
      "epoch": 0.5017452702379624,
      "grad_norm": 0.03218790143728256,
      "learning_rate": 0.00014947641892861126,
      "loss": 0.4331,
      "step": 157400
    },
    {
      "epoch": 0.5020640410576815,
      "grad_norm": 30.446533203125,
      "learning_rate": 0.0001493807876826955,
      "loss": 0.2048,
      "step": 157500
    },
    {
      "epoch": 0.5023828118774007,
      "grad_norm": 0.013713564723730087,
      "learning_rate": 0.00014928515643677977,
      "loss": 0.2518,
      "step": 157600
    },
    {
      "epoch": 0.50270158269712,
      "grad_norm": 0.0011011626338586211,
      "learning_rate": 0.000149189525190864,
      "loss": 0.4332,
      "step": 157700
    },
    {
      "epoch": 0.503020353516839,
      "grad_norm": 0.9383359551429749,
      "learning_rate": 0.00014909389394494828,
      "loss": 0.204,
      "step": 157800
    },
    {
      "epoch": 0.5033391243365583,
      "grad_norm": 0.45925140380859375,
      "learning_rate": 0.00014899826269903252,
      "loss": 0.2233,
      "step": 157900
    },
    {
      "epoch": 0.5036578951562773,
      "grad_norm": 0.0009088889346458018,
      "learning_rate": 0.00014890263145311676,
      "loss": 0.2576,
      "step": 158000
    },
    {
      "epoch": 0.5039766659759966,
      "grad_norm": 0.0020880375523120165,
      "learning_rate": 0.00014880700020720102,
      "loss": 0.327,
      "step": 158100
    },
    {
      "epoch": 0.5042954367957158,
      "grad_norm": 0.00029352804995141923,
      "learning_rate": 0.00014871136896128526,
      "loss": 0.2687,
      "step": 158200
    },
    {
      "epoch": 0.5046142076154349,
      "grad_norm": 0.0016044261865317822,
      "learning_rate": 0.0001486157377153695,
      "loss": 0.2672,
      "step": 158300
    },
    {
      "epoch": 0.5049329784351541,
      "grad_norm": 0.00037920946488156915,
      "learning_rate": 0.00014852010646945377,
      "loss": 0.2965,
      "step": 158400
    },
    {
      "epoch": 0.5052517492548733,
      "grad_norm": 8.99356891750358e-05,
      "learning_rate": 0.000148424475223538,
      "loss": 0.2388,
      "step": 158500
    },
    {
      "epoch": 0.5055705200745924,
      "grad_norm": 0.06036747246980667,
      "learning_rate": 0.00014832884397762228,
      "loss": 0.3049,
      "step": 158600
    },
    {
      "epoch": 0.5058892908943116,
      "grad_norm": 2.737712929956615e-05,
      "learning_rate": 0.00014823321273170652,
      "loss": 0.3993,
      "step": 158700
    },
    {
      "epoch": 0.5062080617140307,
      "grad_norm": 0.0026665690820664167,
      "learning_rate": 0.0001481375814857908,
      "loss": 0.2782,
      "step": 158800
    },
    {
      "epoch": 0.5065268325337499,
      "grad_norm": 1.7979211807250977,
      "learning_rate": 0.00014804195023987503,
      "loss": 0.2994,
      "step": 158900
    },
    {
      "epoch": 0.5068456033534691,
      "grad_norm": 0.02749510295689106,
      "learning_rate": 0.0001479463189939593,
      "loss": 0.431,
      "step": 159000
    },
    {
      "epoch": 0.5071643741731882,
      "grad_norm": 0.0001418415631633252,
      "learning_rate": 0.00014785068774804354,
      "loss": 0.3068,
      "step": 159100
    },
    {
      "epoch": 0.5074831449929074,
      "grad_norm": 0.00022504059597849846,
      "learning_rate": 0.00014775505650212778,
      "loss": 0.3318,
      "step": 159200
    },
    {
      "epoch": 0.5078019158126265,
      "grad_norm": 2.9624109268188477,
      "learning_rate": 0.00014765942525621204,
      "loss": 0.309,
      "step": 159300
    },
    {
      "epoch": 0.5081206866323457,
      "grad_norm": 0.01651945523917675,
      "learning_rate": 0.00014756379401029628,
      "loss": 0.1916,
      "step": 159400
    },
    {
      "epoch": 0.5084394574520649,
      "grad_norm": 0.005867197643965483,
      "learning_rate": 0.00014746816276438055,
      "loss": 0.3881,
      "step": 159500
    },
    {
      "epoch": 0.508758228271784,
      "grad_norm": 6.507976650027558e-05,
      "learning_rate": 0.0001473725315184648,
      "loss": 0.3144,
      "step": 159600
    },
    {
      "epoch": 0.5090769990915032,
      "grad_norm": 0.0524735227227211,
      "learning_rate": 0.00014727690027254903,
      "loss": 0.4871,
      "step": 159700
    },
    {
      "epoch": 0.5093957699112224,
      "grad_norm": 0.0009271063026972115,
      "learning_rate": 0.0001471812690266333,
      "loss": 0.2215,
      "step": 159800
    },
    {
      "epoch": 0.5097145407309415,
      "grad_norm": 0.0002629332593642175,
      "learning_rate": 0.00014708563778071754,
      "loss": 0.415,
      "step": 159900
    },
    {
      "epoch": 0.5100333115506607,
      "grad_norm": 0.0014245709171518683,
      "learning_rate": 0.00014699000653480178,
      "loss": 0.3369,
      "step": 160000
    },
    {
      "epoch": 0.5103520823703798,
      "grad_norm": 55.15156555175781,
      "learning_rate": 0.00014689437528888605,
      "loss": 0.2583,
      "step": 160100
    },
    {
      "epoch": 0.510670853190099,
      "grad_norm": 125.46475982666016,
      "learning_rate": 0.0001467987440429703,
      "loss": 0.2905,
      "step": 160200
    },
    {
      "epoch": 0.5109896240098182,
      "grad_norm": 0.0047097522765398026,
      "learning_rate": 0.00014670311279705456,
      "loss": 0.4924,
      "step": 160300
    },
    {
      "epoch": 0.5113083948295373,
      "grad_norm": 0.0008268673554994166,
      "learning_rate": 0.0001466074815511388,
      "loss": 0.3435,
      "step": 160400
    },
    {
      "epoch": 0.5116271656492565,
      "grad_norm": 11.970599174499512,
      "learning_rate": 0.00014651185030522304,
      "loss": 0.318,
      "step": 160500
    },
    {
      "epoch": 0.5119459364689757,
      "grad_norm": 0.001076035900041461,
      "learning_rate": 0.0001464162190593073,
      "loss": 0.2698,
      "step": 160600
    },
    {
      "epoch": 0.5122647072886948,
      "grad_norm": 0.0001021389034576714,
      "learning_rate": 0.00014632058781339154,
      "loss": 0.1974,
      "step": 160700
    },
    {
      "epoch": 0.512583478108414,
      "grad_norm": 0.002486084820702672,
      "learning_rate": 0.00014622495656747578,
      "loss": 0.199,
      "step": 160800
    },
    {
      "epoch": 0.5129022489281331,
      "grad_norm": 0.0005421617534011602,
      "learning_rate": 0.00014612932532156005,
      "loss": 0.3072,
      "step": 160900
    },
    {
      "epoch": 0.5132210197478523,
      "grad_norm": 0.014583099633455276,
      "learning_rate": 0.0001460336940756443,
      "loss": 0.2681,
      "step": 161000
    },
    {
      "epoch": 0.5135397905675715,
      "grad_norm": 0.0019016219303011894,
      "learning_rate": 0.00014593806282972856,
      "loss": 0.2936,
      "step": 161100
    },
    {
      "epoch": 0.5138585613872906,
      "grad_norm": 0.0020290950778871775,
      "learning_rate": 0.0001458424315838128,
      "loss": 0.2271,
      "step": 161200
    },
    {
      "epoch": 0.5141773322070098,
      "grad_norm": 4.215672492980957,
      "learning_rate": 0.00014574680033789704,
      "loss": 0.2181,
      "step": 161300
    },
    {
      "epoch": 0.5144961030267289,
      "grad_norm": 25.158100128173828,
      "learning_rate": 0.0001456511690919813,
      "loss": 0.477,
      "step": 161400
    },
    {
      "epoch": 0.5148148738464481,
      "grad_norm": 0.0007096503977663815,
      "learning_rate": 0.00014555553784606558,
      "loss": 0.1458,
      "step": 161500
    },
    {
      "epoch": 0.5151336446661673,
      "grad_norm": 0.03212340548634529,
      "learning_rate": 0.00014545990660014982,
      "loss": 0.2438,
      "step": 161600
    },
    {
      "epoch": 0.5154524154858864,
      "grad_norm": 0.7836037874221802,
      "learning_rate": 0.00014536427535423406,
      "loss": 0.3898,
      "step": 161700
    },
    {
      "epoch": 0.5157711863056056,
      "grad_norm": 15.632219314575195,
      "learning_rate": 0.00014526864410831832,
      "loss": 0.3137,
      "step": 161800
    },
    {
      "epoch": 0.5160899571253248,
      "grad_norm": 0.10827852040529251,
      "learning_rate": 0.00014517301286240256,
      "loss": 0.3364,
      "step": 161900
    },
    {
      "epoch": 0.5164087279450439,
      "grad_norm": 0.002998067531734705,
      "learning_rate": 0.00014507738161648683,
      "loss": 0.1847,
      "step": 162000
    },
    {
      "epoch": 0.5167274987647631,
      "grad_norm": 0.00691034272313118,
      "learning_rate": 0.00014498175037057107,
      "loss": 0.5095,
      "step": 162100
    },
    {
      "epoch": 0.5170462695844822,
      "grad_norm": 0.3389888405799866,
      "learning_rate": 0.0001448861191246553,
      "loss": 0.4034,
      "step": 162200
    },
    {
      "epoch": 0.5173650404042014,
      "grad_norm": 0.004087254870682955,
      "learning_rate": 0.00014479048787873958,
      "loss": 0.2902,
      "step": 162300
    },
    {
      "epoch": 0.5176838112239206,
      "grad_norm": 0.0002124878956237808,
      "learning_rate": 0.00014469485663282382,
      "loss": 0.4013,
      "step": 162400
    },
    {
      "epoch": 0.5180025820436397,
      "grad_norm": 0.0009165579685941339,
      "learning_rate": 0.00014459922538690806,
      "loss": 0.1841,
      "step": 162500
    },
    {
      "epoch": 0.5183213528633589,
      "grad_norm": 0.01541187521070242,
      "learning_rate": 0.00014450359414099233,
      "loss": 0.3098,
      "step": 162600
    },
    {
      "epoch": 0.5186401236830781,
      "grad_norm": 0.0012357032392174006,
      "learning_rate": 0.00014440796289507657,
      "loss": 0.2413,
      "step": 162700
    },
    {
      "epoch": 0.5189588945027972,
      "grad_norm": 0.017378533259034157,
      "learning_rate": 0.00014431233164916084,
      "loss": 0.2996,
      "step": 162800
    },
    {
      "epoch": 0.5192776653225164,
      "grad_norm": 0.0004567551368381828,
      "learning_rate": 0.00014421670040324508,
      "loss": 0.2618,
      "step": 162900
    },
    {
      "epoch": 0.5195964361422355,
      "grad_norm": 0.0010439952602609992,
      "learning_rate": 0.00014412106915732932,
      "loss": 0.304,
      "step": 163000
    },
    {
      "epoch": 0.5199152069619547,
      "grad_norm": 0.12000291794538498,
      "learning_rate": 0.00014402543791141358,
      "loss": 0.39,
      "step": 163100
    },
    {
      "epoch": 0.5202339777816739,
      "grad_norm": 0.1931942254304886,
      "learning_rate": 0.00014392980666549782,
      "loss": 0.3779,
      "step": 163200
    },
    {
      "epoch": 0.520552748601393,
      "grad_norm": 0.06922506541013718,
      "learning_rate": 0.00014383417541958206,
      "loss": 0.3471,
      "step": 163300
    },
    {
      "epoch": 0.5208715194211122,
      "grad_norm": 4.959609577781521e-05,
      "learning_rate": 0.00014373854417366633,
      "loss": 0.1652,
      "step": 163400
    },
    {
      "epoch": 0.5211902902408313,
      "grad_norm": 0.0010126953711733222,
      "learning_rate": 0.00014364291292775057,
      "loss": 0.5755,
      "step": 163500
    },
    {
      "epoch": 0.5215090610605505,
      "grad_norm": 3.706897258758545,
      "learning_rate": 0.00014354728168183484,
      "loss": 0.3272,
      "step": 163600
    },
    {
      "epoch": 0.5218278318802697,
      "grad_norm": 0.6263691782951355,
      "learning_rate": 0.00014345165043591908,
      "loss": 0.3171,
      "step": 163700
    },
    {
      "epoch": 0.5221466026999888,
      "grad_norm": 0.0020613151136785746,
      "learning_rate": 0.00014335601919000332,
      "loss": 0.2788,
      "step": 163800
    },
    {
      "epoch": 0.522465373519708,
      "grad_norm": 0.021910028532147408,
      "learning_rate": 0.0001432603879440876,
      "loss": 0.3193,
      "step": 163900
    },
    {
      "epoch": 0.5227841443394272,
      "grad_norm": 0.5400702357292175,
      "learning_rate": 0.00014316475669817183,
      "loss": 0.3694,
      "step": 164000
    },
    {
      "epoch": 0.5231029151591463,
      "grad_norm": 0.00013034076255280524,
      "learning_rate": 0.0001430691254522561,
      "loss": 0.2779,
      "step": 164100
    },
    {
      "epoch": 0.5234216859788655,
      "grad_norm": 0.0009496186976321042,
      "learning_rate": 0.00014297349420634034,
      "loss": 0.1742,
      "step": 164200
    },
    {
      "epoch": 0.5237404567985846,
      "grad_norm": 2.548686279624235e-05,
      "learning_rate": 0.00014287786296042458,
      "loss": 0.3136,
      "step": 164300
    },
    {
      "epoch": 0.5240592276183038,
      "grad_norm": 3.842078967863927e-06,
      "learning_rate": 0.00014278223171450884,
      "loss": 0.3023,
      "step": 164400
    },
    {
      "epoch": 0.524377998438023,
      "grad_norm": 0.01221831887960434,
      "learning_rate": 0.0001426866004685931,
      "loss": 0.3333,
      "step": 164500
    },
    {
      "epoch": 0.5246967692577421,
      "grad_norm": 1.2946335077285767,
      "learning_rate": 0.00014259096922267732,
      "loss": 0.337,
      "step": 164600
    },
    {
      "epoch": 0.5250155400774613,
      "grad_norm": 0.020976364612579346,
      "learning_rate": 0.0001424953379767616,
      "loss": 0.4566,
      "step": 164700
    },
    {
      "epoch": 0.5253343108971805,
      "grad_norm": 91.33769226074219,
      "learning_rate": 0.00014239970673084586,
      "loss": 0.4527,
      "step": 164800
    },
    {
      "epoch": 0.5256530817168996,
      "grad_norm": 7.381180330412462e-05,
      "learning_rate": 0.0001423040754849301,
      "loss": 0.4064,
      "step": 164900
    },
    {
      "epoch": 0.5259718525366188,
      "grad_norm": 0.0013546270783990622,
      "learning_rate": 0.00014220844423901434,
      "loss": 0.1818,
      "step": 165000
    },
    {
      "epoch": 0.5262906233563379,
      "grad_norm": 4.470746353035793e-05,
      "learning_rate": 0.0001421128129930986,
      "loss": 0.3374,
      "step": 165100
    },
    {
      "epoch": 0.5266093941760571,
      "grad_norm": 0.13901761174201965,
      "learning_rate": 0.00014201718174718285,
      "loss": 0.3855,
      "step": 165200
    },
    {
      "epoch": 0.5269281649957763,
      "grad_norm": 0.0003275726630818099,
      "learning_rate": 0.00014192155050126712,
      "loss": 0.2866,
      "step": 165300
    },
    {
      "epoch": 0.5272469358154954,
      "grad_norm": 0.07079867273569107,
      "learning_rate": 0.00014182591925535136,
      "loss": 0.198,
      "step": 165400
    },
    {
      "epoch": 0.5275657066352146,
      "grad_norm": 0.004756341688334942,
      "learning_rate": 0.0001417302880094356,
      "loss": 0.2352,
      "step": 165500
    },
    {
      "epoch": 0.5278844774549337,
      "grad_norm": 48.53300857543945,
      "learning_rate": 0.00014163465676351986,
      "loss": 0.2039,
      "step": 165600
    },
    {
      "epoch": 0.5282032482746529,
      "grad_norm": 0.020962409675121307,
      "learning_rate": 0.0001415390255176041,
      "loss": 0.2603,
      "step": 165700
    },
    {
      "epoch": 0.5285220190943721,
      "grad_norm": 0.011065111495554447,
      "learning_rate": 0.00014144339427168837,
      "loss": 0.2445,
      "step": 165800
    },
    {
      "epoch": 0.5288407899140912,
      "grad_norm": 21.63956642150879,
      "learning_rate": 0.0001413477630257726,
      "loss": 0.4096,
      "step": 165900
    },
    {
      "epoch": 0.5291595607338104,
      "grad_norm": 0.0036758147180080414,
      "learning_rate": 0.00014125213177985685,
      "loss": 0.3479,
      "step": 166000
    },
    {
      "epoch": 0.5294783315535296,
      "grad_norm": 0.002190060680732131,
      "learning_rate": 0.00014115650053394112,
      "loss": 0.4344,
      "step": 166100
    },
    {
      "epoch": 0.5297971023732487,
      "grad_norm": 0.00017722678603604436,
      "learning_rate": 0.00014106086928802536,
      "loss": 0.3928,
      "step": 166200
    },
    {
      "epoch": 0.5301158731929679,
      "grad_norm": 0.048378147184848785,
      "learning_rate": 0.0001409652380421096,
      "loss": 0.3689,
      "step": 166300
    },
    {
      "epoch": 0.530434644012687,
      "grad_norm": 0.0007635987130925059,
      "learning_rate": 0.00014086960679619387,
      "loss": 0.3891,
      "step": 166400
    },
    {
      "epoch": 0.5307534148324062,
      "grad_norm": 0.0020114067010581493,
      "learning_rate": 0.0001407739755502781,
      "loss": 0.5426,
      "step": 166500
    },
    {
      "epoch": 0.5310721856521254,
      "grad_norm": 8.684041858941782e-06,
      "learning_rate": 0.00014067834430436238,
      "loss": 0.332,
      "step": 166600
    },
    {
      "epoch": 0.5313909564718445,
      "grad_norm": 0.0076398300006985664,
      "learning_rate": 0.00014058271305844662,
      "loss": 0.2603,
      "step": 166700
    },
    {
      "epoch": 0.5317097272915637,
      "grad_norm": 19.878643035888672,
      "learning_rate": 0.00014048708181253086,
      "loss": 0.2552,
      "step": 166800
    },
    {
      "epoch": 0.5320284981112829,
      "grad_norm": 0.009054513648152351,
      "learning_rate": 0.00014039145056661512,
      "loss": 0.2627,
      "step": 166900
    },
    {
      "epoch": 0.532347268931002,
      "grad_norm": 4.990777969360352,
      "learning_rate": 0.00014029581932069936,
      "loss": 0.2773,
      "step": 167000
    },
    {
      "epoch": 0.5326660397507212,
      "grad_norm": 1.5657943487167358,
      "learning_rate": 0.0001402001880747836,
      "loss": 0.4832,
      "step": 167100
    },
    {
      "epoch": 0.5329848105704403,
      "grad_norm": 0.14506952464580536,
      "learning_rate": 0.00014010455682886787,
      "loss": 0.3602,
      "step": 167200
    },
    {
      "epoch": 0.5333035813901595,
      "grad_norm": 0.688589870929718,
      "learning_rate": 0.0001400089255829521,
      "loss": 0.2816,
      "step": 167300
    },
    {
      "epoch": 0.5336223522098787,
      "grad_norm": 2.6077377697220072e-05,
      "learning_rate": 0.00013991329433703638,
      "loss": 0.3625,
      "step": 167400
    },
    {
      "epoch": 0.5339411230295978,
      "grad_norm": 38.8975830078125,
      "learning_rate": 0.00013981766309112062,
      "loss": 0.321,
      "step": 167500
    },
    {
      "epoch": 0.534259893849317,
      "grad_norm": 134.64566040039062,
      "learning_rate": 0.0001397220318452049,
      "loss": 0.4262,
      "step": 167600
    },
    {
      "epoch": 0.5345786646690361,
      "grad_norm": 0.0001522723032394424,
      "learning_rate": 0.00013962640059928913,
      "loss": 0.124,
      "step": 167700
    },
    {
      "epoch": 0.5348974354887553,
      "grad_norm": 0.00043612089939415455,
      "learning_rate": 0.0001395307693533734,
      "loss": 0.1776,
      "step": 167800
    },
    {
      "epoch": 0.5352162063084746,
      "grad_norm": 0.1646963208913803,
      "learning_rate": 0.00013943513810745764,
      "loss": 0.333,
      "step": 167900
    },
    {
      "epoch": 0.5355349771281936,
      "grad_norm": 0.004161893390119076,
      "learning_rate": 0.00013933950686154188,
      "loss": 0.2698,
      "step": 168000
    },
    {
      "epoch": 0.5358537479479129,
      "grad_norm": 0.004701609257608652,
      "learning_rate": 0.00013924387561562614,
      "loss": 0.3944,
      "step": 168100
    },
    {
      "epoch": 0.5361725187676321,
      "grad_norm": 0.6938289999961853,
      "learning_rate": 0.00013914824436971038,
      "loss": 0.3159,
      "step": 168200
    },
    {
      "epoch": 0.5364912895873512,
      "grad_norm": 1.469132423400879,
      "learning_rate": 0.00013905261312379465,
      "loss": 0.3768,
      "step": 168300
    },
    {
      "epoch": 0.5368100604070704,
      "grad_norm": 0.0005385298863984644,
      "learning_rate": 0.0001389569818778789,
      "loss": 0.3281,
      "step": 168400
    },
    {
      "epoch": 0.5371288312267894,
      "grad_norm": 0.016718851402401924,
      "learning_rate": 0.00013886135063196313,
      "loss": 0.2048,
      "step": 168500
    },
    {
      "epoch": 0.5374476020465087,
      "grad_norm": 0.0019187084399163723,
      "learning_rate": 0.0001387657193860474,
      "loss": 0.2067,
      "step": 168600
    },
    {
      "epoch": 0.5377663728662279,
      "grad_norm": 0.6005235910415649,
      "learning_rate": 0.00013867008814013164,
      "loss": 0.1285,
      "step": 168700
    },
    {
      "epoch": 0.538085143685947,
      "grad_norm": 10.849942207336426,
      "learning_rate": 0.00013857445689421588,
      "loss": 0.5225,
      "step": 168800
    },
    {
      "epoch": 0.5384039145056662,
      "grad_norm": 0.004717624746263027,
      "learning_rate": 0.00013847882564830015,
      "loss": 0.4147,
      "step": 168900
    },
    {
      "epoch": 0.5387226853253854,
      "grad_norm": 0.000988488201983273,
      "learning_rate": 0.0001383831944023844,
      "loss": 0.4144,
      "step": 169000
    },
    {
      "epoch": 0.5390414561451045,
      "grad_norm": 0.001367919147014618,
      "learning_rate": 0.00013828756315646866,
      "loss": 0.271,
      "step": 169100
    },
    {
      "epoch": 0.5393602269648237,
      "grad_norm": 113.2868423461914,
      "learning_rate": 0.0001381919319105529,
      "loss": 0.3225,
      "step": 169200
    },
    {
      "epoch": 0.5396789977845428,
      "grad_norm": 10.593586921691895,
      "learning_rate": 0.00013809630066463714,
      "loss": 0.342,
      "step": 169300
    },
    {
      "epoch": 0.539997768604262,
      "grad_norm": 0.004658220801502466,
      "learning_rate": 0.0001380006694187214,
      "loss": 0.2552,
      "step": 169400
    },
    {
      "epoch": 0.5403165394239812,
      "grad_norm": 0.004569157492369413,
      "learning_rate": 0.00013790503817280564,
      "loss": 0.2549,
      "step": 169500
    },
    {
      "epoch": 0.5406353102437003,
      "grad_norm": 0.001252798829227686,
      "learning_rate": 0.00013780940692688988,
      "loss": 0.237,
      "step": 169600
    },
    {
      "epoch": 0.5409540810634195,
      "grad_norm": 25.431550979614258,
      "learning_rate": 0.00013771377568097415,
      "loss": 0.2322,
      "step": 169700
    },
    {
      "epoch": 0.5412728518831387,
      "grad_norm": 0.0070540341548621655,
      "learning_rate": 0.0001376181444350584,
      "loss": 0.2184,
      "step": 169800
    },
    {
      "epoch": 0.5415916227028578,
      "grad_norm": 0.0010343370959162712,
      "learning_rate": 0.00013752251318914266,
      "loss": 0.206,
      "step": 169900
    },
    {
      "epoch": 0.541910393522577,
      "grad_norm": 5.519069600268267e-05,
      "learning_rate": 0.0001374268819432269,
      "loss": 0.3801,
      "step": 170000
    },
    {
      "epoch": 0.5422291643422961,
      "grad_norm": 0.00037369003985077143,
      "learning_rate": 0.00013733125069731114,
      "loss": 0.3996,
      "step": 170100
    },
    {
      "epoch": 0.5425479351620153,
      "grad_norm": 11.580548286437988,
      "learning_rate": 0.0001372356194513954,
      "loss": 0.459,
      "step": 170200
    },
    {
      "epoch": 0.5428667059817345,
      "grad_norm": 1.0058531761169434,
      "learning_rate": 0.00013713998820547967,
      "loss": 0.2954,
      "step": 170300
    },
    {
      "epoch": 0.5431854768014536,
      "grad_norm": 74.60611724853516,
      "learning_rate": 0.00013704435695956392,
      "loss": 0.2304,
      "step": 170400
    },
    {
      "epoch": 0.5435042476211728,
      "grad_norm": 1.7499733075965196e-05,
      "learning_rate": 0.00013694872571364816,
      "loss": 0.4654,
      "step": 170500
    },
    {
      "epoch": 0.5438230184408919,
      "grad_norm": 0.00026208258350379765,
      "learning_rate": 0.00013685309446773242,
      "loss": 0.5042,
      "step": 170600
    },
    {
      "epoch": 0.5441417892606111,
      "grad_norm": 0.003401957917958498,
      "learning_rate": 0.00013675746322181666,
      "loss": 0.231,
      "step": 170700
    },
    {
      "epoch": 0.5444605600803303,
      "grad_norm": 0.0006726751453243196,
      "learning_rate": 0.00013666183197590093,
      "loss": 0.3149,
      "step": 170800
    },
    {
      "epoch": 0.5447793309000494,
      "grad_norm": 67.8267822265625,
      "learning_rate": 0.00013656620072998517,
      "loss": 0.2316,
      "step": 170900
    },
    {
      "epoch": 0.5450981017197686,
      "grad_norm": 7.437820022460073e-05,
      "learning_rate": 0.0001364705694840694,
      "loss": 0.2957,
      "step": 171000
    },
    {
      "epoch": 0.5454168725394878,
      "grad_norm": 0.002297390252351761,
      "learning_rate": 0.00013637493823815368,
      "loss": 0.2664,
      "step": 171100
    },
    {
      "epoch": 0.5457356433592069,
      "grad_norm": 0.001586357713676989,
      "learning_rate": 0.00013627930699223792,
      "loss": 0.3597,
      "step": 171200
    },
    {
      "epoch": 0.5460544141789261,
      "grad_norm": 0.001799913588911295,
      "learning_rate": 0.00013618367574632216,
      "loss": 0.3977,
      "step": 171300
    },
    {
      "epoch": 0.5463731849986452,
      "grad_norm": 41.05021286010742,
      "learning_rate": 0.00013608804450040643,
      "loss": 0.4157,
      "step": 171400
    },
    {
      "epoch": 0.5466919558183644,
      "grad_norm": 0.035786550492048264,
      "learning_rate": 0.00013599241325449067,
      "loss": 0.468,
      "step": 171500
    },
    {
      "epoch": 0.5470107266380836,
      "grad_norm": 0.07085702568292618,
      "learning_rate": 0.00013589678200857494,
      "loss": 0.264,
      "step": 171600
    },
    {
      "epoch": 0.5473294974578027,
      "grad_norm": 0.0023045644629746675,
      "learning_rate": 0.00013580115076265918,
      "loss": 0.4244,
      "step": 171700
    },
    {
      "epoch": 0.5476482682775219,
      "grad_norm": 0.0054282983765006065,
      "learning_rate": 0.00013570551951674342,
      "loss": 0.2755,
      "step": 171800
    },
    {
      "epoch": 0.5479670390972411,
      "grad_norm": 60.84783935546875,
      "learning_rate": 0.00013560988827082768,
      "loss": 0.2637,
      "step": 171900
    },
    {
      "epoch": 0.5482858099169602,
      "grad_norm": 27.162841796875,
      "learning_rate": 0.00013551425702491192,
      "loss": 0.1847,
      "step": 172000
    },
    {
      "epoch": 0.5486045807366794,
      "grad_norm": 0.0006828923942521214,
      "learning_rate": 0.0001354186257789962,
      "loss": 0.2986,
      "step": 172100
    },
    {
      "epoch": 0.5489233515563985,
      "grad_norm": 7.794779958203435e-05,
      "learning_rate": 0.00013532299453308043,
      "loss": 0.3237,
      "step": 172200
    },
    {
      "epoch": 0.5492421223761177,
      "grad_norm": 0.0011226359056308866,
      "learning_rate": 0.00013522736328716467,
      "loss": 0.3563,
      "step": 172300
    },
    {
      "epoch": 0.5495608931958369,
      "grad_norm": 0.0007097728666849434,
      "learning_rate": 0.00013513173204124894,
      "loss": 0.3565,
      "step": 172400
    },
    {
      "epoch": 0.549879664015556,
      "grad_norm": 0.01347339991480112,
      "learning_rate": 0.00013503610079533318,
      "loss": 0.2992,
      "step": 172500
    },
    {
      "epoch": 0.5501984348352752,
      "grad_norm": 0.004716442432254553,
      "learning_rate": 0.00013494046954941742,
      "loss": 0.3038,
      "step": 172600
    },
    {
      "epoch": 0.5505172056549943,
      "grad_norm": 0.0024769166484475136,
      "learning_rate": 0.0001348448383035017,
      "loss": 0.3197,
      "step": 172700
    },
    {
      "epoch": 0.5508359764747135,
      "grad_norm": 53.035545349121094,
      "learning_rate": 0.00013474920705758593,
      "loss": 0.2447,
      "step": 172800
    },
    {
      "epoch": 0.5511547472944327,
      "grad_norm": 36.38465118408203,
      "learning_rate": 0.0001346535758116702,
      "loss": 0.3888,
      "step": 172900
    },
    {
      "epoch": 0.5514735181141518,
      "grad_norm": 11.071834564208984,
      "learning_rate": 0.00013455794456575444,
      "loss": 0.2193,
      "step": 173000
    },
    {
      "epoch": 0.551792288933871,
      "grad_norm": 0.0008542102295905352,
      "learning_rate": 0.00013446231331983868,
      "loss": 0.4304,
      "step": 173100
    },
    {
      "epoch": 0.5521110597535902,
      "grad_norm": 3.20477557182312,
      "learning_rate": 0.00013436668207392294,
      "loss": 0.194,
      "step": 173200
    },
    {
      "epoch": 0.5524298305733093,
      "grad_norm": 0.001070192432962358,
      "learning_rate": 0.0001342710508280072,
      "loss": 0.243,
      "step": 173300
    },
    {
      "epoch": 0.5527486013930285,
      "grad_norm": 4.344578266143799,
      "learning_rate": 0.00013417541958209142,
      "loss": 0.2034,
      "step": 173400
    },
    {
      "epoch": 0.5530673722127476,
      "grad_norm": 0.03445977345108986,
      "learning_rate": 0.0001340797883361757,
      "loss": 0.2932,
      "step": 173500
    },
    {
      "epoch": 0.5533861430324668,
      "grad_norm": 0.0012290157610550523,
      "learning_rate": 0.00013398415709025996,
      "loss": 0.2371,
      "step": 173600
    },
    {
      "epoch": 0.553704913852186,
      "grad_norm": 0.0009795618243515491,
      "learning_rate": 0.0001338885258443442,
      "loss": 0.1686,
      "step": 173700
    },
    {
      "epoch": 0.5540236846719051,
      "grad_norm": 0.32705986499786377,
      "learning_rate": 0.00013379289459842844,
      "loss": 0.2493,
      "step": 173800
    },
    {
      "epoch": 0.5543424554916243,
      "grad_norm": 0.0014700470492243767,
      "learning_rate": 0.0001336972633525127,
      "loss": 0.2849,
      "step": 173900
    },
    {
      "epoch": 0.5546612263113435,
      "grad_norm": 90.01373291015625,
      "learning_rate": 0.00013360163210659695,
      "loss": 0.2673,
      "step": 174000
    },
    {
      "epoch": 0.5549799971310626,
      "grad_norm": 0.0005063156713731587,
      "learning_rate": 0.00013350600086068121,
      "loss": 0.1905,
      "step": 174100
    },
    {
      "epoch": 0.5552987679507818,
      "grad_norm": 3.60371996066533e-05,
      "learning_rate": 0.00013341036961476546,
      "loss": 0.2676,
      "step": 174200
    },
    {
      "epoch": 0.5556175387705009,
      "grad_norm": 40.25088119506836,
      "learning_rate": 0.0001333147383688497,
      "loss": 0.2364,
      "step": 174300
    },
    {
      "epoch": 0.5559363095902201,
      "grad_norm": 0.005944627802819014,
      "learning_rate": 0.00013321910712293396,
      "loss": 0.3363,
      "step": 174400
    },
    {
      "epoch": 0.5562550804099393,
      "grad_norm": 5.200811862945557,
      "learning_rate": 0.0001331234758770182,
      "loss": 0.237,
      "step": 174500
    },
    {
      "epoch": 0.5565738512296584,
      "grad_norm": 0.0028166521806269884,
      "learning_rate": 0.00013302784463110247,
      "loss": 0.3653,
      "step": 174600
    },
    {
      "epoch": 0.5568926220493776,
      "grad_norm": 0.00025662791449576616,
      "learning_rate": 0.0001329322133851867,
      "loss": 0.365,
      "step": 174700
    },
    {
      "epoch": 0.5572113928690967,
      "grad_norm": 0.0013412872795015574,
      "learning_rate": 0.00013283658213927095,
      "loss": 0.2969,
      "step": 174800
    },
    {
      "epoch": 0.5575301636888159,
      "grad_norm": 0.004831713158637285,
      "learning_rate": 0.00013274095089335522,
      "loss": 0.2075,
      "step": 174900
    },
    {
      "epoch": 0.5578489345085351,
      "grad_norm": 0.03773540258407593,
      "learning_rate": 0.00013264531964743946,
      "loss": 0.447,
      "step": 175000
    },
    {
      "epoch": 0.5581677053282542,
      "grad_norm": 0.010191702283918858,
      "learning_rate": 0.0001325496884015237,
      "loss": 0.242,
      "step": 175100
    },
    {
      "epoch": 0.5584864761479734,
      "grad_norm": 16.16131019592285,
      "learning_rate": 0.00013245405715560797,
      "loss": 0.5585,
      "step": 175200
    },
    {
      "epoch": 0.5588052469676926,
      "grad_norm": 0.0006695963675156236,
      "learning_rate": 0.0001323584259096922,
      "loss": 0.2124,
      "step": 175300
    },
    {
      "epoch": 0.5591240177874117,
      "grad_norm": 0.00799526460468769,
      "learning_rate": 0.00013226279466377648,
      "loss": 0.4121,
      "step": 175400
    },
    {
      "epoch": 0.5594427886071309,
      "grad_norm": 0.009395424276590347,
      "learning_rate": 0.00013216716341786072,
      "loss": 0.5171,
      "step": 175500
    },
    {
      "epoch": 0.55976155942685,
      "grad_norm": 0.07385025918483734,
      "learning_rate": 0.00013207153217194496,
      "loss": 0.145,
      "step": 175600
    },
    {
      "epoch": 0.5600803302465692,
      "grad_norm": 0.0011374232126399875,
      "learning_rate": 0.00013197590092602922,
      "loss": 0.2376,
      "step": 175700
    },
    {
      "epoch": 0.5603991010662884,
      "grad_norm": 0.0020088725723326206,
      "learning_rate": 0.00013188026968011346,
      "loss": 0.3053,
      "step": 175800
    },
    {
      "epoch": 0.5607178718860075,
      "grad_norm": 2.1488251150003634e-05,
      "learning_rate": 0.0001317846384341977,
      "loss": 0.408,
      "step": 175900
    },
    {
      "epoch": 0.5610366427057267,
      "grad_norm": 13.53234577178955,
      "learning_rate": 0.00013168900718828197,
      "loss": 0.4107,
      "step": 176000
    },
    {
      "epoch": 0.5613554135254459,
      "grad_norm": 0.001013291534036398,
      "learning_rate": 0.0001315933759423662,
      "loss": 0.1739,
      "step": 176100
    },
    {
      "epoch": 0.561674184345165,
      "grad_norm": 0.03233140707015991,
      "learning_rate": 0.00013149774469645048,
      "loss": 0.4556,
      "step": 176200
    },
    {
      "epoch": 0.5619929551648842,
      "grad_norm": 8.542696952819824,
      "learning_rate": 0.00013140211345053472,
      "loss": 0.4894,
      "step": 176300
    },
    {
      "epoch": 0.5623117259846033,
      "grad_norm": 0.007307309657335281,
      "learning_rate": 0.00013130648220461896,
      "loss": 0.5969,
      "step": 176400
    },
    {
      "epoch": 0.5626304968043225,
      "grad_norm": 7.992826431291178e-05,
      "learning_rate": 0.00013121085095870323,
      "loss": 0.388,
      "step": 176500
    },
    {
      "epoch": 0.5629492676240417,
      "grad_norm": 0.025103794410824776,
      "learning_rate": 0.0001311152197127875,
      "loss": 0.2414,
      "step": 176600
    },
    {
      "epoch": 0.5632680384437608,
      "grad_norm": 5.261897087097168,
      "learning_rate": 0.00013101958846687174,
      "loss": 0.1769,
      "step": 176700
    },
    {
      "epoch": 0.56358680926348,
      "grad_norm": 5.747212708229199e-05,
      "learning_rate": 0.00013092395722095598,
      "loss": 0.3232,
      "step": 176800
    },
    {
      "epoch": 0.5639055800831991,
      "grad_norm": 0.00024998874869197607,
      "learning_rate": 0.00013082832597504024,
      "loss": 0.2732,
      "step": 176900
    },
    {
      "epoch": 0.5642243509029183,
      "grad_norm": 0.4438973665237427,
      "learning_rate": 0.00013073269472912448,
      "loss": 0.2125,
      "step": 177000
    },
    {
      "epoch": 0.5645431217226375,
      "grad_norm": 0.009820380248129368,
      "learning_rate": 0.00013063706348320875,
      "loss": 0.22,
      "step": 177100
    },
    {
      "epoch": 0.5648618925423566,
      "grad_norm": 0.0017713168635964394,
      "learning_rate": 0.000130541432237293,
      "loss": 0.2665,
      "step": 177200
    },
    {
      "epoch": 0.5651806633620758,
      "grad_norm": 6.74576367600821e-05,
      "learning_rate": 0.00013044580099137723,
      "loss": 0.2971,
      "step": 177300
    },
    {
      "epoch": 0.565499434181795,
      "grad_norm": 0.0003046330239158124,
      "learning_rate": 0.0001303501697454615,
      "loss": 0.2936,
      "step": 177400
    },
    {
      "epoch": 0.5658182050015141,
      "grad_norm": 66.55182647705078,
      "learning_rate": 0.00013025453849954574,
      "loss": 0.2432,
      "step": 177500
    },
    {
      "epoch": 0.5661369758212333,
      "grad_norm": 0.00015779724344611168,
      "learning_rate": 0.00013015890725362998,
      "loss": 0.2798,
      "step": 177600
    },
    {
      "epoch": 0.5664557466409524,
      "grad_norm": 9.227186092175543e-05,
      "learning_rate": 0.00013006327600771425,
      "loss": 0.3607,
      "step": 177700
    },
    {
      "epoch": 0.5667745174606716,
      "grad_norm": 0.0002541417197789997,
      "learning_rate": 0.0001299676447617985,
      "loss": 0.3068,
      "step": 177800
    },
    {
      "epoch": 0.5670932882803909,
      "grad_norm": 23.36410140991211,
      "learning_rate": 0.00012987201351588276,
      "loss": 0.2026,
      "step": 177900
    },
    {
      "epoch": 0.56741205910011,
      "grad_norm": 0.0005894579226151109,
      "learning_rate": 0.000129776382269967,
      "loss": 0.2552,
      "step": 178000
    },
    {
      "epoch": 0.5677308299198291,
      "grad_norm": 0.0009139307076111436,
      "learning_rate": 0.00012968075102405124,
      "loss": 0.356,
      "step": 178100
    },
    {
      "epoch": 0.5680496007395484,
      "grad_norm": 3.272835601819679e-05,
      "learning_rate": 0.0001295851197781355,
      "loss": 0.298,
      "step": 178200
    },
    {
      "epoch": 0.5683683715592674,
      "grad_norm": 0.02906297706067562,
      "learning_rate": 0.00012948948853221974,
      "loss": 0.5312,
      "step": 178300
    },
    {
      "epoch": 0.5686871423789867,
      "grad_norm": 0.014078669250011444,
      "learning_rate": 0.00012939385728630398,
      "loss": 0.312,
      "step": 178400
    },
    {
      "epoch": 0.5690059131987057,
      "grad_norm": 0.00086131104035303,
      "learning_rate": 0.00012929822604038825,
      "loss": 0.4742,
      "step": 178500
    },
    {
      "epoch": 0.569324684018425,
      "grad_norm": 0.00025804771576076746,
      "learning_rate": 0.0001292025947944725,
      "loss": 0.1674,
      "step": 178600
    },
    {
      "epoch": 0.5696434548381442,
      "grad_norm": 2.653733730316162,
      "learning_rate": 0.00012910696354855676,
      "loss": 0.2369,
      "step": 178700
    },
    {
      "epoch": 0.5699622256578633,
      "grad_norm": 0.0034065679647028446,
      "learning_rate": 0.000129011332302641,
      "loss": 0.2775,
      "step": 178800
    },
    {
      "epoch": 0.5702809964775825,
      "grad_norm": 0.00018045368778984994,
      "learning_rate": 0.00012891570105672524,
      "loss": 0.2698,
      "step": 178900
    },
    {
      "epoch": 0.5705997672973016,
      "grad_norm": 107.77515411376953,
      "learning_rate": 0.0001288200698108095,
      "loss": 0.1826,
      "step": 179000
    },
    {
      "epoch": 0.5709185381170208,
      "grad_norm": 48.12455368041992,
      "learning_rate": 0.00012872443856489377,
      "loss": 0.3318,
      "step": 179100
    },
    {
      "epoch": 0.57123730893674,
      "grad_norm": 0.0012246973346918821,
      "learning_rate": 0.00012862880731897802,
      "loss": 0.1801,
      "step": 179200
    },
    {
      "epoch": 0.5715560797564591,
      "grad_norm": 0.043801967054605484,
      "learning_rate": 0.00012853317607306226,
      "loss": 0.2847,
      "step": 179300
    },
    {
      "epoch": 0.5718748505761783,
      "grad_norm": 0.003046897239983082,
      "learning_rate": 0.00012843754482714652,
      "loss": 0.3718,
      "step": 179400
    },
    {
      "epoch": 0.5721936213958975,
      "grad_norm": 0.008265516720712185,
      "learning_rate": 0.00012834191358123076,
      "loss": 0.4956,
      "step": 179500
    },
    {
      "epoch": 0.5725123922156166,
      "grad_norm": 0.0003832955553662032,
      "learning_rate": 0.00012824628233531503,
      "loss": 0.2906,
      "step": 179600
    },
    {
      "epoch": 0.5728311630353358,
      "grad_norm": 3.476807978586294e-05,
      "learning_rate": 0.00012815065108939927,
      "loss": 0.3533,
      "step": 179700
    },
    {
      "epoch": 0.5731499338550549,
      "grad_norm": 0.00033105124020949006,
      "learning_rate": 0.0001280550198434835,
      "loss": 0.1493,
      "step": 179800
    },
    {
      "epoch": 0.5734687046747741,
      "grad_norm": 5.631582098430954e-05,
      "learning_rate": 0.00012795938859756778,
      "loss": 0.3863,
      "step": 179900
    },
    {
      "epoch": 0.5737874754944933,
      "grad_norm": 5.601077282335609e-05,
      "learning_rate": 0.00012786375735165202,
      "loss": 0.3938,
      "step": 180000
    },
    {
      "epoch": 0.5741062463142124,
      "grad_norm": 0.0003064178745262325,
      "learning_rate": 0.00012776812610573626,
      "loss": 0.2837,
      "step": 180100
    },
    {
      "epoch": 0.5744250171339316,
      "grad_norm": 8.359438652405515e-05,
      "learning_rate": 0.00012767249485982053,
      "loss": 0.1369,
      "step": 180200
    },
    {
      "epoch": 0.5747437879536508,
      "grad_norm": 0.003364257048815489,
      "learning_rate": 0.00012757686361390477,
      "loss": 0.3352,
      "step": 180300
    },
    {
      "epoch": 0.5750625587733699,
      "grad_norm": 0.00014247898070607334,
      "learning_rate": 0.00012748123236798903,
      "loss": 0.3014,
      "step": 180400
    },
    {
      "epoch": 0.5753813295930891,
      "grad_norm": 0.0007032671710476279,
      "learning_rate": 0.00012738560112207328,
      "loss": 0.41,
      "step": 180500
    },
    {
      "epoch": 0.5757001004128082,
      "grad_norm": 0.00087440648349002,
      "learning_rate": 0.00012728996987615752,
      "loss": 0.2932,
      "step": 180600
    },
    {
      "epoch": 0.5760188712325274,
      "grad_norm": 57.92668914794922,
      "learning_rate": 0.00012719433863024178,
      "loss": 0.488,
      "step": 180700
    },
    {
      "epoch": 0.5763376420522466,
      "grad_norm": 83.05481719970703,
      "learning_rate": 0.00012709870738432602,
      "loss": 0.3263,
      "step": 180800
    },
    {
      "epoch": 0.5766564128719657,
      "grad_norm": 1.4333890676498413,
      "learning_rate": 0.0001270030761384103,
      "loss": 0.2153,
      "step": 180900
    },
    {
      "epoch": 0.5769751836916849,
      "grad_norm": 0.0039237188175320625,
      "learning_rate": 0.00012690744489249453,
      "loss": 0.3219,
      "step": 181000
    },
    {
      "epoch": 0.577293954511404,
      "grad_norm": 0.004248934332281351,
      "learning_rate": 0.00012681181364657877,
      "loss": 0.3535,
      "step": 181100
    },
    {
      "epoch": 0.5776127253311232,
      "grad_norm": 0.0004508761048782617,
      "learning_rate": 0.00012671618240066304,
      "loss": 0.2304,
      "step": 181200
    },
    {
      "epoch": 0.5779314961508424,
      "grad_norm": 0.00017171767831314355,
      "learning_rate": 0.00012662055115474728,
      "loss": 0.2977,
      "step": 181300
    },
    {
      "epoch": 0.5782502669705615,
      "grad_norm": 0.7931826710700989,
      "learning_rate": 0.00012652491990883152,
      "loss": 0.224,
      "step": 181400
    },
    {
      "epoch": 0.5785690377902807,
      "grad_norm": 0.006436330731958151,
      "learning_rate": 0.0001264292886629158,
      "loss": 0.2852,
      "step": 181500
    },
    {
      "epoch": 0.5788878086099999,
      "grad_norm": 0.021262869238853455,
      "learning_rate": 0.00012633365741700003,
      "loss": 0.447,
      "step": 181600
    },
    {
      "epoch": 0.579206579429719,
      "grad_norm": 0.042802371084690094,
      "learning_rate": 0.0001262380261710843,
      "loss": 0.1433,
      "step": 181700
    },
    {
      "epoch": 0.5795253502494382,
      "grad_norm": 0.00022422854090109468,
      "learning_rate": 0.00012614239492516854,
      "loss": 0.4509,
      "step": 181800
    },
    {
      "epoch": 0.5798441210691573,
      "grad_norm": 0.0034214667975902557,
      "learning_rate": 0.00012604676367925278,
      "loss": 0.1545,
      "step": 181900
    },
    {
      "epoch": 0.5801628918888765,
      "grad_norm": 0.03642703592777252,
      "learning_rate": 0.00012595113243333704,
      "loss": 0.4256,
      "step": 182000
    },
    {
      "epoch": 0.5804816627085957,
      "grad_norm": 0.00951260607689619,
      "learning_rate": 0.0001258555011874213,
      "loss": 0.1463,
      "step": 182100
    },
    {
      "epoch": 0.5808004335283148,
      "grad_norm": 0.0002253959683002904,
      "learning_rate": 0.00012575986994150552,
      "loss": 0.1143,
      "step": 182200
    },
    {
      "epoch": 0.581119204348034,
      "grad_norm": 2.264557361602783,
      "learning_rate": 0.0001256642386955898,
      "loss": 0.343,
      "step": 182300
    },
    {
      "epoch": 0.5814379751677532,
      "grad_norm": 0.005084867589175701,
      "learning_rate": 0.00012556860744967406,
      "loss": 0.2762,
      "step": 182400
    },
    {
      "epoch": 0.5817567459874723,
      "grad_norm": 0.00014213102986104786,
      "learning_rate": 0.0001254729762037583,
      "loss": 0.2647,
      "step": 182500
    },
    {
      "epoch": 0.5820755168071915,
      "grad_norm": 44.42018127441406,
      "learning_rate": 0.00012537734495784254,
      "loss": 0.2324,
      "step": 182600
    },
    {
      "epoch": 0.5823942876269106,
      "grad_norm": 0.0056312596425414085,
      "learning_rate": 0.0001252817137119268,
      "loss": 0.3885,
      "step": 182700
    },
    {
      "epoch": 0.5827130584466298,
      "grad_norm": 0.0021698891650885344,
      "learning_rate": 0.00012518608246601105,
      "loss": 0.4209,
      "step": 182800
    },
    {
      "epoch": 0.583031829266349,
      "grad_norm": 9.01883831829764e-05,
      "learning_rate": 0.00012509045122009531,
      "loss": 0.3094,
      "step": 182900
    },
    {
      "epoch": 0.5833506000860681,
      "grad_norm": 0.007079099304974079,
      "learning_rate": 0.00012499481997417956,
      "loss": 0.3334,
      "step": 183000
    },
    {
      "epoch": 0.5836693709057873,
      "grad_norm": 30.48975372314453,
      "learning_rate": 0.0001248991887282638,
      "loss": 0.3484,
      "step": 183100
    },
    {
      "epoch": 0.5839881417255064,
      "grad_norm": 0.00040716008516028523,
      "learning_rate": 0.00012480355748234806,
      "loss": 0.3034,
      "step": 183200
    },
    {
      "epoch": 0.5843069125452256,
      "grad_norm": 6.487885548267514e-05,
      "learning_rate": 0.0001247079262364323,
      "loss": 0.2346,
      "step": 183300
    },
    {
      "epoch": 0.5846256833649448,
      "grad_norm": 0.06586793065071106,
      "learning_rate": 0.00012461229499051657,
      "loss": 0.1228,
      "step": 183400
    },
    {
      "epoch": 0.5849444541846639,
      "grad_norm": 0.0004753294342663139,
      "learning_rate": 0.0001245166637446008,
      "loss": 0.3234,
      "step": 183500
    },
    {
      "epoch": 0.5852632250043831,
      "grad_norm": 0.0003998728934675455,
      "learning_rate": 0.00012442103249868505,
      "loss": 0.3981,
      "step": 183600
    },
    {
      "epoch": 0.5855819958241023,
      "grad_norm": 83.38509368896484,
      "learning_rate": 0.00012432540125276932,
      "loss": 0.3939,
      "step": 183700
    },
    {
      "epoch": 0.5859007666438214,
      "grad_norm": 0.0002857022627722472,
      "learning_rate": 0.00012422977000685356,
      "loss": 0.3403,
      "step": 183800
    },
    {
      "epoch": 0.5862195374635406,
      "grad_norm": 0.0009039500146172941,
      "learning_rate": 0.0001241341387609378,
      "loss": 0.1515,
      "step": 183900
    },
    {
      "epoch": 0.5865383082832597,
      "grad_norm": 0.0078439237549901,
      "learning_rate": 0.00012403850751502207,
      "loss": 0.3443,
      "step": 184000
    },
    {
      "epoch": 0.5868570791029789,
      "grad_norm": 56.093807220458984,
      "learning_rate": 0.0001239428762691063,
      "loss": 0.3024,
      "step": 184100
    },
    {
      "epoch": 0.5871758499226981,
      "grad_norm": 0.0005729461554437876,
      "learning_rate": 0.00012384724502319058,
      "loss": 0.2431,
      "step": 184200
    },
    {
      "epoch": 0.5874946207424172,
      "grad_norm": 0.005205749999731779,
      "learning_rate": 0.00012375161377727482,
      "loss": 0.2333,
      "step": 184300
    },
    {
      "epoch": 0.5878133915621364,
      "grad_norm": 6.862334703328088e-05,
      "learning_rate": 0.00012365598253135906,
      "loss": 0.3784,
      "step": 184400
    },
    {
      "epoch": 0.5881321623818556,
      "grad_norm": 27.59018325805664,
      "learning_rate": 0.00012356035128544332,
      "loss": 0.2088,
      "step": 184500
    },
    {
      "epoch": 0.5884509332015747,
      "grad_norm": 23.550827026367188,
      "learning_rate": 0.00012346472003952756,
      "loss": 0.2961,
      "step": 184600
    },
    {
      "epoch": 0.5887697040212939,
      "grad_norm": 0.022478053346276283,
      "learning_rate": 0.0001233690887936118,
      "loss": 0.2305,
      "step": 184700
    },
    {
      "epoch": 0.589088474841013,
      "grad_norm": 6.616441532969475e-05,
      "learning_rate": 0.00012327345754769607,
      "loss": 0.1876,
      "step": 184800
    },
    {
      "epoch": 0.5894072456607322,
      "grad_norm": 0.001162756234407425,
      "learning_rate": 0.0001231778263017803,
      "loss": 0.2289,
      "step": 184900
    },
    {
      "epoch": 0.5897260164804514,
      "grad_norm": 0.017435221001505852,
      "learning_rate": 0.00012308219505586458,
      "loss": 0.3872,
      "step": 185000
    },
    {
      "epoch": 0.5900447873001705,
      "grad_norm": 0.0009976442670449615,
      "learning_rate": 0.00012298656380994882,
      "loss": 0.438,
      "step": 185100
    },
    {
      "epoch": 0.5903635581198897,
      "grad_norm": 0.0002969801425933838,
      "learning_rate": 0.00012289093256403306,
      "loss": 0.1841,
      "step": 185200
    },
    {
      "epoch": 0.5906823289396088,
      "grad_norm": 9.056417184183374e-05,
      "learning_rate": 0.00012279530131811733,
      "loss": 0.4362,
      "step": 185300
    },
    {
      "epoch": 0.591001099759328,
      "grad_norm": 41.28779983520508,
      "learning_rate": 0.0001226996700722016,
      "loss": 0.1093,
      "step": 185400
    },
    {
      "epoch": 0.5913198705790472,
      "grad_norm": 6.827680772403255e-05,
      "learning_rate": 0.00012260403882628584,
      "loss": 0.3003,
      "step": 185500
    },
    {
      "epoch": 0.5916386413987663,
      "grad_norm": 0.6113381385803223,
      "learning_rate": 0.00012250840758037008,
      "loss": 0.3141,
      "step": 185600
    },
    {
      "epoch": 0.5919574122184855,
      "grad_norm": 0.44969111680984497,
      "learning_rate": 0.00012241277633445434,
      "loss": 0.1861,
      "step": 185700
    },
    {
      "epoch": 0.5922761830382047,
      "grad_norm": 19.14798355102539,
      "learning_rate": 0.00012231714508853858,
      "loss": 0.2631,
      "step": 185800
    },
    {
      "epoch": 0.5925949538579238,
      "grad_norm": 0.0008940226980485022,
      "learning_rate": 0.00012222151384262285,
      "loss": 0.2725,
      "step": 185900
    },
    {
      "epoch": 0.592913724677643,
      "grad_norm": 7.374639244517311e-05,
      "learning_rate": 0.0001221258825967071,
      "loss": 0.2471,
      "step": 186000
    },
    {
      "epoch": 0.5932324954973621,
      "grad_norm": 0.23656320571899414,
      "learning_rate": 0.00012203025135079133,
      "loss": 0.2749,
      "step": 186100
    },
    {
      "epoch": 0.5935512663170813,
      "grad_norm": 0.0014595179818570614,
      "learning_rate": 0.00012193462010487559,
      "loss": 0.2615,
      "step": 186200
    },
    {
      "epoch": 0.5938700371368005,
      "grad_norm": 0.0001928246347233653,
      "learning_rate": 0.00012183898885895985,
      "loss": 0.3032,
      "step": 186300
    },
    {
      "epoch": 0.5941888079565196,
      "grad_norm": 0.022132160142064095,
      "learning_rate": 0.00012174335761304408,
      "loss": 0.3414,
      "step": 186400
    },
    {
      "epoch": 0.5945075787762388,
      "grad_norm": 0.0006899432046338916,
      "learning_rate": 0.00012164772636712833,
      "loss": 0.3804,
      "step": 186500
    },
    {
      "epoch": 0.594826349595958,
      "grad_norm": 0.012381654232740402,
      "learning_rate": 0.0001215520951212126,
      "loss": 0.1151,
      "step": 186600
    },
    {
      "epoch": 0.5951451204156771,
      "grad_norm": 0.0002663637278601527,
      "learning_rate": 0.00012145646387529685,
      "loss": 0.1724,
      "step": 186700
    },
    {
      "epoch": 0.5954638912353963,
      "grad_norm": 0.000748171703889966,
      "learning_rate": 0.00012136083262938108,
      "loss": 0.184,
      "step": 186800
    },
    {
      "epoch": 0.5957826620551154,
      "grad_norm": 0.0013217004016041756,
      "learning_rate": 0.00012126520138346535,
      "loss": 0.3361,
      "step": 186900
    },
    {
      "epoch": 0.5961014328748346,
      "grad_norm": 0.0008945020381361246,
      "learning_rate": 0.0001211695701375496,
      "loss": 0.2845,
      "step": 187000
    },
    {
      "epoch": 0.5964202036945538,
      "grad_norm": 0.0008905346039682627,
      "learning_rate": 0.00012107393889163386,
      "loss": 0.1482,
      "step": 187100
    },
    {
      "epoch": 0.5967389745142729,
      "grad_norm": 0.0010642196284607053,
      "learning_rate": 0.0001209783076457181,
      "loss": 0.2358,
      "step": 187200
    },
    {
      "epoch": 0.5970577453339921,
      "grad_norm": 0.003381646005436778,
      "learning_rate": 0.00012088267639980235,
      "loss": 0.2453,
      "step": 187300
    },
    {
      "epoch": 0.5973765161537112,
      "grad_norm": 0.00022746625472791493,
      "learning_rate": 0.0001207870451538866,
      "loss": 0.2795,
      "step": 187400
    },
    {
      "epoch": 0.5976952869734304,
      "grad_norm": 0.09513296186923981,
      "learning_rate": 0.00012069141390797086,
      "loss": 0.3073,
      "step": 187500
    },
    {
      "epoch": 0.5980140577931496,
      "grad_norm": 0.0007914763409644365,
      "learning_rate": 0.00012059578266205511,
      "loss": 0.4251,
      "step": 187600
    },
    {
      "epoch": 0.5983328286128687,
      "grad_norm": 9.999694884754717e-05,
      "learning_rate": 0.00012050015141613935,
      "loss": 0.1861,
      "step": 187700
    },
    {
      "epoch": 0.5986515994325879,
      "grad_norm": 0.0005503561114892364,
      "learning_rate": 0.00012040452017022361,
      "loss": 0.1771,
      "step": 187800
    },
    {
      "epoch": 0.5989703702523071,
      "grad_norm": 0.02994643710553646,
      "learning_rate": 0.00012030888892430786,
      "loss": 0.128,
      "step": 187900
    },
    {
      "epoch": 0.5992891410720262,
      "grad_norm": 0.004532952792942524,
      "learning_rate": 0.00012021325767839212,
      "loss": 0.2741,
      "step": 188000
    },
    {
      "epoch": 0.5996079118917454,
      "grad_norm": 9.554001007927582e-05,
      "learning_rate": 0.00012011762643247636,
      "loss": 0.2603,
      "step": 188100
    },
    {
      "epoch": 0.5999266827114645,
      "grad_norm": 0.00033979356521740556,
      "learning_rate": 0.00012002199518656061,
      "loss": 0.2548,
      "step": 188200
    },
    {
      "epoch": 0.6002454535311837,
      "grad_norm": 0.0007619598181918263,
      "learning_rate": 0.00011992636394064486,
      "loss": 0.2973,
      "step": 188300
    },
    {
      "epoch": 0.600564224350903,
      "grad_norm": 0.0008608087082393467,
      "learning_rate": 0.00011983073269472912,
      "loss": 0.2803,
      "step": 188400
    },
    {
      "epoch": 0.600882995170622,
      "grad_norm": 0.00016255940136034042,
      "learning_rate": 0.00011973510144881336,
      "loss": 0.2627,
      "step": 188500
    },
    {
      "epoch": 0.6012017659903413,
      "grad_norm": 3.080569877056405e-05,
      "learning_rate": 0.00011963947020289761,
      "loss": 0.2091,
      "step": 188600
    },
    {
      "epoch": 0.6015205368100605,
      "grad_norm": 0.02561359852552414,
      "learning_rate": 0.00011954383895698187,
      "loss": 0.2342,
      "step": 188700
    },
    {
      "epoch": 0.6018393076297796,
      "grad_norm": 0.0006414243252947927,
      "learning_rate": 0.00011944820771106612,
      "loss": 0.2308,
      "step": 188800
    },
    {
      "epoch": 0.6021580784494988,
      "grad_norm": 0.00013523596862796694,
      "learning_rate": 0.00011935257646515036,
      "loss": 0.3407,
      "step": 188900
    },
    {
      "epoch": 0.6024768492692179,
      "grad_norm": 0.0014466585125774145,
      "learning_rate": 0.00011925694521923461,
      "loss": 0.2449,
      "step": 189000
    },
    {
      "epoch": 0.6027956200889371,
      "grad_norm": 30.197826385498047,
      "learning_rate": 0.00011916131397331887,
      "loss": 0.5296,
      "step": 189100
    },
    {
      "epoch": 0.6031143909086563,
      "grad_norm": 107.75431060791016,
      "learning_rate": 0.00011906568272740312,
      "loss": 0.2467,
      "step": 189200
    },
    {
      "epoch": 0.6034331617283754,
      "grad_norm": 0.0010136931668967009,
      "learning_rate": 0.00011897005148148736,
      "loss": 0.2247,
      "step": 189300
    },
    {
      "epoch": 0.6037519325480946,
      "grad_norm": 10.306699752807617,
      "learning_rate": 0.00011887442023557162,
      "loss": 0.2766,
      "step": 189400
    },
    {
      "epoch": 0.6040707033678137,
      "grad_norm": 0.0006201494834385812,
      "learning_rate": 0.00011877878898965588,
      "loss": 0.2081,
      "step": 189500
    },
    {
      "epoch": 0.6043894741875329,
      "grad_norm": 0.005851502995938063,
      "learning_rate": 0.00011868315774374014,
      "loss": 0.2688,
      "step": 189600
    },
    {
      "epoch": 0.6047082450072521,
      "grad_norm": 0.0003000370052177459,
      "learning_rate": 0.00011858752649782439,
      "loss": 0.3036,
      "step": 189700
    },
    {
      "epoch": 0.6050270158269712,
      "grad_norm": 0.018899928778409958,
      "learning_rate": 0.00011849189525190863,
      "loss": 0.1473,
      "step": 189800
    },
    {
      "epoch": 0.6053457866466904,
      "grad_norm": 0.004812959115952253,
      "learning_rate": 0.00011839626400599289,
      "loss": 0.217,
      "step": 189900
    },
    {
      "epoch": 0.6056645574664096,
      "grad_norm": 0.002937359968200326,
      "learning_rate": 0.00011830063276007714,
      "loss": 0.3641,
      "step": 190000
    },
    {
      "epoch": 0.6059833282861287,
      "grad_norm": 0.007646291982382536,
      "learning_rate": 0.00011820500151416139,
      "loss": 0.2933,
      "step": 190100
    },
    {
      "epoch": 0.6063020991058479,
      "grad_norm": 0.0019023717613890767,
      "learning_rate": 0.00011810937026824563,
      "loss": 0.3303,
      "step": 190200
    },
    {
      "epoch": 0.606620869925567,
      "grad_norm": 0.00044291140511631966,
      "learning_rate": 0.00011801373902232989,
      "loss": 0.1526,
      "step": 190300
    },
    {
      "epoch": 0.6069396407452862,
      "grad_norm": 0.00030970622901804745,
      "learning_rate": 0.00011791810777641414,
      "loss": 0.2693,
      "step": 190400
    },
    {
      "epoch": 0.6072584115650054,
      "grad_norm": 0.004411534871906042,
      "learning_rate": 0.0001178224765304984,
      "loss": 0.261,
      "step": 190500
    },
    {
      "epoch": 0.6075771823847245,
      "grad_norm": 0.0026262393221259117,
      "learning_rate": 0.00011772684528458264,
      "loss": 0.4702,
      "step": 190600
    },
    {
      "epoch": 0.6078959532044437,
      "grad_norm": 0.0009187499526888132,
      "learning_rate": 0.00011763121403866689,
      "loss": 0.3376,
      "step": 190700
    },
    {
      "epoch": 0.6082147240241629,
      "grad_norm": 0.0002863613481167704,
      "learning_rate": 0.00011753558279275114,
      "loss": 0.1813,
      "step": 190800
    },
    {
      "epoch": 0.608533494843882,
      "grad_norm": 16.894102096557617,
      "learning_rate": 0.0001174399515468354,
      "loss": 0.3488,
      "step": 190900
    },
    {
      "epoch": 0.6088522656636012,
      "grad_norm": 1.259622695215512e-05,
      "learning_rate": 0.00011734432030091964,
      "loss": 0.2318,
      "step": 191000
    },
    {
      "epoch": 0.6091710364833203,
      "grad_norm": 29.346521377563477,
      "learning_rate": 0.00011724868905500389,
      "loss": 0.3122,
      "step": 191100
    },
    {
      "epoch": 0.6094898073030395,
      "grad_norm": 22.063533782958984,
      "learning_rate": 0.00011715305780908815,
      "loss": 0.2998,
      "step": 191200
    },
    {
      "epoch": 0.6098085781227587,
      "grad_norm": 0.0013844444183632731,
      "learning_rate": 0.0001170574265631724,
      "loss": 0.2038,
      "step": 191300
    },
    {
      "epoch": 0.6101273489424778,
      "grad_norm": 0.11052461713552475,
      "learning_rate": 0.00011696179531725664,
      "loss": 0.2889,
      "step": 191400
    },
    {
      "epoch": 0.610446119762197,
      "grad_norm": 0.00023555013467557728,
      "learning_rate": 0.0001168661640713409,
      "loss": 0.2571,
      "step": 191500
    },
    {
      "epoch": 0.6107648905819162,
      "grad_norm": 0.002140643773600459,
      "learning_rate": 0.00011677053282542515,
      "loss": 0.2074,
      "step": 191600
    },
    {
      "epoch": 0.6110836614016353,
      "grad_norm": 0.023528466001152992,
      "learning_rate": 0.0001166749015795094,
      "loss": 0.3964,
      "step": 191700
    },
    {
      "epoch": 0.6114024322213545,
      "grad_norm": 0.00029719830490648746,
      "learning_rate": 0.00011657927033359366,
      "loss": 0.2931,
      "step": 191800
    },
    {
      "epoch": 0.6117212030410736,
      "grad_norm": 0.0020211597438901663,
      "learning_rate": 0.0001164836390876779,
      "loss": 0.2653,
      "step": 191900
    },
    {
      "epoch": 0.6120399738607928,
      "grad_norm": 0.9869177341461182,
      "learning_rate": 0.00011638800784176215,
      "loss": 0.2248,
      "step": 192000
    },
    {
      "epoch": 0.612358744680512,
      "grad_norm": 5.52163619431667e-05,
      "learning_rate": 0.0001162923765958464,
      "loss": 0.2489,
      "step": 192100
    },
    {
      "epoch": 0.6126775155002311,
      "grad_norm": 0.010060359723865986,
      "learning_rate": 0.00011619674534993067,
      "loss": 0.5189,
      "step": 192200
    },
    {
      "epoch": 0.6129962863199503,
      "grad_norm": 20.319116592407227,
      "learning_rate": 0.0001161011141040149,
      "loss": 0.3106,
      "step": 192300
    },
    {
      "epoch": 0.6133150571396694,
      "grad_norm": 0.003523040795698762,
      "learning_rate": 0.00011600548285809915,
      "loss": 0.1811,
      "step": 192400
    },
    {
      "epoch": 0.6136338279593886,
      "grad_norm": 4.319348335266113,
      "learning_rate": 0.00011590985161218342,
      "loss": 0.341,
      "step": 192500
    },
    {
      "epoch": 0.6139525987791078,
      "grad_norm": 0.0003934172564186156,
      "learning_rate": 0.00011581422036626767,
      "loss": 0.3161,
      "step": 192600
    },
    {
      "epoch": 0.6142713695988269,
      "grad_norm": 66.81560516357422,
      "learning_rate": 0.00011571858912035191,
      "loss": 0.2668,
      "step": 192700
    },
    {
      "epoch": 0.6145901404185461,
      "grad_norm": 0.0070651923306286335,
      "learning_rate": 0.00011562295787443617,
      "loss": 0.286,
      "step": 192800
    },
    {
      "epoch": 0.6149089112382653,
      "grad_norm": 0.002237752778455615,
      "learning_rate": 0.00011552732662852042,
      "loss": 0.2185,
      "step": 192900
    },
    {
      "epoch": 0.6152276820579844,
      "grad_norm": 0.00021505597396753728,
      "learning_rate": 0.00011543169538260467,
      "loss": 0.3082,
      "step": 193000
    },
    {
      "epoch": 0.6155464528777036,
      "grad_norm": 0.0009954535635188222,
      "learning_rate": 0.00011533606413668892,
      "loss": 0.3707,
      "step": 193100
    },
    {
      "epoch": 0.6158652236974227,
      "grad_norm": 0.0004059959901496768,
      "learning_rate": 0.00011524043289077317,
      "loss": 0.1577,
      "step": 193200
    },
    {
      "epoch": 0.6161839945171419,
      "grad_norm": 0.000746516278013587,
      "learning_rate": 0.00011514480164485742,
      "loss": 0.2354,
      "step": 193300
    },
    {
      "epoch": 0.6165027653368611,
      "grad_norm": 0.0012715642806142569,
      "learning_rate": 0.00011504917039894168,
      "loss": 0.1417,
      "step": 193400
    },
    {
      "epoch": 0.6168215361565802,
      "grad_norm": 0.00016241332923527807,
      "learning_rate": 0.00011495353915302592,
      "loss": 0.2459,
      "step": 193500
    },
    {
      "epoch": 0.6171403069762994,
      "grad_norm": 3.1355724786408246e-05,
      "learning_rate": 0.00011485790790711017,
      "loss": 0.2886,
      "step": 193600
    },
    {
      "epoch": 0.6174590777960186,
      "grad_norm": 0.01986975595355034,
      "learning_rate": 0.00011476227666119443,
      "loss": 0.0959,
      "step": 193700
    },
    {
      "epoch": 0.6177778486157377,
      "grad_norm": 3.399114211788401e-05,
      "learning_rate": 0.00011466664541527868,
      "loss": 0.2469,
      "step": 193800
    },
    {
      "epoch": 0.6180966194354569,
      "grad_norm": 0.40954551100730896,
      "learning_rate": 0.00011457101416936293,
      "loss": 0.2826,
      "step": 193900
    },
    {
      "epoch": 0.618415390255176,
      "grad_norm": 0.3073589503765106,
      "learning_rate": 0.00011447538292344717,
      "loss": 0.1914,
      "step": 194000
    },
    {
      "epoch": 0.6187341610748952,
      "grad_norm": 7.706649194005877e-05,
      "learning_rate": 0.00011437975167753143,
      "loss": 0.3799,
      "step": 194100
    },
    {
      "epoch": 0.6190529318946144,
      "grad_norm": 0.3668935298919678,
      "learning_rate": 0.00011428412043161568,
      "loss": 0.1832,
      "step": 194200
    },
    {
      "epoch": 0.6193717027143335,
      "grad_norm": 0.0014433126198127866,
      "learning_rate": 0.00011418848918569994,
      "loss": 0.3326,
      "step": 194300
    },
    {
      "epoch": 0.6196904735340527,
      "grad_norm": 0.0012869721977040172,
      "learning_rate": 0.00011409285793978418,
      "loss": 0.288,
      "step": 194400
    },
    {
      "epoch": 0.6200092443537718,
      "grad_norm": 3.360013484954834,
      "learning_rate": 0.00011399722669386843,
      "loss": 0.307,
      "step": 194500
    },
    {
      "epoch": 0.620328015173491,
      "grad_norm": 0.0021298842038959265,
      "learning_rate": 0.00011390159544795268,
      "loss": 0.226,
      "step": 194600
    },
    {
      "epoch": 0.6206467859932102,
      "grad_norm": 0.08358198404312134,
      "learning_rate": 0.00011380596420203694,
      "loss": 0.3137,
      "step": 194700
    },
    {
      "epoch": 0.6209655568129293,
      "grad_norm": 0.007010981906205416,
      "learning_rate": 0.00011371033295612118,
      "loss": 0.1008,
      "step": 194800
    },
    {
      "epoch": 0.6212843276326485,
      "grad_norm": 0.10980963706970215,
      "learning_rate": 0.00011361470171020543,
      "loss": 0.1268,
      "step": 194900
    },
    {
      "epoch": 0.6216030984523677,
      "grad_norm": 0.7148608565330505,
      "learning_rate": 0.00011351907046428969,
      "loss": 0.2305,
      "step": 195000
    },
    {
      "epoch": 0.6219218692720868,
      "grad_norm": 0.0031871842220425606,
      "learning_rate": 0.00011342343921837395,
      "loss": 0.2011,
      "step": 195100
    },
    {
      "epoch": 0.622240640091806,
      "grad_norm": 8.995949610834941e-05,
      "learning_rate": 0.00011332780797245818,
      "loss": 0.2082,
      "step": 195200
    },
    {
      "epoch": 0.6225594109115251,
      "grad_norm": 0.0003598571347538382,
      "learning_rate": 0.00011323217672654243,
      "loss": 0.4211,
      "step": 195300
    },
    {
      "epoch": 0.6228781817312443,
      "grad_norm": 6.669940921710804e-05,
      "learning_rate": 0.0001131365454806267,
      "loss": 0.3865,
      "step": 195400
    },
    {
      "epoch": 0.6231969525509635,
      "grad_norm": 0.006279894150793552,
      "learning_rate": 0.00011304091423471095,
      "loss": 0.2781,
      "step": 195500
    },
    {
      "epoch": 0.6235157233706826,
      "grad_norm": 2.3946819055709057e-05,
      "learning_rate": 0.00011294528298879518,
      "loss": 0.203,
      "step": 195600
    },
    {
      "epoch": 0.6238344941904018,
      "grad_norm": 0.0010133407777175307,
      "learning_rate": 0.00011284965174287945,
      "loss": 0.3093,
      "step": 195700
    },
    {
      "epoch": 0.624153265010121,
      "grad_norm": 0.0024867523461580276,
      "learning_rate": 0.0001127540204969637,
      "loss": 0.3054,
      "step": 195800
    },
    {
      "epoch": 0.6244720358298401,
      "grad_norm": 5.499943654285744e-05,
      "learning_rate": 0.00011265838925104796,
      "loss": 0.1469,
      "step": 195900
    },
    {
      "epoch": 0.6247908066495593,
      "grad_norm": 0.000914171221666038,
      "learning_rate": 0.00011256275800513221,
      "loss": 0.2797,
      "step": 196000
    },
    {
      "epoch": 0.6251095774692784,
      "grad_norm": 0.0009797880193218589,
      "learning_rate": 0.00011246712675921645,
      "loss": 0.2547,
      "step": 196100
    },
    {
      "epoch": 0.6254283482889976,
      "grad_norm": 0.00012793304631486535,
      "learning_rate": 0.0001123714955133007,
      "loss": 0.2385,
      "step": 196200
    },
    {
      "epoch": 0.6257471191087168,
      "grad_norm": 9.102543830871582,
      "learning_rate": 0.00011227586426738496,
      "loss": 0.1818,
      "step": 196300
    },
    {
      "epoch": 0.6260658899284359,
      "grad_norm": 0.006953624077141285,
      "learning_rate": 0.00011218023302146921,
      "loss": 0.3572,
      "step": 196400
    },
    {
      "epoch": 0.6263846607481551,
      "grad_norm": 0.0012848679907619953,
      "learning_rate": 0.00011208460177555345,
      "loss": 0.208,
      "step": 196500
    },
    {
      "epoch": 0.6267034315678742,
      "grad_norm": 0.00016406000941060483,
      "learning_rate": 0.00011198897052963771,
      "loss": 0.1918,
      "step": 196600
    },
    {
      "epoch": 0.6270222023875934,
      "grad_norm": 6.44773530960083,
      "learning_rate": 0.00011189333928372196,
      "loss": 0.3524,
      "step": 196700
    },
    {
      "epoch": 0.6273409732073126,
      "grad_norm": 8.001238893484697e-05,
      "learning_rate": 0.00011179770803780621,
      "loss": 0.3364,
      "step": 196800
    },
    {
      "epoch": 0.6276597440270317,
      "grad_norm": 18.650861740112305,
      "learning_rate": 0.00011170207679189046,
      "loss": 0.2634,
      "step": 196900
    },
    {
      "epoch": 0.6279785148467509,
      "grad_norm": 0.003205742221325636,
      "learning_rate": 0.00011160644554597471,
      "loss": 0.0881,
      "step": 197000
    },
    {
      "epoch": 0.6282972856664701,
      "grad_norm": 1.7966148853302002,
      "learning_rate": 0.00011151081430005896,
      "loss": 0.1389,
      "step": 197100
    },
    {
      "epoch": 0.6286160564861892,
      "grad_norm": 0.008486801758408546,
      "learning_rate": 0.00011141518305414322,
      "loss": 0.272,
      "step": 197200
    },
    {
      "epoch": 0.6289348273059084,
      "grad_norm": 0.002540637506172061,
      "learning_rate": 0.00011131955180822746,
      "loss": 0.2405,
      "step": 197300
    },
    {
      "epoch": 0.6292535981256275,
      "grad_norm": 67.1299057006836,
      "learning_rate": 0.00011122392056231171,
      "loss": 0.2887,
      "step": 197400
    },
    {
      "epoch": 0.6295723689453467,
      "grad_norm": 0.08831504732370377,
      "learning_rate": 0.00011112828931639597,
      "loss": 0.2423,
      "step": 197500
    },
    {
      "epoch": 0.6298911397650659,
      "grad_norm": 49.41442108154297,
      "learning_rate": 0.00011103265807048022,
      "loss": 0.2956,
      "step": 197600
    },
    {
      "epoch": 0.630209910584785,
      "grad_norm": 0.12503109872341156,
      "learning_rate": 0.00011093702682456446,
      "loss": 0.1861,
      "step": 197700
    },
    {
      "epoch": 0.6305286814045042,
      "grad_norm": 0.002341215033084154,
      "learning_rate": 0.00011084139557864871,
      "loss": 0.1095,
      "step": 197800
    },
    {
      "epoch": 0.6308474522242234,
      "grad_norm": 0.0016900916816666722,
      "learning_rate": 0.00011074576433273297,
      "loss": 0.2623,
      "step": 197900
    },
    {
      "epoch": 0.6311662230439425,
      "grad_norm": 0.000388138898415491,
      "learning_rate": 0.00011065013308681722,
      "loss": 0.2274,
      "step": 198000
    },
    {
      "epoch": 0.6314849938636617,
      "grad_norm": 0.040041837841272354,
      "learning_rate": 0.00011055450184090146,
      "loss": 0.207,
      "step": 198100
    },
    {
      "epoch": 0.6318037646833808,
      "grad_norm": 0.00029212015215307474,
      "learning_rate": 0.00011045887059498572,
      "loss": 0.1773,
      "step": 198200
    },
    {
      "epoch": 0.6321225355031,
      "grad_norm": 0.16611216962337494,
      "learning_rate": 0.00011036323934906998,
      "loss": 0.1978,
      "step": 198300
    },
    {
      "epoch": 0.6324413063228193,
      "grad_norm": 0.005226821172982454,
      "learning_rate": 0.00011026760810315424,
      "loss": 0.3483,
      "step": 198400
    },
    {
      "epoch": 0.6327600771425383,
      "grad_norm": 0.003986189607530832,
      "learning_rate": 0.00011017197685723849,
      "loss": 0.2151,
      "step": 198500
    },
    {
      "epoch": 0.6330788479622576,
      "grad_norm": 0.0020554668735712767,
      "learning_rate": 0.00011007634561132273,
      "loss": 0.1861,
      "step": 198600
    },
    {
      "epoch": 0.6333976187819766,
      "grad_norm": 0.043467193841934204,
      "learning_rate": 0.00010998071436540698,
      "loss": 0.2181,
      "step": 198700
    },
    {
      "epoch": 0.6337163896016959,
      "grad_norm": 0.0001910805731313303,
      "learning_rate": 0.00010988508311949124,
      "loss": 0.2254,
      "step": 198800
    },
    {
      "epoch": 0.6340351604214151,
      "grad_norm": 0.007024249527603388,
      "learning_rate": 0.00010978945187357549,
      "loss": 0.3061,
      "step": 198900
    },
    {
      "epoch": 0.6343539312411342,
      "grad_norm": 0.0002707155072130263,
      "learning_rate": 0.00010969382062765973,
      "loss": 0.1991,
      "step": 199000
    },
    {
      "epoch": 0.6346727020608534,
      "grad_norm": 4.5121501898393035e-05,
      "learning_rate": 0.00010959818938174399,
      "loss": 0.4125,
      "step": 199100
    },
    {
      "epoch": 0.6349914728805726,
      "grad_norm": 0.001547680119983852,
      "learning_rate": 0.00010950255813582824,
      "loss": 0.3007,
      "step": 199200
    },
    {
      "epoch": 0.6353102437002917,
      "grad_norm": 2.4876422685338184e-05,
      "learning_rate": 0.0001094069268899125,
      "loss": 0.2482,
      "step": 199300
    },
    {
      "epoch": 0.6356290145200109,
      "grad_norm": 96.46134185791016,
      "learning_rate": 0.00010931129564399674,
      "loss": 0.3506,
      "step": 199400
    },
    {
      "epoch": 0.63594778533973,
      "grad_norm": 1.039940357208252,
      "learning_rate": 0.00010921566439808099,
      "loss": 0.1837,
      "step": 199500
    },
    {
      "epoch": 0.6362665561594492,
      "grad_norm": 0.7976131439208984,
      "learning_rate": 0.00010912003315216524,
      "loss": 0.1977,
      "step": 199600
    },
    {
      "epoch": 0.6365853269791684,
      "grad_norm": 0.002787358360365033,
      "learning_rate": 0.0001090244019062495,
      "loss": 0.1894,
      "step": 199700
    },
    {
      "epoch": 0.6369040977988875,
      "grad_norm": 32.8459587097168,
      "learning_rate": 0.00010892877066033374,
      "loss": 0.3864,
      "step": 199800
    },
    {
      "epoch": 0.6372228686186067,
      "grad_norm": 5.88528455409687e-05,
      "learning_rate": 0.00010883313941441799,
      "loss": 0.246,
      "step": 199900
    },
    {
      "epoch": 0.6375416394383259,
      "grad_norm": 4.274758975952864e-05,
      "learning_rate": 0.00010873750816850225,
      "loss": 0.2424,
      "step": 200000
    },
    {
      "epoch": 0.637860410258045,
      "grad_norm": 7.095684122759849e-05,
      "learning_rate": 0.0001086418769225865,
      "loss": 0.2796,
      "step": 200100
    },
    {
      "epoch": 0.6381791810777642,
      "grad_norm": 0.08233165740966797,
      "learning_rate": 0.00010854624567667074,
      "loss": 0.2014,
      "step": 200200
    },
    {
      "epoch": 0.6384979518974833,
      "grad_norm": 0.0001926479599205777,
      "learning_rate": 0.000108450614430755,
      "loss": 0.1623,
      "step": 200300
    },
    {
      "epoch": 0.6388167227172025,
      "grad_norm": 0.019793717190623283,
      "learning_rate": 0.00010835498318483925,
      "loss": 0.1747,
      "step": 200400
    },
    {
      "epoch": 0.6391354935369217,
      "grad_norm": 1.2763761281967163,
      "learning_rate": 0.0001082593519389235,
      "loss": 0.0968,
      "step": 200500
    },
    {
      "epoch": 0.6394542643566408,
      "grad_norm": 0.0005851858877576888,
      "learning_rate": 0.00010816372069300776,
      "loss": 0.2583,
      "step": 200600
    },
    {
      "epoch": 0.63977303517636,
      "grad_norm": 0.00013412900443654507,
      "learning_rate": 0.000108068089447092,
      "loss": 0.1503,
      "step": 200700
    },
    {
      "epoch": 0.6400918059960791,
      "grad_norm": 0.002262112684547901,
      "learning_rate": 0.00010797245820117625,
      "loss": 0.223,
      "step": 200800
    },
    {
      "epoch": 0.6404105768157983,
      "grad_norm": 0.5792698264122009,
      "learning_rate": 0.0001078768269552605,
      "loss": 0.3438,
      "step": 200900
    },
    {
      "epoch": 0.6407293476355175,
      "grad_norm": 0.0008278858149424195,
      "learning_rate": 0.00010778119570934477,
      "loss": 0.414,
      "step": 201000
    },
    {
      "epoch": 0.6410481184552366,
      "grad_norm": 0.0006494777626357973,
      "learning_rate": 0.000107685564463429,
      "loss": 0.2926,
      "step": 201100
    },
    {
      "epoch": 0.6413668892749558,
      "grad_norm": 0.00041394372237846255,
      "learning_rate": 0.00010758993321751325,
      "loss": 0.2268,
      "step": 201200
    },
    {
      "epoch": 0.641685660094675,
      "grad_norm": 16.949182510375977,
      "learning_rate": 0.00010749430197159752,
      "loss": 0.1716,
      "step": 201300
    },
    {
      "epoch": 0.6420044309143941,
      "grad_norm": 0.0014665370108559728,
      "learning_rate": 0.00010739867072568177,
      "loss": 0.2555,
      "step": 201400
    },
    {
      "epoch": 0.6423232017341133,
      "grad_norm": 0.0009710637386888266,
      "learning_rate": 0.00010730303947976601,
      "loss": 0.2833,
      "step": 201500
    },
    {
      "epoch": 0.6426419725538324,
      "grad_norm": 0.0252375528216362,
      "learning_rate": 0.00010720740823385027,
      "loss": 0.3285,
      "step": 201600
    },
    {
      "epoch": 0.6429607433735516,
      "grad_norm": 0.0004612483608070761,
      "learning_rate": 0.00010711177698793452,
      "loss": 0.4097,
      "step": 201700
    },
    {
      "epoch": 0.6432795141932708,
      "grad_norm": 86.2362289428711,
      "learning_rate": 0.00010701614574201877,
      "loss": 0.2316,
      "step": 201800
    },
    {
      "epoch": 0.6435982850129899,
      "grad_norm": 0.012691721320152283,
      "learning_rate": 0.00010692051449610302,
      "loss": 0.2398,
      "step": 201900
    },
    {
      "epoch": 0.6439170558327091,
      "grad_norm": 0.0007037707837298512,
      "learning_rate": 0.00010682488325018727,
      "loss": 0.2749,
      "step": 202000
    },
    {
      "epoch": 0.6442358266524283,
      "grad_norm": 0.0005538669065572321,
      "learning_rate": 0.00010672925200427152,
      "loss": 0.1568,
      "step": 202100
    },
    {
      "epoch": 0.6445545974721474,
      "grad_norm": 0.008088386617600918,
      "learning_rate": 0.00010663362075835578,
      "loss": 0.2027,
      "step": 202200
    },
    {
      "epoch": 0.6448733682918666,
      "grad_norm": 48.0740852355957,
      "learning_rate": 0.00010653798951244002,
      "loss": 0.1934,
      "step": 202300
    },
    {
      "epoch": 0.6451921391115857,
      "grad_norm": 0.09412942826747894,
      "learning_rate": 0.00010644235826652427,
      "loss": 0.2324,
      "step": 202400
    },
    {
      "epoch": 0.6455109099313049,
      "grad_norm": 0.011514748446643353,
      "learning_rate": 0.00010634672702060853,
      "loss": 0.0829,
      "step": 202500
    },
    {
      "epoch": 0.6458296807510241,
      "grad_norm": 70.75669860839844,
      "learning_rate": 0.00010625109577469278,
      "loss": 0.1414,
      "step": 202600
    },
    {
      "epoch": 0.6461484515707432,
      "grad_norm": 0.0018490611109882593,
      "learning_rate": 0.00010615546452877703,
      "loss": 0.4476,
      "step": 202700
    },
    {
      "epoch": 0.6464672223904624,
      "grad_norm": 0.0020720763131976128,
      "learning_rate": 0.00010605983328286127,
      "loss": 0.2785,
      "step": 202800
    },
    {
      "epoch": 0.6467859932101815,
      "grad_norm": 0.000667704560328275,
      "learning_rate": 0.00010596420203694553,
      "loss": 0.2157,
      "step": 202900
    },
    {
      "epoch": 0.6471047640299007,
      "grad_norm": 0.0007197760278359056,
      "learning_rate": 0.00010586857079102978,
      "loss": 0.0801,
      "step": 203000
    },
    {
      "epoch": 0.6474235348496199,
      "grad_norm": 0.0032587687019258738,
      "learning_rate": 0.00010577293954511403,
      "loss": 0.2649,
      "step": 203100
    },
    {
      "epoch": 0.647742305669339,
      "grad_norm": 0.012120630592107773,
      "learning_rate": 0.00010567730829919828,
      "loss": 0.2895,
      "step": 203200
    },
    {
      "epoch": 0.6480610764890582,
      "grad_norm": 12.321335792541504,
      "learning_rate": 0.00010558167705328253,
      "loss": 0.2199,
      "step": 203300
    },
    {
      "epoch": 0.6483798473087774,
      "grad_norm": 90.16815185546875,
      "learning_rate": 0.00010548604580736678,
      "loss": 0.3873,
      "step": 203400
    },
    {
      "epoch": 0.6486986181284965,
      "grad_norm": 0.005104751791805029,
      "learning_rate": 0.00010539041456145104,
      "loss": 0.2527,
      "step": 203500
    },
    {
      "epoch": 0.6490173889482157,
      "grad_norm": 0.4893701672554016,
      "learning_rate": 0.00010529478331553528,
      "loss": 0.2355,
      "step": 203600
    },
    {
      "epoch": 0.6493361597679348,
      "grad_norm": 0.0002029669121839106,
      "learning_rate": 0.00010519915206961953,
      "loss": 0.4004,
      "step": 203700
    },
    {
      "epoch": 0.649654930587654,
      "grad_norm": 0.0015226354589685798,
      "learning_rate": 0.00010510352082370379,
      "loss": 0.1651,
      "step": 203800
    },
    {
      "epoch": 0.6499737014073732,
      "grad_norm": 0.00021796462533529848,
      "learning_rate": 0.00010500788957778804,
      "loss": 0.2371,
      "step": 203900
    },
    {
      "epoch": 0.6502924722270923,
      "grad_norm": 24.58010482788086,
      "learning_rate": 0.00010491225833187228,
      "loss": 0.1666,
      "step": 204000
    },
    {
      "epoch": 0.6506112430468115,
      "grad_norm": 0.001264454796910286,
      "learning_rate": 0.00010481662708595653,
      "loss": 0.1122,
      "step": 204100
    },
    {
      "epoch": 0.6509300138665307,
      "grad_norm": 0.0007512311567552388,
      "learning_rate": 0.0001047209958400408,
      "loss": 0.1591,
      "step": 204200
    },
    {
      "epoch": 0.6512487846862498,
      "grad_norm": 0.0009558849269524217,
      "learning_rate": 0.00010462536459412505,
      "loss": 0.3391,
      "step": 204300
    },
    {
      "epoch": 0.651567555505969,
      "grad_norm": 8.797202110290527,
      "learning_rate": 0.00010452973334820928,
      "loss": 0.306,
      "step": 204400
    },
    {
      "epoch": 0.6518863263256881,
      "grad_norm": 0.0005451429169625044,
      "learning_rate": 0.00010443410210229355,
      "loss": 0.2021,
      "step": 204500
    },
    {
      "epoch": 0.6522050971454073,
      "grad_norm": 0.004518372006714344,
      "learning_rate": 0.0001043384708563778,
      "loss": 0.3785,
      "step": 204600
    },
    {
      "epoch": 0.6525238679651265,
      "grad_norm": 0.002452403772622347,
      "learning_rate": 0.00010424283961046206,
      "loss": 0.1957,
      "step": 204700
    },
    {
      "epoch": 0.6528426387848456,
      "grad_norm": 0.0006284063565544784,
      "learning_rate": 0.00010414720836454631,
      "loss": 0.216,
      "step": 204800
    },
    {
      "epoch": 0.6531614096045648,
      "grad_norm": 0.0003727868606802076,
      "learning_rate": 0.00010405157711863055,
      "loss": 0.2998,
      "step": 204900
    },
    {
      "epoch": 0.6534801804242839,
      "grad_norm": 49.3916015625,
      "learning_rate": 0.0001039559458727148,
      "loss": 0.1002,
      "step": 205000
    },
    {
      "epoch": 0.6537989512440031,
      "grad_norm": 2.160362482070923,
      "learning_rate": 0.00010386031462679906,
      "loss": 0.2635,
      "step": 205100
    },
    {
      "epoch": 0.6541177220637223,
      "grad_norm": 0.12834036350250244,
      "learning_rate": 0.00010376468338088331,
      "loss": 0.2693,
      "step": 205200
    },
    {
      "epoch": 0.6544364928834414,
      "grad_norm": 5.840665471623652e-05,
      "learning_rate": 0.00010366905213496755,
      "loss": 0.2007,
      "step": 205300
    },
    {
      "epoch": 0.6547552637031606,
      "grad_norm": 0.0009049949585460126,
      "learning_rate": 0.00010357342088905181,
      "loss": 0.1967,
      "step": 205400
    },
    {
      "epoch": 0.6550740345228798,
      "grad_norm": 0.00028086791280657053,
      "learning_rate": 0.00010347778964313606,
      "loss": 0.3447,
      "step": 205500
    },
    {
      "epoch": 0.6553928053425989,
      "grad_norm": 122.11245727539062,
      "learning_rate": 0.00010338215839722031,
      "loss": 0.371,
      "step": 205600
    },
    {
      "epoch": 0.6557115761623181,
      "grad_norm": 1.1733900308609009,
      "learning_rate": 0.00010328652715130456,
      "loss": 0.1877,
      "step": 205700
    },
    {
      "epoch": 0.6560303469820372,
      "grad_norm": 0.0006049767252989113,
      "learning_rate": 0.00010319089590538881,
      "loss": 0.3075,
      "step": 205800
    },
    {
      "epoch": 0.6563491178017564,
      "grad_norm": 7.655254739802331e-05,
      "learning_rate": 0.00010309526465947306,
      "loss": 0.3097,
      "step": 205900
    },
    {
      "epoch": 0.6566678886214756,
      "grad_norm": 75.6922378540039,
      "learning_rate": 0.00010299963341355732,
      "loss": 0.24,
      "step": 206000
    },
    {
      "epoch": 0.6569866594411947,
      "grad_norm": 0.04864542931318283,
      "learning_rate": 0.00010290400216764156,
      "loss": 0.2049,
      "step": 206100
    },
    {
      "epoch": 0.6573054302609139,
      "grad_norm": 9.003000741358846e-05,
      "learning_rate": 0.00010280837092172581,
      "loss": 0.2222,
      "step": 206200
    },
    {
      "epoch": 0.6576242010806331,
      "grad_norm": 0.0015506040072068572,
      "learning_rate": 0.00010271273967581007,
      "loss": 0.1904,
      "step": 206300
    },
    {
      "epoch": 0.6579429719003522,
      "grad_norm": 0.00019400032761041075,
      "learning_rate": 0.00010261710842989432,
      "loss": 0.1113,
      "step": 206400
    },
    {
      "epoch": 0.6582617427200714,
      "grad_norm": 10.002994537353516,
      "learning_rate": 0.00010252147718397856,
      "loss": 0.2392,
      "step": 206500
    },
    {
      "epoch": 0.6585805135397905,
      "grad_norm": 21.502729415893555,
      "learning_rate": 0.00010242584593806281,
      "loss": 0.2619,
      "step": 206600
    },
    {
      "epoch": 0.6588992843595097,
      "grad_norm": 0.00022095022723078728,
      "learning_rate": 0.00010233021469214707,
      "loss": 0.3094,
      "step": 206700
    },
    {
      "epoch": 0.6592180551792289,
      "grad_norm": 0.00034447567304596305,
      "learning_rate": 0.00010223458344623132,
      "loss": 0.2396,
      "step": 206800
    },
    {
      "epoch": 0.659536825998948,
      "grad_norm": 0.00027259672060608864,
      "learning_rate": 0.00010213895220031559,
      "loss": 0.2431,
      "step": 206900
    },
    {
      "epoch": 0.6598555968186672,
      "grad_norm": 2.4374938220717013e-05,
      "learning_rate": 0.00010204332095439982,
      "loss": 0.2092,
      "step": 207000
    },
    {
      "epoch": 0.6601743676383863,
      "grad_norm": 25.96634292602539,
      "learning_rate": 0.00010194768970848408,
      "loss": 0.1803,
      "step": 207100
    },
    {
      "epoch": 0.6604931384581055,
      "grad_norm": 0.007355717476457357,
      "learning_rate": 0.00010185205846256834,
      "loss": 0.2746,
      "step": 207200
    },
    {
      "epoch": 0.6608119092778247,
      "grad_norm": 0.0007206741138361394,
      "learning_rate": 0.00010175642721665259,
      "loss": 0.2928,
      "step": 207300
    },
    {
      "epoch": 0.6611306800975438,
      "grad_norm": 0.03432242572307587,
      "learning_rate": 0.00010166079597073683,
      "loss": 0.1607,
      "step": 207400
    },
    {
      "epoch": 0.661449450917263,
      "grad_norm": 0.0003717287036124617,
      "learning_rate": 0.00010156516472482108,
      "loss": 0.3096,
      "step": 207500
    },
    {
      "epoch": 0.6617682217369822,
      "grad_norm": 0.0015373019268736243,
      "learning_rate": 0.00010146953347890534,
      "loss": 0.2187,
      "step": 207600
    },
    {
      "epoch": 0.6620869925567013,
      "grad_norm": 0.0022863377816975117,
      "learning_rate": 0.00010137390223298959,
      "loss": 0.1788,
      "step": 207700
    },
    {
      "epoch": 0.6624057633764205,
      "grad_norm": 0.004718490410596132,
      "learning_rate": 0.00010127827098707383,
      "loss": 0.3364,
      "step": 207800
    },
    {
      "epoch": 0.6627245341961396,
      "grad_norm": 6.44156607449986e-05,
      "learning_rate": 0.00010118263974115809,
      "loss": 0.233,
      "step": 207900
    },
    {
      "epoch": 0.6630433050158588,
      "grad_norm": 0.0028308373875916004,
      "learning_rate": 0.00010108700849524234,
      "loss": 0.2862,
      "step": 208000
    },
    {
      "epoch": 0.663362075835578,
      "grad_norm": 0.7713948488235474,
      "learning_rate": 0.0001009913772493266,
      "loss": 0.1854,
      "step": 208100
    },
    {
      "epoch": 0.6636808466552971,
      "grad_norm": 132.5229949951172,
      "learning_rate": 0.00010089574600341084,
      "loss": 0.1388,
      "step": 208200
    },
    {
      "epoch": 0.6639996174750163,
      "grad_norm": 56.62007141113281,
      "learning_rate": 0.00010080011475749509,
      "loss": 0.2629,
      "step": 208300
    },
    {
      "epoch": 0.6643183882947356,
      "grad_norm": 0.0003464497276581824,
      "learning_rate": 0.00010070448351157934,
      "loss": 0.2392,
      "step": 208400
    },
    {
      "epoch": 0.6646371591144546,
      "grad_norm": 3.368500620126724e-05,
      "learning_rate": 0.0001006088522656636,
      "loss": 0.1894,
      "step": 208500
    },
    {
      "epoch": 0.6649559299341739,
      "grad_norm": 0.0001666314055910334,
      "learning_rate": 0.00010051322101974784,
      "loss": 0.3017,
      "step": 208600
    },
    {
      "epoch": 0.665274700753893,
      "grad_norm": 0.02311607636511326,
      "learning_rate": 0.00010041758977383209,
      "loss": 0.2861,
      "step": 208700
    },
    {
      "epoch": 0.6655934715736121,
      "grad_norm": 0.5131884813308716,
      "learning_rate": 0.00010032195852791635,
      "loss": 0.3885,
      "step": 208800
    },
    {
      "epoch": 0.6659122423933314,
      "grad_norm": 0.0019061880884692073,
      "learning_rate": 0.0001002263272820006,
      "loss": 0.2069,
      "step": 208900
    },
    {
      "epoch": 0.6662310132130504,
      "grad_norm": 71.52021026611328,
      "learning_rate": 0.00010013069603608484,
      "loss": 0.3935,
      "step": 209000
    },
    {
      "epoch": 0.6665497840327697,
      "grad_norm": 0.0009150314726866782,
      "learning_rate": 0.00010003506479016909,
      "loss": 0.2925,
      "step": 209100
    },
    {
      "epoch": 0.6668685548524887,
      "grad_norm": 0.970913290977478,
      "learning_rate": 9.993943354425335e-05,
      "loss": 0.3233,
      "step": 209200
    },
    {
      "epoch": 0.667187325672208,
      "grad_norm": 0.002924711676314473,
      "learning_rate": 9.98438022983376e-05,
      "loss": 0.315,
      "step": 209300
    },
    {
      "epoch": 0.6675060964919272,
      "grad_norm": 8.445964340353385e-05,
      "learning_rate": 9.974817105242185e-05,
      "loss": 0.0982,
      "step": 209400
    },
    {
      "epoch": 0.6678248673116463,
      "grad_norm": 0.0005787337431684136,
      "learning_rate": 9.96525398065061e-05,
      "loss": 0.139,
      "step": 209500
    },
    {
      "epoch": 0.6681436381313655,
      "grad_norm": 0.011473579332232475,
      "learning_rate": 9.955690856059035e-05,
      "loss": 0.2883,
      "step": 209600
    },
    {
      "epoch": 0.6684624089510847,
      "grad_norm": 0.01239293534308672,
      "learning_rate": 9.94612773146746e-05,
      "loss": 0.1735,
      "step": 209700
    },
    {
      "epoch": 0.6687811797708038,
      "grad_norm": 5.765866080764681e-05,
      "learning_rate": 9.936564606875887e-05,
      "loss": 0.1697,
      "step": 209800
    },
    {
      "epoch": 0.669099950590523,
      "grad_norm": 123.65758514404297,
      "learning_rate": 9.92700148228431e-05,
      "loss": 0.1404,
      "step": 209900
    },
    {
      "epoch": 0.6694187214102421,
      "grad_norm": 0.0703277513384819,
      "learning_rate": 9.917438357692735e-05,
      "loss": 0.2272,
      "step": 210000
    },
    {
      "epoch": 0.6697374922299613,
      "grad_norm": 0.00010828559607034549,
      "learning_rate": 9.907875233101162e-05,
      "loss": 0.347,
      "step": 210100
    },
    {
      "epoch": 0.6700562630496805,
      "grad_norm": 0.00070645083906129,
      "learning_rate": 9.898312108509587e-05,
      "loss": 0.1832,
      "step": 210200
    },
    {
      "epoch": 0.6703750338693996,
      "grad_norm": 0.12429525703191757,
      "learning_rate": 9.888748983918011e-05,
      "loss": 0.1139,
      "step": 210300
    },
    {
      "epoch": 0.6706938046891188,
      "grad_norm": 0.0022301063872873783,
      "learning_rate": 9.879185859326437e-05,
      "loss": 0.1451,
      "step": 210400
    },
    {
      "epoch": 0.671012575508838,
      "grad_norm": 0.0036860238760709763,
      "learning_rate": 9.869622734734862e-05,
      "loss": 0.1951,
      "step": 210500
    },
    {
      "epoch": 0.6713313463285571,
      "grad_norm": 0.0011145370081067085,
      "learning_rate": 9.860059610143287e-05,
      "loss": 0.1746,
      "step": 210600
    },
    {
      "epoch": 0.6716501171482763,
      "grad_norm": 0.006873982027173042,
      "learning_rate": 9.850496485551712e-05,
      "loss": 0.3515,
      "step": 210700
    },
    {
      "epoch": 0.6719688879679954,
      "grad_norm": 0.00026837544282898307,
      "learning_rate": 9.840933360960137e-05,
      "loss": 0.2349,
      "step": 210800
    },
    {
      "epoch": 0.6722876587877146,
      "grad_norm": 0.00048813148168846965,
      "learning_rate": 9.831370236368562e-05,
      "loss": 0.2316,
      "step": 210900
    },
    {
      "epoch": 0.6726064296074338,
      "grad_norm": 0.00015518517466261983,
      "learning_rate": 9.821807111776988e-05,
      "loss": 0.2653,
      "step": 211000
    },
    {
      "epoch": 0.6729252004271529,
      "grad_norm": 0.005675585474818945,
      "learning_rate": 9.812243987185412e-05,
      "loss": 0.3495,
      "step": 211100
    },
    {
      "epoch": 0.6732439712468721,
      "grad_norm": 44.24553680419922,
      "learning_rate": 9.802680862593837e-05,
      "loss": 0.275,
      "step": 211200
    },
    {
      "epoch": 0.6735627420665912,
      "grad_norm": 0.009177776984870434,
      "learning_rate": 9.793117738002262e-05,
      "loss": 0.2332,
      "step": 211300
    },
    {
      "epoch": 0.6738815128863104,
      "grad_norm": 0.002240475732833147,
      "learning_rate": 9.783554613410688e-05,
      "loss": 0.229,
      "step": 211400
    },
    {
      "epoch": 0.6742002837060296,
      "grad_norm": 24.239171981811523,
      "learning_rate": 9.773991488819113e-05,
      "loss": 0.3817,
      "step": 211500
    },
    {
      "epoch": 0.6745190545257487,
      "grad_norm": 0.0008184760226868093,
      "learning_rate": 9.764428364227537e-05,
      "loss": 0.3021,
      "step": 211600
    },
    {
      "epoch": 0.6748378253454679,
      "grad_norm": 8.785697900748346e-06,
      "learning_rate": 9.754865239635963e-05,
      "loss": 0.1734,
      "step": 211700
    },
    {
      "epoch": 0.6751565961651871,
      "grad_norm": 22.672651290893555,
      "learning_rate": 9.745302115044388e-05,
      "loss": 0.3551,
      "step": 211800
    },
    {
      "epoch": 0.6754753669849062,
      "grad_norm": 92.24207305908203,
      "learning_rate": 9.735738990452813e-05,
      "loss": 0.2665,
      "step": 211900
    },
    {
      "epoch": 0.6757941378046254,
      "grad_norm": 0.0005476577789522707,
      "learning_rate": 9.726175865861238e-05,
      "loss": 0.2897,
      "step": 212000
    },
    {
      "epoch": 0.6761129086243445,
      "grad_norm": 0.033176448196172714,
      "learning_rate": 9.716612741269663e-05,
      "loss": 0.2665,
      "step": 212100
    },
    {
      "epoch": 0.6764316794440637,
      "grad_norm": 0.0009057832648977637,
      "learning_rate": 9.707049616678088e-05,
      "loss": 0.1723,
      "step": 212200
    },
    {
      "epoch": 0.6767504502637829,
      "grad_norm": 2.7914746169699356e-05,
      "learning_rate": 9.697486492086514e-05,
      "loss": 0.4024,
      "step": 212300
    },
    {
      "epoch": 0.677069221083502,
      "grad_norm": 0.00033296336187049747,
      "learning_rate": 9.687923367494938e-05,
      "loss": 0.2757,
      "step": 212400
    },
    {
      "epoch": 0.6773879919032212,
      "grad_norm": 0.059298232197761536,
      "learning_rate": 9.678360242903363e-05,
      "loss": 0.163,
      "step": 212500
    },
    {
      "epoch": 0.6777067627229404,
      "grad_norm": 0.000275070546194911,
      "learning_rate": 9.668797118311789e-05,
      "loss": 0.3692,
      "step": 212600
    },
    {
      "epoch": 0.6780255335426595,
      "grad_norm": 1.1111751794815063,
      "learning_rate": 9.659233993720214e-05,
      "loss": 0.21,
      "step": 212700
    },
    {
      "epoch": 0.6783443043623787,
      "grad_norm": 0.00028143919189460576,
      "learning_rate": 9.649670869128638e-05,
      "loss": 0.1569,
      "step": 212800
    },
    {
      "epoch": 0.6786630751820978,
      "grad_norm": 0.0017438095528632402,
      "learning_rate": 9.640107744537063e-05,
      "loss": 0.3317,
      "step": 212900
    },
    {
      "epoch": 0.678981846001817,
      "grad_norm": 0.6136817336082458,
      "learning_rate": 9.63054461994549e-05,
      "loss": 0.104,
      "step": 213000
    },
    {
      "epoch": 0.6793006168215362,
      "grad_norm": 0.08271152526140213,
      "learning_rate": 9.620981495353915e-05,
      "loss": 0.1535,
      "step": 213100
    },
    {
      "epoch": 0.6796193876412553,
      "grad_norm": 0.26105672121047974,
      "learning_rate": 9.611418370762338e-05,
      "loss": 0.1968,
      "step": 213200
    },
    {
      "epoch": 0.6799381584609745,
      "grad_norm": 0.0008681312901899219,
      "learning_rate": 9.601855246170765e-05,
      "loss": 0.228,
      "step": 213300
    },
    {
      "epoch": 0.6802569292806937,
      "grad_norm": 0.0019777133129537106,
      "learning_rate": 9.59229212157919e-05,
      "loss": 0.1487,
      "step": 213400
    },
    {
      "epoch": 0.6805757001004128,
      "grad_norm": 0.01439551543444395,
      "learning_rate": 9.582728996987616e-05,
      "loss": 0.2014,
      "step": 213500
    },
    {
      "epoch": 0.680894470920132,
      "grad_norm": 0.0002658766752574593,
      "learning_rate": 9.573165872396041e-05,
      "loss": 0.2629,
      "step": 213600
    },
    {
      "epoch": 0.6812132417398511,
      "grad_norm": 0.0015826255548745394,
      "learning_rate": 9.563602747804465e-05,
      "loss": 0.1322,
      "step": 213700
    },
    {
      "epoch": 0.6815320125595703,
      "grad_norm": 0.08468279242515564,
      "learning_rate": 9.55403962321289e-05,
      "loss": 0.1621,
      "step": 213800
    },
    {
      "epoch": 0.6818507833792895,
      "grad_norm": 0.14661318063735962,
      "learning_rate": 9.544476498621316e-05,
      "loss": 0.2885,
      "step": 213900
    },
    {
      "epoch": 0.6821695541990086,
      "grad_norm": 5.330780550139025e-05,
      "learning_rate": 9.534913374029741e-05,
      "loss": 0.2975,
      "step": 214000
    },
    {
      "epoch": 0.6824883250187278,
      "grad_norm": 0.0018018725095316768,
      "learning_rate": 9.525350249438165e-05,
      "loss": 0.3362,
      "step": 214100
    },
    {
      "epoch": 0.6828070958384469,
      "grad_norm": 0.0005929035251028836,
      "learning_rate": 9.515787124846591e-05,
      "loss": 0.1826,
      "step": 214200
    },
    {
      "epoch": 0.6831258666581661,
      "grad_norm": 72.16154479980469,
      "learning_rate": 9.506224000255016e-05,
      "loss": 0.156,
      "step": 214300
    },
    {
      "epoch": 0.6834446374778853,
      "grad_norm": 0.08595769107341766,
      "learning_rate": 9.496660875663441e-05,
      "loss": 0.2243,
      "step": 214400
    },
    {
      "epoch": 0.6837634082976044,
      "grad_norm": 0.0019536176696419716,
      "learning_rate": 9.487097751071866e-05,
      "loss": 0.1799,
      "step": 214500
    },
    {
      "epoch": 0.6840821791173236,
      "grad_norm": 2.824609873641748e-05,
      "learning_rate": 9.477534626480291e-05,
      "loss": 0.2709,
      "step": 214600
    },
    {
      "epoch": 0.6844009499370428,
      "grad_norm": 9.024847984313965,
      "learning_rate": 9.467971501888716e-05,
      "loss": 0.2235,
      "step": 214700
    },
    {
      "epoch": 0.6847197207567619,
      "grad_norm": 0.0036009454634040594,
      "learning_rate": 9.458408377297142e-05,
      "loss": 0.1953,
      "step": 214800
    },
    {
      "epoch": 0.6850384915764811,
      "grad_norm": 4.3230953451711684e-05,
      "learning_rate": 9.448845252705566e-05,
      "loss": 0.3621,
      "step": 214900
    },
    {
      "epoch": 0.6853572623962002,
      "grad_norm": 0.0004108372377231717,
      "learning_rate": 9.439282128113991e-05,
      "loss": 0.1699,
      "step": 215000
    },
    {
      "epoch": 0.6856760332159194,
      "grad_norm": 0.00042273508734069765,
      "learning_rate": 9.429719003522416e-05,
      "loss": 0.1102,
      "step": 215100
    },
    {
      "epoch": 0.6859948040356386,
      "grad_norm": 0.003440156811848283,
      "learning_rate": 9.420155878930842e-05,
      "loss": 0.2821,
      "step": 215200
    },
    {
      "epoch": 0.6863135748553577,
      "grad_norm": 0.0002458108065184206,
      "learning_rate": 9.410592754339266e-05,
      "loss": 0.4523,
      "step": 215300
    },
    {
      "epoch": 0.6866323456750769,
      "grad_norm": 0.001472330535762012,
      "learning_rate": 9.401029629747691e-05,
      "loss": 0.452,
      "step": 215400
    },
    {
      "epoch": 0.6869511164947961,
      "grad_norm": 2.7001391572412103e-05,
      "learning_rate": 9.391466505156117e-05,
      "loss": 0.3647,
      "step": 215500
    },
    {
      "epoch": 0.6872698873145152,
      "grad_norm": 0.006925608031451702,
      "learning_rate": 9.381903380564542e-05,
      "loss": 0.1973,
      "step": 215600
    },
    {
      "epoch": 0.6875886581342344,
      "grad_norm": 0.00015239584899973124,
      "learning_rate": 9.372340255972969e-05,
      "loss": 0.1629,
      "step": 215700
    },
    {
      "epoch": 0.6879074289539535,
      "grad_norm": 4.19581301684957e-05,
      "learning_rate": 9.362777131381392e-05,
      "loss": 0.1698,
      "step": 215800
    },
    {
      "epoch": 0.6882261997736727,
      "grad_norm": 0.0003749056486412883,
      "learning_rate": 9.353214006789817e-05,
      "loss": 0.4326,
      "step": 215900
    },
    {
      "epoch": 0.6885449705933919,
      "grad_norm": 0.006859052460640669,
      "learning_rate": 9.343650882198244e-05,
      "loss": 0.2075,
      "step": 216000
    },
    {
      "epoch": 0.688863741413111,
      "grad_norm": 0.00046292427578009665,
      "learning_rate": 9.334087757606669e-05,
      "loss": 0.3669,
      "step": 216100
    },
    {
      "epoch": 0.6891825122328302,
      "grad_norm": 30.40629005432129,
      "learning_rate": 9.324524633015093e-05,
      "loss": 0.1891,
      "step": 216200
    },
    {
      "epoch": 0.6895012830525493,
      "grad_norm": 0.006455009803175926,
      "learning_rate": 9.314961508423518e-05,
      "loss": 0.2148,
      "step": 216300
    },
    {
      "epoch": 0.6898200538722685,
      "grad_norm": 0.14819324016571045,
      "learning_rate": 9.305398383831944e-05,
      "loss": 0.2204,
      "step": 216400
    },
    {
      "epoch": 0.6901388246919877,
      "grad_norm": 0.0010736079420894384,
      "learning_rate": 9.295835259240369e-05,
      "loss": 0.2511,
      "step": 216500
    },
    {
      "epoch": 0.6904575955117068,
      "grad_norm": 0.00012787285959348083,
      "learning_rate": 9.286272134648793e-05,
      "loss": 0.2331,
      "step": 216600
    },
    {
      "epoch": 0.690776366331426,
      "grad_norm": 0.03454656898975372,
      "learning_rate": 9.276709010057219e-05,
      "loss": 0.3092,
      "step": 216700
    },
    {
      "epoch": 0.6910951371511452,
      "grad_norm": 0.020067540928721428,
      "learning_rate": 9.267145885465644e-05,
      "loss": 0.286,
      "step": 216800
    },
    {
      "epoch": 0.6914139079708643,
      "grad_norm": 0.12866447865962982,
      "learning_rate": 9.25758276087407e-05,
      "loss": 0.1346,
      "step": 216900
    },
    {
      "epoch": 0.6917326787905835,
      "grad_norm": 0.0019537226762622595,
      "learning_rate": 9.248019636282493e-05,
      "loss": 0.2694,
      "step": 217000
    },
    {
      "epoch": 0.6920514496103026,
      "grad_norm": 0.002490306505933404,
      "learning_rate": 9.238456511690919e-05,
      "loss": 0.4004,
      "step": 217100
    },
    {
      "epoch": 0.6923702204300218,
      "grad_norm": 0.0031026357319206,
      "learning_rate": 9.228893387099344e-05,
      "loss": 0.2054,
      "step": 217200
    },
    {
      "epoch": 0.692688991249741,
      "grad_norm": 0.0009632911533117294,
      "learning_rate": 9.21933026250777e-05,
      "loss": 0.3307,
      "step": 217300
    },
    {
      "epoch": 0.6930077620694601,
      "grad_norm": 0.2145712971687317,
      "learning_rate": 9.209767137916194e-05,
      "loss": 0.3816,
      "step": 217400
    },
    {
      "epoch": 0.6933265328891793,
      "grad_norm": 0.0031242151744663715,
      "learning_rate": 9.200204013324619e-05,
      "loss": 0.2852,
      "step": 217500
    },
    {
      "epoch": 0.6936453037088985,
      "grad_norm": 0.0003597205795813352,
      "learning_rate": 9.190640888733044e-05,
      "loss": 0.2106,
      "step": 217600
    },
    {
      "epoch": 0.6939640745286176,
      "grad_norm": 0.013738174922764301,
      "learning_rate": 9.18107776414147e-05,
      "loss": 0.2934,
      "step": 217700
    },
    {
      "epoch": 0.6942828453483368,
      "grad_norm": 2.9120139515725896e-05,
      "learning_rate": 9.171514639549895e-05,
      "loss": 0.2395,
      "step": 217800
    },
    {
      "epoch": 0.6946016161680559,
      "grad_norm": 0.00011042768892366439,
      "learning_rate": 9.161951514958319e-05,
      "loss": 0.3342,
      "step": 217900
    },
    {
      "epoch": 0.6949203869877751,
      "grad_norm": 0.0001747098140185699,
      "learning_rate": 9.152388390366745e-05,
      "loss": 0.379,
      "step": 218000
    },
    {
      "epoch": 0.6952391578074943,
      "grad_norm": 0.022840827703475952,
      "learning_rate": 9.14282526577517e-05,
      "loss": 0.1169,
      "step": 218100
    },
    {
      "epoch": 0.6955579286272134,
      "grad_norm": 0.0017351879505440593,
      "learning_rate": 9.133262141183595e-05,
      "loss": 0.2871,
      "step": 218200
    },
    {
      "epoch": 0.6958766994469326,
      "grad_norm": 0.00014820789510849863,
      "learning_rate": 9.12369901659202e-05,
      "loss": 0.164,
      "step": 218300
    },
    {
      "epoch": 0.6961954702666517,
      "grad_norm": 0.32212474942207336,
      "learning_rate": 9.114135892000445e-05,
      "loss": 0.3341,
      "step": 218400
    },
    {
      "epoch": 0.6965142410863709,
      "grad_norm": 0.008472425863146782,
      "learning_rate": 9.10457276740887e-05,
      "loss": 0.2612,
      "step": 218500
    },
    {
      "epoch": 0.6968330119060901,
      "grad_norm": 0.020275378599762917,
      "learning_rate": 9.095009642817297e-05,
      "loss": 0.2121,
      "step": 218600
    },
    {
      "epoch": 0.6971517827258092,
      "grad_norm": 0.002540457295253873,
      "learning_rate": 9.08544651822572e-05,
      "loss": 0.2527,
      "step": 218700
    },
    {
      "epoch": 0.6974705535455284,
      "grad_norm": 54.847530364990234,
      "learning_rate": 9.075883393634145e-05,
      "loss": 0.2368,
      "step": 218800
    },
    {
      "epoch": 0.6977893243652477,
      "grad_norm": 11.77306842803955,
      "learning_rate": 9.066320269042572e-05,
      "loss": 0.285,
      "step": 218900
    },
    {
      "epoch": 0.6981080951849667,
      "grad_norm": 0.0002590332005638629,
      "learning_rate": 9.056757144450997e-05,
      "loss": 0.3233,
      "step": 219000
    },
    {
      "epoch": 0.698426866004686,
      "grad_norm": 0.0007739535067230463,
      "learning_rate": 9.04719401985942e-05,
      "loss": 0.3157,
      "step": 219100
    },
    {
      "epoch": 0.698745636824405,
      "grad_norm": 116.43382263183594,
      "learning_rate": 9.037630895267847e-05,
      "loss": 0.2362,
      "step": 219200
    },
    {
      "epoch": 0.6990644076441243,
      "grad_norm": 0.001194185228087008,
      "learning_rate": 9.028067770676272e-05,
      "loss": 0.0893,
      "step": 219300
    },
    {
      "epoch": 0.6993831784638435,
      "grad_norm": 0.0010857494780793786,
      "learning_rate": 9.018504646084697e-05,
      "loss": 0.2131,
      "step": 219400
    },
    {
      "epoch": 0.6997019492835626,
      "grad_norm": 6.876890256535262e-05,
      "learning_rate": 9.008941521493121e-05,
      "loss": 0.0932,
      "step": 219500
    },
    {
      "epoch": 0.7000207201032818,
      "grad_norm": 41.118629455566406,
      "learning_rate": 8.999378396901547e-05,
      "loss": 0.2305,
      "step": 219600
    },
    {
      "epoch": 0.700339490923001,
      "grad_norm": 0.00016748980851843953,
      "learning_rate": 8.989815272309972e-05,
      "loss": 0.2931,
      "step": 219700
    },
    {
      "epoch": 0.7006582617427201,
      "grad_norm": 154.15843200683594,
      "learning_rate": 8.980252147718398e-05,
      "loss": 0.3383,
      "step": 219800
    },
    {
      "epoch": 0.7009770325624393,
      "grad_norm": 2.2969683413975872e-05,
      "learning_rate": 8.970689023126822e-05,
      "loss": 0.1552,
      "step": 219900
    },
    {
      "epoch": 0.7012958033821584,
      "grad_norm": 0.002153649227693677,
      "learning_rate": 8.961125898535247e-05,
      "loss": 0.2011,
      "step": 220000
    },
    {
      "epoch": 0.7016145742018776,
      "grad_norm": 0.000770679151173681,
      "learning_rate": 8.951562773943672e-05,
      "loss": 0.2467,
      "step": 220100
    },
    {
      "epoch": 0.7019333450215968,
      "grad_norm": 9.092948312172666e-05,
      "learning_rate": 8.941999649352098e-05,
      "loss": 0.3212,
      "step": 220200
    },
    {
      "epoch": 0.7022521158413159,
      "grad_norm": 0.0010549004655331373,
      "learning_rate": 8.932436524760523e-05,
      "loss": 0.1148,
      "step": 220300
    },
    {
      "epoch": 0.7025708866610351,
      "grad_norm": 0.0009179060580208898,
      "learning_rate": 8.922873400168947e-05,
      "loss": 0.2814,
      "step": 220400
    },
    {
      "epoch": 0.7028896574807542,
      "grad_norm": 0.0002987013722304255,
      "learning_rate": 8.913310275577373e-05,
      "loss": 0.2492,
      "step": 220500
    },
    {
      "epoch": 0.7032084283004734,
      "grad_norm": 5.364861488342285,
      "learning_rate": 8.903747150985798e-05,
      "loss": 0.0782,
      "step": 220600
    },
    {
      "epoch": 0.7035271991201926,
      "grad_norm": 0.006597009487450123,
      "learning_rate": 8.894184026394223e-05,
      "loss": 0.1377,
      "step": 220700
    },
    {
      "epoch": 0.7038459699399117,
      "grad_norm": 23.387731552124023,
      "learning_rate": 8.884620901802648e-05,
      "loss": 0.2656,
      "step": 220800
    },
    {
      "epoch": 0.7041647407596309,
      "grad_norm": 5.413022994995117,
      "learning_rate": 8.875057777211073e-05,
      "loss": 0.1381,
      "step": 220900
    },
    {
      "epoch": 0.7044835115793501,
      "grad_norm": 50.585262298583984,
      "learning_rate": 8.865494652619498e-05,
      "loss": 0.2746,
      "step": 221000
    },
    {
      "epoch": 0.7048022823990692,
      "grad_norm": 0.0005256549920886755,
      "learning_rate": 8.855931528027924e-05,
      "loss": 0.2629,
      "step": 221100
    },
    {
      "epoch": 0.7051210532187884,
      "grad_norm": 0.004468678496778011,
      "learning_rate": 8.846368403436348e-05,
      "loss": 0.2163,
      "step": 221200
    },
    {
      "epoch": 0.7054398240385075,
      "grad_norm": 0.003049778286367655,
      "learning_rate": 8.836805278844773e-05,
      "loss": 0.1751,
      "step": 221300
    },
    {
      "epoch": 0.7057585948582267,
      "grad_norm": 4.48286700702738e-05,
      "learning_rate": 8.827242154253198e-05,
      "loss": 0.14,
      "step": 221400
    },
    {
      "epoch": 0.7060773656779459,
      "grad_norm": 0.0033302539959549904,
      "learning_rate": 8.817679029661624e-05,
      "loss": 0.2648,
      "step": 221500
    },
    {
      "epoch": 0.706396136497665,
      "grad_norm": 0.016154704615473747,
      "learning_rate": 8.808115905070048e-05,
      "loss": 0.3303,
      "step": 221600
    },
    {
      "epoch": 0.7067149073173842,
      "grad_norm": 3.478670259937644e-05,
      "learning_rate": 8.798552780478473e-05,
      "loss": 0.1804,
      "step": 221700
    },
    {
      "epoch": 0.7070336781371034,
      "grad_norm": 0.0006856420077383518,
      "learning_rate": 8.7889896558869e-05,
      "loss": 0.2181,
      "step": 221800
    },
    {
      "epoch": 0.7073524489568225,
      "grad_norm": 63.884613037109375,
      "learning_rate": 8.779426531295325e-05,
      "loss": 0.1961,
      "step": 221900
    },
    {
      "epoch": 0.7076712197765417,
      "grad_norm": 0.6577847003936768,
      "learning_rate": 8.769863406703748e-05,
      "loss": 0.2683,
      "step": 222000
    },
    {
      "epoch": 0.7079899905962608,
      "grad_norm": 0.0010549338767305017,
      "learning_rate": 8.760300282112175e-05,
      "loss": 0.2403,
      "step": 222100
    },
    {
      "epoch": 0.70830876141598,
      "grad_norm": 9.4140566943679e-05,
      "learning_rate": 8.7507371575206e-05,
      "loss": 0.2499,
      "step": 222200
    },
    {
      "epoch": 0.7086275322356992,
      "grad_norm": 4.786204590345733e-05,
      "learning_rate": 8.741174032929026e-05,
      "loss": 0.1678,
      "step": 222300
    },
    {
      "epoch": 0.7089463030554183,
      "grad_norm": 20.652162551879883,
      "learning_rate": 8.731610908337451e-05,
      "loss": 0.2115,
      "step": 222400
    },
    {
      "epoch": 0.7092650738751375,
      "grad_norm": 0.00023143320868257433,
      "learning_rate": 8.722047783745875e-05,
      "loss": 0.1845,
      "step": 222500
    },
    {
      "epoch": 0.7095838446948566,
      "grad_norm": 0.026394236832857132,
      "learning_rate": 8.7124846591543e-05,
      "loss": 0.2083,
      "step": 222600
    },
    {
      "epoch": 0.7099026155145758,
      "grad_norm": 0.00181641336530447,
      "learning_rate": 8.702921534562726e-05,
      "loss": 0.2771,
      "step": 222700
    },
    {
      "epoch": 0.710221386334295,
      "grad_norm": 0.17641712725162506,
      "learning_rate": 8.693358409971151e-05,
      "loss": 0.2818,
      "step": 222800
    },
    {
      "epoch": 0.7105401571540141,
      "grad_norm": 0.0004982124664820731,
      "learning_rate": 8.683795285379575e-05,
      "loss": 0.1351,
      "step": 222900
    },
    {
      "epoch": 0.7108589279737333,
      "grad_norm": 3.9001593589782715,
      "learning_rate": 8.674232160788001e-05,
      "loss": 0.1328,
      "step": 223000
    },
    {
      "epoch": 0.7111776987934525,
      "grad_norm": 22.577777862548828,
      "learning_rate": 8.664669036196426e-05,
      "loss": 0.2044,
      "step": 223100
    },
    {
      "epoch": 0.7114964696131716,
      "grad_norm": 0.00831765029579401,
      "learning_rate": 8.655105911604851e-05,
      "loss": 0.361,
      "step": 223200
    },
    {
      "epoch": 0.7118152404328908,
      "grad_norm": 0.0019399758893996477,
      "learning_rate": 8.645542787013275e-05,
      "loss": 0.0703,
      "step": 223300
    },
    {
      "epoch": 0.7121340112526099,
      "grad_norm": 0.00010923900117632002,
      "learning_rate": 8.635979662421701e-05,
      "loss": 0.2666,
      "step": 223400
    },
    {
      "epoch": 0.7124527820723291,
      "grad_norm": 6.165798549773172e-05,
      "learning_rate": 8.626416537830126e-05,
      "loss": 0.2415,
      "step": 223500
    },
    {
      "epoch": 0.7127715528920483,
      "grad_norm": 0.0111321359872818,
      "learning_rate": 8.616853413238552e-05,
      "loss": 0.1242,
      "step": 223600
    },
    {
      "epoch": 0.7130903237117674,
      "grad_norm": 51.75531768798828,
      "learning_rate": 8.607290288646976e-05,
      "loss": 0.1718,
      "step": 223700
    },
    {
      "epoch": 0.7134090945314866,
      "grad_norm": 0.021647389978170395,
      "learning_rate": 8.597727164055401e-05,
      "loss": 0.4064,
      "step": 223800
    },
    {
      "epoch": 0.7137278653512058,
      "grad_norm": 0.0001586197759024799,
      "learning_rate": 8.588164039463826e-05,
      "loss": 0.2889,
      "step": 223900
    },
    {
      "epoch": 0.7140466361709249,
      "grad_norm": 19.08455467224121,
      "learning_rate": 8.578600914872252e-05,
      "loss": 0.3154,
      "step": 224000
    },
    {
      "epoch": 0.7143654069906441,
      "grad_norm": 0.0005561225698329508,
      "learning_rate": 8.569037790280676e-05,
      "loss": 0.2594,
      "step": 224100
    },
    {
      "epoch": 0.7146841778103632,
      "grad_norm": 0.00015991716645658016,
      "learning_rate": 8.559474665689101e-05,
      "loss": 0.3753,
      "step": 224200
    },
    {
      "epoch": 0.7150029486300824,
      "grad_norm": 48.232032775878906,
      "learning_rate": 8.549911541097527e-05,
      "loss": 0.2324,
      "step": 224300
    },
    {
      "epoch": 0.7153217194498016,
      "grad_norm": 0.0008320460910908878,
      "learning_rate": 8.540348416505952e-05,
      "loss": 0.1936,
      "step": 224400
    },
    {
      "epoch": 0.7156404902695207,
      "grad_norm": 0.00038800155743956566,
      "learning_rate": 8.530785291914379e-05,
      "loss": 0.3265,
      "step": 224500
    },
    {
      "epoch": 0.7159592610892399,
      "grad_norm": 0.00031557970214635134,
      "learning_rate": 8.521222167322802e-05,
      "loss": 0.2791,
      "step": 224600
    },
    {
      "epoch": 0.716278031908959,
      "grad_norm": 0.02022378519177437,
      "learning_rate": 8.511659042731227e-05,
      "loss": 0.0379,
      "step": 224700
    },
    {
      "epoch": 0.7165968027286782,
      "grad_norm": 0.31160444021224976,
      "learning_rate": 8.502095918139654e-05,
      "loss": 0.227,
      "step": 224800
    },
    {
      "epoch": 0.7169155735483974,
      "grad_norm": 78.05963897705078,
      "learning_rate": 8.492532793548079e-05,
      "loss": 0.4075,
      "step": 224900
    },
    {
      "epoch": 0.7172343443681165,
      "grad_norm": 0.010620133951306343,
      "learning_rate": 8.482969668956503e-05,
      "loss": 0.3559,
      "step": 225000
    },
    {
      "epoch": 0.7175531151878357,
      "grad_norm": 0.01976403407752514,
      "learning_rate": 8.473406544364928e-05,
      "loss": 0.2929,
      "step": 225100
    },
    {
      "epoch": 0.7178718860075549,
      "grad_norm": 1.7967438697814941,
      "learning_rate": 8.463843419773354e-05,
      "loss": 0.1649,
      "step": 225200
    },
    {
      "epoch": 0.718190656827274,
      "grad_norm": 0.0008538353140465915,
      "learning_rate": 8.454280295181779e-05,
      "loss": 0.1535,
      "step": 225300
    },
    {
      "epoch": 0.7185094276469932,
      "grad_norm": 0.0019779661670327187,
      "learning_rate": 8.444717170590203e-05,
      "loss": 0.2603,
      "step": 225400
    },
    {
      "epoch": 0.7188281984667123,
      "grad_norm": 0.0011663809418678284,
      "learning_rate": 8.435154045998629e-05,
      "loss": 0.2357,
      "step": 225500
    },
    {
      "epoch": 0.7191469692864315,
      "grad_norm": 1.100316047668457,
      "learning_rate": 8.425590921407054e-05,
      "loss": 0.2279,
      "step": 225600
    },
    {
      "epoch": 0.7194657401061507,
      "grad_norm": 5.3327938076108694e-05,
      "learning_rate": 8.41602779681548e-05,
      "loss": 0.0818,
      "step": 225700
    },
    {
      "epoch": 0.7197845109258698,
      "grad_norm": 3.690212906803936e-05,
      "learning_rate": 8.406464672223903e-05,
      "loss": 0.2782,
      "step": 225800
    },
    {
      "epoch": 0.720103281745589,
      "grad_norm": 41.15911102294922,
      "learning_rate": 8.396901547632329e-05,
      "loss": 0.2752,
      "step": 225900
    },
    {
      "epoch": 0.7204220525653082,
      "grad_norm": 0.07935710996389389,
      "learning_rate": 8.387338423040754e-05,
      "loss": 0.3,
      "step": 226000
    },
    {
      "epoch": 0.7207408233850273,
      "grad_norm": 0.0004266265605110675,
      "learning_rate": 8.37777529844918e-05,
      "loss": 0.3475,
      "step": 226100
    },
    {
      "epoch": 0.7210595942047465,
      "grad_norm": 0.0002667068038135767,
      "learning_rate": 8.368212173857604e-05,
      "loss": 0.2226,
      "step": 226200
    },
    {
      "epoch": 0.7213783650244656,
      "grad_norm": 0.03835105523467064,
      "learning_rate": 8.358649049266029e-05,
      "loss": 0.1747,
      "step": 226300
    },
    {
      "epoch": 0.7216971358441848,
      "grad_norm": 0.00017353426665067673,
      "learning_rate": 8.349085924674454e-05,
      "loss": 0.0448,
      "step": 226400
    },
    {
      "epoch": 0.722015906663904,
      "grad_norm": 0.0003406970645301044,
      "learning_rate": 8.33952280008288e-05,
      "loss": 0.4524,
      "step": 226500
    },
    {
      "epoch": 0.7223346774836231,
      "grad_norm": 0.006365937180817127,
      "learning_rate": 8.329959675491305e-05,
      "loss": 0.2197,
      "step": 226600
    },
    {
      "epoch": 0.7226534483033423,
      "grad_norm": 1.1341688150423579e-05,
      "learning_rate": 8.320396550899729e-05,
      "loss": 0.2945,
      "step": 226700
    },
    {
      "epoch": 0.7229722191230614,
      "grad_norm": 0.009991993196308613,
      "learning_rate": 8.310833426308155e-05,
      "loss": 0.2233,
      "step": 226800
    },
    {
      "epoch": 0.7232909899427806,
      "grad_norm": 0.001856950344517827,
      "learning_rate": 8.30127030171658e-05,
      "loss": 0.1855,
      "step": 226900
    },
    {
      "epoch": 0.7236097607624998,
      "grad_norm": 0.015551751479506493,
      "learning_rate": 8.291707177125005e-05,
      "loss": 0.1951,
      "step": 227000
    },
    {
      "epoch": 0.7239285315822189,
      "grad_norm": 0.047095246613025665,
      "learning_rate": 8.28214405253343e-05,
      "loss": 0.199,
      "step": 227100
    },
    {
      "epoch": 0.7242473024019381,
      "grad_norm": 0.0006550429970957339,
      "learning_rate": 8.272580927941855e-05,
      "loss": 0.2486,
      "step": 227200
    },
    {
      "epoch": 0.7245660732216573,
      "grad_norm": 0.0006708527798764408,
      "learning_rate": 8.26301780335028e-05,
      "loss": 0.2967,
      "step": 227300
    },
    {
      "epoch": 0.7248848440413764,
      "grad_norm": 0.000208451587241143,
      "learning_rate": 8.253454678758707e-05,
      "loss": 0.3105,
      "step": 227400
    },
    {
      "epoch": 0.7252036148610956,
      "grad_norm": 40.56173324584961,
      "learning_rate": 8.24389155416713e-05,
      "loss": 0.2854,
      "step": 227500
    },
    {
      "epoch": 0.7255223856808147,
      "grad_norm": 0.00020599829440470785,
      "learning_rate": 8.234328429575555e-05,
      "loss": 0.342,
      "step": 227600
    },
    {
      "epoch": 0.7258411565005339,
      "grad_norm": 0.006923504173755646,
      "learning_rate": 8.224765304983982e-05,
      "loss": 0.2469,
      "step": 227700
    },
    {
      "epoch": 0.7261599273202531,
      "grad_norm": 0.0011023120023310184,
      "learning_rate": 8.215202180392407e-05,
      "loss": 0.1528,
      "step": 227800
    },
    {
      "epoch": 0.7264786981399722,
      "grad_norm": 0.0008128249319270253,
      "learning_rate": 8.20563905580083e-05,
      "loss": 0.1309,
      "step": 227900
    },
    {
      "epoch": 0.7267974689596914,
      "grad_norm": 6.435471004806459e-05,
      "learning_rate": 8.196075931209257e-05,
      "loss": 0.3529,
      "step": 228000
    },
    {
      "epoch": 0.7271162397794106,
      "grad_norm": 0.00013439077883958817,
      "learning_rate": 8.186512806617682e-05,
      "loss": 0.2687,
      "step": 228100
    },
    {
      "epoch": 0.7274350105991297,
      "grad_norm": 0.00020607082115020603,
      "learning_rate": 8.176949682026107e-05,
      "loss": 0.1615,
      "step": 228200
    },
    {
      "epoch": 0.7277537814188489,
      "grad_norm": 0.010945712216198444,
      "learning_rate": 8.167386557434531e-05,
      "loss": 0.3138,
      "step": 228300
    },
    {
      "epoch": 0.728072552238568,
      "grad_norm": 65.66170501708984,
      "learning_rate": 8.157823432842957e-05,
      "loss": 0.2912,
      "step": 228400
    },
    {
      "epoch": 0.7283913230582872,
      "grad_norm": 0.028531985357403755,
      "learning_rate": 8.148260308251382e-05,
      "loss": 0.3954,
      "step": 228500
    },
    {
      "epoch": 0.7287100938780064,
      "grad_norm": 11.92883586883545,
      "learning_rate": 8.138697183659808e-05,
      "loss": 0.1525,
      "step": 228600
    },
    {
      "epoch": 0.7290288646977255,
      "grad_norm": 0.00012815491936635226,
      "learning_rate": 8.129134059068233e-05,
      "loss": 0.3776,
      "step": 228700
    },
    {
      "epoch": 0.7293476355174447,
      "grad_norm": 0.00021864802693016827,
      "learning_rate": 8.119570934476657e-05,
      "loss": 0.2157,
      "step": 228800
    },
    {
      "epoch": 0.7296664063371638,
      "grad_norm": 0.0016052216524258256,
      "learning_rate": 8.110007809885082e-05,
      "loss": 0.258,
      "step": 228900
    },
    {
      "epoch": 0.729985177156883,
      "grad_norm": 22.182159423828125,
      "learning_rate": 8.100444685293508e-05,
      "loss": 0.2894,
      "step": 229000
    },
    {
      "epoch": 0.7303039479766023,
      "grad_norm": 6.904995098011568e-05,
      "learning_rate": 8.090881560701933e-05,
      "loss": 0.3691,
      "step": 229100
    },
    {
      "epoch": 0.7306227187963213,
      "grad_norm": 34.726261138916016,
      "learning_rate": 8.081318436110357e-05,
      "loss": 0.5221,
      "step": 229200
    },
    {
      "epoch": 0.7309414896160406,
      "grad_norm": 0.0027763957623392344,
      "learning_rate": 8.071755311518783e-05,
      "loss": 0.3749,
      "step": 229300
    },
    {
      "epoch": 0.7312602604357598,
      "grad_norm": 11.926322937011719,
      "learning_rate": 8.062192186927208e-05,
      "loss": 0.3102,
      "step": 229400
    },
    {
      "epoch": 0.7315790312554789,
      "grad_norm": 0.006944007705897093,
      "learning_rate": 8.052629062335633e-05,
      "loss": 0.1217,
      "step": 229500
    },
    {
      "epoch": 0.7318978020751981,
      "grad_norm": 0.0020132381469011307,
      "learning_rate": 8.043065937744057e-05,
      "loss": 0.1332,
      "step": 229600
    },
    {
      "epoch": 0.7322165728949172,
      "grad_norm": 0.00019989750580862164,
      "learning_rate": 8.033502813152483e-05,
      "loss": 0.1616,
      "step": 229700
    },
    {
      "epoch": 0.7325353437146364,
      "grad_norm": 19.28618812561035,
      "learning_rate": 8.023939688560908e-05,
      "loss": 0.3127,
      "step": 229800
    },
    {
      "epoch": 0.7328541145343556,
      "grad_norm": 0.001569138839840889,
      "learning_rate": 8.014376563969334e-05,
      "loss": 0.2485,
      "step": 229900
    },
    {
      "epoch": 0.7331728853540747,
      "grad_norm": 0.00020224387117195874,
      "learning_rate": 8.004813439377758e-05,
      "loss": 0.2994,
      "step": 230000
    },
    {
      "epoch": 0.7334916561737939,
      "grad_norm": 1.0071819815493654e-05,
      "learning_rate": 7.995250314786183e-05,
      "loss": 0.2556,
      "step": 230100
    },
    {
      "epoch": 0.7338104269935131,
      "grad_norm": 0.00032230952638201416,
      "learning_rate": 7.985687190194608e-05,
      "loss": 0.3297,
      "step": 230200
    },
    {
      "epoch": 0.7341291978132322,
      "grad_norm": 58.830650329589844,
      "learning_rate": 7.976124065603034e-05,
      "loss": 0.1974,
      "step": 230300
    },
    {
      "epoch": 0.7344479686329514,
      "grad_norm": 0.004118931945413351,
      "learning_rate": 7.966560941011458e-05,
      "loss": 0.3033,
      "step": 230400
    },
    {
      "epoch": 0.7347667394526705,
      "grad_norm": 0.0015316128265112638,
      "learning_rate": 7.956997816419883e-05,
      "loss": 0.1065,
      "step": 230500
    },
    {
      "epoch": 0.7350855102723897,
      "grad_norm": 3.688571086968295e-05,
      "learning_rate": 7.94743469182831e-05,
      "loss": 0.2475,
      "step": 230600
    },
    {
      "epoch": 0.7354042810921089,
      "grad_norm": 0.0044418806210160255,
      "learning_rate": 7.937871567236735e-05,
      "loss": 0.4083,
      "step": 230700
    },
    {
      "epoch": 0.735723051911828,
      "grad_norm": 1.9830670356750488,
      "learning_rate": 7.928308442645158e-05,
      "loss": 0.2455,
      "step": 230800
    },
    {
      "epoch": 0.7360418227315472,
      "grad_norm": 0.004071259871125221,
      "learning_rate": 7.918745318053585e-05,
      "loss": 0.2496,
      "step": 230900
    },
    {
      "epoch": 0.7363605935512663,
      "grad_norm": 0.00026243526372127235,
      "learning_rate": 7.90918219346201e-05,
      "loss": 0.201,
      "step": 231000
    },
    {
      "epoch": 0.7366793643709855,
      "grad_norm": 1.2697342754108831e-05,
      "learning_rate": 7.899619068870436e-05,
      "loss": 0.1495,
      "step": 231100
    },
    {
      "epoch": 0.7369981351907047,
      "grad_norm": 0.005922108422964811,
      "learning_rate": 7.890055944278861e-05,
      "loss": 0.3612,
      "step": 231200
    },
    {
      "epoch": 0.7373169060104238,
      "grad_norm": 0.011821102350950241,
      "learning_rate": 7.880492819687285e-05,
      "loss": 0.2027,
      "step": 231300
    },
    {
      "epoch": 0.737635676830143,
      "grad_norm": 38.91357421875,
      "learning_rate": 7.87092969509571e-05,
      "loss": 0.3874,
      "step": 231400
    },
    {
      "epoch": 0.7379544476498622,
      "grad_norm": 47.577293395996094,
      "learning_rate": 7.861366570504136e-05,
      "loss": 0.1978,
      "step": 231500
    },
    {
      "epoch": 0.7382732184695813,
      "grad_norm": 4.57683636341244e-05,
      "learning_rate": 7.851803445912561e-05,
      "loss": 0.2453,
      "step": 231600
    },
    {
      "epoch": 0.7385919892893005,
      "grad_norm": 0.004199105314910412,
      "learning_rate": 7.842240321320985e-05,
      "loss": 0.2908,
      "step": 231700
    },
    {
      "epoch": 0.7389107601090196,
      "grad_norm": 0.004280627705156803,
      "learning_rate": 7.83267719672941e-05,
      "loss": 0.1902,
      "step": 231800
    },
    {
      "epoch": 0.7392295309287388,
      "grad_norm": 0.0007829959504306316,
      "learning_rate": 7.823114072137836e-05,
      "loss": 0.1329,
      "step": 231900
    },
    {
      "epoch": 0.739548301748458,
      "grad_norm": 0.00024300366931129247,
      "learning_rate": 7.813550947546261e-05,
      "loss": 0.2487,
      "step": 232000
    },
    {
      "epoch": 0.7398670725681771,
      "grad_norm": 0.0007010400877334177,
      "learning_rate": 7.803987822954685e-05,
      "loss": 0.1992,
      "step": 232100
    },
    {
      "epoch": 0.7401858433878963,
      "grad_norm": 29.89656639099121,
      "learning_rate": 7.794424698363111e-05,
      "loss": 0.0841,
      "step": 232200
    },
    {
      "epoch": 0.7405046142076155,
      "grad_norm": 0.0006056444835849106,
      "learning_rate": 7.784861573771536e-05,
      "loss": 0.1446,
      "step": 232300
    },
    {
      "epoch": 0.7408233850273346,
      "grad_norm": 0.0009614540613256395,
      "learning_rate": 7.775298449179962e-05,
      "loss": 0.2964,
      "step": 232400
    },
    {
      "epoch": 0.7411421558470538,
      "grad_norm": 14.277256965637207,
      "learning_rate": 7.765735324588386e-05,
      "loss": 0.1283,
      "step": 232500
    },
    {
      "epoch": 0.7414609266667729,
      "grad_norm": 17.95102882385254,
      "learning_rate": 7.756172199996811e-05,
      "loss": 0.1783,
      "step": 232600
    },
    {
      "epoch": 0.7417796974864921,
      "grad_norm": 0.0002242844639113173,
      "learning_rate": 7.746609075405236e-05,
      "loss": 0.2976,
      "step": 232700
    },
    {
      "epoch": 0.7420984683062113,
      "grad_norm": 87.1314926147461,
      "learning_rate": 7.737045950813662e-05,
      "loss": 0.1934,
      "step": 232800
    },
    {
      "epoch": 0.7424172391259304,
      "grad_norm": 6.336777005344629e-05,
      "learning_rate": 7.727482826222086e-05,
      "loss": 0.1991,
      "step": 232900
    },
    {
      "epoch": 0.7427360099456496,
      "grad_norm": 31.06398582458496,
      "learning_rate": 7.717919701630511e-05,
      "loss": 0.2149,
      "step": 233000
    },
    {
      "epoch": 0.7430547807653687,
      "grad_norm": 0.019586503505706787,
      "learning_rate": 7.708356577038937e-05,
      "loss": 0.2108,
      "step": 233100
    },
    {
      "epoch": 0.7433735515850879,
      "grad_norm": 0.000963333819527179,
      "learning_rate": 7.698793452447362e-05,
      "loss": 0.3674,
      "step": 233200
    },
    {
      "epoch": 0.7436923224048071,
      "grad_norm": 0.00023986361338756979,
      "learning_rate": 7.689230327855789e-05,
      "loss": 0.1742,
      "step": 233300
    },
    {
      "epoch": 0.7440110932245262,
      "grad_norm": 0.00032306116190738976,
      "learning_rate": 7.679667203264211e-05,
      "loss": 0.1882,
      "step": 233400
    },
    {
      "epoch": 0.7443298640442454,
      "grad_norm": 0.0023446427658200264,
      "learning_rate": 7.670104078672637e-05,
      "loss": 0.2747,
      "step": 233500
    },
    {
      "epoch": 0.7446486348639646,
      "grad_norm": 0.000770388578530401,
      "learning_rate": 7.660540954081064e-05,
      "loss": 0.1561,
      "step": 233600
    },
    {
      "epoch": 0.7449674056836837,
      "grad_norm": 0.052848875522613525,
      "learning_rate": 7.650977829489489e-05,
      "loss": 0.2148,
      "step": 233700
    },
    {
      "epoch": 0.7452861765034029,
      "grad_norm": 0.0001325637276750058,
      "learning_rate": 7.641414704897913e-05,
      "loss": 0.2958,
      "step": 233800
    },
    {
      "epoch": 0.745604947323122,
      "grad_norm": 0.0008468070300295949,
      "learning_rate": 7.631851580306338e-05,
      "loss": 0.3346,
      "step": 233900
    },
    {
      "epoch": 0.7459237181428412,
      "grad_norm": 0.0004232583742123097,
      "learning_rate": 7.622288455714764e-05,
      "loss": 0.3548,
      "step": 234000
    },
    {
      "epoch": 0.7462424889625604,
      "grad_norm": 0.0002590423682704568,
      "learning_rate": 7.612725331123189e-05,
      "loss": 0.1712,
      "step": 234100
    },
    {
      "epoch": 0.7465612597822795,
      "grad_norm": 0.0001223227591253817,
      "learning_rate": 7.603162206531613e-05,
      "loss": 0.2221,
      "step": 234200
    },
    {
      "epoch": 0.7468800306019987,
      "grad_norm": 0.0024266222026199102,
      "learning_rate": 7.593599081940039e-05,
      "loss": 0.2703,
      "step": 234300
    },
    {
      "epoch": 0.7471988014217179,
      "grad_norm": 0.31910887360572815,
      "learning_rate": 7.584035957348464e-05,
      "loss": 0.1986,
      "step": 234400
    },
    {
      "epoch": 0.747517572241437,
      "grad_norm": 0.0003839704440906644,
      "learning_rate": 7.57447283275689e-05,
      "loss": 0.1792,
      "step": 234500
    },
    {
      "epoch": 0.7478363430611562,
      "grad_norm": 4.127504325879272e-06,
      "learning_rate": 7.564909708165313e-05,
      "loss": 0.2038,
      "step": 234600
    },
    {
      "epoch": 0.7481551138808753,
      "grad_norm": 0.0004293674137443304,
      "learning_rate": 7.555346583573739e-05,
      "loss": 0.3439,
      "step": 234700
    },
    {
      "epoch": 0.7484738847005945,
      "grad_norm": 0.002936671022325754,
      "learning_rate": 7.545783458982164e-05,
      "loss": 0.2236,
      "step": 234800
    },
    {
      "epoch": 0.7487926555203137,
      "grad_norm": 0.00011715850268956274,
      "learning_rate": 7.53622033439059e-05,
      "loss": 0.1679,
      "step": 234900
    },
    {
      "epoch": 0.7491114263400328,
      "grad_norm": 0.00045655464055016637,
      "learning_rate": 7.526657209799014e-05,
      "loss": 0.1011,
      "step": 235000
    },
    {
      "epoch": 0.749430197159752,
      "grad_norm": 4.960251317243092e-05,
      "learning_rate": 7.517094085207439e-05,
      "loss": 0.27,
      "step": 235100
    },
    {
      "epoch": 0.7497489679794711,
      "grad_norm": 0.0015054030809551477,
      "learning_rate": 7.507530960615864e-05,
      "loss": 0.2742,
      "step": 235200
    },
    {
      "epoch": 0.7500677387991903,
      "grad_norm": 0.00042122468585148454,
      "learning_rate": 7.49796783602429e-05,
      "loss": 0.2072,
      "step": 235300
    },
    {
      "epoch": 0.7503865096189095,
      "grad_norm": 0.005122094880789518,
      "learning_rate": 7.488404711432715e-05,
      "loss": 0.2611,
      "step": 235400
    },
    {
      "epoch": 0.7507052804386286,
      "grad_norm": 4.764923869515769e-05,
      "learning_rate": 7.47884158684114e-05,
      "loss": 0.1373,
      "step": 235500
    },
    {
      "epoch": 0.7510240512583478,
      "grad_norm": 0.012887263670563698,
      "learning_rate": 7.469278462249565e-05,
      "loss": 0.1862,
      "step": 235600
    },
    {
      "epoch": 0.751342822078067,
      "grad_norm": 0.0007848030072636902,
      "learning_rate": 7.45971533765799e-05,
      "loss": 0.1579,
      "step": 235700
    },
    {
      "epoch": 0.7516615928977861,
      "grad_norm": 0.001272316207177937,
      "learning_rate": 7.450152213066415e-05,
      "loss": 0.2756,
      "step": 235800
    },
    {
      "epoch": 0.7519803637175053,
      "grad_norm": 0.0027671970892697573,
      "learning_rate": 7.440589088474841e-05,
      "loss": 0.315,
      "step": 235900
    },
    {
      "epoch": 0.7522991345372244,
      "grad_norm": 3.829886554740369e-05,
      "learning_rate": 7.431025963883265e-05,
      "loss": 0.1465,
      "step": 236000
    },
    {
      "epoch": 0.7526179053569436,
      "grad_norm": 0.0014319306937977672,
      "learning_rate": 7.42146283929169e-05,
      "loss": 0.1661,
      "step": 236100
    },
    {
      "epoch": 0.7529366761766628,
      "grad_norm": 0.0003023749450221658,
      "learning_rate": 7.411899714700116e-05,
      "loss": 0.3616,
      "step": 236200
    },
    {
      "epoch": 0.7532554469963819,
      "grad_norm": 0.00012527139915619045,
      "learning_rate": 7.402336590108541e-05,
      "loss": 0.1846,
      "step": 236300
    },
    {
      "epoch": 0.7535742178161011,
      "grad_norm": 6.957201549084857e-05,
      "learning_rate": 7.392773465516965e-05,
      "loss": 0.2333,
      "step": 236400
    },
    {
      "epoch": 0.7538929886358203,
      "grad_norm": 0.0013545069377869368,
      "learning_rate": 7.383210340925392e-05,
      "loss": 0.1048,
      "step": 236500
    },
    {
      "epoch": 0.7542117594555394,
      "grad_norm": 0.00034124925150536,
      "learning_rate": 7.373647216333816e-05,
      "loss": 0.261,
      "step": 236600
    },
    {
      "epoch": 0.7545305302752586,
      "grad_norm": 0.003321083029732108,
      "learning_rate": 7.364084091742241e-05,
      "loss": 0.4039,
      "step": 236700
    },
    {
      "epoch": 0.7548493010949777,
      "grad_norm": 0.0007165439310483634,
      "learning_rate": 7.354520967150667e-05,
      "loss": 0.2035,
      "step": 236800
    },
    {
      "epoch": 0.7551680719146969,
      "grad_norm": 0.0009036542614921927,
      "learning_rate": 7.344957842559092e-05,
      "loss": 0.2333,
      "step": 236900
    },
    {
      "epoch": 0.7554868427344161,
      "grad_norm": 0.00027458666590973735,
      "learning_rate": 7.335394717967516e-05,
      "loss": 0.1384,
      "step": 237000
    },
    {
      "epoch": 0.7558056135541352,
      "grad_norm": 4.108468055725098,
      "learning_rate": 7.325831593375941e-05,
      "loss": 0.183,
      "step": 237100
    },
    {
      "epoch": 0.7561243843738544,
      "grad_norm": 0.001210894901305437,
      "learning_rate": 7.316268468784367e-05,
      "loss": 0.1107,
      "step": 237200
    },
    {
      "epoch": 0.7564431551935736,
      "grad_norm": 3.7787220207974315e-05,
      "learning_rate": 7.306705344192792e-05,
      "loss": 0.2026,
      "step": 237300
    },
    {
      "epoch": 0.7567619260132927,
      "grad_norm": 0.00012054880789946765,
      "learning_rate": 7.297142219601218e-05,
      "loss": 0.1674,
      "step": 237400
    },
    {
      "epoch": 0.7570806968330119,
      "grad_norm": 0.0008491213666275144,
      "learning_rate": 7.287579095009642e-05,
      "loss": 0.2053,
      "step": 237500
    },
    {
      "epoch": 0.757399467652731,
      "grad_norm": 0.006935827899724245,
      "learning_rate": 7.278015970418067e-05,
      "loss": 0.1681,
      "step": 237600
    },
    {
      "epoch": 0.7577182384724502,
      "grad_norm": 2.025114190473687e-05,
      "learning_rate": 7.268452845826492e-05,
      "loss": 0.1716,
      "step": 237700
    },
    {
      "epoch": 0.7580370092921694,
      "grad_norm": 0.00013759735156781971,
      "learning_rate": 7.258889721234918e-05,
      "loss": 0.2494,
      "step": 237800
    },
    {
      "epoch": 0.7583557801118885,
      "grad_norm": 0.0007733192178420722,
      "learning_rate": 7.249326596643342e-05,
      "loss": 0.3279,
      "step": 237900
    },
    {
      "epoch": 0.7586745509316077,
      "grad_norm": 0.006294888909906149,
      "learning_rate": 7.239763472051769e-05,
      "loss": 0.0887,
      "step": 238000
    },
    {
      "epoch": 0.7589933217513268,
      "grad_norm": 0.001122375950217247,
      "learning_rate": 7.230200347460193e-05,
      "loss": 0.2077,
      "step": 238100
    },
    {
      "epoch": 0.759312092571046,
      "grad_norm": 0.05102330073714256,
      "learning_rate": 7.220637222868618e-05,
      "loss": 0.1706,
      "step": 238200
    },
    {
      "epoch": 0.7596308633907652,
      "grad_norm": 1.6448057067464106e-05,
      "learning_rate": 7.211074098277043e-05,
      "loss": 0.2485,
      "step": 238300
    },
    {
      "epoch": 0.7599496342104843,
      "grad_norm": 10.72061824798584,
      "learning_rate": 7.201510973685469e-05,
      "loss": 0.2401,
      "step": 238400
    },
    {
      "epoch": 0.7602684050302035,
      "grad_norm": 22.550186157226562,
      "learning_rate": 7.191947849093893e-05,
      "loss": 0.2497,
      "step": 238500
    },
    {
      "epoch": 0.7605871758499227,
      "grad_norm": 92.03973388671875,
      "learning_rate": 7.182384724502318e-05,
      "loss": 0.2125,
      "step": 238600
    },
    {
      "epoch": 0.7609059466696418,
      "grad_norm": 0.0006826103781349957,
      "learning_rate": 7.172821599910744e-05,
      "loss": 0.1378,
      "step": 238700
    },
    {
      "epoch": 0.761224717489361,
      "grad_norm": 0.0005093443905934691,
      "learning_rate": 7.163258475319169e-05,
      "loss": 0.2196,
      "step": 238800
    },
    {
      "epoch": 0.7615434883090801,
      "grad_norm": 0.0004223432042635977,
      "learning_rate": 7.153695350727593e-05,
      "loss": 0.1841,
      "step": 238900
    },
    {
      "epoch": 0.7618622591287993,
      "grad_norm": 51.39134979248047,
      "learning_rate": 7.144132226136018e-05,
      "loss": 0.2826,
      "step": 239000
    },
    {
      "epoch": 0.7621810299485186,
      "grad_norm": 0.00010710309288697317,
      "learning_rate": 7.134569101544444e-05,
      "loss": 0.2645,
      "step": 239100
    },
    {
      "epoch": 0.7624998007682376,
      "grad_norm": 0.0042418548837304115,
      "learning_rate": 7.125005976952869e-05,
      "loss": 0.1448,
      "step": 239200
    },
    {
      "epoch": 0.7628185715879569,
      "grad_norm": 9.170987323159352e-05,
      "learning_rate": 7.115442852361293e-05,
      "loss": 0.3611,
      "step": 239300
    },
    {
      "epoch": 0.7631373424076761,
      "grad_norm": 0.0006468171486631036,
      "learning_rate": 7.10587972776972e-05,
      "loss": 0.2631,
      "step": 239400
    },
    {
      "epoch": 0.7634561132273952,
      "grad_norm": 0.0011779890628531575,
      "learning_rate": 7.096316603178144e-05,
      "loss": 0.1153,
      "step": 239500
    },
    {
      "epoch": 0.7637748840471144,
      "grad_norm": 0.0013747572666034102,
      "learning_rate": 7.08675347858657e-05,
      "loss": 0.2319,
      "step": 239600
    },
    {
      "epoch": 0.7640936548668334,
      "grad_norm": 0.005790843162685633,
      "learning_rate": 7.077190353994995e-05,
      "loss": 0.2016,
      "step": 239700
    },
    {
      "epoch": 0.7644124256865527,
      "grad_norm": 59.14432907104492,
      "learning_rate": 7.06762722940342e-05,
      "loss": 0.3492,
      "step": 239800
    },
    {
      "epoch": 0.7647311965062719,
      "grad_norm": 0.002269107149913907,
      "learning_rate": 7.058064104811846e-05,
      "loss": 0.089,
      "step": 239900
    },
    {
      "epoch": 0.765049967325991,
      "grad_norm": 7.691136852372438e-05,
      "learning_rate": 7.04850098022027e-05,
      "loss": 0.2368,
      "step": 240000
    },
    {
      "epoch": 0.7653687381457102,
      "grad_norm": 0.8712760210037231,
      "learning_rate": 7.038937855628695e-05,
      "loss": 0.3995,
      "step": 240100
    },
    {
      "epoch": 0.7656875089654293,
      "grad_norm": 56.490867614746094,
      "learning_rate": 7.02937473103712e-05,
      "loss": 0.3031,
      "step": 240200
    },
    {
      "epoch": 0.7660062797851485,
      "grad_norm": 0.00024275975010823458,
      "learning_rate": 7.019811606445546e-05,
      "loss": 0.1859,
      "step": 240300
    },
    {
      "epoch": 0.7663250506048677,
      "grad_norm": 0.0007405958604067564,
      "learning_rate": 7.01024848185397e-05,
      "loss": 0.1402,
      "step": 240400
    },
    {
      "epoch": 0.7666438214245868,
      "grad_norm": 0.00033532187808305025,
      "learning_rate": 7.000685357262395e-05,
      "loss": 0.3254,
      "step": 240500
    },
    {
      "epoch": 0.766962592244306,
      "grad_norm": 0.0034497412852942944,
      "learning_rate": 6.99112223267082e-05,
      "loss": 0.3354,
      "step": 240600
    },
    {
      "epoch": 0.7672813630640252,
      "grad_norm": 0.0002638709847815335,
      "learning_rate": 6.981559108079246e-05,
      "loss": 0.1343,
      "step": 240700
    },
    {
      "epoch": 0.7676001338837443,
      "grad_norm": 0.013130956329405308,
      "learning_rate": 6.97199598348767e-05,
      "loss": 0.2671,
      "step": 240800
    },
    {
      "epoch": 0.7679189047034635,
      "grad_norm": 0.0053717996925115585,
      "learning_rate": 6.962432858896097e-05,
      "loss": 0.1566,
      "step": 240900
    },
    {
      "epoch": 0.7682376755231826,
      "grad_norm": 2.7221425625612028e-05,
      "learning_rate": 6.952869734304521e-05,
      "loss": 0.2165,
      "step": 241000
    },
    {
      "epoch": 0.7685564463429018,
      "grad_norm": 3.372363426024094e-05,
      "learning_rate": 6.943306609712946e-05,
      "loss": 0.2256,
      "step": 241100
    },
    {
      "epoch": 0.768875217162621,
      "grad_norm": 0.0003442202869337052,
      "learning_rate": 6.933743485121372e-05,
      "loss": 0.1506,
      "step": 241200
    },
    {
      "epoch": 0.7691939879823401,
      "grad_norm": 2.0234858311596327e-05,
      "learning_rate": 6.924180360529797e-05,
      "loss": 0.3003,
      "step": 241300
    },
    {
      "epoch": 0.7695127588020593,
      "grad_norm": 0.0005900683463551104,
      "learning_rate": 6.914617235938221e-05,
      "loss": 0.2186,
      "step": 241400
    },
    {
      "epoch": 0.7698315296217785,
      "grad_norm": 0.010163204744458199,
      "learning_rate": 6.905054111346646e-05,
      "loss": 0.1042,
      "step": 241500
    },
    {
      "epoch": 0.7701503004414976,
      "grad_norm": 0.00044395143049769104,
      "learning_rate": 6.895490986755072e-05,
      "loss": 0.2142,
      "step": 241600
    },
    {
      "epoch": 0.7704690712612168,
      "grad_norm": 59.224822998046875,
      "learning_rate": 6.885927862163497e-05,
      "loss": 0.1929,
      "step": 241700
    },
    {
      "epoch": 0.7707878420809359,
      "grad_norm": 28.476064682006836,
      "learning_rate": 6.876364737571923e-05,
      "loss": 0.1782,
      "step": 241800
    },
    {
      "epoch": 0.7711066129006551,
      "grad_norm": 0.00013858832244295627,
      "learning_rate": 6.866801612980347e-05,
      "loss": 0.1156,
      "step": 241900
    },
    {
      "epoch": 0.7714253837203743,
      "grad_norm": 0.005056475754827261,
      "learning_rate": 6.857238488388772e-05,
      "loss": 0.1977,
      "step": 242000
    },
    {
      "epoch": 0.7717441545400934,
      "grad_norm": 0.0005837228382006288,
      "learning_rate": 6.847675363797197e-05,
      "loss": 0.1038,
      "step": 242100
    },
    {
      "epoch": 0.7720629253598126,
      "grad_norm": 0.20453739166259766,
      "learning_rate": 6.838112239205623e-05,
      "loss": 0.1734,
      "step": 242200
    },
    {
      "epoch": 0.7723816961795317,
      "grad_norm": 0.016476407647132874,
      "learning_rate": 6.828549114614047e-05,
      "loss": 0.1014,
      "step": 242300
    },
    {
      "epoch": 0.7727004669992509,
      "grad_norm": 0.00020030781161040068,
      "learning_rate": 6.818985990022474e-05,
      "loss": 0.2424,
      "step": 242400
    },
    {
      "epoch": 0.7730192378189701,
      "grad_norm": 0.001628673984669149,
      "learning_rate": 6.809422865430898e-05,
      "loss": 0.1265,
      "step": 242500
    },
    {
      "epoch": 0.7733380086386892,
      "grad_norm": 0.009080651216208935,
      "learning_rate": 6.799859740839323e-05,
      "loss": 0.199,
      "step": 242600
    },
    {
      "epoch": 0.7736567794584084,
      "grad_norm": 0.0001038378759403713,
      "learning_rate": 6.790296616247748e-05,
      "loss": 0.0918,
      "step": 242700
    },
    {
      "epoch": 0.7739755502781276,
      "grad_norm": 67.50433349609375,
      "learning_rate": 6.780733491656174e-05,
      "loss": 0.2458,
      "step": 242800
    },
    {
      "epoch": 0.7742943210978467,
      "grad_norm": 0.004406552296131849,
      "learning_rate": 6.771170367064598e-05,
      "loss": 0.0862,
      "step": 242900
    },
    {
      "epoch": 0.7746130919175659,
      "grad_norm": 0.00019618141232058406,
      "learning_rate": 6.761607242473023e-05,
      "loss": 0.2029,
      "step": 243000
    },
    {
      "epoch": 0.774931862737285,
      "grad_norm": 0.005142904352396727,
      "learning_rate": 6.752044117881449e-05,
      "loss": 0.2024,
      "step": 243100
    },
    {
      "epoch": 0.7752506335570042,
      "grad_norm": 0.5372175574302673,
      "learning_rate": 6.742480993289874e-05,
      "loss": 0.2128,
      "step": 243200
    },
    {
      "epoch": 0.7755694043767234,
      "grad_norm": 0.0007170754252001643,
      "learning_rate": 6.732917868698298e-05,
      "loss": 0.2371,
      "step": 243300
    },
    {
      "epoch": 0.7758881751964425,
      "grad_norm": 2.5412422473891638e-05,
      "learning_rate": 6.723354744106723e-05,
      "loss": 0.1851,
      "step": 243400
    },
    {
      "epoch": 0.7762069460161617,
      "grad_norm": 0.03294171020388603,
      "learning_rate": 6.713791619515149e-05,
      "loss": 0.2059,
      "step": 243500
    },
    {
      "epoch": 0.7765257168358809,
      "grad_norm": 0.000136236849357374,
      "learning_rate": 6.704228494923574e-05,
      "loss": 0.171,
      "step": 243600
    },
    {
      "epoch": 0.7768444876556,
      "grad_norm": 0.0008870773017406464,
      "learning_rate": 6.694665370331998e-05,
      "loss": 0.1993,
      "step": 243700
    },
    {
      "epoch": 0.7771632584753192,
      "grad_norm": 4.07638890465023e-06,
      "learning_rate": 6.685102245740425e-05,
      "loss": 0.1503,
      "step": 243800
    },
    {
      "epoch": 0.7774820292950383,
      "grad_norm": 0.0054633901454508305,
      "learning_rate": 6.675539121148849e-05,
      "loss": 0.1919,
      "step": 243900
    },
    {
      "epoch": 0.7778008001147575,
      "grad_norm": 52.19990539550781,
      "learning_rate": 6.665975996557274e-05,
      "loss": 0.3743,
      "step": 244000
    },
    {
      "epoch": 0.7781195709344767,
      "grad_norm": 0.00033319962676614523,
      "learning_rate": 6.6564128719657e-05,
      "loss": 0.2445,
      "step": 244100
    },
    {
      "epoch": 0.7784383417541958,
      "grad_norm": 0.02612624503672123,
      "learning_rate": 6.646849747374125e-05,
      "loss": 0.2545,
      "step": 244200
    },
    {
      "epoch": 0.778757112573915,
      "grad_norm": 0.9977179765701294,
      "learning_rate": 6.63728662278255e-05,
      "loss": 0.2256,
      "step": 244300
    },
    {
      "epoch": 0.7790758833936341,
      "grad_norm": 5.9663398133125156e-05,
      "learning_rate": 6.627723498190975e-05,
      "loss": 0.2464,
      "step": 244400
    },
    {
      "epoch": 0.7793946542133533,
      "grad_norm": 0.3300541937351227,
      "learning_rate": 6.6181603735994e-05,
      "loss": 0.3123,
      "step": 244500
    },
    {
      "epoch": 0.7797134250330725,
      "grad_norm": 12.967812538146973,
      "learning_rate": 6.608597249007825e-05,
      "loss": 0.4248,
      "step": 244600
    },
    {
      "epoch": 0.7800321958527916,
      "grad_norm": 0.01914091408252716,
      "learning_rate": 6.599034124416251e-05,
      "loss": 0.1964,
      "step": 244700
    },
    {
      "epoch": 0.7803509666725108,
      "grad_norm": 0.0006317444494925439,
      "learning_rate": 6.589470999824675e-05,
      "loss": 0.1729,
      "step": 244800
    },
    {
      "epoch": 0.78066973749223,
      "grad_norm": 9.63070011138916,
      "learning_rate": 6.5799078752331e-05,
      "loss": 0.0689,
      "step": 244900
    },
    {
      "epoch": 0.7809885083119491,
      "grad_norm": 0.0002769113052636385,
      "learning_rate": 6.570344750641526e-05,
      "loss": 0.3034,
      "step": 245000
    },
    {
      "epoch": 0.7813072791316683,
      "grad_norm": 12.176165580749512,
      "learning_rate": 6.560781626049951e-05,
      "loss": 0.2237,
      "step": 245100
    },
    {
      "epoch": 0.7816260499513874,
      "grad_norm": 0.021372998133301735,
      "learning_rate": 6.551218501458375e-05,
      "loss": 0.2192,
      "step": 245200
    },
    {
      "epoch": 0.7819448207711066,
      "grad_norm": 0.0015340339159592986,
      "learning_rate": 6.541655376866802e-05,
      "loss": 0.09,
      "step": 245300
    },
    {
      "epoch": 0.7822635915908258,
      "grad_norm": 38.62004089355469,
      "learning_rate": 6.532092252275226e-05,
      "loss": 0.1773,
      "step": 245400
    },
    {
      "epoch": 0.7825823624105449,
      "grad_norm": 0.00043978769099339843,
      "learning_rate": 6.522529127683651e-05,
      "loss": 0.1336,
      "step": 245500
    },
    {
      "epoch": 0.7829011332302641,
      "grad_norm": 0.0005625616759061813,
      "learning_rate": 6.512966003092077e-05,
      "loss": 0.1867,
      "step": 245600
    },
    {
      "epoch": 0.7832199040499833,
      "grad_norm": 0.0012629079865291715,
      "learning_rate": 6.503402878500502e-05,
      "loss": 0.1965,
      "step": 245700
    },
    {
      "epoch": 0.7835386748697024,
      "grad_norm": 59.594337463378906,
      "learning_rate": 6.493839753908926e-05,
      "loss": 0.3785,
      "step": 245800
    },
    {
      "epoch": 0.7838574456894216,
      "grad_norm": 0.00042798841604962945,
      "learning_rate": 6.484276629317351e-05,
      "loss": 0.0927,
      "step": 245900
    },
    {
      "epoch": 0.7841762165091407,
      "grad_norm": 2.616148049128242e-05,
      "learning_rate": 6.474713504725777e-05,
      "loss": 0.2718,
      "step": 246000
    },
    {
      "epoch": 0.7844949873288599,
      "grad_norm": 0.0003639321366790682,
      "learning_rate": 6.465150380134202e-05,
      "loss": 0.0961,
      "step": 246100
    },
    {
      "epoch": 0.7848137581485791,
      "grad_norm": 0.0004898287588730454,
      "learning_rate": 6.455587255542628e-05,
      "loss": 0.1373,
      "step": 246200
    },
    {
      "epoch": 0.7851325289682982,
      "grad_norm": 0.46772393584251404,
      "learning_rate": 6.446024130951052e-05,
      "loss": 0.1616,
      "step": 246300
    },
    {
      "epoch": 0.7854512997880174,
      "grad_norm": 0.0005872351466678083,
      "learning_rate": 6.436461006359477e-05,
      "loss": 0.2233,
      "step": 246400
    },
    {
      "epoch": 0.7857700706077365,
      "grad_norm": 0.0006112541886977851,
      "learning_rate": 6.426897881767902e-05,
      "loss": 0.2296,
      "step": 246500
    },
    {
      "epoch": 0.7860888414274557,
      "grad_norm": 1.178733229637146,
      "learning_rate": 6.417334757176328e-05,
      "loss": 0.1781,
      "step": 246600
    },
    {
      "epoch": 0.7864076122471749,
      "grad_norm": 0.0008101853891275823,
      "learning_rate": 6.407771632584752e-05,
      "loss": 0.2424,
      "step": 246700
    },
    {
      "epoch": 0.786726383066894,
      "grad_norm": 0.002193064196035266,
      "learning_rate": 6.398208507993179e-05,
      "loss": 0.1945,
      "step": 246800
    },
    {
      "epoch": 0.7870451538866132,
      "grad_norm": 0.0005909137544222176,
      "learning_rate": 6.388645383401603e-05,
      "loss": 0.1686,
      "step": 246900
    },
    {
      "epoch": 0.7873639247063324,
      "grad_norm": 8.558459376217797e-05,
      "learning_rate": 6.379082258810028e-05,
      "loss": 0.1019,
      "step": 247000
    },
    {
      "epoch": 0.7876826955260515,
      "grad_norm": 86.01160430908203,
      "learning_rate": 6.369519134218453e-05,
      "loss": 0.1011,
      "step": 247100
    },
    {
      "epoch": 0.7880014663457707,
      "grad_norm": 0.11826497316360474,
      "learning_rate": 6.359956009626879e-05,
      "loss": 0.2073,
      "step": 247200
    },
    {
      "epoch": 0.7883202371654898,
      "grad_norm": 5.2307226724224165e-05,
      "learning_rate": 6.350392885035303e-05,
      "loss": 0.0615,
      "step": 247300
    },
    {
      "epoch": 0.788639007985209,
      "grad_norm": 0.0013317455304786563,
      "learning_rate": 6.340829760443728e-05,
      "loss": 0.1695,
      "step": 247400
    },
    {
      "epoch": 0.7889577788049282,
      "grad_norm": 0.000605278997682035,
      "learning_rate": 6.331266635852154e-05,
      "loss": 0.1651,
      "step": 247500
    },
    {
      "epoch": 0.7892765496246473,
      "grad_norm": 5.824563503265381,
      "learning_rate": 6.321703511260579e-05,
      "loss": 0.1735,
      "step": 247600
    },
    {
      "epoch": 0.7895953204443665,
      "grad_norm": 0.000131530367070809,
      "learning_rate": 6.312140386669003e-05,
      "loss": 0.1151,
      "step": 247700
    },
    {
      "epoch": 0.7899140912640857,
      "grad_norm": 61.08411407470703,
      "learning_rate": 6.302577262077428e-05,
      "loss": 0.2904,
      "step": 247800
    },
    {
      "epoch": 0.7902328620838048,
      "grad_norm": 0.004006341565400362,
      "learning_rate": 6.293014137485854e-05,
      "loss": 0.1581,
      "step": 247900
    },
    {
      "epoch": 0.790551632903524,
      "grad_norm": 0.021798226982355118,
      "learning_rate": 6.283451012894279e-05,
      "loss": 0.2412,
      "step": 248000
    },
    {
      "epoch": 0.7908704037232431,
      "grad_norm": 0.0005086854216642678,
      "learning_rate": 6.273887888302703e-05,
      "loss": 0.2957,
      "step": 248100
    },
    {
      "epoch": 0.7911891745429623,
      "grad_norm": 0.010248350910842419,
      "learning_rate": 6.26432476371113e-05,
      "loss": 0.1523,
      "step": 248200
    },
    {
      "epoch": 0.7915079453626815,
      "grad_norm": 1.6368991055060178e-05,
      "learning_rate": 6.254761639119555e-05,
      "loss": 0.1095,
      "step": 248300
    },
    {
      "epoch": 0.7918267161824006,
      "grad_norm": 0.0010738696437329054,
      "learning_rate": 6.24519851452798e-05,
      "loss": 0.105,
      "step": 248400
    },
    {
      "epoch": 0.7921454870021198,
      "grad_norm": 0.02576102502644062,
      "learning_rate": 6.235635389936405e-05,
      "loss": 0.1894,
      "step": 248500
    },
    {
      "epoch": 0.7924642578218389,
      "grad_norm": 0.002364040818065405,
      "learning_rate": 6.22607226534483e-05,
      "loss": 0.1623,
      "step": 248600
    },
    {
      "epoch": 0.7927830286415581,
      "grad_norm": 6.243451207410544e-05,
      "learning_rate": 6.216509140753256e-05,
      "loss": 0.1694,
      "step": 248700
    },
    {
      "epoch": 0.7931017994612773,
      "grad_norm": 0.0006085691857151687,
      "learning_rate": 6.20694601616168e-05,
      "loss": 0.174,
      "step": 248800
    },
    {
      "epoch": 0.7934205702809964,
      "grad_norm": 0.011573085561394691,
      "learning_rate": 6.197382891570105e-05,
      "loss": 0.1492,
      "step": 248900
    },
    {
      "epoch": 0.7937393411007156,
      "grad_norm": 6.752840272383764e-05,
      "learning_rate": 6.18781976697853e-05,
      "loss": 0.4611,
      "step": 249000
    },
    {
      "epoch": 0.7940581119204349,
      "grad_norm": 0.009572029113769531,
      "learning_rate": 6.178256642386956e-05,
      "loss": 0.216,
      "step": 249100
    },
    {
      "epoch": 0.794376882740154,
      "grad_norm": 0.00019381355377845466,
      "learning_rate": 6.16869351779538e-05,
      "loss": 0.1343,
      "step": 249200
    },
    {
      "epoch": 0.7946956535598731,
      "grad_norm": 13.586296081542969,
      "learning_rate": 6.159130393203805e-05,
      "loss": 0.4426,
      "step": 249300
    },
    {
      "epoch": 0.7950144243795922,
      "grad_norm": 0.00255843554623425,
      "learning_rate": 6.14956726861223e-05,
      "loss": 0.2426,
      "step": 249400
    },
    {
      "epoch": 0.7953331951993114,
      "grad_norm": 0.0003023889730684459,
      "learning_rate": 6.140004144020656e-05,
      "loss": 0.1617,
      "step": 249500
    },
    {
      "epoch": 0.7956519660190307,
      "grad_norm": 70.41803741455078,
      "learning_rate": 6.13044101942908e-05,
      "loss": 0.1009,
      "step": 249600
    },
    {
      "epoch": 0.7959707368387497,
      "grad_norm": 0.0006588842952623963,
      "learning_rate": 6.120877894837507e-05,
      "loss": 0.1384,
      "step": 249700
    },
    {
      "epoch": 0.796289507658469,
      "grad_norm": 0.005225364118814468,
      "learning_rate": 6.111314770245931e-05,
      "loss": 0.1825,
      "step": 249800
    },
    {
      "epoch": 0.7966082784781882,
      "grad_norm": 0.0066908132284879684,
      "learning_rate": 6.101751645654356e-05,
      "loss": 0.268,
      "step": 249900
    },
    {
      "epoch": 0.7969270492979073,
      "grad_norm": 0.00012731268361676484,
      "learning_rate": 6.092188521062781e-05,
      "loss": 0.3552,
      "step": 250000
    },
    {
      "epoch": 0.7972458201176265,
      "grad_norm": 0.004764941520988941,
      "learning_rate": 6.0826253964712063e-05,
      "loss": 0.1808,
      "step": 250100
    },
    {
      "epoch": 0.7975645909373456,
      "grad_norm": 0.0004313178069423884,
      "learning_rate": 6.073062271879631e-05,
      "loss": 0.1749,
      "step": 250200
    },
    {
      "epoch": 0.7978833617570648,
      "grad_norm": 4.746607373817824e-05,
      "learning_rate": 6.0634991472880565e-05,
      "loss": 0.1782,
      "step": 250300
    },
    {
      "epoch": 0.798202132576784,
      "grad_norm": 4.567458381643519e-05,
      "learning_rate": 6.053936022696481e-05,
      "loss": 0.2266,
      "step": 250400
    },
    {
      "epoch": 0.7985209033965031,
      "grad_norm": 53.413902282714844,
      "learning_rate": 6.044372898104907e-05,
      "loss": 0.2114,
      "step": 250500
    },
    {
      "epoch": 0.7988396742162223,
      "grad_norm": 0.0006125045474618673,
      "learning_rate": 6.0348097735133326e-05,
      "loss": 0.2259,
      "step": 250600
    },
    {
      "epoch": 0.7991584450359414,
      "grad_norm": 0.0005143294110894203,
      "learning_rate": 6.025246648921757e-05,
      "loss": 0.171,
      "step": 250700
    },
    {
      "epoch": 0.7994772158556606,
      "grad_norm": 0.0005694745341315866,
      "learning_rate": 6.015683524330183e-05,
      "loss": 0.1812,
      "step": 250800
    },
    {
      "epoch": 0.7997959866753798,
      "grad_norm": 6.76619092701003e-05,
      "learning_rate": 6.0061203997386074e-05,
      "loss": 0.2588,
      "step": 250900
    },
    {
      "epoch": 0.8001147574950989,
      "grad_norm": 0.9815394878387451,
      "learning_rate": 5.996557275147033e-05,
      "loss": 0.0819,
      "step": 251000
    },
    {
      "epoch": 0.8004335283148181,
      "grad_norm": 0.0005708672688342631,
      "learning_rate": 5.9869941505554575e-05,
      "loss": 0.3926,
      "step": 251100
    },
    {
      "epoch": 0.8007522991345373,
      "grad_norm": 5.672247789334506e-05,
      "learning_rate": 5.977431025963883e-05,
      "loss": 0.0843,
      "step": 251200
    },
    {
      "epoch": 0.8010710699542564,
      "grad_norm": 0.0036423273850232363,
      "learning_rate": 5.9678679013723076e-05,
      "loss": 0.2285,
      "step": 251300
    },
    {
      "epoch": 0.8013898407739756,
      "grad_norm": 0.004101132974028587,
      "learning_rate": 5.958304776780733e-05,
      "loss": 0.2831,
      "step": 251400
    },
    {
      "epoch": 0.8017086115936947,
      "grad_norm": 0.00011736853775801137,
      "learning_rate": 5.948741652189158e-05,
      "loss": 0.1021,
      "step": 251500
    },
    {
      "epoch": 0.8020273824134139,
      "grad_norm": 0.14297576248645782,
      "learning_rate": 5.939178527597583e-05,
      "loss": 0.1413,
      "step": 251600
    },
    {
      "epoch": 0.8023461532331331,
      "grad_norm": 0.00037730566691607237,
      "learning_rate": 5.929615403006008e-05,
      "loss": 0.1167,
      "step": 251700
    },
    {
      "epoch": 0.8026649240528522,
      "grad_norm": 0.005117608234286308,
      "learning_rate": 5.920052278414433e-05,
      "loss": 0.1294,
      "step": 251800
    },
    {
      "epoch": 0.8029836948725714,
      "grad_norm": 0.025935348123311996,
      "learning_rate": 5.910489153822858e-05,
      "loss": 0.1512,
      "step": 251900
    },
    {
      "epoch": 0.8033024656922906,
      "grad_norm": 0.00018591794651001692,
      "learning_rate": 5.900926029231284e-05,
      "loss": 0.2349,
      "step": 252000
    },
    {
      "epoch": 0.8036212365120097,
      "grad_norm": 3.365476368344389e-05,
      "learning_rate": 5.891362904639709e-05,
      "loss": 0.1309,
      "step": 252100
    },
    {
      "epoch": 0.8039400073317289,
      "grad_norm": 1.0610978603363037,
      "learning_rate": 5.881799780048134e-05,
      "loss": 0.0684,
      "step": 252200
    },
    {
      "epoch": 0.804258778151448,
      "grad_norm": 33.38682556152344,
      "learning_rate": 5.872236655456559e-05,
      "loss": 0.1904,
      "step": 252300
    },
    {
      "epoch": 0.8045775489711672,
      "grad_norm": 0.00012781983241438866,
      "learning_rate": 5.862673530864984e-05,
      "loss": 0.1671,
      "step": 252400
    },
    {
      "epoch": 0.8048963197908864,
      "grad_norm": 0.0018493295647203922,
      "learning_rate": 5.853110406273409e-05,
      "loss": 0.1214,
      "step": 252500
    },
    {
      "epoch": 0.8052150906106055,
      "grad_norm": 0.005740709602832794,
      "learning_rate": 5.8435472816818343e-05,
      "loss": 0.1336,
      "step": 252600
    },
    {
      "epoch": 0.8055338614303247,
      "grad_norm": 0.06953463703393936,
      "learning_rate": 5.83398415709026e-05,
      "loss": 0.1549,
      "step": 252700
    },
    {
      "epoch": 0.8058526322500438,
      "grad_norm": 0.0002520736015867442,
      "learning_rate": 5.8244210324986844e-05,
      "loss": 0.2227,
      "step": 252800
    },
    {
      "epoch": 0.806171403069763,
      "grad_norm": 0.0003487691574264318,
      "learning_rate": 5.81485790790711e-05,
      "loss": 0.1709,
      "step": 252900
    },
    {
      "epoch": 0.8064901738894822,
      "grad_norm": 0.00406651571393013,
      "learning_rate": 5.8052947833155345e-05,
      "loss": 0.2746,
      "step": 253000
    },
    {
      "epoch": 0.8068089447092013,
      "grad_norm": 0.00044826394878327847,
      "learning_rate": 5.79573165872396e-05,
      "loss": 0.2308,
      "step": 253100
    },
    {
      "epoch": 0.8071277155289205,
      "grad_norm": 0.004769997205585241,
      "learning_rate": 5.7861685341323847e-05,
      "loss": 0.1628,
      "step": 253200
    },
    {
      "epoch": 0.8074464863486397,
      "grad_norm": 0.0001319691218668595,
      "learning_rate": 5.776605409540811e-05,
      "loss": 0.1293,
      "step": 253300
    },
    {
      "epoch": 0.8077652571683588,
      "grad_norm": 0.0002270472323289141,
      "learning_rate": 5.767042284949235e-05,
      "loss": 0.1864,
      "step": 253400
    },
    {
      "epoch": 0.808084027988078,
      "grad_norm": 5.00299283885397e-05,
      "learning_rate": 5.757479160357661e-05,
      "loss": 0.099,
      "step": 253500
    },
    {
      "epoch": 0.8084027988077971,
      "grad_norm": 0.05669134482741356,
      "learning_rate": 5.7479160357660855e-05,
      "loss": 0.0948,
      "step": 253600
    },
    {
      "epoch": 0.8087215696275163,
      "grad_norm": 38.488460540771484,
      "learning_rate": 5.738352911174511e-05,
      "loss": 0.215,
      "step": 253700
    },
    {
      "epoch": 0.8090403404472355,
      "grad_norm": 68.95340728759766,
      "learning_rate": 5.7287897865829356e-05,
      "loss": 0.2679,
      "step": 253800
    },
    {
      "epoch": 0.8093591112669546,
      "grad_norm": 96.15199279785156,
      "learning_rate": 5.719226661991361e-05,
      "loss": 0.1208,
      "step": 253900
    },
    {
      "epoch": 0.8096778820866738,
      "grad_norm": 0.0007310073706321418,
      "learning_rate": 5.709663537399786e-05,
      "loss": 0.2353,
      "step": 254000
    },
    {
      "epoch": 0.809996652906393,
      "grad_norm": 0.0001601275580469519,
      "learning_rate": 5.700100412808211e-05,
      "loss": 0.1708,
      "step": 254100
    },
    {
      "epoch": 0.8103154237261121,
      "grad_norm": 0.010481981560587883,
      "learning_rate": 5.690537288216636e-05,
      "loss": 0.1236,
      "step": 254200
    },
    {
      "epoch": 0.8106341945458313,
      "grad_norm": 0.0011961732525378466,
      "learning_rate": 5.680974163625061e-05,
      "loss": 0.0701,
      "step": 254300
    },
    {
      "epoch": 0.8109529653655504,
      "grad_norm": 0.013152533210814,
      "learning_rate": 5.671411039033486e-05,
      "loss": 0.1266,
      "step": 254400
    },
    {
      "epoch": 0.8112717361852696,
      "grad_norm": 0.0007259880076162517,
      "learning_rate": 5.6618479144419113e-05,
      "loss": 0.1371,
      "step": 254500
    },
    {
      "epoch": 0.8115905070049888,
      "grad_norm": 0.0001205813096021302,
      "learning_rate": 5.652284789850336e-05,
      "loss": 0.2252,
      "step": 254600
    },
    {
      "epoch": 0.8119092778247079,
      "grad_norm": 0.06592592597007751,
      "learning_rate": 5.6427216652587614e-05,
      "loss": 0.2322,
      "step": 254700
    },
    {
      "epoch": 0.8122280486444271,
      "grad_norm": 4.924674067297019e-05,
      "learning_rate": 5.6331585406671875e-05,
      "loss": 0.2428,
      "step": 254800
    },
    {
      "epoch": 0.8125468194641462,
      "grad_norm": 0.00018694358004722744,
      "learning_rate": 5.623595416075612e-05,
      "loss": 0.2027,
      "step": 254900
    },
    {
      "epoch": 0.8128655902838654,
      "grad_norm": 0.0001749047514749691,
      "learning_rate": 5.6140322914840376e-05,
      "loss": 0.2503,
      "step": 255000
    },
    {
      "epoch": 0.8131843611035846,
      "grad_norm": 0.0003352150379214436,
      "learning_rate": 5.604469166892462e-05,
      "loss": 0.2572,
      "step": 255100
    },
    {
      "epoch": 0.8135031319233037,
      "grad_norm": 0.02758678048849106,
      "learning_rate": 5.594906042300888e-05,
      "loss": 0.1822,
      "step": 255200
    },
    {
      "epoch": 0.8138219027430229,
      "grad_norm": 63.95756530761719,
      "learning_rate": 5.5853429177093124e-05,
      "loss": 0.1445,
      "step": 255300
    },
    {
      "epoch": 0.8141406735627421,
      "grad_norm": 0.00034572070580907166,
      "learning_rate": 5.575779793117738e-05,
      "loss": 0.1391,
      "step": 255400
    },
    {
      "epoch": 0.8144594443824612,
      "grad_norm": 7.718116830801591e-05,
      "learning_rate": 5.5662166685261625e-05,
      "loss": 0.2775,
      "step": 255500
    },
    {
      "epoch": 0.8147782152021804,
      "grad_norm": 0.10996349155902863,
      "learning_rate": 5.556653543934588e-05,
      "loss": 0.1564,
      "step": 255600
    },
    {
      "epoch": 0.8150969860218995,
      "grad_norm": 0.0022080973722040653,
      "learning_rate": 5.5470904193430126e-05,
      "loss": 0.188,
      "step": 255700
    },
    {
      "epoch": 0.8154157568416187,
      "grad_norm": 0.0017212131060659885,
      "learning_rate": 5.537527294751438e-05,
      "loss": 0.1799,
      "step": 255800
    },
    {
      "epoch": 0.8157345276613379,
      "grad_norm": 7.947686390252784e-05,
      "learning_rate": 5.527964170159863e-05,
      "loss": 0.3991,
      "step": 255900
    },
    {
      "epoch": 0.816053298481057,
      "grad_norm": 0.0008251168765127659,
      "learning_rate": 5.518401045568288e-05,
      "loss": 0.2278,
      "step": 256000
    },
    {
      "epoch": 0.8163720693007762,
      "grad_norm": 0.0004231717612128705,
      "learning_rate": 5.508837920976713e-05,
      "loss": 0.264,
      "step": 256100
    },
    {
      "epoch": 0.8166908401204954,
      "grad_norm": 0.04162612184882164,
      "learning_rate": 5.499274796385138e-05,
      "loss": 0.255,
      "step": 256200
    },
    {
      "epoch": 0.8170096109402145,
      "grad_norm": 0.019753430038690567,
      "learning_rate": 5.489711671793563e-05,
      "loss": 0.1763,
      "step": 256300
    },
    {
      "epoch": 0.8173283817599337,
      "grad_norm": 0.000946496264077723,
      "learning_rate": 5.480148547201989e-05,
      "loss": 0.3226,
      "step": 256400
    },
    {
      "epoch": 0.8176471525796528,
      "grad_norm": 0.00012165016960352659,
      "learning_rate": 5.470585422610414e-05,
      "loss": 0.1111,
      "step": 256500
    },
    {
      "epoch": 0.817965923399372,
      "grad_norm": 0.007858881726861,
      "learning_rate": 5.461022298018839e-05,
      "loss": 0.2673,
      "step": 256600
    },
    {
      "epoch": 0.8182846942190912,
      "grad_norm": 9.299955854658037e-05,
      "learning_rate": 5.451459173427264e-05,
      "loss": 0.1458,
      "step": 256700
    },
    {
      "epoch": 0.8186034650388103,
      "grad_norm": 0.0003842878504656255,
      "learning_rate": 5.441896048835689e-05,
      "loss": 0.1999,
      "step": 256800
    },
    {
      "epoch": 0.8189222358585295,
      "grad_norm": 43.3282585144043,
      "learning_rate": 5.432332924244114e-05,
      "loss": 0.1692,
      "step": 256900
    },
    {
      "epoch": 0.8192410066782486,
      "grad_norm": 9.41315374802798e-05,
      "learning_rate": 5.422769799652539e-05,
      "loss": 0.0866,
      "step": 257000
    },
    {
      "epoch": 0.8195597774979678,
      "grad_norm": 8.481693657813594e-05,
      "learning_rate": 5.413206675060965e-05,
      "loss": 0.1452,
      "step": 257100
    },
    {
      "epoch": 0.819878548317687,
      "grad_norm": 7.349116640398279e-05,
      "learning_rate": 5.4036435504693894e-05,
      "loss": 0.2359,
      "step": 257200
    },
    {
      "epoch": 0.8201973191374061,
      "grad_norm": 0.0002630093658808619,
      "learning_rate": 5.394080425877815e-05,
      "loss": 0.128,
      "step": 257300
    },
    {
      "epoch": 0.8205160899571253,
      "grad_norm": 1.9185828932677396e-05,
      "learning_rate": 5.3845173012862395e-05,
      "loss": 0.2695,
      "step": 257400
    },
    {
      "epoch": 0.8208348607768445,
      "grad_norm": 0.0004504144308157265,
      "learning_rate": 5.374954176694665e-05,
      "loss": 0.2787,
      "step": 257500
    },
    {
      "epoch": 0.8211536315965636,
      "grad_norm": 0.00016557359776925296,
      "learning_rate": 5.3653910521030896e-05,
      "loss": 0.1666,
      "step": 257600
    },
    {
      "epoch": 0.8214724024162828,
      "grad_norm": 0.00023429334396496415,
      "learning_rate": 5.355827927511516e-05,
      "loss": 0.1905,
      "step": 257700
    },
    {
      "epoch": 0.8217911732360019,
      "grad_norm": 0.09482761472463608,
      "learning_rate": 5.34626480291994e-05,
      "loss": 0.3077,
      "step": 257800
    },
    {
      "epoch": 0.8221099440557211,
      "grad_norm": 0.002025120658800006,
      "learning_rate": 5.336701678328366e-05,
      "loss": 0.2275,
      "step": 257900
    },
    {
      "epoch": 0.8224287148754403,
      "grad_norm": 0.04152696579694748,
      "learning_rate": 5.3271385537367905e-05,
      "loss": 0.1348,
      "step": 258000
    },
    {
      "epoch": 0.8227474856951594,
      "grad_norm": 9.804614091990516e-05,
      "learning_rate": 5.317575429145216e-05,
      "loss": 0.0414,
      "step": 258100
    },
    {
      "epoch": 0.8230662565148786,
      "grad_norm": 0.00026390558923594654,
      "learning_rate": 5.3080123045536406e-05,
      "loss": 0.1224,
      "step": 258200
    },
    {
      "epoch": 0.8233850273345978,
      "grad_norm": 0.0011376447509974241,
      "learning_rate": 5.298449179962066e-05,
      "loss": 0.0508,
      "step": 258300
    },
    {
      "epoch": 0.8237037981543169,
      "grad_norm": 0.1610705554485321,
      "learning_rate": 5.288886055370491e-05,
      "loss": 0.2346,
      "step": 258400
    },
    {
      "epoch": 0.8240225689740361,
      "grad_norm": 0.0016902291681617498,
      "learning_rate": 5.279322930778916e-05,
      "loss": 0.2718,
      "step": 258500
    },
    {
      "epoch": 0.8243413397937552,
      "grad_norm": 0.0002526167081668973,
      "learning_rate": 5.269759806187341e-05,
      "loss": 0.1491,
      "step": 258600
    },
    {
      "epoch": 0.8246601106134744,
      "grad_norm": 0.00013648989261128008,
      "learning_rate": 5.260196681595766e-05,
      "loss": 0.1526,
      "step": 258700
    },
    {
      "epoch": 0.8249788814331936,
      "grad_norm": 0.00022704304137732834,
      "learning_rate": 5.250633557004191e-05,
      "loss": 0.15,
      "step": 258800
    },
    {
      "epoch": 0.8252976522529127,
      "grad_norm": 12.310256958007812,
      "learning_rate": 5.241070432412616e-05,
      "loss": 0.2745,
      "step": 258900
    },
    {
      "epoch": 0.8256164230726319,
      "grad_norm": 3.630173683166504,
      "learning_rate": 5.231507307821041e-05,
      "loss": 0.2146,
      "step": 259000
    },
    {
      "epoch": 0.8259351938923511,
      "grad_norm": 0.0004067535628564656,
      "learning_rate": 5.2219441832294664e-05,
      "loss": 0.1175,
      "step": 259100
    },
    {
      "epoch": 0.8262539647120702,
      "grad_norm": 0.02569909393787384,
      "learning_rate": 5.2123810586378925e-05,
      "loss": 0.1558,
      "step": 259200
    },
    {
      "epoch": 0.8265727355317894,
      "grad_norm": 0.00022684248688165098,
      "learning_rate": 5.202817934046317e-05,
      "loss": 0.1716,
      "step": 259300
    },
    {
      "epoch": 0.8268915063515085,
      "grad_norm": 0.00031585225951857865,
      "learning_rate": 5.1932548094547426e-05,
      "loss": 0.1743,
      "step": 259400
    },
    {
      "epoch": 0.8272102771712277,
      "grad_norm": 13.481096267700195,
      "learning_rate": 5.183691684863167e-05,
      "loss": 0.2038,
      "step": 259500
    },
    {
      "epoch": 0.827529047990947,
      "grad_norm": 0.0002711563720367849,
      "learning_rate": 5.174128560271593e-05,
      "loss": 0.282,
      "step": 259600
    },
    {
      "epoch": 0.827847818810666,
      "grad_norm": 0.0012244755635038018,
      "learning_rate": 5.1645654356800174e-05,
      "loss": 0.2932,
      "step": 259700
    },
    {
      "epoch": 0.8281665896303853,
      "grad_norm": 0.00016159618098754436,
      "learning_rate": 5.155002311088443e-05,
      "loss": 0.0814,
      "step": 259800
    },
    {
      "epoch": 0.8284853604501043,
      "grad_norm": 0.10164371877908707,
      "learning_rate": 5.1454391864968675e-05,
      "loss": 0.1581,
      "step": 259900
    },
    {
      "epoch": 0.8288041312698236,
      "grad_norm": 0.0003262351674493402,
      "learning_rate": 5.135876061905293e-05,
      "loss": 0.2059,
      "step": 260000
    },
    {
      "epoch": 0.8291229020895428,
      "grad_norm": 0.11308220028877258,
      "learning_rate": 5.1263129373137176e-05,
      "loss": 0.1993,
      "step": 260100
    },
    {
      "epoch": 0.8294416729092619,
      "grad_norm": 0.0004666576278395951,
      "learning_rate": 5.116749812722143e-05,
      "loss": 0.3333,
      "step": 260200
    },
    {
      "epoch": 0.8297604437289811,
      "grad_norm": 0.0012124617351219058,
      "learning_rate": 5.107186688130568e-05,
      "loss": 0.1543,
      "step": 260300
    },
    {
      "epoch": 0.8300792145487003,
      "grad_norm": 0.0013718490954488516,
      "learning_rate": 5.097623563538993e-05,
      "loss": 0.2487,
      "step": 260400
    },
    {
      "epoch": 0.8303979853684194,
      "grad_norm": 0.0001367463410133496,
      "learning_rate": 5.088060438947418e-05,
      "loss": 0.2493,
      "step": 260500
    },
    {
      "epoch": 0.8307167561881386,
      "grad_norm": 1.199387788772583,
      "learning_rate": 5.078497314355843e-05,
      "loss": 0.2213,
      "step": 260600
    },
    {
      "epoch": 0.8310355270078577,
      "grad_norm": 45.335975646972656,
      "learning_rate": 5.068934189764268e-05,
      "loss": 0.2143,
      "step": 260700
    },
    {
      "epoch": 0.8313542978275769,
      "grad_norm": 15.197786331176758,
      "learning_rate": 5.059371065172694e-05,
      "loss": 0.1231,
      "step": 260800
    },
    {
      "epoch": 0.8316730686472961,
      "grad_norm": 0.0001886961836135015,
      "learning_rate": 5.049807940581119e-05,
      "loss": 0.1663,
      "step": 260900
    },
    {
      "epoch": 0.8319918394670152,
      "grad_norm": 6.987384796142578,
      "learning_rate": 5.040244815989544e-05,
      "loss": 0.1977,
      "step": 261000
    },
    {
      "epoch": 0.8323106102867344,
      "grad_norm": 0.009152891114354134,
      "learning_rate": 5.030681691397969e-05,
      "loss": 0.1805,
      "step": 261100
    },
    {
      "epoch": 0.8326293811064536,
      "grad_norm": 0.002588546136394143,
      "learning_rate": 5.021118566806394e-05,
      "loss": 0.1372,
      "step": 261200
    },
    {
      "epoch": 0.8329481519261727,
      "grad_norm": 0.00011762580106733367,
      "learning_rate": 5.011555442214819e-05,
      "loss": 0.2293,
      "step": 261300
    },
    {
      "epoch": 0.8332669227458919,
      "grad_norm": 0.0018092343816533685,
      "learning_rate": 5.001992317623244e-05,
      "loss": 0.0895,
      "step": 261400
    },
    {
      "epoch": 0.833585693565611,
      "grad_norm": 2.653096271387767e-05,
      "learning_rate": 4.99242919303167e-05,
      "loss": 0.0988,
      "step": 261500
    },
    {
      "epoch": 0.8339044643853302,
      "grad_norm": 7.776277197990566e-05,
      "learning_rate": 4.9828660684400944e-05,
      "loss": 0.1537,
      "step": 261600
    },
    {
      "epoch": 0.8342232352050494,
      "grad_norm": 15.59591293334961,
      "learning_rate": 4.97330294384852e-05,
      "loss": 0.0263,
      "step": 261700
    },
    {
      "epoch": 0.8345420060247685,
      "grad_norm": 0.03354722261428833,
      "learning_rate": 4.9637398192569445e-05,
      "loss": 0.2252,
      "step": 261800
    },
    {
      "epoch": 0.8348607768444877,
      "grad_norm": 0.0002912751806434244,
      "learning_rate": 4.95417669466537e-05,
      "loss": 0.2446,
      "step": 261900
    },
    {
      "epoch": 0.8351795476642068,
      "grad_norm": 0.0023749498650431633,
      "learning_rate": 4.9446135700737946e-05,
      "loss": 0.1118,
      "step": 262000
    },
    {
      "epoch": 0.835498318483926,
      "grad_norm": 96.28668975830078,
      "learning_rate": 4.935050445482221e-05,
      "loss": 0.3157,
      "step": 262100
    },
    {
      "epoch": 0.8358170893036452,
      "grad_norm": 6.374502658843994,
      "learning_rate": 4.925487320890645e-05,
      "loss": 0.0651,
      "step": 262200
    },
    {
      "epoch": 0.8361358601233643,
      "grad_norm": 0.0030492525547742844,
      "learning_rate": 4.915924196299071e-05,
      "loss": 0.3129,
      "step": 262300
    },
    {
      "epoch": 0.8364546309430835,
      "grad_norm": 0.00030569906812161207,
      "learning_rate": 4.9063610717074955e-05,
      "loss": 0.1574,
      "step": 262400
    },
    {
      "epoch": 0.8367734017628027,
      "grad_norm": 0.0010247876634821296,
      "learning_rate": 4.896797947115921e-05,
      "loss": 0.24,
      "step": 262500
    },
    {
      "epoch": 0.8370921725825218,
      "grad_norm": 0.0006691532908007503,
      "learning_rate": 4.8872348225243456e-05,
      "loss": 0.1742,
      "step": 262600
    },
    {
      "epoch": 0.837410943402241,
      "grad_norm": 4.825999712920748e-05,
      "learning_rate": 4.877671697932771e-05,
      "loss": 0.4014,
      "step": 262700
    },
    {
      "epoch": 0.8377297142219601,
      "grad_norm": 0.0023361400235444307,
      "learning_rate": 4.868108573341196e-05,
      "loss": 0.1806,
      "step": 262800
    },
    {
      "epoch": 0.8380484850416793,
      "grad_norm": 0.00039647784433327615,
      "learning_rate": 4.858545448749621e-05,
      "loss": 0.1903,
      "step": 262900
    },
    {
      "epoch": 0.8383672558613985,
      "grad_norm": 0.002691535744816065,
      "learning_rate": 4.848982324158046e-05,
      "loss": 0.0651,
      "step": 263000
    },
    {
      "epoch": 0.8386860266811176,
      "grad_norm": 0.0004930950817652047,
      "learning_rate": 4.839419199566471e-05,
      "loss": 0.2171,
      "step": 263100
    },
    {
      "epoch": 0.8390047975008368,
      "grad_norm": 5.768852588516893e-06,
      "learning_rate": 4.829856074974896e-05,
      "loss": 0.318,
      "step": 263200
    },
    {
      "epoch": 0.839323568320556,
      "grad_norm": 0.01609090156853199,
      "learning_rate": 4.820292950383321e-05,
      "loss": 0.1566,
      "step": 263300
    },
    {
      "epoch": 0.8396423391402751,
      "grad_norm": 0.00028926233062520623,
      "learning_rate": 4.810729825791746e-05,
      "loss": 0.2559,
      "step": 263400
    },
    {
      "epoch": 0.8399611099599943,
      "grad_norm": 0.0016496112802997231,
      "learning_rate": 4.8011667012001714e-05,
      "loss": 0.1155,
      "step": 263500
    },
    {
      "epoch": 0.8402798807797134,
      "grad_norm": 7.970928192138672,
      "learning_rate": 4.7916035766085975e-05,
      "loss": 0.2112,
      "step": 263600
    },
    {
      "epoch": 0.8405986515994326,
      "grad_norm": 0.004443730227649212,
      "learning_rate": 4.782040452017022e-05,
      "loss": 0.2185,
      "step": 263700
    },
    {
      "epoch": 0.8409174224191518,
      "grad_norm": 0.0005891683977097273,
      "learning_rate": 4.7724773274254476e-05,
      "loss": 0.1515,
      "step": 263800
    },
    {
      "epoch": 0.8412361932388709,
      "grad_norm": 0.0010132385650649667,
      "learning_rate": 4.762914202833872e-05,
      "loss": 0.137,
      "step": 263900
    },
    {
      "epoch": 0.8415549640585901,
      "grad_norm": 4.180714130401611,
      "learning_rate": 4.753351078242298e-05,
      "loss": 0.1444,
      "step": 264000
    },
    {
      "epoch": 0.8418737348783092,
      "grad_norm": 0.0006097788573242724,
      "learning_rate": 4.7437879536507224e-05,
      "loss": 0.1826,
      "step": 264100
    },
    {
      "epoch": 0.8421925056980284,
      "grad_norm": 34.07847595214844,
      "learning_rate": 4.734224829059148e-05,
      "loss": 0.1041,
      "step": 264200
    },
    {
      "epoch": 0.8425112765177476,
      "grad_norm": 0.00021390945767052472,
      "learning_rate": 4.7246617044675725e-05,
      "loss": 0.2307,
      "step": 264300
    },
    {
      "epoch": 0.8428300473374667,
      "grad_norm": 0.00013567693531513214,
      "learning_rate": 4.715098579875998e-05,
      "loss": 0.2144,
      "step": 264400
    },
    {
      "epoch": 0.8431488181571859,
      "grad_norm": 69.53639221191406,
      "learning_rate": 4.7055354552844226e-05,
      "loss": 0.2344,
      "step": 264500
    },
    {
      "epoch": 0.8434675889769051,
      "grad_norm": 0.010802275501191616,
      "learning_rate": 4.695972330692848e-05,
      "loss": 0.1825,
      "step": 264600
    },
    {
      "epoch": 0.8437863597966242,
      "grad_norm": 1.086997151374817,
      "learning_rate": 4.686409206101273e-05,
      "loss": 0.1618,
      "step": 264700
    },
    {
      "epoch": 0.8441051306163434,
      "grad_norm": 70.31299591064453,
      "learning_rate": 4.676846081509698e-05,
      "loss": 0.2439,
      "step": 264800
    },
    {
      "epoch": 0.8444239014360625,
      "grad_norm": 0.0004073869495186955,
      "learning_rate": 4.667282956918123e-05,
      "loss": 0.1353,
      "step": 264900
    },
    {
      "epoch": 0.8447426722557817,
      "grad_norm": 0.006145937833935022,
      "learning_rate": 4.657719832326548e-05,
      "loss": 0.2051,
      "step": 265000
    },
    {
      "epoch": 0.8450614430755009,
      "grad_norm": 0.0009949328377842903,
      "learning_rate": 4.648156707734973e-05,
      "loss": 0.1828,
      "step": 265100
    },
    {
      "epoch": 0.84538021389522,
      "grad_norm": 0.000416477007092908,
      "learning_rate": 4.638593583143399e-05,
      "loss": 0.2193,
      "step": 265200
    },
    {
      "epoch": 0.8456989847149392,
      "grad_norm": 8.235186396632344e-05,
      "learning_rate": 4.629030458551824e-05,
      "loss": 0.1804,
      "step": 265300
    },
    {
      "epoch": 0.8460177555346584,
      "grad_norm": 0.00021160676260478795,
      "learning_rate": 4.619467333960249e-05,
      "loss": 0.1674,
      "step": 265400
    },
    {
      "epoch": 0.8463365263543775,
      "grad_norm": 0.0010450581321492791,
      "learning_rate": 4.609904209368674e-05,
      "loss": 0.1819,
      "step": 265500
    },
    {
      "epoch": 0.8466552971740967,
      "grad_norm": 4.67054960608948e-05,
      "learning_rate": 4.600341084777099e-05,
      "loss": 0.2001,
      "step": 265600
    },
    {
      "epoch": 0.8469740679938158,
      "grad_norm": 7.878787437221035e-05,
      "learning_rate": 4.5907779601855246e-05,
      "loss": 0.1701,
      "step": 265700
    },
    {
      "epoch": 0.847292838813535,
      "grad_norm": 1.132770466938382e-05,
      "learning_rate": 4.581214835593949e-05,
      "loss": 0.1808,
      "step": 265800
    },
    {
      "epoch": 0.8476116096332542,
      "grad_norm": 41.60955810546875,
      "learning_rate": 4.571651711002375e-05,
      "loss": 0.2453,
      "step": 265900
    },
    {
      "epoch": 0.8479303804529733,
      "grad_norm": 0.0001600224495632574,
      "learning_rate": 4.5620885864107994e-05,
      "loss": 0.252,
      "step": 266000
    },
    {
      "epoch": 0.8482491512726925,
      "grad_norm": 5.5038340178725775e-06,
      "learning_rate": 4.552525461819225e-05,
      "loss": 0.1504,
      "step": 266100
    },
    {
      "epoch": 0.8485679220924116,
      "grad_norm": 9.195938110351562,
      "learning_rate": 4.5429623372276495e-05,
      "loss": 0.2648,
      "step": 266200
    },
    {
      "epoch": 0.8488866929121308,
      "grad_norm": 0.0005779323983006179,
      "learning_rate": 4.533399212636075e-05,
      "loss": 0.3352,
      "step": 266300
    },
    {
      "epoch": 0.84920546373185,
      "grad_norm": 0.009103362448513508,
      "learning_rate": 4.5238360880444996e-05,
      "loss": 0.0987,
      "step": 266400
    },
    {
      "epoch": 0.8495242345515691,
      "grad_norm": 0.24319404363632202,
      "learning_rate": 4.514272963452926e-05,
      "loss": 0.1542,
      "step": 266500
    },
    {
      "epoch": 0.8498430053712883,
      "grad_norm": 0.0008920249529182911,
      "learning_rate": 4.50470983886135e-05,
      "loss": 0.1207,
      "step": 266600
    },
    {
      "epoch": 0.8501617761910075,
      "grad_norm": 0.019139621406793594,
      "learning_rate": 4.495146714269776e-05,
      "loss": 0.151,
      "step": 266700
    },
    {
      "epoch": 0.8504805470107266,
      "grad_norm": 0.00011083118442911655,
      "learning_rate": 4.4855835896782005e-05,
      "loss": 0.1756,
      "step": 266800
    },
    {
      "epoch": 0.8507993178304458,
      "grad_norm": 0.0005483903805725276,
      "learning_rate": 4.476020465086626e-05,
      "loss": 0.1353,
      "step": 266900
    },
    {
      "epoch": 0.8511180886501649,
      "grad_norm": 1.2596467733383179,
      "learning_rate": 4.4664573404950506e-05,
      "loss": 0.2733,
      "step": 267000
    },
    {
      "epoch": 0.8514368594698841,
      "grad_norm": 2.570104697952047e-05,
      "learning_rate": 4.456894215903476e-05,
      "loss": 0.1899,
      "step": 267100
    },
    {
      "epoch": 0.8517556302896033,
      "grad_norm": 22.29415512084961,
      "learning_rate": 4.447331091311901e-05,
      "loss": 0.2507,
      "step": 267200
    },
    {
      "epoch": 0.8520744011093224,
      "grad_norm": 64.01571655273438,
      "learning_rate": 4.437767966720326e-05,
      "loss": 0.1587,
      "step": 267300
    },
    {
      "epoch": 0.8523931719290416,
      "grad_norm": 7.609929084777832,
      "learning_rate": 4.428204842128751e-05,
      "loss": 0.2117,
      "step": 267400
    },
    {
      "epoch": 0.8527119427487608,
      "grad_norm": 0.00024387349549215287,
      "learning_rate": 4.418641717537176e-05,
      "loss": 0.2374,
      "step": 267500
    },
    {
      "epoch": 0.8530307135684799,
      "grad_norm": 0.00013485476665664464,
      "learning_rate": 4.409078592945601e-05,
      "loss": 0.1038,
      "step": 267600
    },
    {
      "epoch": 0.8533494843881991,
      "grad_norm": 0.10284382849931717,
      "learning_rate": 4.399515468354026e-05,
      "loss": 0.1931,
      "step": 267700
    },
    {
      "epoch": 0.8536682552079182,
      "grad_norm": 0.00036186794750392437,
      "learning_rate": 4.389952343762451e-05,
      "loss": 0.2708,
      "step": 267800
    },
    {
      "epoch": 0.8539870260276374,
      "grad_norm": 0.005361943505704403,
      "learning_rate": 4.3803892191708764e-05,
      "loss": 0.1033,
      "step": 267900
    },
    {
      "epoch": 0.8543057968473566,
      "grad_norm": 32.01218032836914,
      "learning_rate": 4.3708260945793025e-05,
      "loss": 0.221,
      "step": 268000
    },
    {
      "epoch": 0.8546245676670757,
      "grad_norm": 0.00041745827184058726,
      "learning_rate": 4.361262969987727e-05,
      "loss": 0.1439,
      "step": 268100
    },
    {
      "epoch": 0.8549433384867949,
      "grad_norm": 0.0012181730708107352,
      "learning_rate": 4.3516998453961526e-05,
      "loss": 0.1325,
      "step": 268200
    },
    {
      "epoch": 0.855262109306514,
      "grad_norm": 0.0021636139135807753,
      "learning_rate": 4.342136720804577e-05,
      "loss": 0.1802,
      "step": 268300
    },
    {
      "epoch": 0.8555808801262332,
      "grad_norm": 8.107715984806418e-05,
      "learning_rate": 4.332573596213003e-05,
      "loss": 0.243,
      "step": 268400
    },
    {
      "epoch": 0.8558996509459524,
      "grad_norm": 0.00011891273607034236,
      "learning_rate": 4.3230104716214274e-05,
      "loss": 0.1668,
      "step": 268500
    },
    {
      "epoch": 0.8562184217656715,
      "grad_norm": 0.00010753928654594347,
      "learning_rate": 4.313447347029853e-05,
      "loss": 0.1964,
      "step": 268600
    },
    {
      "epoch": 0.8565371925853907,
      "grad_norm": 7.071955678839004e-06,
      "learning_rate": 4.3038842224382775e-05,
      "loss": 0.126,
      "step": 268700
    },
    {
      "epoch": 0.8568559634051099,
      "grad_norm": 0.004196591675281525,
      "learning_rate": 4.294321097846703e-05,
      "loss": 0.0776,
      "step": 268800
    },
    {
      "epoch": 0.857174734224829,
      "grad_norm": 2.08359670068603e-05,
      "learning_rate": 4.2847579732551276e-05,
      "loss": 0.0956,
      "step": 268900
    },
    {
      "epoch": 0.8574935050445482,
      "grad_norm": 0.00019021501066163182,
      "learning_rate": 4.275194848663553e-05,
      "loss": 0.1018,
      "step": 269000
    },
    {
      "epoch": 0.8578122758642673,
      "grad_norm": 91.09041595458984,
      "learning_rate": 4.265631724071978e-05,
      "loss": 0.2028,
      "step": 269100
    },
    {
      "epoch": 0.8581310466839865,
      "grad_norm": 0.08868168294429779,
      "learning_rate": 4.256068599480403e-05,
      "loss": 0.1022,
      "step": 269200
    },
    {
      "epoch": 0.8584498175037057,
      "grad_norm": 0.00033176896977238357,
      "learning_rate": 4.246505474888828e-05,
      "loss": 0.2841,
      "step": 269300
    },
    {
      "epoch": 0.8587685883234248,
      "grad_norm": 8.977834701538086,
      "learning_rate": 4.236942350297253e-05,
      "loss": 0.2777,
      "step": 269400
    },
    {
      "epoch": 0.859087359143144,
      "grad_norm": 0.007846503518521786,
      "learning_rate": 4.227379225705678e-05,
      "loss": 0.0554,
      "step": 269500
    },
    {
      "epoch": 0.8594061299628633,
      "grad_norm": 2.7278658308205195e-05,
      "learning_rate": 4.217816101114104e-05,
      "loss": 0.1785,
      "step": 269600
    },
    {
      "epoch": 0.8597249007825823,
      "grad_norm": 0.1073838397860527,
      "learning_rate": 4.208252976522529e-05,
      "loss": 0.1085,
      "step": 269700
    },
    {
      "epoch": 0.8600436716023016,
      "grad_norm": 0.0011501158587634563,
      "learning_rate": 4.198689851930954e-05,
      "loss": 0.2055,
      "step": 269800
    },
    {
      "epoch": 0.8603624424220206,
      "grad_norm": 0.0018601131159812212,
      "learning_rate": 4.189126727339379e-05,
      "loss": 0.0724,
      "step": 269900
    },
    {
      "epoch": 0.8606812132417399,
      "grad_norm": 0.002420867094770074,
      "learning_rate": 4.179563602747804e-05,
      "loss": 0.2564,
      "step": 270000
    },
    {
      "epoch": 0.8609999840614591,
      "grad_norm": 0.0022011895198374987,
      "learning_rate": 4.1700004781562296e-05,
      "loss": 0.101,
      "step": 270100
    },
    {
      "epoch": 0.8613187548811782,
      "grad_norm": 0.5731236934661865,
      "learning_rate": 4.160437353564654e-05,
      "loss": 0.1477,
      "step": 270200
    },
    {
      "epoch": 0.8616375257008974,
      "grad_norm": 0.005416504107415676,
      "learning_rate": 4.15087422897308e-05,
      "loss": 0.2188,
      "step": 270300
    },
    {
      "epoch": 0.8619562965206164,
      "grad_norm": 0.0006590245175175369,
      "learning_rate": 4.1413111043815044e-05,
      "loss": 0.1933,
      "step": 270400
    },
    {
      "epoch": 0.8622750673403357,
      "grad_norm": 0.3794194161891937,
      "learning_rate": 4.13174797978993e-05,
      "loss": 0.3031,
      "step": 270500
    },
    {
      "epoch": 0.8625938381600549,
      "grad_norm": 17.056791305541992,
      "learning_rate": 4.1221848551983545e-05,
      "loss": 0.1884,
      "step": 270600
    },
    {
      "epoch": 0.862912608979774,
      "grad_norm": 0.0004700802091974765,
      "learning_rate": 4.11262173060678e-05,
      "loss": 0.3699,
      "step": 270700
    },
    {
      "epoch": 0.8632313797994932,
      "grad_norm": 0.0021517029963433743,
      "learning_rate": 4.1030586060152046e-05,
      "loss": 0.2286,
      "step": 270800
    },
    {
      "epoch": 0.8635501506192124,
      "grad_norm": 0.0005083394353277981,
      "learning_rate": 4.093495481423631e-05,
      "loss": 0.1257,
      "step": 270900
    },
    {
      "epoch": 0.8638689214389315,
      "grad_norm": 0.00045395109918899834,
      "learning_rate": 4.083932356832055e-05,
      "loss": 0.2829,
      "step": 271000
    },
    {
      "epoch": 0.8641876922586507,
      "grad_norm": 0.01816108077764511,
      "learning_rate": 4.074369232240481e-05,
      "loss": 0.0903,
      "step": 271100
    },
    {
      "epoch": 0.8645064630783698,
      "grad_norm": 0.0014048643643036485,
      "learning_rate": 4.0648061076489055e-05,
      "loss": 0.1773,
      "step": 271200
    },
    {
      "epoch": 0.864825233898089,
      "grad_norm": 0.00011418348731240258,
      "learning_rate": 4.055242983057331e-05,
      "loss": 0.1139,
      "step": 271300
    },
    {
      "epoch": 0.8651440047178082,
      "grad_norm": 0.0008612478268332779,
      "learning_rate": 4.0456798584657556e-05,
      "loss": 0.1237,
      "step": 271400
    },
    {
      "epoch": 0.8654627755375273,
      "grad_norm": 0.0010046915849670768,
      "learning_rate": 4.036116733874181e-05,
      "loss": 0.3435,
      "step": 271500
    },
    {
      "epoch": 0.8657815463572465,
      "grad_norm": 0.0016021989285945892,
      "learning_rate": 4.026553609282606e-05,
      "loss": 0.2636,
      "step": 271600
    },
    {
      "epoch": 0.8661003171769657,
      "grad_norm": 0.00040805063326843083,
      "learning_rate": 4.016990484691031e-05,
      "loss": 0.3856,
      "step": 271700
    },
    {
      "epoch": 0.8664190879966848,
      "grad_norm": 10.812705993652344,
      "learning_rate": 4.007427360099456e-05,
      "loss": 0.2113,
      "step": 271800
    },
    {
      "epoch": 0.866737858816404,
      "grad_norm": 0.03480519726872444,
      "learning_rate": 3.997864235507881e-05,
      "loss": 0.2043,
      "step": 271900
    },
    {
      "epoch": 0.8670566296361231,
      "grad_norm": 0.0015506327617913485,
      "learning_rate": 3.988301110916306e-05,
      "loss": 0.073,
      "step": 272000
    },
    {
      "epoch": 0.8673754004558423,
      "grad_norm": 0.1844518929719925,
      "learning_rate": 3.978737986324731e-05,
      "loss": 0.1509,
      "step": 272100
    },
    {
      "epoch": 0.8676941712755615,
      "grad_norm": 24.85141372680664,
      "learning_rate": 3.969174861733156e-05,
      "loss": 0.1118,
      "step": 272200
    },
    {
      "epoch": 0.8680129420952806,
      "grad_norm": 49.00794982910156,
      "learning_rate": 3.9596117371415814e-05,
      "loss": 0.1476,
      "step": 272300
    },
    {
      "epoch": 0.8683317129149998,
      "grad_norm": 36.78227615356445,
      "learning_rate": 3.9500486125500075e-05,
      "loss": 0.201,
      "step": 272400
    },
    {
      "epoch": 0.8686504837347189,
      "grad_norm": 0.005307330749928951,
      "learning_rate": 3.940485487958432e-05,
      "loss": 0.1,
      "step": 272500
    },
    {
      "epoch": 0.8689692545544381,
      "grad_norm": 70.3640365600586,
      "learning_rate": 3.9309223633668576e-05,
      "loss": 0.2246,
      "step": 272600
    },
    {
      "epoch": 0.8692880253741573,
      "grad_norm": 6.871265941299498e-05,
      "learning_rate": 3.921359238775282e-05,
      "loss": 0.1714,
      "step": 272700
    },
    {
      "epoch": 0.8696067961938764,
      "grad_norm": 9.90723492577672e-05,
      "learning_rate": 3.911796114183708e-05,
      "loss": 0.1104,
      "step": 272800
    },
    {
      "epoch": 0.8699255670135956,
      "grad_norm": 0.0009906379273161292,
      "learning_rate": 3.9022329895921324e-05,
      "loss": 0.0873,
      "step": 272900
    },
    {
      "epoch": 0.8702443378333148,
      "grad_norm": 0.0043871100060641766,
      "learning_rate": 3.892669865000558e-05,
      "loss": 0.2447,
      "step": 273000
    },
    {
      "epoch": 0.8705631086530339,
      "grad_norm": 0.006073446944355965,
      "learning_rate": 3.8831067404089825e-05,
      "loss": 0.1915,
      "step": 273100
    },
    {
      "epoch": 0.8708818794727531,
      "grad_norm": 0.0019298192346468568,
      "learning_rate": 3.873543615817408e-05,
      "loss": 0.0909,
      "step": 273200
    },
    {
      "epoch": 0.8712006502924722,
      "grad_norm": 9.897195816040039,
      "learning_rate": 3.8639804912258326e-05,
      "loss": 0.2209,
      "step": 273300
    },
    {
      "epoch": 0.8715194211121914,
      "grad_norm": 0.1236041933298111,
      "learning_rate": 3.854417366634258e-05,
      "loss": 0.2972,
      "step": 273400
    },
    {
      "epoch": 0.8718381919319106,
      "grad_norm": 0.027841225266456604,
      "learning_rate": 3.844854242042683e-05,
      "loss": 0.1951,
      "step": 273500
    },
    {
      "epoch": 0.8721569627516297,
      "grad_norm": 0.00024436431704089046,
      "learning_rate": 3.835291117451108e-05,
      "loss": 0.1304,
      "step": 273600
    },
    {
      "epoch": 0.8724757335713489,
      "grad_norm": 71.98888397216797,
      "learning_rate": 3.825727992859533e-05,
      "loss": 0.1739,
      "step": 273700
    },
    {
      "epoch": 0.8727945043910681,
      "grad_norm": 6.751357555389404,
      "learning_rate": 3.816164868267958e-05,
      "loss": 0.2008,
      "step": 273800
    },
    {
      "epoch": 0.8731132752107872,
      "grad_norm": 4.926770998281427e-05,
      "learning_rate": 3.806601743676383e-05,
      "loss": 0.1569,
      "step": 273900
    },
    {
      "epoch": 0.8734320460305064,
      "grad_norm": 0.0003524169442243874,
      "learning_rate": 3.797038619084809e-05,
      "loss": 0.1159,
      "step": 274000
    },
    {
      "epoch": 0.8737508168502255,
      "grad_norm": 0.01955784112215042,
      "learning_rate": 3.787475494493234e-05,
      "loss": 0.1514,
      "step": 274100
    },
    {
      "epoch": 0.8740695876699447,
      "grad_norm": 0.0002865568676497787,
      "learning_rate": 3.777912369901659e-05,
      "loss": 0.2357,
      "step": 274200
    },
    {
      "epoch": 0.8743883584896639,
      "grad_norm": 0.0026414559688419104,
      "learning_rate": 3.768349245310084e-05,
      "loss": 0.1288,
      "step": 274300
    },
    {
      "epoch": 0.874707129309383,
      "grad_norm": 4.953838651999831e-05,
      "learning_rate": 3.758786120718509e-05,
      "loss": 0.1139,
      "step": 274400
    },
    {
      "epoch": 0.8750259001291022,
      "grad_norm": 0.000699360272847116,
      "learning_rate": 3.749222996126934e-05,
      "loss": 0.2516,
      "step": 274500
    },
    {
      "epoch": 0.8753446709488213,
      "grad_norm": 0.0022263173013925552,
      "learning_rate": 3.739659871535359e-05,
      "loss": 0.1213,
      "step": 274600
    },
    {
      "epoch": 0.8756634417685405,
      "grad_norm": 0.0009028790518641472,
      "learning_rate": 3.730096746943785e-05,
      "loss": 0.1762,
      "step": 274700
    },
    {
      "epoch": 0.8759822125882597,
      "grad_norm": 0.0003166856768075377,
      "learning_rate": 3.7205336223522094e-05,
      "loss": 0.0316,
      "step": 274800
    },
    {
      "epoch": 0.8763009834079788,
      "grad_norm": 6.59687866573222e-05,
      "learning_rate": 3.710970497760635e-05,
      "loss": 0.1671,
      "step": 274900
    },
    {
      "epoch": 0.876619754227698,
      "grad_norm": 0.0004443721263669431,
      "learning_rate": 3.7014073731690595e-05,
      "loss": 0.2033,
      "step": 275000
    },
    {
      "epoch": 0.8769385250474172,
      "grad_norm": 9.815755038289353e-05,
      "learning_rate": 3.691844248577485e-05,
      "loss": 0.122,
      "step": 275100
    },
    {
      "epoch": 0.8772572958671363,
      "grad_norm": 0.419202595949173,
      "learning_rate": 3.6822811239859096e-05,
      "loss": 0.0806,
      "step": 275200
    },
    {
      "epoch": 0.8775760666868555,
      "grad_norm": 0.00041102556861005723,
      "learning_rate": 3.672717999394335e-05,
      "loss": 0.2233,
      "step": 275300
    },
    {
      "epoch": 0.8778948375065746,
      "grad_norm": 5.8264431572752073e-05,
      "learning_rate": 3.66315487480276e-05,
      "loss": 0.2316,
      "step": 275400
    },
    {
      "epoch": 0.8782136083262938,
      "grad_norm": 0.00033795551280491054,
      "learning_rate": 3.653591750211186e-05,
      "loss": 0.1833,
      "step": 275500
    },
    {
      "epoch": 0.878532379146013,
      "grad_norm": 0.777742326259613,
      "learning_rate": 3.6440286256196105e-05,
      "loss": 0.1401,
      "step": 275600
    },
    {
      "epoch": 0.8788511499657321,
      "grad_norm": 0.0008152190130203962,
      "learning_rate": 3.634465501028036e-05,
      "loss": 0.1325,
      "step": 275700
    },
    {
      "epoch": 0.8791699207854513,
      "grad_norm": 0.0001432608114555478,
      "learning_rate": 3.6249023764364606e-05,
      "loss": 0.1628,
      "step": 275800
    },
    {
      "epoch": 0.8794886916051705,
      "grad_norm": 0.000940032594371587,
      "learning_rate": 3.615339251844886e-05,
      "loss": 0.1624,
      "step": 275900
    },
    {
      "epoch": 0.8798074624248896,
      "grad_norm": 0.0011885035783052444,
      "learning_rate": 3.605776127253311e-05,
      "loss": 0.2259,
      "step": 276000
    },
    {
      "epoch": 0.8801262332446088,
      "grad_norm": 6.8728108406066895,
      "learning_rate": 3.596213002661736e-05,
      "loss": 0.0653,
      "step": 276100
    },
    {
      "epoch": 0.8804450040643279,
      "grad_norm": 0.0016771260416135192,
      "learning_rate": 3.5866498780701615e-05,
      "loss": 0.1205,
      "step": 276200
    },
    {
      "epoch": 0.8807637748840471,
      "grad_norm": 0.07860440015792847,
      "learning_rate": 3.577086753478586e-05,
      "loss": 0.1732,
      "step": 276300
    },
    {
      "epoch": 0.8810825457037663,
      "grad_norm": 0.0020083643030375242,
      "learning_rate": 3.5675236288870116e-05,
      "loss": 0.3004,
      "step": 276400
    },
    {
      "epoch": 0.8814013165234854,
      "grad_norm": 0.00044669798808172345,
      "learning_rate": 3.557960504295436e-05,
      "loss": 0.079,
      "step": 276500
    },
    {
      "epoch": 0.8817200873432046,
      "grad_norm": 0.00432228110730648,
      "learning_rate": 3.548397379703862e-05,
      "loss": 0.0654,
      "step": 276600
    },
    {
      "epoch": 0.8820388581629237,
      "grad_norm": 0.01571510173380375,
      "learning_rate": 3.5388342551122864e-05,
      "loss": 0.0948,
      "step": 276700
    },
    {
      "epoch": 0.8823576289826429,
      "grad_norm": 0.00047940213698893785,
      "learning_rate": 3.529271130520712e-05,
      "loss": 0.1868,
      "step": 276800
    },
    {
      "epoch": 0.8826763998023621,
      "grad_norm": 3.433761594351381e-05,
      "learning_rate": 3.519708005929137e-05,
      "loss": 0.1189,
      "step": 276900
    },
    {
      "epoch": 0.8829951706220812,
      "grad_norm": 0.00016817344294395298,
      "learning_rate": 3.510144881337562e-05,
      "loss": 0.1619,
      "step": 277000
    },
    {
      "epoch": 0.8833139414418004,
      "grad_norm": 8.251746476162225e-05,
      "learning_rate": 3.500581756745987e-05,
      "loss": 0.0816,
      "step": 277100
    },
    {
      "epoch": 0.8836327122615196,
      "grad_norm": 0.0002749673731159419,
      "learning_rate": 3.491018632154412e-05,
      "loss": 0.1159,
      "step": 277200
    },
    {
      "epoch": 0.8839514830812387,
      "grad_norm": 0.00021205290977377445,
      "learning_rate": 3.4814555075628374e-05,
      "loss": 0.1851,
      "step": 277300
    },
    {
      "epoch": 0.8842702539009579,
      "grad_norm": 1.158617942564888e-05,
      "learning_rate": 3.471892382971262e-05,
      "loss": 0.2188,
      "step": 277400
    },
    {
      "epoch": 0.884589024720677,
      "grad_norm": 0.0004002025816589594,
      "learning_rate": 3.4623292583796875e-05,
      "loss": 0.0598,
      "step": 277500
    },
    {
      "epoch": 0.8849077955403962,
      "grad_norm": 0.04492414742708206,
      "learning_rate": 3.452766133788112e-05,
      "loss": 0.1159,
      "step": 277600
    },
    {
      "epoch": 0.8852265663601154,
      "grad_norm": 0.0004164038982708007,
      "learning_rate": 3.443203009196538e-05,
      "loss": 0.222,
      "step": 277700
    },
    {
      "epoch": 0.8855453371798345,
      "grad_norm": 16.402677536010742,
      "learning_rate": 3.433639884604963e-05,
      "loss": 0.0896,
      "step": 277800
    },
    {
      "epoch": 0.8858641079995537,
      "grad_norm": 0.26418399810791016,
      "learning_rate": 3.4240767600133884e-05,
      "loss": 0.0931,
      "step": 277900
    },
    {
      "epoch": 0.8861828788192729,
      "grad_norm": 0.0029496846254915,
      "learning_rate": 3.414513635421813e-05,
      "loss": 0.1432,
      "step": 278000
    },
    {
      "epoch": 0.886501649638992,
      "grad_norm": 0.0011457301443442702,
      "learning_rate": 3.4049505108302385e-05,
      "loss": 0.1118,
      "step": 278100
    },
    {
      "epoch": 0.8868204204587112,
      "grad_norm": 0.000996684073470533,
      "learning_rate": 3.395387386238663e-05,
      "loss": 0.2022,
      "step": 278200
    },
    {
      "epoch": 0.8871391912784303,
      "grad_norm": 0.0007208551396615803,
      "learning_rate": 3.3858242616470886e-05,
      "loss": 0.0701,
      "step": 278300
    },
    {
      "epoch": 0.8874579620981495,
      "grad_norm": 0.005184307228773832,
      "learning_rate": 3.376261137055514e-05,
      "loss": 0.1568,
      "step": 278400
    },
    {
      "epoch": 0.8877767329178687,
      "grad_norm": 0.00017289975949097425,
      "learning_rate": 3.366698012463939e-05,
      "loss": 0.2087,
      "step": 278500
    },
    {
      "epoch": 0.8880955037375878,
      "grad_norm": 0.014179626479744911,
      "learning_rate": 3.357134887872364e-05,
      "loss": 0.0959,
      "step": 278600
    },
    {
      "epoch": 0.888414274557307,
      "grad_norm": 7.554951298516244e-05,
      "learning_rate": 3.347571763280789e-05,
      "loss": 0.2034,
      "step": 278700
    },
    {
      "epoch": 0.8887330453770261,
      "grad_norm": 0.338984876871109,
      "learning_rate": 3.338008638689214e-05,
      "loss": 0.1029,
      "step": 278800
    },
    {
      "epoch": 0.8890518161967453,
      "grad_norm": 0.0002717937750276178,
      "learning_rate": 3.328445514097639e-05,
      "loss": 0.1722,
      "step": 278900
    },
    {
      "epoch": 0.8893705870164645,
      "grad_norm": 0.00013205742288846523,
      "learning_rate": 3.318882389506064e-05,
      "loss": 0.1402,
      "step": 279000
    },
    {
      "epoch": 0.8896893578361836,
      "grad_norm": 1.1446946859359741,
      "learning_rate": 3.30931926491449e-05,
      "loss": 0.1548,
      "step": 279100
    },
    {
      "epoch": 0.8900081286559028,
      "grad_norm": 0.0005702050984837115,
      "learning_rate": 3.2997561403229144e-05,
      "loss": 0.2186,
      "step": 279200
    },
    {
      "epoch": 0.890326899475622,
      "grad_norm": 0.00011137319961562753,
      "learning_rate": 3.29019301573134e-05,
      "loss": 0.145,
      "step": 279300
    },
    {
      "epoch": 0.8906456702953411,
      "grad_norm": 0.0617302842438221,
      "learning_rate": 3.2806298911397645e-05,
      "loss": 0.172,
      "step": 279400
    },
    {
      "epoch": 0.8909644411150603,
      "grad_norm": 0.004512113053351641,
      "learning_rate": 3.27106676654819e-05,
      "loss": 0.1829,
      "step": 279500
    },
    {
      "epoch": 0.8912832119347794,
      "grad_norm": 2.4055096218944527e-05,
      "learning_rate": 3.2615036419566146e-05,
      "loss": 0.1627,
      "step": 279600
    },
    {
      "epoch": 0.8916019827544986,
      "grad_norm": 0.00019699834228958935,
      "learning_rate": 3.25194051736504e-05,
      "loss": 0.2089,
      "step": 279700
    },
    {
      "epoch": 0.8919207535742179,
      "grad_norm": 0.00010930959979305044,
      "learning_rate": 3.242377392773465e-05,
      "loss": 0.1443,
      "step": 279800
    },
    {
      "epoch": 0.892239524393937,
      "grad_norm": 0.013572478666901588,
      "learning_rate": 3.232814268181891e-05,
      "loss": 0.1125,
      "step": 279900
    },
    {
      "epoch": 0.8925582952136561,
      "grad_norm": 5.0676466344157234e-05,
      "learning_rate": 3.2232511435903155e-05,
      "loss": 0.242,
      "step": 280000
    },
    {
      "epoch": 0.8928770660333754,
      "grad_norm": 0.00026341513148508966,
      "learning_rate": 3.213688018998741e-05,
      "loss": 0.0982,
      "step": 280100
    },
    {
      "epoch": 0.8931958368530944,
      "grad_norm": 0.000212584956898354,
      "learning_rate": 3.2041248944071656e-05,
      "loss": 0.3472,
      "step": 280200
    },
    {
      "epoch": 0.8935146076728137,
      "grad_norm": 5.0395912694511935e-05,
      "learning_rate": 3.194561769815591e-05,
      "loss": 0.2131,
      "step": 280300
    },
    {
      "epoch": 0.8938333784925327,
      "grad_norm": 0.7294939756393433,
      "learning_rate": 3.184998645224016e-05,
      "loss": 0.1762,
      "step": 280400
    },
    {
      "epoch": 0.894152149312252,
      "grad_norm": 0.0020180935971438885,
      "learning_rate": 3.175435520632441e-05,
      "loss": 0.1681,
      "step": 280500
    },
    {
      "epoch": 0.8944709201319712,
      "grad_norm": 0.005687027703970671,
      "learning_rate": 3.1658723960408665e-05,
      "loss": 0.1312,
      "step": 280600
    },
    {
      "epoch": 0.8947896909516903,
      "grad_norm": 0.0005401675007306039,
      "learning_rate": 3.156309271449291e-05,
      "loss": 0.1066,
      "step": 280700
    },
    {
      "epoch": 0.8951084617714095,
      "grad_norm": 0.00024394226784352213,
      "learning_rate": 3.1467461468577166e-05,
      "loss": 0.1698,
      "step": 280800
    },
    {
      "epoch": 0.8954272325911287,
      "grad_norm": 0.002511287573724985,
      "learning_rate": 3.137183022266141e-05,
      "loss": 0.1275,
      "step": 280900
    },
    {
      "epoch": 0.8957460034108478,
      "grad_norm": 0.0002487565216142684,
      "learning_rate": 3.127619897674567e-05,
      "loss": 0.1648,
      "step": 281000
    },
    {
      "epoch": 0.896064774230567,
      "grad_norm": 0.0017997107934206724,
      "learning_rate": 3.1180567730829914e-05,
      "loss": 0.2307,
      "step": 281100
    },
    {
      "epoch": 0.8963835450502861,
      "grad_norm": 8.396726608276367,
      "learning_rate": 3.108493648491417e-05,
      "loss": 0.1799,
      "step": 281200
    },
    {
      "epoch": 0.8967023158700053,
      "grad_norm": 0.0016576333437114954,
      "learning_rate": 3.098930523899842e-05,
      "loss": 0.1732,
      "step": 281300
    },
    {
      "epoch": 0.8970210866897245,
      "grad_norm": 0.0002035570068983361,
      "learning_rate": 3.089367399308267e-05,
      "loss": 0.2103,
      "step": 281400
    },
    {
      "epoch": 0.8973398575094436,
      "grad_norm": 0.0008175823022611439,
      "learning_rate": 3.079804274716692e-05,
      "loss": 0.2068,
      "step": 281500
    },
    {
      "epoch": 0.8976586283291628,
      "grad_norm": 55.468711853027344,
      "learning_rate": 3.070241150125117e-05,
      "loss": 0.101,
      "step": 281600
    },
    {
      "epoch": 0.8979773991488819,
      "grad_norm": 0.0005961615825071931,
      "learning_rate": 3.0606780255335424e-05,
      "loss": 0.1796,
      "step": 281700
    },
    {
      "epoch": 0.8982961699686011,
      "grad_norm": 0.005401231348514557,
      "learning_rate": 3.0511149009419675e-05,
      "loss": 0.1465,
      "step": 281800
    },
    {
      "epoch": 0.8986149407883203,
      "grad_norm": 0.0019044379005208611,
      "learning_rate": 3.0415517763503925e-05,
      "loss": 0.0841,
      "step": 281900
    },
    {
      "epoch": 0.8989337116080394,
      "grad_norm": 0.0015872599324211478,
      "learning_rate": 3.0319886517588176e-05,
      "loss": 0.2292,
      "step": 282000
    },
    {
      "epoch": 0.8992524824277586,
      "grad_norm": 0.002546100877225399,
      "learning_rate": 3.022425527167243e-05,
      "loss": 0.0999,
      "step": 282100
    },
    {
      "epoch": 0.8995712532474778,
      "grad_norm": 0.005842107813805342,
      "learning_rate": 3.012862402575668e-05,
      "loss": 0.2241,
      "step": 282200
    },
    {
      "epoch": 0.8998900240671969,
      "grad_norm": 0.00011465569696156308,
      "learning_rate": 3.0032992779840934e-05,
      "loss": 0.0385,
      "step": 282300
    },
    {
      "epoch": 0.9002087948869161,
      "grad_norm": 6.366051820805296e-05,
      "learning_rate": 2.9937361533925184e-05,
      "loss": 0.0692,
      "step": 282400
    },
    {
      "epoch": 0.9005275657066352,
      "grad_norm": 6.875355029478669e-05,
      "learning_rate": 2.9841730288009435e-05,
      "loss": 0.1391,
      "step": 282500
    },
    {
      "epoch": 0.9008463365263544,
      "grad_norm": 0.027156822383403778,
      "learning_rate": 2.9746099042093685e-05,
      "loss": 0.177,
      "step": 282600
    },
    {
      "epoch": 0.9011651073460736,
      "grad_norm": 6.9563727378845215,
      "learning_rate": 2.9650467796177936e-05,
      "loss": 0.1574,
      "step": 282700
    },
    {
      "epoch": 0.9014838781657927,
      "grad_norm": 64.71023559570312,
      "learning_rate": 2.9554836550262187e-05,
      "loss": 0.2246,
      "step": 282800
    },
    {
      "epoch": 0.9018026489855119,
      "grad_norm": 3.654982356238179e-05,
      "learning_rate": 2.9459205304346437e-05,
      "loss": 0.2817,
      "step": 282900
    },
    {
      "epoch": 0.9021214198052311,
      "grad_norm": 3.7502009868621826,
      "learning_rate": 2.9363574058430688e-05,
      "loss": 0.1155,
      "step": 283000
    },
    {
      "epoch": 0.9024401906249502,
      "grad_norm": 0.0006562561029568315,
      "learning_rate": 2.926794281251494e-05,
      "loss": 0.0393,
      "step": 283100
    },
    {
      "epoch": 0.9027589614446694,
      "grad_norm": 0.05081205442547798,
      "learning_rate": 2.9172311566599192e-05,
      "loss": 0.1706,
      "step": 283200
    },
    {
      "epoch": 0.9030777322643885,
      "grad_norm": 48.07939910888672,
      "learning_rate": 2.9076680320683442e-05,
      "loss": 0.1374,
      "step": 283300
    },
    {
      "epoch": 0.9033965030841077,
      "grad_norm": 0.0010116095654666424,
      "learning_rate": 2.8981049074767693e-05,
      "loss": 0.3158,
      "step": 283400
    },
    {
      "epoch": 0.9037152739038269,
      "grad_norm": 0.14983688294887543,
      "learning_rate": 2.8885417828851944e-05,
      "loss": 0.1774,
      "step": 283500
    },
    {
      "epoch": 0.904034044723546,
      "grad_norm": 0.257839560508728,
      "learning_rate": 2.8789786582936194e-05,
      "loss": 0.1558,
      "step": 283600
    },
    {
      "epoch": 0.9043528155432652,
      "grad_norm": 2.826613126671873e-05,
      "learning_rate": 2.8694155337020445e-05,
      "loss": 0.3032,
      "step": 283700
    },
    {
      "epoch": 0.9046715863629843,
      "grad_norm": 0.00022579052892979234,
      "learning_rate": 2.8598524091104695e-05,
      "loss": 0.1443,
      "step": 283800
    },
    {
      "epoch": 0.9049903571827035,
      "grad_norm": 2.7773894544225186e-05,
      "learning_rate": 2.850289284518895e-05,
      "loss": 0.1783,
      "step": 283900
    },
    {
      "epoch": 0.9053091280024227,
      "grad_norm": 30.00473976135254,
      "learning_rate": 2.84072615992732e-05,
      "loss": 0.1954,
      "step": 284000
    },
    {
      "epoch": 0.9056278988221418,
      "grad_norm": 0.0008369686547666788,
      "learning_rate": 2.831163035335745e-05,
      "loss": 0.0816,
      "step": 284100
    },
    {
      "epoch": 0.905946669641861,
      "grad_norm": 0.04666799679398537,
      "learning_rate": 2.8215999107441704e-05,
      "loss": 0.1833,
      "step": 284200
    },
    {
      "epoch": 0.9062654404615802,
      "grad_norm": 0.001494420925155282,
      "learning_rate": 2.8120367861525954e-05,
      "loss": 0.1483,
      "step": 284300
    },
    {
      "epoch": 0.9065842112812993,
      "grad_norm": 0.0002038956299657002,
      "learning_rate": 2.8024736615610205e-05,
      "loss": 0.0365,
      "step": 284400
    },
    {
      "epoch": 0.9069029821010185,
      "grad_norm": 0.0008822393720038235,
      "learning_rate": 2.792910536969446e-05,
      "loss": 0.1555,
      "step": 284500
    },
    {
      "epoch": 0.9072217529207376,
      "grad_norm": 0.002655782038345933,
      "learning_rate": 2.783347412377871e-05,
      "loss": 0.2968,
      "step": 284600
    },
    {
      "epoch": 0.9075405237404568,
      "grad_norm": 3.6993911635363474e-05,
      "learning_rate": 2.773784287786296e-05,
      "loss": 0.2411,
      "step": 284700
    },
    {
      "epoch": 0.907859294560176,
      "grad_norm": 0.0004206764861010015,
      "learning_rate": 2.764221163194721e-05,
      "loss": 0.1473,
      "step": 284800
    },
    {
      "epoch": 0.9081780653798951,
      "grad_norm": 0.00023030028387438506,
      "learning_rate": 2.754658038603146e-05,
      "loss": 0.196,
      "step": 284900
    },
    {
      "epoch": 0.9084968361996143,
      "grad_norm": 0.0002399791992502287,
      "learning_rate": 2.745094914011571e-05,
      "loss": 0.2114,
      "step": 285000
    },
    {
      "epoch": 0.9088156070193335,
      "grad_norm": 0.0002218978916062042,
      "learning_rate": 2.7355317894199962e-05,
      "loss": 0.2108,
      "step": 285100
    },
    {
      "epoch": 0.9091343778390526,
      "grad_norm": 0.0003975951985921711,
      "learning_rate": 2.7259686648284213e-05,
      "loss": 0.1507,
      "step": 285200
    },
    {
      "epoch": 0.9094531486587718,
      "grad_norm": 0.00039320671930909157,
      "learning_rate": 2.7164055402368466e-05,
      "loss": 0.2062,
      "step": 285300
    },
    {
      "epoch": 0.9097719194784909,
      "grad_norm": 0.004235715139657259,
      "learning_rate": 2.7068424156452717e-05,
      "loss": 0.1644,
      "step": 285400
    },
    {
      "epoch": 0.9100906902982101,
      "grad_norm": 91.88157653808594,
      "learning_rate": 2.6972792910536967e-05,
      "loss": 0.1356,
      "step": 285500
    },
    {
      "epoch": 0.9104094611179293,
      "grad_norm": 9.686036355560645e-05,
      "learning_rate": 2.6877161664621218e-05,
      "loss": 0.1932,
      "step": 285600
    },
    {
      "epoch": 0.9107282319376484,
      "grad_norm": 0.0006978470482863486,
      "learning_rate": 2.678153041870547e-05,
      "loss": 0.0952,
      "step": 285700
    },
    {
      "epoch": 0.9110470027573676,
      "grad_norm": 12.876818656921387,
      "learning_rate": 2.668589917278972e-05,
      "loss": 0.0941,
      "step": 285800
    },
    {
      "epoch": 0.9113657735770867,
      "grad_norm": 4.4668875489151105e-05,
      "learning_rate": 2.659026792687397e-05,
      "loss": 0.1773,
      "step": 285900
    },
    {
      "epoch": 0.9116845443968059,
      "grad_norm": 0.0012827690225094557,
      "learning_rate": 2.649463668095822e-05,
      "loss": 0.1758,
      "step": 286000
    },
    {
      "epoch": 0.9120033152165251,
      "grad_norm": 10.397346496582031,
      "learning_rate": 2.6399005435042474e-05,
      "loss": 0.3532,
      "step": 286100
    },
    {
      "epoch": 0.9123220860362442,
      "grad_norm": 0.009625446982681751,
      "learning_rate": 2.6303374189126724e-05,
      "loss": 0.1493,
      "step": 286200
    },
    {
      "epoch": 0.9126408568559634,
      "grad_norm": 0.002512937178835273,
      "learning_rate": 2.6207742943210975e-05,
      "loss": 0.2703,
      "step": 286300
    },
    {
      "epoch": 0.9129596276756826,
      "grad_norm": 0.005057828035205603,
      "learning_rate": 2.611211169729523e-05,
      "loss": 0.0652,
      "step": 286400
    },
    {
      "epoch": 0.9132783984954017,
      "grad_norm": 7.332072709687054e-05,
      "learning_rate": 2.601648045137948e-05,
      "loss": 0.1569,
      "step": 286500
    },
    {
      "epoch": 0.9135971693151209,
      "grad_norm": 6.275714258663356e-05,
      "learning_rate": 2.592084920546373e-05,
      "loss": 0.1666,
      "step": 286600
    },
    {
      "epoch": 0.91391594013484,
      "grad_norm": 0.0246844794601202,
      "learning_rate": 2.5825217959547984e-05,
      "loss": 0.081,
      "step": 286700
    },
    {
      "epoch": 0.9142347109545592,
      "grad_norm": 0.040130864828825,
      "learning_rate": 2.5729586713632234e-05,
      "loss": 0.1204,
      "step": 286800
    },
    {
      "epoch": 0.9145534817742784,
      "grad_norm": 0.00013781589223071933,
      "learning_rate": 2.5633955467716485e-05,
      "loss": 0.2326,
      "step": 286900
    },
    {
      "epoch": 0.9148722525939975,
      "grad_norm": 0.00020994729129597545,
      "learning_rate": 2.5538324221800735e-05,
      "loss": 0.1441,
      "step": 287000
    },
    {
      "epoch": 0.9151910234137167,
      "grad_norm": 2.9606833457946777,
      "learning_rate": 2.5442692975884986e-05,
      "loss": 0.2287,
      "step": 287100
    },
    {
      "epoch": 0.9155097942334359,
      "grad_norm": 0.0011579492129385471,
      "learning_rate": 2.5347061729969236e-05,
      "loss": 0.0895,
      "step": 287200
    },
    {
      "epoch": 0.915828565053155,
      "grad_norm": 0.00022222900588531047,
      "learning_rate": 2.5251430484053487e-05,
      "loss": 0.2068,
      "step": 287300
    },
    {
      "epoch": 0.9161473358728742,
      "grad_norm": 0.00045691744890064,
      "learning_rate": 2.5155799238137737e-05,
      "loss": 0.0603,
      "step": 287400
    },
    {
      "epoch": 0.9164661066925933,
      "grad_norm": 25.790103912353516,
      "learning_rate": 2.506016799222199e-05,
      "loss": 0.0981,
      "step": 287500
    },
    {
      "epoch": 0.9167848775123125,
      "grad_norm": 0.00011046722647733986,
      "learning_rate": 2.4964536746306242e-05,
      "loss": 0.3026,
      "step": 287600
    },
    {
      "epoch": 0.9171036483320317,
      "grad_norm": 0.005920058581978083,
      "learning_rate": 2.4868905500390492e-05,
      "loss": 0.4454,
      "step": 287700
    },
    {
      "epoch": 0.9174224191517508,
      "grad_norm": 0.0022558204364031553,
      "learning_rate": 2.4773274254474743e-05,
      "loss": 0.1566,
      "step": 287800
    },
    {
      "epoch": 0.91774118997147,
      "grad_norm": 0.00010937213664874434,
      "learning_rate": 2.4677643008558993e-05,
      "loss": 0.0644,
      "step": 287900
    },
    {
      "epoch": 0.9180599607911891,
      "grad_norm": 0.0005206045461818576,
      "learning_rate": 2.4582011762643244e-05,
      "loss": 0.1623,
      "step": 288000
    },
    {
      "epoch": 0.9183787316109083,
      "grad_norm": 0.05026229843497276,
      "learning_rate": 2.4486380516727495e-05,
      "loss": 0.1526,
      "step": 288100
    },
    {
      "epoch": 0.9186975024306275,
      "grad_norm": 0.0003765655274037272,
      "learning_rate": 2.4390749270811745e-05,
      "loss": 0.1975,
      "step": 288200
    },
    {
      "epoch": 0.9190162732503466,
      "grad_norm": 0.00018695126345846802,
      "learning_rate": 2.4295118024896e-05,
      "loss": 0.2733,
      "step": 288300
    },
    {
      "epoch": 0.9193350440700658,
      "grad_norm": 0.00017184055468533188,
      "learning_rate": 2.419948677898025e-05,
      "loss": 0.1777,
      "step": 288400
    },
    {
      "epoch": 0.919653814889785,
      "grad_norm": 0.0018349081510677934,
      "learning_rate": 2.41038555330645e-05,
      "loss": 0.2279,
      "step": 288500
    },
    {
      "epoch": 0.9199725857095041,
      "grad_norm": 0.0002162069285986945,
      "learning_rate": 2.4008224287148754e-05,
      "loss": 0.0611,
      "step": 288600
    },
    {
      "epoch": 0.9202913565292233,
      "grad_norm": 8.175291441148147e-05,
      "learning_rate": 2.3912593041233004e-05,
      "loss": 0.0502,
      "step": 288700
    },
    {
      "epoch": 0.9206101273489424,
      "grad_norm": 0.000441430980572477,
      "learning_rate": 2.3816961795317255e-05,
      "loss": 0.0839,
      "step": 288800
    },
    {
      "epoch": 0.9209288981686616,
      "grad_norm": 0.0029813996516168118,
      "learning_rate": 2.3721330549401505e-05,
      "loss": 0.0326,
      "step": 288900
    },
    {
      "epoch": 0.9212476689883808,
      "grad_norm": 0.00011910018656635657,
      "learning_rate": 2.362569930348576e-05,
      "loss": 0.065,
      "step": 289000
    },
    {
      "epoch": 0.9215664398080999,
      "grad_norm": 3.846254912787117e-05,
      "learning_rate": 2.353006805757001e-05,
      "loss": 0.1838,
      "step": 289100
    },
    {
      "epoch": 0.9218852106278191,
      "grad_norm": 3.4006960049737245e-05,
      "learning_rate": 2.343443681165426e-05,
      "loss": 0.161,
      "step": 289200
    },
    {
      "epoch": 0.9222039814475383,
      "grad_norm": 0.00010991707677021623,
      "learning_rate": 2.333880556573851e-05,
      "loss": 0.2331,
      "step": 289300
    },
    {
      "epoch": 0.9225227522672574,
      "grad_norm": 2.208247661590576,
      "learning_rate": 2.324317431982276e-05,
      "loss": 0.1489,
      "step": 289400
    },
    {
      "epoch": 0.9228415230869766,
      "grad_norm": 0.0001597990922164172,
      "learning_rate": 2.3147543073907012e-05,
      "loss": 0.1405,
      "step": 289500
    },
    {
      "epoch": 0.9231602939066957,
      "grad_norm": 5.0844097131630406e-05,
      "learning_rate": 2.3051911827991262e-05,
      "loss": 0.0635,
      "step": 289600
    },
    {
      "epoch": 0.9234790647264149,
      "grad_norm": 0.0001898097398225218,
      "learning_rate": 2.2956280582075516e-05,
      "loss": 0.1403,
      "step": 289700
    },
    {
      "epoch": 0.9237978355461341,
      "grad_norm": 0.00035257957642897964,
      "learning_rate": 2.2860649336159767e-05,
      "loss": 0.3178,
      "step": 289800
    },
    {
      "epoch": 0.9241166063658532,
      "grad_norm": 0.00134492595680058,
      "learning_rate": 2.2765018090244017e-05,
      "loss": 0.1347,
      "step": 289900
    },
    {
      "epoch": 0.9244353771855724,
      "grad_norm": 0.0003851293586194515,
      "learning_rate": 2.2669386844328268e-05,
      "loss": 0.2059,
      "step": 290000
    },
    {
      "epoch": 0.9247541480052915,
      "grad_norm": 0.0005476941587403417,
      "learning_rate": 2.257375559841252e-05,
      "loss": 0.1067,
      "step": 290100
    },
    {
      "epoch": 0.9250729188250107,
      "grad_norm": 0.00021721923258155584,
      "learning_rate": 2.247812435249677e-05,
      "loss": 0.2081,
      "step": 290200
    },
    {
      "epoch": 0.92539168964473,
      "grad_norm": 5.33097299921792e-05,
      "learning_rate": 2.238249310658102e-05,
      "loss": 0.0981,
      "step": 290300
    },
    {
      "epoch": 0.925710460464449,
      "grad_norm": 6.362736166920513e-05,
      "learning_rate": 2.228686186066527e-05,
      "loss": 0.1088,
      "step": 290400
    },
    {
      "epoch": 0.9260292312841683,
      "grad_norm": 0.054252155125141144,
      "learning_rate": 2.2191230614749524e-05,
      "loss": 0.1192,
      "step": 290500
    },
    {
      "epoch": 0.9263480021038875,
      "grad_norm": 4.9180751375388354e-05,
      "learning_rate": 2.2095599368833774e-05,
      "loss": 0.1866,
      "step": 290600
    },
    {
      "epoch": 0.9266667729236066,
      "grad_norm": 20.744304656982422,
      "learning_rate": 2.1999968122918025e-05,
      "loss": 0.1045,
      "step": 290700
    },
    {
      "epoch": 0.9269855437433258,
      "grad_norm": 0.0008051682962104678,
      "learning_rate": 2.190433687700228e-05,
      "loss": 0.0995,
      "step": 290800
    },
    {
      "epoch": 0.9273043145630449,
      "grad_norm": 0.016717277467250824,
      "learning_rate": 2.180870563108653e-05,
      "loss": 0.2755,
      "step": 290900
    },
    {
      "epoch": 0.9276230853827641,
      "grad_norm": 0.0009031785884872079,
      "learning_rate": 2.171307438517078e-05,
      "loss": 0.0757,
      "step": 291000
    },
    {
      "epoch": 0.9279418562024833,
      "grad_norm": 4.810499376617372e-05,
      "learning_rate": 2.161744313925503e-05,
      "loss": 0.2471,
      "step": 291100
    },
    {
      "epoch": 0.9282606270222024,
      "grad_norm": 0.0034644051920622587,
      "learning_rate": 2.1521811893339284e-05,
      "loss": 0.1895,
      "step": 291200
    },
    {
      "epoch": 0.9285793978419216,
      "grad_norm": 20.323701858520508,
      "learning_rate": 2.1426180647423535e-05,
      "loss": 0.1088,
      "step": 291300
    },
    {
      "epoch": 0.9288981686616408,
      "grad_norm": 0.004723891615867615,
      "learning_rate": 2.1330549401507785e-05,
      "loss": 0.1303,
      "step": 291400
    },
    {
      "epoch": 0.9292169394813599,
      "grad_norm": 9.505106572760269e-05,
      "learning_rate": 2.1234918155592036e-05,
      "loss": 0.1258,
      "step": 291500
    },
    {
      "epoch": 0.9295357103010791,
      "grad_norm": 0.0004932591109536588,
      "learning_rate": 2.1139286909676286e-05,
      "loss": 0.0991,
      "step": 291600
    },
    {
      "epoch": 0.9298544811207982,
      "grad_norm": 0.0013229157775640488,
      "learning_rate": 2.1043655663760537e-05,
      "loss": 0.1573,
      "step": 291700
    },
    {
      "epoch": 0.9301732519405174,
      "grad_norm": 9.049999061971903e-05,
      "learning_rate": 2.0948024417844787e-05,
      "loss": 0.1396,
      "step": 291800
    },
    {
      "epoch": 0.9304920227602366,
      "grad_norm": 0.0018503996543586254,
      "learning_rate": 2.0852393171929038e-05,
      "loss": 0.1838,
      "step": 291900
    },
    {
      "epoch": 0.9308107935799557,
      "grad_norm": 0.0010830448009073734,
      "learning_rate": 2.0756761926013292e-05,
      "loss": 0.1022,
      "step": 292000
    },
    {
      "epoch": 0.9311295643996749,
      "grad_norm": 0.00038772064726799726,
      "learning_rate": 2.0661130680097542e-05,
      "loss": 0.1905,
      "step": 292100
    },
    {
      "epoch": 0.931448335219394,
      "grad_norm": 0.00043385743629187346,
      "learning_rate": 2.0565499434181793e-05,
      "loss": 0.1702,
      "step": 292200
    },
    {
      "epoch": 0.9317671060391132,
      "grad_norm": 0.0010918303159996867,
      "learning_rate": 2.0469868188266043e-05,
      "loss": 0.1267,
      "step": 292300
    },
    {
      "epoch": 0.9320858768588324,
      "grad_norm": 27.20111846923828,
      "learning_rate": 2.0374236942350294e-05,
      "loss": 0.2914,
      "step": 292400
    },
    {
      "epoch": 0.9324046476785515,
      "grad_norm": 38.91180419921875,
      "learning_rate": 2.0278605696434544e-05,
      "loss": 0.1128,
      "step": 292500
    },
    {
      "epoch": 0.9327234184982707,
      "grad_norm": 8.907407755032182e-05,
      "learning_rate": 2.0182974450518795e-05,
      "loss": 0.2533,
      "step": 292600
    },
    {
      "epoch": 0.9330421893179899,
      "grad_norm": 0.00010060292697744444,
      "learning_rate": 2.008734320460305e-05,
      "loss": 0.1247,
      "step": 292700
    },
    {
      "epoch": 0.933360960137709,
      "grad_norm": 0.006527787074446678,
      "learning_rate": 1.99917119586873e-05,
      "loss": 0.1428,
      "step": 292800
    },
    {
      "epoch": 0.9336797309574282,
      "grad_norm": 0.00013796950224786997,
      "learning_rate": 1.989608071277155e-05,
      "loss": 0.0973,
      "step": 292900
    },
    {
      "epoch": 0.9339985017771473,
      "grad_norm": 8.198463910957798e-05,
      "learning_rate": 1.9800449466855804e-05,
      "loss": 0.1979,
      "step": 293000
    },
    {
      "epoch": 0.9343172725968665,
      "grad_norm": 0.0002620284503791481,
      "learning_rate": 1.9704818220940054e-05,
      "loss": 0.1893,
      "step": 293100
    },
    {
      "epoch": 0.9346360434165857,
      "grad_norm": 0.01823076605796814,
      "learning_rate": 1.9609186975024305e-05,
      "loss": 0.1563,
      "step": 293200
    },
    {
      "epoch": 0.9349548142363048,
      "grad_norm": 9.056123235495761e-05,
      "learning_rate": 1.9513555729108555e-05,
      "loss": 0.3969,
      "step": 293300
    },
    {
      "epoch": 0.935273585056024,
      "grad_norm": 0.04010210558772087,
      "learning_rate": 1.941792448319281e-05,
      "loss": 0.1993,
      "step": 293400
    },
    {
      "epoch": 0.9355923558757432,
      "grad_norm": 67.4021987915039,
      "learning_rate": 1.932229323727706e-05,
      "loss": 0.3001,
      "step": 293500
    },
    {
      "epoch": 0.9359111266954623,
      "grad_norm": 0.000768470112234354,
      "learning_rate": 1.922666199136131e-05,
      "loss": 0.1867,
      "step": 293600
    },
    {
      "epoch": 0.9362298975151815,
      "grad_norm": 0.0002832048339769244,
      "learning_rate": 1.913103074544556e-05,
      "loss": 0.1186,
      "step": 293700
    },
    {
      "epoch": 0.9365486683349006,
      "grad_norm": 8.872918988345191e-05,
      "learning_rate": 1.903539949952981e-05,
      "loss": 0.2244,
      "step": 293800
    },
    {
      "epoch": 0.9368674391546198,
      "grad_norm": 13.866901397705078,
      "learning_rate": 1.8939768253614062e-05,
      "loss": 0.1361,
      "step": 293900
    },
    {
      "epoch": 0.937186209974339,
      "grad_norm": 0.0007388637168332934,
      "learning_rate": 1.8844137007698312e-05,
      "loss": 0.2049,
      "step": 294000
    },
    {
      "epoch": 0.9375049807940581,
      "grad_norm": 3.1112336728256196e-05,
      "learning_rate": 1.8748505761782563e-05,
      "loss": 0.2169,
      "step": 294100
    },
    {
      "epoch": 0.9378237516137773,
      "grad_norm": 3.6627236113417894e-05,
      "learning_rate": 1.8652874515866817e-05,
      "loss": 0.1779,
      "step": 294200
    },
    {
      "epoch": 0.9381425224334964,
      "grad_norm": 2.604919791338034e-05,
      "learning_rate": 1.8557243269951067e-05,
      "loss": 0.147,
      "step": 294300
    },
    {
      "epoch": 0.9384612932532156,
      "grad_norm": 0.010629796423017979,
      "learning_rate": 1.8461612024035318e-05,
      "loss": 0.1921,
      "step": 294400
    },
    {
      "epoch": 0.9387800640729348,
      "grad_norm": 0.016931673511862755,
      "learning_rate": 1.836598077811957e-05,
      "loss": 0.0959,
      "step": 294500
    },
    {
      "epoch": 0.9390988348926539,
      "grad_norm": 0.0048934901133179665,
      "learning_rate": 1.8270349532203822e-05,
      "loss": 0.0856,
      "step": 294600
    },
    {
      "epoch": 0.9394176057123731,
      "grad_norm": 0.00011987602192675695,
      "learning_rate": 1.8174718286288073e-05,
      "loss": 0.1694,
      "step": 294700
    },
    {
      "epoch": 0.9397363765320923,
      "grad_norm": 0.01334892027080059,
      "learning_rate": 1.8079087040372323e-05,
      "loss": 0.1642,
      "step": 294800
    },
    {
      "epoch": 0.9400551473518114,
      "grad_norm": 0.0003028962528333068,
      "learning_rate": 1.7983455794456574e-05,
      "loss": 0.1386,
      "step": 294900
    },
    {
      "epoch": 0.9403739181715306,
      "grad_norm": 0.0007496905163861811,
      "learning_rate": 1.7887824548540824e-05,
      "loss": 0.0565,
      "step": 295000
    },
    {
      "epoch": 0.9406926889912497,
      "grad_norm": 6.661741645075381e-05,
      "learning_rate": 1.7792193302625075e-05,
      "loss": 0.1552,
      "step": 295100
    },
    {
      "epoch": 0.9410114598109689,
      "grad_norm": 1.0574310181254987e-05,
      "learning_rate": 1.7696562056709325e-05,
      "loss": 0.1863,
      "step": 295200
    },
    {
      "epoch": 0.9413302306306881,
      "grad_norm": 14.094175338745117,
      "learning_rate": 1.760093081079358e-05,
      "loss": 0.2425,
      "step": 295300
    },
    {
      "epoch": 0.9416490014504072,
      "grad_norm": 0.00021131662651896477,
      "learning_rate": 1.750529956487783e-05,
      "loss": 0.1854,
      "step": 295400
    },
    {
      "epoch": 0.9419677722701264,
      "grad_norm": 0.0027961651794612408,
      "learning_rate": 1.740966831896208e-05,
      "loss": 0.1568,
      "step": 295500
    },
    {
      "epoch": 0.9422865430898456,
      "grad_norm": 0.00035135773941874504,
      "learning_rate": 1.731403707304633e-05,
      "loss": 0.1057,
      "step": 295600
    },
    {
      "epoch": 0.9426053139095647,
      "grad_norm": 0.020226355642080307,
      "learning_rate": 1.7218405827130585e-05,
      "loss": 0.0754,
      "step": 295700
    },
    {
      "epoch": 0.9429240847292839,
      "grad_norm": 0.00012796730152331293,
      "learning_rate": 1.7122774581214835e-05,
      "loss": 0.2351,
      "step": 295800
    },
    {
      "epoch": 0.943242855549003,
      "grad_norm": 0.0012450255453586578,
      "learning_rate": 1.7027143335299086e-05,
      "loss": 0.1296,
      "step": 295900
    },
    {
      "epoch": 0.9435616263687222,
      "grad_norm": 0.00010645196016412228,
      "learning_rate": 1.6931512089383336e-05,
      "loss": 0.0883,
      "step": 296000
    },
    {
      "epoch": 0.9438803971884414,
      "grad_norm": 0.0018892191583290696,
      "learning_rate": 1.6835880843467587e-05,
      "loss": 0.2393,
      "step": 296100
    },
    {
      "epoch": 0.9441991680081605,
      "grad_norm": 10.231821060180664,
      "learning_rate": 1.6740249597551837e-05,
      "loss": 0.3065,
      "step": 296200
    },
    {
      "epoch": 0.9445179388278797,
      "grad_norm": 0.0015737988287582994,
      "learning_rate": 1.6644618351636088e-05,
      "loss": 0.0885,
      "step": 296300
    },
    {
      "epoch": 0.9448367096475988,
      "grad_norm": 0.00018572769477032125,
      "learning_rate": 1.6548987105720342e-05,
      "loss": 0.1066,
      "step": 296400
    },
    {
      "epoch": 0.945155480467318,
      "grad_norm": 8.355964382644743e-06,
      "learning_rate": 1.6453355859804592e-05,
      "loss": 0.1039,
      "step": 296500
    },
    {
      "epoch": 0.9454742512870372,
      "grad_norm": 0.0006016655825078487,
      "learning_rate": 1.6357724613888843e-05,
      "loss": 0.2019,
      "step": 296600
    },
    {
      "epoch": 0.9457930221067563,
      "grad_norm": 0.006379593629390001,
      "learning_rate": 1.6262093367973093e-05,
      "loss": 0.1819,
      "step": 296700
    },
    {
      "epoch": 0.9461117929264755,
      "grad_norm": 0.00026836435426957905,
      "learning_rate": 1.6166462122057347e-05,
      "loss": 0.2208,
      "step": 296800
    },
    {
      "epoch": 0.9464305637461947,
      "grad_norm": 0.17321045696735382,
      "learning_rate": 1.6070830876141598e-05,
      "loss": 0.1409,
      "step": 296900
    },
    {
      "epoch": 0.9467493345659138,
      "grad_norm": 0.000417652860051021,
      "learning_rate": 1.5975199630225848e-05,
      "loss": 0.215,
      "step": 297000
    },
    {
      "epoch": 0.947068105385633,
      "grad_norm": 0.0007923014345578849,
      "learning_rate": 1.58795683843101e-05,
      "loss": 0.1802,
      "step": 297100
    },
    {
      "epoch": 0.9473868762053521,
      "grad_norm": 0.00011066543083870783,
      "learning_rate": 1.578393713839435e-05,
      "loss": 0.1342,
      "step": 297200
    },
    {
      "epoch": 0.9477056470250713,
      "grad_norm": 0.00012473424430936575,
      "learning_rate": 1.56883058924786e-05,
      "loss": 0.0997,
      "step": 297300
    },
    {
      "epoch": 0.9480244178447905,
      "grad_norm": 0.00014522478159051389,
      "learning_rate": 1.559267464656285e-05,
      "loss": 0.2837,
      "step": 297400
    },
    {
      "epoch": 0.9483431886645096,
      "grad_norm": 0.5035264492034912,
      "learning_rate": 1.5497043400647104e-05,
      "loss": 0.0338,
      "step": 297500
    },
    {
      "epoch": 0.9486619594842288,
      "grad_norm": 2.3260517991730012e-05,
      "learning_rate": 1.5401412154731355e-05,
      "loss": 0.1104,
      "step": 297600
    },
    {
      "epoch": 0.948980730303948,
      "grad_norm": 0.00018878008995670825,
      "learning_rate": 1.5305780908815605e-05,
      "loss": 0.1285,
      "step": 297700
    },
    {
      "epoch": 0.9492995011236671,
      "grad_norm": 0.0019632456824183464,
      "learning_rate": 1.5210149662899856e-05,
      "loss": 0.2505,
      "step": 297800
    },
    {
      "epoch": 0.9496182719433863,
      "grad_norm": 29.110126495361328,
      "learning_rate": 1.5114518416984108e-05,
      "loss": 0.1529,
      "step": 297900
    },
    {
      "epoch": 0.9499370427631054,
      "grad_norm": 0.0014563644072040915,
      "learning_rate": 1.501888717106836e-05,
      "loss": 0.1177,
      "step": 298000
    },
    {
      "epoch": 0.9502558135828246,
      "grad_norm": 0.02121993899345398,
      "learning_rate": 1.492325592515261e-05,
      "loss": 0.1957,
      "step": 298100
    },
    {
      "epoch": 0.9505745844025438,
      "grad_norm": 1.3368077278137207,
      "learning_rate": 1.4827624679236861e-05,
      "loss": 0.1244,
      "step": 298200
    },
    {
      "epoch": 0.9508933552222629,
      "grad_norm": 0.00012623195652849972,
      "learning_rate": 1.4731993433321112e-05,
      "loss": 0.0949,
      "step": 298300
    },
    {
      "epoch": 0.9512121260419821,
      "grad_norm": 0.00023675395641475916,
      "learning_rate": 1.4636362187405364e-05,
      "loss": 0.1533,
      "step": 298400
    },
    {
      "epoch": 0.9515308968617012,
      "grad_norm": 2.6068817533086985e-05,
      "learning_rate": 1.4540730941489615e-05,
      "loss": 0.1674,
      "step": 298500
    },
    {
      "epoch": 0.9518496676814204,
      "grad_norm": 0.00013301333819981664,
      "learning_rate": 1.4445099695573865e-05,
      "loss": 0.1518,
      "step": 298600
    },
    {
      "epoch": 0.9521684385011396,
      "grad_norm": 0.005699864123016596,
      "learning_rate": 1.4349468449658116e-05,
      "loss": 0.0417,
      "step": 298700
    },
    {
      "epoch": 0.9524872093208587,
      "grad_norm": 0.00014427033602260053,
      "learning_rate": 1.4253837203742368e-05,
      "loss": 0.0881,
      "step": 298800
    },
    {
      "epoch": 0.9528059801405779,
      "grad_norm": 0.0930468812584877,
      "learning_rate": 1.415820595782662e-05,
      "loss": 0.1874,
      "step": 298900
    },
    {
      "epoch": 0.9531247509602971,
      "grad_norm": 0.0022436908911913633,
      "learning_rate": 1.406257471191087e-05,
      "loss": 0.2671,
      "step": 299000
    },
    {
      "epoch": 0.9534435217800162,
      "grad_norm": 0.008926229551434517,
      "learning_rate": 1.3966943465995123e-05,
      "loss": 0.1057,
      "step": 299100
    },
    {
      "epoch": 0.9537622925997354,
      "grad_norm": 0.00021790042228531092,
      "learning_rate": 1.3871312220079373e-05,
      "loss": 0.1224,
      "step": 299200
    },
    {
      "epoch": 0.9540810634194545,
      "grad_norm": 0.00021615017612930387,
      "learning_rate": 1.3775680974163624e-05,
      "loss": 0.1197,
      "step": 299300
    },
    {
      "epoch": 0.9543998342391737,
      "grad_norm": 0.0010863557690754533,
      "learning_rate": 1.3680049728247874e-05,
      "loss": 0.1303,
      "step": 299400
    },
    {
      "epoch": 0.9547186050588929,
      "grad_norm": 4.553206599666737e-05,
      "learning_rate": 1.3584418482332127e-05,
      "loss": 0.1922,
      "step": 299500
    },
    {
      "epoch": 0.955037375878612,
      "grad_norm": 0.002199756447225809,
      "learning_rate": 1.3488787236416377e-05,
      "loss": 0.0809,
      "step": 299600
    },
    {
      "epoch": 0.9553561466983312,
      "grad_norm": 0.002199595095589757,
      "learning_rate": 1.3393155990500628e-05,
      "loss": 0.1287,
      "step": 299700
    },
    {
      "epoch": 0.9556749175180504,
      "grad_norm": 0.03324194625020027,
      "learning_rate": 1.3297524744584878e-05,
      "loss": 0.4098,
      "step": 299800
    },
    {
      "epoch": 0.9559936883377695,
      "grad_norm": 0.00011714261199813336,
      "learning_rate": 1.320189349866913e-05,
      "loss": 0.0957,
      "step": 299900
    },
    {
      "epoch": 0.9563124591574887,
      "grad_norm": 5.255691212369129e-05,
      "learning_rate": 1.3106262252753383e-05,
      "loss": 0.2293,
      "step": 300000
    },
    {
      "epoch": 0.9566312299772078,
      "grad_norm": 0.11757708340883255,
      "learning_rate": 1.3010631006837633e-05,
      "loss": 0.0668,
      "step": 300100
    },
    {
      "epoch": 0.956950000796927,
      "grad_norm": 0.00012366108421701938,
      "learning_rate": 1.2914999760921885e-05,
      "loss": 0.2889,
      "step": 300200
    },
    {
      "epoch": 0.9572687716166463,
      "grad_norm": 0.0017652094829827547,
      "learning_rate": 1.2819368515006136e-05,
      "loss": 0.1086,
      "step": 300300
    },
    {
      "epoch": 0.9575875424363653,
      "grad_norm": 6.51329755783081e-05,
      "learning_rate": 1.2723737269090386e-05,
      "loss": 0.0481,
      "step": 300400
    },
    {
      "epoch": 0.9579063132560846,
      "grad_norm": 36.508724212646484,
      "learning_rate": 1.2628106023174637e-05,
      "loss": 0.121,
      "step": 300500
    },
    {
      "epoch": 0.9582250840758036,
      "grad_norm": 0.00017368124099448323,
      "learning_rate": 1.2532474777258889e-05,
      "loss": 0.1509,
      "step": 300600
    },
    {
      "epoch": 0.9585438548955229,
      "grad_norm": 48.28497314453125,
      "learning_rate": 1.243684353134314e-05,
      "loss": 0.0744,
      "step": 300700
    },
    {
      "epoch": 0.9588626257152421,
      "grad_norm": 0.0194451455026865,
      "learning_rate": 1.234121228542739e-05,
      "loss": 0.2152,
      "step": 300800
    },
    {
      "epoch": 0.9591813965349612,
      "grad_norm": 0.004012237768620253,
      "learning_rate": 1.224558103951164e-05,
      "loss": 0.2161,
      "step": 300900
    },
    {
      "epoch": 0.9595001673546804,
      "grad_norm": 37.91436004638672,
      "learning_rate": 1.2149949793595893e-05,
      "loss": 0.2835,
      "step": 301000
    },
    {
      "epoch": 0.9598189381743996,
      "grad_norm": 0.00017942491103895009,
      "learning_rate": 1.2054318547680145e-05,
      "loss": 0.0882,
      "step": 301100
    },
    {
      "epoch": 0.9601377089941187,
      "grad_norm": 62.9083137512207,
      "learning_rate": 1.1958687301764396e-05,
      "loss": 0.1508,
      "step": 301200
    },
    {
      "epoch": 0.9604564798138379,
      "grad_norm": 48.38505172729492,
      "learning_rate": 1.1863056055848648e-05,
      "loss": 0.1835,
      "step": 301300
    },
    {
      "epoch": 0.960775250633557,
      "grad_norm": 0.00020049524027854204,
      "learning_rate": 1.1767424809932898e-05,
      "loss": 0.128,
      "step": 301400
    },
    {
      "epoch": 0.9610940214532762,
      "grad_norm": 7.946472760522738e-05,
      "learning_rate": 1.1671793564017149e-05,
      "loss": 0.1273,
      "step": 301500
    },
    {
      "epoch": 0.9614127922729954,
      "grad_norm": 0.00016611596220172942,
      "learning_rate": 1.15761623181014e-05,
      "loss": 0.1152,
      "step": 301600
    },
    {
      "epoch": 0.9617315630927145,
      "grad_norm": 1.5474144674954005e-05,
      "learning_rate": 1.1480531072185651e-05,
      "loss": 0.1759,
      "step": 301700
    },
    {
      "epoch": 0.9620503339124337,
      "grad_norm": 0.0003081324102822691,
      "learning_rate": 1.1384899826269902e-05,
      "loss": 0.1479,
      "step": 301800
    },
    {
      "epoch": 0.9623691047321529,
      "grad_norm": 0.014639514498412609,
      "learning_rate": 1.1289268580354153e-05,
      "loss": 0.1737,
      "step": 301900
    },
    {
      "epoch": 0.962687875551872,
      "grad_norm": 0.00112322682980448,
      "learning_rate": 1.1193637334438403e-05,
      "loss": 0.1406,
      "step": 302000
    },
    {
      "epoch": 0.9630066463715912,
      "grad_norm": 0.00026148525648750365,
      "learning_rate": 1.1098006088522655e-05,
      "loss": 0.0901,
      "step": 302100
    },
    {
      "epoch": 0.9633254171913103,
      "grad_norm": 4.097207056474872e-05,
      "learning_rate": 1.1002374842606907e-05,
      "loss": 0.1948,
      "step": 302200
    },
    {
      "epoch": 0.9636441880110295,
      "grad_norm": 24.3173770904541,
      "learning_rate": 1.0906743596691158e-05,
      "loss": 0.2085,
      "step": 302300
    },
    {
      "epoch": 0.9639629588307487,
      "grad_norm": 0.024040158838033676,
      "learning_rate": 1.081111235077541e-05,
      "loss": 0.0593,
      "step": 302400
    },
    {
      "epoch": 0.9642817296504678,
      "grad_norm": 0.00072863680543378,
      "learning_rate": 1.071548110485966e-05,
      "loss": 0.1964,
      "step": 302500
    },
    {
      "epoch": 0.964600500470187,
      "grad_norm": 0.002840885892510414,
      "learning_rate": 1.0619849858943911e-05,
      "loss": 0.2099,
      "step": 302600
    },
    {
      "epoch": 0.9649192712899062,
      "grad_norm": 0.00015258020721375942,
      "learning_rate": 1.0524218613028162e-05,
      "loss": 0.0746,
      "step": 302700
    },
    {
      "epoch": 0.9652380421096253,
      "grad_norm": 0.00053699582349509,
      "learning_rate": 1.0428587367112414e-05,
      "loss": 0.2211,
      "step": 302800
    },
    {
      "epoch": 0.9655568129293445,
      "grad_norm": 0.00027941810549236834,
      "learning_rate": 1.0332956121196665e-05,
      "loss": 0.1738,
      "step": 302900
    },
    {
      "epoch": 0.9658755837490636,
      "grad_norm": 0.0007033415022306144,
      "learning_rate": 1.0237324875280915e-05,
      "loss": 0.2099,
      "step": 303000
    },
    {
      "epoch": 0.9661943545687828,
      "grad_norm": 3.869833017233759e-05,
      "learning_rate": 1.0141693629365166e-05,
      "loss": 0.1075,
      "step": 303100
    },
    {
      "epoch": 0.966513125388502,
      "grad_norm": 0.0009394297958351672,
      "learning_rate": 1.0046062383449418e-05,
      "loss": 0.1718,
      "step": 303200
    },
    {
      "epoch": 0.9668318962082211,
      "grad_norm": 0.007836204953491688,
      "learning_rate": 9.95043113753367e-06,
      "loss": 0.0713,
      "step": 303300
    },
    {
      "epoch": 0.9671506670279403,
      "grad_norm": 0.00022520855418406427,
      "learning_rate": 9.85479989161792e-06,
      "loss": 0.1567,
      "step": 303400
    },
    {
      "epoch": 0.9674694378476594,
      "grad_norm": 3.574803486117162e-05,
      "learning_rate": 9.759168645702173e-06,
      "loss": 0.1488,
      "step": 303500
    },
    {
      "epoch": 0.9677882086673786,
      "grad_norm": 0.000355773838236928,
      "learning_rate": 9.663537399786423e-06,
      "loss": 0.1802,
      "step": 303600
    },
    {
      "epoch": 0.9681069794870978,
      "grad_norm": 0.0037656875792890787,
      "learning_rate": 9.567906153870674e-06,
      "loss": 0.0885,
      "step": 303700
    },
    {
      "epoch": 0.9684257503068169,
      "grad_norm": 0.0010111090959981084,
      "learning_rate": 9.472274907954924e-06,
      "loss": 0.1663,
      "step": 303800
    },
    {
      "epoch": 0.9687445211265361,
      "grad_norm": 7.106779230525717e-05,
      "learning_rate": 9.376643662039176e-06,
      "loss": 0.1394,
      "step": 303900
    },
    {
      "epoch": 0.9690632919462553,
      "grad_norm": 0.00024137404398061335,
      "learning_rate": 9.281012416123427e-06,
      "loss": 0.1265,
      "step": 304000
    },
    {
      "epoch": 0.9693820627659744,
      "grad_norm": 3.556813453542418e-06,
      "learning_rate": 9.18538117020768e-06,
      "loss": 0.1272,
      "step": 304100
    },
    {
      "epoch": 0.9697008335856936,
      "grad_norm": 0.021077729761600494,
      "learning_rate": 9.08974992429193e-06,
      "loss": 0.0907,
      "step": 304200
    },
    {
      "epoch": 0.9700196044054127,
      "grad_norm": 0.0001763992040650919,
      "learning_rate": 8.99411867837618e-06,
      "loss": 0.1313,
      "step": 304300
    },
    {
      "epoch": 0.9703383752251319,
      "grad_norm": 5.441608664114028e-05,
      "learning_rate": 8.898487432460432e-06,
      "loss": 0.1275,
      "step": 304400
    },
    {
      "epoch": 0.9706571460448511,
      "grad_norm": 0.0010133159812539816,
      "learning_rate": 8.802856186544683e-06,
      "loss": 0.2431,
      "step": 304500
    },
    {
      "epoch": 0.9709759168645702,
      "grad_norm": 0.0007693827501498163,
      "learning_rate": 8.707224940628933e-06,
      "loss": 0.0778,
      "step": 304600
    },
    {
      "epoch": 0.9712946876842894,
      "grad_norm": 54.42509078979492,
      "learning_rate": 8.611593694713186e-06,
      "loss": 0.1145,
      "step": 304700
    },
    {
      "epoch": 0.9716134585040086,
      "grad_norm": 6.112510891398415e-05,
      "learning_rate": 8.515962448797436e-06,
      "loss": 0.147,
      "step": 304800
    },
    {
      "epoch": 0.9719322293237277,
      "grad_norm": 0.00022483934299089015,
      "learning_rate": 8.420331202881687e-06,
      "loss": 0.1826,
      "step": 304900
    },
    {
      "epoch": 0.9722510001434469,
      "grad_norm": 0.015884364023804665,
      "learning_rate": 8.324699956965939e-06,
      "loss": 0.043,
      "step": 305000
    },
    {
      "epoch": 0.972569770963166,
      "grad_norm": 0.0009359153336845338,
      "learning_rate": 8.22906871105019e-06,
      "loss": 0.1714,
      "step": 305100
    },
    {
      "epoch": 0.9728885417828852,
      "grad_norm": 0.0002807644777931273,
      "learning_rate": 8.133437465134442e-06,
      "loss": 0.0796,
      "step": 305200
    },
    {
      "epoch": 0.9732073126026044,
      "grad_norm": 0.10216431319713593,
      "learning_rate": 8.037806219218692e-06,
      "loss": 0.3265,
      "step": 305300
    },
    {
      "epoch": 0.9735260834223235,
      "grad_norm": 0.00020777643658220768,
      "learning_rate": 7.942174973302943e-06,
      "loss": 0.1762,
      "step": 305400
    },
    {
      "epoch": 0.9738448542420427,
      "grad_norm": 0.22695571184158325,
      "learning_rate": 7.846543727387195e-06,
      "loss": 0.225,
      "step": 305500
    },
    {
      "epoch": 0.9741636250617618,
      "grad_norm": 0.011000064201653004,
      "learning_rate": 7.750912481471445e-06,
      "loss": 0.0753,
      "step": 305600
    },
    {
      "epoch": 0.974482395881481,
      "grad_norm": 0.0034183075185865164,
      "learning_rate": 7.655281235555696e-06,
      "loss": 0.1501,
      "step": 305700
    },
    {
      "epoch": 0.9748011667012002,
      "grad_norm": 2.6941745281219482,
      "learning_rate": 7.559649989639948e-06,
      "loss": 0.0931,
      "step": 305800
    },
    {
      "epoch": 0.9751199375209193,
      "grad_norm": 0.024384116753935814,
      "learning_rate": 7.464018743724199e-06,
      "loss": 0.1734,
      "step": 305900
    },
    {
      "epoch": 0.9754387083406385,
      "grad_norm": 9.060060983756557e-05,
      "learning_rate": 7.36838749780845e-06,
      "loss": 0.1582,
      "step": 306000
    },
    {
      "epoch": 0.9757574791603577,
      "grad_norm": 7.453614671248943e-05,
      "learning_rate": 7.272756251892701e-06,
      "loss": 0.0342,
      "step": 306100
    },
    {
      "epoch": 0.9760762499800768,
      "grad_norm": 5.0170638132840395e-05,
      "learning_rate": 7.177125005976952e-06,
      "loss": 0.166,
      "step": 306200
    },
    {
      "epoch": 0.976395020799796,
      "grad_norm": 125.25852966308594,
      "learning_rate": 7.081493760061204e-06,
      "loss": 0.1291,
      "step": 306300
    },
    {
      "epoch": 0.9767137916195151,
      "grad_norm": 24.95402717590332,
      "learning_rate": 6.985862514145455e-06,
      "loss": 0.1492,
      "step": 306400
    },
    {
      "epoch": 0.9770325624392343,
      "grad_norm": 4.26303522544913e-05,
      "learning_rate": 6.890231268229706e-06,
      "loss": 0.2167,
      "step": 306500
    },
    {
      "epoch": 0.9773513332589535,
      "grad_norm": 2.3286054783966392e-05,
      "learning_rate": 6.7946000223139566e-06,
      "loss": 0.181,
      "step": 306600
    },
    {
      "epoch": 0.9776701040786726,
      "grad_norm": 4.893328666687012,
      "learning_rate": 6.698968776398208e-06,
      "loss": 0.1729,
      "step": 306700
    },
    {
      "epoch": 0.9779888748983918,
      "grad_norm": 0.00011281592742307112,
      "learning_rate": 6.603337530482459e-06,
      "loss": 0.066,
      "step": 306800
    },
    {
      "epoch": 0.978307645718111,
      "grad_norm": 20.838699340820312,
      "learning_rate": 6.507706284566711e-06,
      "loss": 0.1097,
      "step": 306900
    },
    {
      "epoch": 0.9786264165378301,
      "grad_norm": 0.20334067940711975,
      "learning_rate": 6.412075038650961e-06,
      "loss": 0.1051,
      "step": 307000
    },
    {
      "epoch": 0.9789451873575493,
      "grad_norm": 0.0012307455763220787,
      "learning_rate": 6.3164437927352126e-06,
      "loss": 0.1052,
      "step": 307100
    },
    {
      "epoch": 0.9792639581772684,
      "grad_norm": 0.00033088753116317093,
      "learning_rate": 6.220812546819463e-06,
      "loss": 0.0841,
      "step": 307200
    },
    {
      "epoch": 0.9795827289969876,
      "grad_norm": 0.0002993286761920899,
      "learning_rate": 6.1251813009037144e-06,
      "loss": 0.1574,
      "step": 307300
    },
    {
      "epoch": 0.9799014998167068,
      "grad_norm": 5.964151205262169e-05,
      "learning_rate": 6.029550054987967e-06,
      "loss": 0.2829,
      "step": 307400
    },
    {
      "epoch": 0.9802202706364259,
      "grad_norm": 6.438491982407868e-05,
      "learning_rate": 5.933918809072217e-06,
      "loss": 0.1202,
      "step": 307500
    },
    {
      "epoch": 0.9805390414561451,
      "grad_norm": 8.783311204751953e-05,
      "learning_rate": 5.8382875631564685e-06,
      "loss": 0.152,
      "step": 307600
    },
    {
      "epoch": 0.9808578122758642,
      "grad_norm": 0.023012254387140274,
      "learning_rate": 5.742656317240719e-06,
      "loss": 0.2832,
      "step": 307700
    },
    {
      "epoch": 0.9811765830955834,
      "grad_norm": 0.0026844115927815437,
      "learning_rate": 5.6470250713249704e-06,
      "loss": 0.074,
      "step": 307800
    },
    {
      "epoch": 0.9814953539153026,
      "grad_norm": 9.94371876004152e-05,
      "learning_rate": 5.551393825409222e-06,
      "loss": 0.0864,
      "step": 307900
    },
    {
      "epoch": 0.9818141247350217,
      "grad_norm": 19.84101676940918,
      "learning_rate": 5.455762579493473e-06,
      "loss": 0.1466,
      "step": 308000
    },
    {
      "epoch": 0.9821328955547409,
      "grad_norm": 0.000436523521784693,
      "learning_rate": 5.360131333577724e-06,
      "loss": 0.2375,
      "step": 308100
    },
    {
      "epoch": 0.9824516663744601,
      "grad_norm": 23.241418838500977,
      "learning_rate": 5.264500087661975e-06,
      "loss": 0.1432,
      "step": 308200
    },
    {
      "epoch": 0.9827704371941792,
      "grad_norm": 0.0005031016189604998,
      "learning_rate": 5.1688688417462256e-06,
      "loss": 0.1259,
      "step": 308300
    },
    {
      "epoch": 0.9830892080138984,
      "grad_norm": 0.04860889911651611,
      "learning_rate": 5.073237595830477e-06,
      "loss": 0.1679,
      "step": 308400
    },
    {
      "epoch": 0.9834079788336175,
      "grad_norm": 0.27251964807510376,
      "learning_rate": 4.977606349914729e-06,
      "loss": 0.245,
      "step": 308500
    },
    {
      "epoch": 0.9837267496533367,
      "grad_norm": 1.0911752724496182e-05,
      "learning_rate": 4.88197510399898e-06,
      "loss": 0.1167,
      "step": 308600
    },
    {
      "epoch": 0.9840455204730559,
      "grad_norm": 21.613445281982422,
      "learning_rate": 4.786343858083231e-06,
      "loss": 0.1874,
      "step": 308700
    },
    {
      "epoch": 0.984364291292775,
      "grad_norm": 0.0001779230951797217,
      "learning_rate": 4.6907126121674815e-06,
      "loss": 0.1238,
      "step": 308800
    },
    {
      "epoch": 0.9846830621124942,
      "grad_norm": 0.3125896751880646,
      "learning_rate": 4.595081366251733e-06,
      "loss": 0.0886,
      "step": 308900
    },
    {
      "epoch": 0.9850018329322134,
      "grad_norm": 5.595888978859875e-06,
      "learning_rate": 4.499450120335984e-06,
      "loss": 0.1627,
      "step": 309000
    },
    {
      "epoch": 0.9853206037519325,
      "grad_norm": 0.0019405401544645429,
      "learning_rate": 4.403818874420235e-06,
      "loss": 0.3117,
      "step": 309100
    },
    {
      "epoch": 0.9856393745716517,
      "grad_norm": 0.0005892256740480661,
      "learning_rate": 4.308187628504486e-06,
      "loss": 0.1121,
      "step": 309200
    },
    {
      "epoch": 0.9859581453913708,
      "grad_norm": 0.000336617260472849,
      "learning_rate": 4.2125563825887375e-06,
      "loss": 0.2163,
      "step": 309300
    },
    {
      "epoch": 0.98627691621109,
      "grad_norm": 0.001923191943205893,
      "learning_rate": 4.116925136672989e-06,
      "loss": 0.1424,
      "step": 309400
    },
    {
      "epoch": 0.9865956870308092,
      "grad_norm": 0.0002986917388625443,
      "learning_rate": 4.021293890757239e-06,
      "loss": 0.3974,
      "step": 309500
    },
    {
      "epoch": 0.9869144578505283,
      "grad_norm": 0.0003760033578146249,
      "learning_rate": 3.925662644841491e-06,
      "loss": 0.2705,
      "step": 309600
    },
    {
      "epoch": 0.9872332286702475,
      "grad_norm": 0.04529226943850517,
      "learning_rate": 3.830031398925742e-06,
      "loss": 0.1291,
      "step": 309700
    },
    {
      "epoch": 0.9875519994899666,
      "grad_norm": 0.00022680513211525977,
      "learning_rate": 3.734400153009993e-06,
      "loss": 0.2819,
      "step": 309800
    },
    {
      "epoch": 0.9878707703096858,
      "grad_norm": 0.0017138764960691333,
      "learning_rate": 3.638768907094244e-06,
      "loss": 0.2387,
      "step": 309900
    },
    {
      "epoch": 0.988189541129405,
      "grad_norm": 0.0032627384644001722,
      "learning_rate": 3.5431376611784954e-06,
      "loss": 0.1089,
      "step": 310000
    },
    {
      "epoch": 0.9885083119491241,
      "grad_norm": 0.00046959397150203586,
      "learning_rate": 3.4475064152627463e-06,
      "loss": 0.0682,
      "step": 310100
    },
    {
      "epoch": 0.9888270827688433,
      "grad_norm": 0.002258323598653078,
      "learning_rate": 3.351875169346998e-06,
      "loss": 0.1595,
      "step": 310200
    },
    {
      "epoch": 0.9891458535885626,
      "grad_norm": 0.00019075693853665143,
      "learning_rate": 3.256243923431249e-06,
      "loss": 0.2671,
      "step": 310300
    },
    {
      "epoch": 0.9894646244082816,
      "grad_norm": 0.17086558043956757,
      "learning_rate": 3.1606126775155e-06,
      "loss": 0.056,
      "step": 310400
    },
    {
      "epoch": 0.9897833952280009,
      "grad_norm": 5.724203947465867e-05,
      "learning_rate": 3.0649814315997514e-06,
      "loss": 0.0845,
      "step": 310500
    },
    {
      "epoch": 0.99010216604772,
      "grad_norm": 0.0031379368156194687,
      "learning_rate": 2.9693501856840023e-06,
      "loss": 0.1514,
      "step": 310600
    },
    {
      "epoch": 0.9904209368674392,
      "grad_norm": 11.125232696533203,
      "learning_rate": 2.8737189397682533e-06,
      "loss": 0.2342,
      "step": 310700
    },
    {
      "epoch": 0.9907397076871584,
      "grad_norm": 0.00013345769548323005,
      "learning_rate": 2.7780876938525046e-06,
      "loss": 0.0856,
      "step": 310800
    },
    {
      "epoch": 0.9910584785068774,
      "grad_norm": 0.0032820014748722315,
      "learning_rate": 2.6824564479367556e-06,
      "loss": 0.0525,
      "step": 310900
    },
    {
      "epoch": 0.9913772493265967,
      "grad_norm": 0.043598514050245285,
      "learning_rate": 2.5868252020210065e-06,
      "loss": 0.074,
      "step": 311000
    },
    {
      "epoch": 0.9916960201463159,
      "grad_norm": 151.29150390625,
      "learning_rate": 2.491193956105258e-06,
      "loss": 0.0995,
      "step": 311100
    },
    {
      "epoch": 0.992014790966035,
      "grad_norm": 0.00022303155856207013,
      "learning_rate": 2.395562710189509e-06,
      "loss": 0.2236,
      "step": 311200
    },
    {
      "epoch": 0.9923335617857542,
      "grad_norm": 0.006302230060100555,
      "learning_rate": 2.29993146427376e-06,
      "loss": 0.1136,
      "step": 311300
    },
    {
      "epoch": 0.9926523326054733,
      "grad_norm": 0.00022642490512225777,
      "learning_rate": 2.2043002183580116e-06,
      "loss": 0.1424,
      "step": 311400
    },
    {
      "epoch": 0.9929711034251925,
      "grad_norm": 0.11957623809576035,
      "learning_rate": 2.1086689724422625e-06,
      "loss": 0.0541,
      "step": 311500
    },
    {
      "epoch": 0.9932898742449117,
      "grad_norm": 0.0013806490460410714,
      "learning_rate": 2.0130377265265134e-06,
      "loss": 0.0704,
      "step": 311600
    },
    {
      "epoch": 0.9936086450646308,
      "grad_norm": 62.5367317199707,
      "learning_rate": 1.917406480610765e-06,
      "loss": 0.0946,
      "step": 311700
    },
    {
      "epoch": 0.99392741588435,
      "grad_norm": 0.06935183703899384,
      "learning_rate": 1.821775234695016e-06,
      "loss": 0.0955,
      "step": 311800
    },
    {
      "epoch": 0.9942461867040691,
      "grad_norm": 5.1791805162793025e-05,
      "learning_rate": 1.7261439887792671e-06,
      "loss": 0.0718,
      "step": 311900
    },
    {
      "epoch": 0.9945649575237883,
      "grad_norm": 0.0006468201172538102,
      "learning_rate": 1.630512742863518e-06,
      "loss": 0.0954,
      "step": 312000
    },
    {
      "epoch": 0.9948837283435075,
      "grad_norm": 4.182782504358329e-05,
      "learning_rate": 1.5348814969477692e-06,
      "loss": 0.1807,
      "step": 312100
    },
    {
      "epoch": 0.9952024991632266,
      "grad_norm": 0.3072739839553833,
      "learning_rate": 1.4392502510320204e-06,
      "loss": 0.1515,
      "step": 312200
    },
    {
      "epoch": 0.9955212699829458,
      "grad_norm": 0.004255634266883135,
      "learning_rate": 1.3436190051162713e-06,
      "loss": 0.1048,
      "step": 312300
    },
    {
      "epoch": 0.995840040802665,
      "grad_norm": 0.10155512392520905,
      "learning_rate": 1.2479877592005227e-06,
      "loss": 0.1807,
      "step": 312400
    },
    {
      "epoch": 0.9961588116223841,
      "grad_norm": 0.0010335659608244896,
      "learning_rate": 1.1523565132847738e-06,
      "loss": 0.1325,
      "step": 312500
    },
    {
      "epoch": 0.9964775824421033,
      "grad_norm": 2.075806332868524e-05,
      "learning_rate": 1.056725267369025e-06,
      "loss": 0.1293,
      "step": 312600
    },
    {
      "epoch": 0.9967963532618224,
      "grad_norm": 0.0008819145732559264,
      "learning_rate": 9.610940214532761e-07,
      "loss": 0.0397,
      "step": 312700
    },
    {
      "epoch": 0.9971151240815416,
      "grad_norm": 0.0001794052659533918,
      "learning_rate": 8.654627755375272e-07,
      "loss": 0.2301,
      "step": 312800
    },
    {
      "epoch": 0.9974338949012608,
      "grad_norm": 0.00030387641163542867,
      "learning_rate": 7.698315296217783e-07,
      "loss": 0.1341,
      "step": 312900
    },
    {
      "epoch": 0.9977526657209799,
      "grad_norm": 0.00011464115232229233,
      "learning_rate": 6.742002837060295e-07,
      "loss": 0.1441,
      "step": 313000
    },
    {
      "epoch": 0.9980714365406991,
      "grad_norm": 0.00024305848637595773,
      "learning_rate": 5.785690377902805e-07,
      "loss": 0.1009,
      "step": 313100
    },
    {
      "epoch": 0.9983902073604183,
      "grad_norm": 19.941068649291992,
      "learning_rate": 4.829377918745317e-07,
      "loss": 0.1641,
      "step": 313200
    },
    {
      "epoch": 0.9987089781801374,
      "grad_norm": 5.798312486149371e-05,
      "learning_rate": 3.8730654595878286e-07,
      "loss": 0.1155,
      "step": 313300
    },
    {
      "epoch": 0.9990277489998566,
      "grad_norm": 0.00020667666103690863,
      "learning_rate": 2.91675300043034e-07,
      "loss": 0.1876,
      "step": 313400
    },
    {
      "epoch": 0.9993465198195757,
      "grad_norm": 4.2481173295527697e-05,
      "learning_rate": 1.9604405412728517e-07,
      "loss": 0.2196,
      "step": 313500
    },
    {
      "epoch": 0.9996652906392949,
      "grad_norm": 0.009511261247098446,
      "learning_rate": 1.0041280821153629e-07,
      "loss": 0.2029,
      "step": 313600
    },
    {
      "epoch": 0.9999840614590141,
      "grad_norm": 0.00027682719519361854,
      "learning_rate": 4.781562295787443e-09,
      "loss": 0.1352,
      "step": 313700
    }
  ],
  "logging_steps": 100,
  "max_steps": 313705,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2708361170940104e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
