{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 282532,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00035394220831622613,
      "grad_norm": 3.0797128677368164,
      "learning_rate": 0.0002998938173375051,
      "loss": 3.6282,
      "step": 100
    },
    {
      "epoch": 0.0007078844166324523,
      "grad_norm": 4.113222122192383,
      "learning_rate": 0.00029978763467501023,
      "loss": 3.6175,
      "step": 200
    },
    {
      "epoch": 0.0010618266249486784,
      "grad_norm": 3.9394683837890625,
      "learning_rate": 0.00029968145201251535,
      "loss": 3.6304,
      "step": 300
    },
    {
      "epoch": 0.0014157688332649045,
      "grad_norm": 5.143721103668213,
      "learning_rate": 0.0002995752693500205,
      "loss": 3.5708,
      "step": 400
    },
    {
      "epoch": 0.0017697110415811307,
      "grad_norm": 3.3231332302093506,
      "learning_rate": 0.00029946908668752566,
      "loss": 3.5044,
      "step": 500
    },
    {
      "epoch": 0.002123653249897357,
      "grad_norm": 4.880591869354248,
      "learning_rate": 0.00029936290402503074,
      "loss": 3.3404,
      "step": 600
    },
    {
      "epoch": 0.002477595458213583,
      "grad_norm": 7.059172630310059,
      "learning_rate": 0.0002992567213625359,
      "loss": 3.3829,
      "step": 700
    },
    {
      "epoch": 0.002831537666529809,
      "grad_norm": 8.619158744812012,
      "learning_rate": 0.00029915053870004104,
      "loss": 3.101,
      "step": 800
    },
    {
      "epoch": 0.003185479874846035,
      "grad_norm": 4.6344828605651855,
      "learning_rate": 0.00029904435603754617,
      "loss": 3.2229,
      "step": 900
    },
    {
      "epoch": 0.0035394220831622613,
      "grad_norm": 5.523018836975098,
      "learning_rate": 0.0002989381733750513,
      "loss": 3.125,
      "step": 1000
    },
    {
      "epoch": 0.0038933642914784875,
      "grad_norm": 6.216672420501709,
      "learning_rate": 0.0002988319907125564,
      "loss": 3.1056,
      "step": 1100
    },
    {
      "epoch": 0.004247306499794714,
      "grad_norm": 7.5100555419921875,
      "learning_rate": 0.00029872580805006155,
      "loss": 3.1128,
      "step": 1200
    },
    {
      "epoch": 0.004601248708110939,
      "grad_norm": 6.958288669586182,
      "learning_rate": 0.0002986196253875667,
      "loss": 3.1163,
      "step": 1300
    },
    {
      "epoch": 0.004955190916427166,
      "grad_norm": 7.23723840713501,
      "learning_rate": 0.0002985134427250718,
      "loss": 2.917,
      "step": 1400
    },
    {
      "epoch": 0.0053091331247433916,
      "grad_norm": 11.84220027923584,
      "learning_rate": 0.000298407260062577,
      "loss": 3.031,
      "step": 1500
    },
    {
      "epoch": 0.005663075333059618,
      "grad_norm": 7.752233982086182,
      "learning_rate": 0.00029830107740008206,
      "loss": 2.8024,
      "step": 1600
    },
    {
      "epoch": 0.006017017541375844,
      "grad_norm": 6.60910177230835,
      "learning_rate": 0.00029819489473758724,
      "loss": 2.874,
      "step": 1700
    },
    {
      "epoch": 0.00637095974969207,
      "grad_norm": 6.26533842086792,
      "learning_rate": 0.00029808871207509237,
      "loss": 2.8763,
      "step": 1800
    },
    {
      "epoch": 0.006724901958008296,
      "grad_norm": 7.51314115524292,
      "learning_rate": 0.0002979825294125975,
      "loss": 2.8022,
      "step": 1900
    },
    {
      "epoch": 0.007078844166324523,
      "grad_norm": 9.490415573120117,
      "learning_rate": 0.0002978763467501026,
      "loss": 2.8601,
      "step": 2000
    },
    {
      "epoch": 0.007432786374640748,
      "grad_norm": 9.356842994689941,
      "learning_rate": 0.00029777016408760775,
      "loss": 2.6041,
      "step": 2100
    },
    {
      "epoch": 0.007786728582956975,
      "grad_norm": 10.05516529083252,
      "learning_rate": 0.0002976639814251129,
      "loss": 2.7229,
      "step": 2200
    },
    {
      "epoch": 0.0081406707912732,
      "grad_norm": 8.622462272644043,
      "learning_rate": 0.000297557798762618,
      "loss": 2.4552,
      "step": 2300
    },
    {
      "epoch": 0.008494612999589427,
      "grad_norm": 9.771451950073242,
      "learning_rate": 0.00029745161610012313,
      "loss": 2.6185,
      "step": 2400
    },
    {
      "epoch": 0.008848555207905654,
      "grad_norm": 15.872042655944824,
      "learning_rate": 0.0002973454334376283,
      "loss": 2.6246,
      "step": 2500
    },
    {
      "epoch": 0.009202497416221879,
      "grad_norm": 8.367987632751465,
      "learning_rate": 0.0002972392507751334,
      "loss": 2.4065,
      "step": 2600
    },
    {
      "epoch": 0.009556439624538105,
      "grad_norm": 10.240523338317871,
      "learning_rate": 0.00029713306811263857,
      "loss": 2.3983,
      "step": 2700
    },
    {
      "epoch": 0.009910381832854332,
      "grad_norm": 7.8997392654418945,
      "learning_rate": 0.0002970268854501437,
      "loss": 2.2866,
      "step": 2800
    },
    {
      "epoch": 0.010264324041170558,
      "grad_norm": 27.71622657775879,
      "learning_rate": 0.0002969207027876488,
      "loss": 2.6073,
      "step": 2900
    },
    {
      "epoch": 0.010618266249486783,
      "grad_norm": 7.670232772827148,
      "learning_rate": 0.00029681452012515395,
      "loss": 2.3259,
      "step": 3000
    },
    {
      "epoch": 0.01097220845780301,
      "grad_norm": 8.467453002929688,
      "learning_rate": 0.0002967083374626591,
      "loss": 2.4351,
      "step": 3100
    },
    {
      "epoch": 0.011326150666119236,
      "grad_norm": 6.601491928100586,
      "learning_rate": 0.0002966021548001642,
      "loss": 2.2515,
      "step": 3200
    },
    {
      "epoch": 0.011680092874435463,
      "grad_norm": 10.33377456665039,
      "learning_rate": 0.00029649597213766933,
      "loss": 2.2346,
      "step": 3300
    },
    {
      "epoch": 0.012034035082751688,
      "grad_norm": 8.096602439880371,
      "learning_rate": 0.00029638978947517445,
      "loss": 2.194,
      "step": 3400
    },
    {
      "epoch": 0.012387977291067914,
      "grad_norm": 14.41287899017334,
      "learning_rate": 0.0002962836068126796,
      "loss": 2.0986,
      "step": 3500
    },
    {
      "epoch": 0.01274191949938414,
      "grad_norm": 5.808352470397949,
      "learning_rate": 0.0002961774241501847,
      "loss": 2.0843,
      "step": 3600
    },
    {
      "epoch": 0.013095861707700367,
      "grad_norm": 4.728914260864258,
      "learning_rate": 0.00029607124148768984,
      "loss": 2.109,
      "step": 3700
    },
    {
      "epoch": 0.013449803916016592,
      "grad_norm": 13.819183349609375,
      "learning_rate": 0.000295965058825195,
      "loss": 2.0717,
      "step": 3800
    },
    {
      "epoch": 0.013803746124332819,
      "grad_norm": 10.592469215393066,
      "learning_rate": 0.00029585887616270014,
      "loss": 2.1106,
      "step": 3900
    },
    {
      "epoch": 0.014157688332649045,
      "grad_norm": 9.214916229248047,
      "learning_rate": 0.00029575269350020527,
      "loss": 1.9725,
      "step": 4000
    },
    {
      "epoch": 0.014511630540965272,
      "grad_norm": 8.51617431640625,
      "learning_rate": 0.0002956465108377104,
      "loss": 1.9478,
      "step": 4100
    },
    {
      "epoch": 0.014865572749281497,
      "grad_norm": 2.431062936782837,
      "learning_rate": 0.0002955403281752155,
      "loss": 1.9216,
      "step": 4200
    },
    {
      "epoch": 0.015219514957597723,
      "grad_norm": 12.873579025268555,
      "learning_rate": 0.00029543414551272065,
      "loss": 2.0362,
      "step": 4300
    },
    {
      "epoch": 0.01557345716591395,
      "grad_norm": 8.406074523925781,
      "learning_rate": 0.0002953279628502258,
      "loss": 1.9523,
      "step": 4400
    },
    {
      "epoch": 0.015927399374230176,
      "grad_norm": 8.31376838684082,
      "learning_rate": 0.0002952217801877309,
      "loss": 1.8886,
      "step": 4500
    },
    {
      "epoch": 0.0162813415825464,
      "grad_norm": 29.916637420654297,
      "learning_rate": 0.0002951155975252361,
      "loss": 1.9543,
      "step": 4600
    },
    {
      "epoch": 0.01663528379086263,
      "grad_norm": 15.630803108215332,
      "learning_rate": 0.00029500941486274116,
      "loss": 1.8607,
      "step": 4700
    },
    {
      "epoch": 0.016989225999178854,
      "grad_norm": 15.898445129394531,
      "learning_rate": 0.00029490323220024634,
      "loss": 2.1249,
      "step": 4800
    },
    {
      "epoch": 0.01734316820749508,
      "grad_norm": 7.908193588256836,
      "learning_rate": 0.00029479704953775147,
      "loss": 1.8706,
      "step": 4900
    },
    {
      "epoch": 0.017697110415811308,
      "grad_norm": 11.604695320129395,
      "learning_rate": 0.0002946908668752566,
      "loss": 2.0096,
      "step": 5000
    },
    {
      "epoch": 0.018051052624127532,
      "grad_norm": 21.944019317626953,
      "learning_rate": 0.0002945846842127617,
      "loss": 1.9115,
      "step": 5100
    },
    {
      "epoch": 0.018404994832443757,
      "grad_norm": 8.529573440551758,
      "learning_rate": 0.00029447850155026685,
      "loss": 1.907,
      "step": 5200
    },
    {
      "epoch": 0.018758937040759985,
      "grad_norm": 1.1504565477371216,
      "learning_rate": 0.000294372318887772,
      "loss": 1.7653,
      "step": 5300
    },
    {
      "epoch": 0.01911287924907621,
      "grad_norm": 5.317996978759766,
      "learning_rate": 0.0002942661362252771,
      "loss": 1.7028,
      "step": 5400
    },
    {
      "epoch": 0.01946682145739244,
      "grad_norm": 26.590600967407227,
      "learning_rate": 0.00029415995356278223,
      "loss": 1.6346,
      "step": 5500
    },
    {
      "epoch": 0.019820763665708663,
      "grad_norm": 12.673898696899414,
      "learning_rate": 0.0002940537709002874,
      "loss": 2.0068,
      "step": 5600
    },
    {
      "epoch": 0.02017470587402489,
      "grad_norm": 7.654232501983643,
      "learning_rate": 0.0002939475882377925,
      "loss": 1.5373,
      "step": 5700
    },
    {
      "epoch": 0.020528648082341117,
      "grad_norm": 15.375248908996582,
      "learning_rate": 0.00029384140557529766,
      "loss": 1.8346,
      "step": 5800
    },
    {
      "epoch": 0.02088259029065734,
      "grad_norm": 9.815024375915527,
      "learning_rate": 0.0002937352229128028,
      "loss": 1.6168,
      "step": 5900
    },
    {
      "epoch": 0.021236532498973566,
      "grad_norm": 5.968212604522705,
      "learning_rate": 0.00029362904025030786,
      "loss": 1.7675,
      "step": 6000
    },
    {
      "epoch": 0.021590474707289795,
      "grad_norm": 9.886685371398926,
      "learning_rate": 0.00029352285758781305,
      "loss": 1.7511,
      "step": 6100
    },
    {
      "epoch": 0.02194441691560602,
      "grad_norm": 13.78359603881836,
      "learning_rate": 0.0002934166749253182,
      "loss": 1.7813,
      "step": 6200
    },
    {
      "epoch": 0.022298359123922248,
      "grad_norm": 26.587677001953125,
      "learning_rate": 0.0002933104922628233,
      "loss": 1.7906,
      "step": 6300
    },
    {
      "epoch": 0.022652301332238473,
      "grad_norm": 7.4112772941589355,
      "learning_rate": 0.0002932043096003284,
      "loss": 1.8134,
      "step": 6400
    },
    {
      "epoch": 0.023006243540554697,
      "grad_norm": 15.605725288391113,
      "learning_rate": 0.00029309812693783355,
      "loss": 1.5291,
      "step": 6500
    },
    {
      "epoch": 0.023360185748870926,
      "grad_norm": 40.20622253417969,
      "learning_rate": 0.00029299194427533874,
      "loss": 1.5366,
      "step": 6600
    },
    {
      "epoch": 0.02371412795718715,
      "grad_norm": 7.45778226852417,
      "learning_rate": 0.0002928857616128438,
      "loss": 1.7849,
      "step": 6700
    },
    {
      "epoch": 0.024068070165503375,
      "grad_norm": 15.388608932495117,
      "learning_rate": 0.00029277957895034893,
      "loss": 1.6504,
      "step": 6800
    },
    {
      "epoch": 0.024422012373819604,
      "grad_norm": 32.289703369140625,
      "learning_rate": 0.0002926733962878541,
      "loss": 1.4456,
      "step": 6900
    },
    {
      "epoch": 0.02477595458213583,
      "grad_norm": 0.38684284687042236,
      "learning_rate": 0.0002925672136253592,
      "loss": 1.5297,
      "step": 7000
    },
    {
      "epoch": 0.025129896790452053,
      "grad_norm": 58.42936706542969,
      "learning_rate": 0.00029246103096286437,
      "loss": 1.3163,
      "step": 7100
    },
    {
      "epoch": 0.02548383899876828,
      "grad_norm": 69.41352844238281,
      "learning_rate": 0.0002923548483003695,
      "loss": 1.7528,
      "step": 7200
    },
    {
      "epoch": 0.025837781207084506,
      "grad_norm": 14.574505805969238,
      "learning_rate": 0.0002922486656378746,
      "loss": 1.5478,
      "step": 7300
    },
    {
      "epoch": 0.026191723415400735,
      "grad_norm": 14.325804710388184,
      "learning_rate": 0.00029214248297537975,
      "loss": 1.5092,
      "step": 7400
    },
    {
      "epoch": 0.02654566562371696,
      "grad_norm": 1.500665307044983,
      "learning_rate": 0.0002920363003128849,
      "loss": 1.1658,
      "step": 7500
    },
    {
      "epoch": 0.026899607832033184,
      "grad_norm": 44.22132873535156,
      "learning_rate": 0.00029193011765039,
      "loss": 1.5852,
      "step": 7600
    },
    {
      "epoch": 0.027253550040349413,
      "grad_norm": 1.6329479217529297,
      "learning_rate": 0.00029182393498789513,
      "loss": 1.215,
      "step": 7700
    },
    {
      "epoch": 0.027607492248665638,
      "grad_norm": 0.3768845498561859,
      "learning_rate": 0.00029171775232540026,
      "loss": 1.4627,
      "step": 7800
    },
    {
      "epoch": 0.027961434456981862,
      "grad_norm": 20.475847244262695,
      "learning_rate": 0.00029161156966290544,
      "loss": 1.322,
      "step": 7900
    },
    {
      "epoch": 0.02831537666529809,
      "grad_norm": 22.61309242248535,
      "learning_rate": 0.00029150538700041057,
      "loss": 1.5386,
      "step": 8000
    },
    {
      "epoch": 0.028669318873614315,
      "grad_norm": 18.16078758239746,
      "learning_rate": 0.0002913992043379157,
      "loss": 1.3812,
      "step": 8100
    },
    {
      "epoch": 0.029023261081930544,
      "grad_norm": 40.858951568603516,
      "learning_rate": 0.0002912930216754208,
      "loss": 1.4329,
      "step": 8200
    },
    {
      "epoch": 0.02937720329024677,
      "grad_norm": 13.172513961791992,
      "learning_rate": 0.00029118683901292595,
      "loss": 1.2561,
      "step": 8300
    },
    {
      "epoch": 0.029731145498562993,
      "grad_norm": 5.040307521820068,
      "learning_rate": 0.0002910806563504311,
      "loss": 1.5365,
      "step": 8400
    },
    {
      "epoch": 0.030085087706879222,
      "grad_norm": 13.012154579162598,
      "learning_rate": 0.0002909744736879362,
      "loss": 1.1408,
      "step": 8500
    },
    {
      "epoch": 0.030439029915195447,
      "grad_norm": 6.487624645233154,
      "learning_rate": 0.00029086829102544133,
      "loss": 1.4667,
      "step": 8600
    },
    {
      "epoch": 0.03079297212351167,
      "grad_norm": 13.047269821166992,
      "learning_rate": 0.0002907621083629465,
      "loss": 1.6294,
      "step": 8700
    },
    {
      "epoch": 0.0311469143318279,
      "grad_norm": 1.0249419212341309,
      "learning_rate": 0.0002906559257004516,
      "loss": 1.219,
      "step": 8800
    },
    {
      "epoch": 0.031500856540144125,
      "grad_norm": 16.73762321472168,
      "learning_rate": 0.00029054974303795676,
      "loss": 1.4992,
      "step": 8900
    },
    {
      "epoch": 0.03185479874846035,
      "grad_norm": 18.558317184448242,
      "learning_rate": 0.0002904435603754619,
      "loss": 1.4687,
      "step": 9000
    },
    {
      "epoch": 0.032208740956776574,
      "grad_norm": 15.460572242736816,
      "learning_rate": 0.00029033737771296696,
      "loss": 1.2322,
      "step": 9100
    },
    {
      "epoch": 0.0325626831650928,
      "grad_norm": 25.900476455688477,
      "learning_rate": 0.00029023119505047215,
      "loss": 1.5844,
      "step": 9200
    },
    {
      "epoch": 0.03291662537340903,
      "grad_norm": 9.684282302856445,
      "learning_rate": 0.00029012501238797727,
      "loss": 1.3952,
      "step": 9300
    },
    {
      "epoch": 0.03327056758172526,
      "grad_norm": 11.370343208312988,
      "learning_rate": 0.0002900188297254824,
      "loss": 1.4326,
      "step": 9400
    },
    {
      "epoch": 0.03362450979004148,
      "grad_norm": 0.3653923273086548,
      "learning_rate": 0.0002899126470629875,
      "loss": 1.5187,
      "step": 9500
    },
    {
      "epoch": 0.03397845199835771,
      "grad_norm": 11.835969924926758,
      "learning_rate": 0.00028980646440049265,
      "loss": 1.1542,
      "step": 9600
    },
    {
      "epoch": 0.03433239420667394,
      "grad_norm": 13.220891952514648,
      "learning_rate": 0.00028970028173799783,
      "loss": 1.3876,
      "step": 9700
    },
    {
      "epoch": 0.03468633641499016,
      "grad_norm": 29.33639907836914,
      "learning_rate": 0.0002895940990755029,
      "loss": 1.3197,
      "step": 9800
    },
    {
      "epoch": 0.03504027862330639,
      "grad_norm": 12.141915321350098,
      "learning_rate": 0.00028948791641300803,
      "loss": 1.336,
      "step": 9900
    },
    {
      "epoch": 0.035394220831622615,
      "grad_norm": 17.838109970092773,
      "learning_rate": 0.0002893817337505132,
      "loss": 1.5429,
      "step": 10000
    },
    {
      "epoch": 0.035748163039938836,
      "grad_norm": 18.66781234741211,
      "learning_rate": 0.0002892755510880183,
      "loss": 1.7663,
      "step": 10100
    },
    {
      "epoch": 0.036102105248255065,
      "grad_norm": 7.956620216369629,
      "learning_rate": 0.00028916936842552347,
      "loss": 1.3163,
      "step": 10200
    },
    {
      "epoch": 0.03645604745657129,
      "grad_norm": 7.164918422698975,
      "learning_rate": 0.0002890631857630286,
      "loss": 1.3151,
      "step": 10300
    },
    {
      "epoch": 0.036809989664887514,
      "grad_norm": 2.7211921215057373,
      "learning_rate": 0.0002889570031005337,
      "loss": 1.3882,
      "step": 10400
    },
    {
      "epoch": 0.03716393187320374,
      "grad_norm": 19.685302734375,
      "learning_rate": 0.00028885082043803885,
      "loss": 1.2915,
      "step": 10500
    },
    {
      "epoch": 0.03751787408151997,
      "grad_norm": 22.09730339050293,
      "learning_rate": 0.000288744637775544,
      "loss": 1.0411,
      "step": 10600
    },
    {
      "epoch": 0.03787181628983619,
      "grad_norm": 19.00144386291504,
      "learning_rate": 0.0002886384551130491,
      "loss": 1.309,
      "step": 10700
    },
    {
      "epoch": 0.03822575849815242,
      "grad_norm": 51.18357467651367,
      "learning_rate": 0.00028853227245055423,
      "loss": 1.0602,
      "step": 10800
    },
    {
      "epoch": 0.03857970070646865,
      "grad_norm": 26.166988372802734,
      "learning_rate": 0.00028842608978805936,
      "loss": 1.1797,
      "step": 10900
    },
    {
      "epoch": 0.03893364291478488,
      "grad_norm": 8.63282585144043,
      "learning_rate": 0.00028831990712556454,
      "loss": 1.4961,
      "step": 11000
    },
    {
      "epoch": 0.0392875851231011,
      "grad_norm": 15.375970840454102,
      "learning_rate": 0.0002882137244630696,
      "loss": 1.3433,
      "step": 11100
    },
    {
      "epoch": 0.03964152733141733,
      "grad_norm": 7.904313564300537,
      "learning_rate": 0.0002881075418005748,
      "loss": 1.3287,
      "step": 11200
    },
    {
      "epoch": 0.039995469539733555,
      "grad_norm": 23.686647415161133,
      "learning_rate": 0.0002880013591380799,
      "loss": 1.1819,
      "step": 11300
    },
    {
      "epoch": 0.04034941174804978,
      "grad_norm": 17.656206130981445,
      "learning_rate": 0.00028789517647558505,
      "loss": 1.3782,
      "step": 11400
    },
    {
      "epoch": 0.040703353956366005,
      "grad_norm": 0.033828455954790115,
      "learning_rate": 0.0002877889938130902,
      "loss": 1.381,
      "step": 11500
    },
    {
      "epoch": 0.04105729616468223,
      "grad_norm": 23.560035705566406,
      "learning_rate": 0.0002876828111505953,
      "loss": 1.2702,
      "step": 11600
    },
    {
      "epoch": 0.041411238372998455,
      "grad_norm": 17.243709564208984,
      "learning_rate": 0.00028757662848810043,
      "loss": 1.2558,
      "step": 11700
    },
    {
      "epoch": 0.04176518058131468,
      "grad_norm": 46.2504997253418,
      "learning_rate": 0.00028747044582560556,
      "loss": 0.9156,
      "step": 11800
    },
    {
      "epoch": 0.04211912278963091,
      "grad_norm": 47.80097198486328,
      "learning_rate": 0.0002873642631631107,
      "loss": 1.2003,
      "step": 11900
    },
    {
      "epoch": 0.04247306499794713,
      "grad_norm": 30.82619285583496,
      "learning_rate": 0.00028725808050061586,
      "loss": 1.2734,
      "step": 12000
    },
    {
      "epoch": 0.04282700720626336,
      "grad_norm": 2.040473222732544,
      "learning_rate": 0.000287151897838121,
      "loss": 1.2045,
      "step": 12100
    },
    {
      "epoch": 0.04318094941457959,
      "grad_norm": 66.75481414794922,
      "learning_rate": 0.00028704571517562606,
      "loss": 0.8974,
      "step": 12200
    },
    {
      "epoch": 0.04353489162289581,
      "grad_norm": 12.863380432128906,
      "learning_rate": 0.00028693953251313125,
      "loss": 1.3089,
      "step": 12300
    },
    {
      "epoch": 0.04388883383121204,
      "grad_norm": 40.40564727783203,
      "learning_rate": 0.00028683334985063637,
      "loss": 0.9333,
      "step": 12400
    },
    {
      "epoch": 0.04424277603952827,
      "grad_norm": 1.3447076082229614,
      "learning_rate": 0.0002867271671881415,
      "loss": 1.3889,
      "step": 12500
    },
    {
      "epoch": 0.044596718247844495,
      "grad_norm": 17.472658157348633,
      "learning_rate": 0.0002866209845256466,
      "loss": 1.3132,
      "step": 12600
    },
    {
      "epoch": 0.04495066045616072,
      "grad_norm": 13.471134185791016,
      "learning_rate": 0.00028651480186315175,
      "loss": 1.7105,
      "step": 12700
    },
    {
      "epoch": 0.045304602664476945,
      "grad_norm": 9.38218879699707,
      "learning_rate": 0.00028640861920065693,
      "loss": 0.9879,
      "step": 12800
    },
    {
      "epoch": 0.04565854487279317,
      "grad_norm": 0.2047397941350937,
      "learning_rate": 0.000286302436538162,
      "loss": 1.4213,
      "step": 12900
    },
    {
      "epoch": 0.046012487081109395,
      "grad_norm": 29.523601531982422,
      "learning_rate": 0.00028619625387566713,
      "loss": 1.2235,
      "step": 13000
    },
    {
      "epoch": 0.04636642928942562,
      "grad_norm": 7.938358783721924,
      "learning_rate": 0.0002860900712131723,
      "loss": 1.1484,
      "step": 13100
    },
    {
      "epoch": 0.04672037149774185,
      "grad_norm": 3.77630877494812,
      "learning_rate": 0.0002859838885506774,
      "loss": 1.0862,
      "step": 13200
    },
    {
      "epoch": 0.04707431370605807,
      "grad_norm": 7.8471760749816895,
      "learning_rate": 0.00028587770588818257,
      "loss": 1.1085,
      "step": 13300
    },
    {
      "epoch": 0.0474282559143743,
      "grad_norm": 38.30613327026367,
      "learning_rate": 0.0002857715232256877,
      "loss": 1.1979,
      "step": 13400
    },
    {
      "epoch": 0.04778219812269053,
      "grad_norm": 27.47764015197754,
      "learning_rate": 0.0002856653405631928,
      "loss": 1.3035,
      "step": 13500
    },
    {
      "epoch": 0.04813614033100675,
      "grad_norm": 37.959815979003906,
      "learning_rate": 0.00028555915790069795,
      "loss": 1.1177,
      "step": 13600
    },
    {
      "epoch": 0.04849008253932298,
      "grad_norm": 36.1486701965332,
      "learning_rate": 0.0002854529752382031,
      "loss": 1.14,
      "step": 13700
    },
    {
      "epoch": 0.04884402474763921,
      "grad_norm": 24.63286018371582,
      "learning_rate": 0.0002853467925757082,
      "loss": 0.9712,
      "step": 13800
    },
    {
      "epoch": 0.04919796695595543,
      "grad_norm": 0.05335698276758194,
      "learning_rate": 0.00028524060991321333,
      "loss": 1.1006,
      "step": 13900
    },
    {
      "epoch": 0.04955190916427166,
      "grad_norm": 16.793472290039062,
      "learning_rate": 0.00028513442725071846,
      "loss": 1.0126,
      "step": 14000
    },
    {
      "epoch": 0.049905851372587885,
      "grad_norm": 0.09081652015447617,
      "learning_rate": 0.00028502824458822364,
      "loss": 0.7525,
      "step": 14100
    },
    {
      "epoch": 0.05025979358090411,
      "grad_norm": 0.10270252078771591,
      "learning_rate": 0.0002849220619257287,
      "loss": 1.0085,
      "step": 14200
    },
    {
      "epoch": 0.050613735789220335,
      "grad_norm": 44.404415130615234,
      "learning_rate": 0.0002848158792632339,
      "loss": 1.2425,
      "step": 14300
    },
    {
      "epoch": 0.05096767799753656,
      "grad_norm": 3.904973268508911,
      "learning_rate": 0.000284709696600739,
      "loss": 1.2247,
      "step": 14400
    },
    {
      "epoch": 0.05132162020585279,
      "grad_norm": 8.798967361450195,
      "learning_rate": 0.00028460351393824415,
      "loss": 1.2301,
      "step": 14500
    },
    {
      "epoch": 0.05167556241416901,
      "grad_norm": 0.9706961512565613,
      "learning_rate": 0.0002844973312757493,
      "loss": 1.0912,
      "step": 14600
    },
    {
      "epoch": 0.05202950462248524,
      "grad_norm": 32.49786376953125,
      "learning_rate": 0.0002843911486132544,
      "loss": 1.2286,
      "step": 14700
    },
    {
      "epoch": 0.05238344683080147,
      "grad_norm": 20.275941848754883,
      "learning_rate": 0.00028428496595075953,
      "loss": 1.1236,
      "step": 14800
    },
    {
      "epoch": 0.05273738903911769,
      "grad_norm": 43.75487518310547,
      "learning_rate": 0.00028417878328826466,
      "loss": 0.8483,
      "step": 14900
    },
    {
      "epoch": 0.05309133124743392,
      "grad_norm": 7.956369876861572,
      "learning_rate": 0.0002840726006257698,
      "loss": 1.2811,
      "step": 15000
    },
    {
      "epoch": 0.05344527345575015,
      "grad_norm": 0.03267636522650719,
      "learning_rate": 0.00028396641796327496,
      "loss": 1.1886,
      "step": 15100
    },
    {
      "epoch": 0.05379921566406637,
      "grad_norm": 1.3625372648239136,
      "learning_rate": 0.00028386023530078004,
      "loss": 1.164,
      "step": 15200
    },
    {
      "epoch": 0.0541531578723826,
      "grad_norm": 14.182209968566895,
      "learning_rate": 0.00028375405263828516,
      "loss": 1.214,
      "step": 15300
    },
    {
      "epoch": 0.054507100080698825,
      "grad_norm": 24.47503662109375,
      "learning_rate": 0.00028364786997579034,
      "loss": 1.0991,
      "step": 15400
    },
    {
      "epoch": 0.05486104228901505,
      "grad_norm": 43.8753662109375,
      "learning_rate": 0.00028354168731329547,
      "loss": 1.0831,
      "step": 15500
    },
    {
      "epoch": 0.055214984497331275,
      "grad_norm": 0.006493363995105028,
      "learning_rate": 0.0002834355046508006,
      "loss": 1.2351,
      "step": 15600
    },
    {
      "epoch": 0.0555689267056475,
      "grad_norm": 4.731532573699951,
      "learning_rate": 0.0002833293219883057,
      "loss": 1.0159,
      "step": 15700
    },
    {
      "epoch": 0.055922868913963725,
      "grad_norm": 16.148164749145508,
      "learning_rate": 0.00028322313932581085,
      "loss": 1.0208,
      "step": 15800
    },
    {
      "epoch": 0.05627681112227995,
      "grad_norm": 0.08178208768367767,
      "learning_rate": 0.000283116956663316,
      "loss": 0.9666,
      "step": 15900
    },
    {
      "epoch": 0.05663075333059618,
      "grad_norm": 1.642223834991455,
      "learning_rate": 0.0002830107740008211,
      "loss": 1.1626,
      "step": 16000
    },
    {
      "epoch": 0.05698469553891241,
      "grad_norm": 0.014746417291462421,
      "learning_rate": 0.00028290459133832623,
      "loss": 1.0766,
      "step": 16100
    },
    {
      "epoch": 0.05733863774722863,
      "grad_norm": 32.012184143066406,
      "learning_rate": 0.0002827984086758314,
      "loss": 0.7811,
      "step": 16200
    },
    {
      "epoch": 0.05769257995554486,
      "grad_norm": 3.7686734199523926,
      "learning_rate": 0.0002826922260133365,
      "loss": 1.29,
      "step": 16300
    },
    {
      "epoch": 0.05804652216386109,
      "grad_norm": 11.490291595458984,
      "learning_rate": 0.00028258604335084167,
      "loss": 0.9768,
      "step": 16400
    },
    {
      "epoch": 0.05840046437217731,
      "grad_norm": 16.655330657958984,
      "learning_rate": 0.0002824798606883468,
      "loss": 0.9181,
      "step": 16500
    },
    {
      "epoch": 0.05875440658049354,
      "grad_norm": 0.257258802652359,
      "learning_rate": 0.0002823736780258519,
      "loss": 1.1655,
      "step": 16600
    },
    {
      "epoch": 0.059108348788809766,
      "grad_norm": 0.020152147859334946,
      "learning_rate": 0.00028226749536335705,
      "loss": 0.9435,
      "step": 16700
    },
    {
      "epoch": 0.05946229099712599,
      "grad_norm": 18.673410415649414,
      "learning_rate": 0.0002821613127008622,
      "loss": 1.0949,
      "step": 16800
    },
    {
      "epoch": 0.059816233205442215,
      "grad_norm": 11.97504711151123,
      "learning_rate": 0.0002820551300383673,
      "loss": 1.1086,
      "step": 16900
    },
    {
      "epoch": 0.060170175413758444,
      "grad_norm": 0.6076053977012634,
      "learning_rate": 0.00028194894737587243,
      "loss": 1.2477,
      "step": 17000
    },
    {
      "epoch": 0.060524117622074665,
      "grad_norm": 29.214859008789062,
      "learning_rate": 0.00028184276471337756,
      "loss": 0.969,
      "step": 17100
    },
    {
      "epoch": 0.06087805983039089,
      "grad_norm": 0.4889763593673706,
      "learning_rate": 0.00028173658205088274,
      "loss": 1.2085,
      "step": 17200
    },
    {
      "epoch": 0.06123200203870712,
      "grad_norm": 41.51328659057617,
      "learning_rate": 0.0002816303993883878,
      "loss": 1.2844,
      "step": 17300
    },
    {
      "epoch": 0.06158594424702334,
      "grad_norm": 0.08471062034368515,
      "learning_rate": 0.000281524216725893,
      "loss": 0.9185,
      "step": 17400
    },
    {
      "epoch": 0.06193988645533957,
      "grad_norm": 0.38175728917121887,
      "learning_rate": 0.0002814180340633981,
      "loss": 0.8955,
      "step": 17500
    },
    {
      "epoch": 0.0622938286636558,
      "grad_norm": 30.24183464050293,
      "learning_rate": 0.00028131185140090325,
      "loss": 0.8242,
      "step": 17600
    },
    {
      "epoch": 0.06264777087197203,
      "grad_norm": 36.04325485229492,
      "learning_rate": 0.0002812056687384084,
      "loss": 0.9262,
      "step": 17700
    },
    {
      "epoch": 0.06300171308028825,
      "grad_norm": 7.642167568206787,
      "learning_rate": 0.0002810994860759135,
      "loss": 0.9879,
      "step": 17800
    },
    {
      "epoch": 0.06335565528860447,
      "grad_norm": 26.759132385253906,
      "learning_rate": 0.00028099330341341863,
      "loss": 1.2594,
      "step": 17900
    },
    {
      "epoch": 0.0637095974969207,
      "grad_norm": 13.40213680267334,
      "learning_rate": 0.00028088712075092376,
      "loss": 1.474,
      "step": 18000
    },
    {
      "epoch": 0.06406353970523693,
      "grad_norm": 31.994457244873047,
      "learning_rate": 0.0002807809380884289,
      "loss": 1.3632,
      "step": 18100
    },
    {
      "epoch": 0.06441748191355315,
      "grad_norm": 18.972639083862305,
      "learning_rate": 0.00028067475542593406,
      "loss": 0.983,
      "step": 18200
    },
    {
      "epoch": 0.06477142412186938,
      "grad_norm": 75.14164733886719,
      "learning_rate": 0.00028056857276343914,
      "loss": 1.1498,
      "step": 18300
    },
    {
      "epoch": 0.0651253663301856,
      "grad_norm": 63.96908187866211,
      "learning_rate": 0.00028046239010094426,
      "loss": 0.8593,
      "step": 18400
    },
    {
      "epoch": 0.06547930853850184,
      "grad_norm": 0.018392790108919144,
      "learning_rate": 0.00028035620743844944,
      "loss": 1.0084,
      "step": 18500
    },
    {
      "epoch": 0.06583325074681806,
      "grad_norm": 45.75974655151367,
      "learning_rate": 0.00028025002477595457,
      "loss": 0.9886,
      "step": 18600
    },
    {
      "epoch": 0.06618719295513428,
      "grad_norm": 17.04198455810547,
      "learning_rate": 0.0002801438421134597,
      "loss": 0.7936,
      "step": 18700
    },
    {
      "epoch": 0.06654113516345052,
      "grad_norm": 98.77215576171875,
      "learning_rate": 0.0002800376594509648,
      "loss": 1.182,
      "step": 18800
    },
    {
      "epoch": 0.06689507737176674,
      "grad_norm": 49.1094856262207,
      "learning_rate": 0.00027993147678846995,
      "loss": 1.2956,
      "step": 18900
    },
    {
      "epoch": 0.06724901958008296,
      "grad_norm": 41.05095672607422,
      "learning_rate": 0.0002798252941259751,
      "loss": 0.9554,
      "step": 19000
    },
    {
      "epoch": 0.0676029617883992,
      "grad_norm": 90.01890563964844,
      "learning_rate": 0.0002797191114634802,
      "loss": 0.9401,
      "step": 19100
    },
    {
      "epoch": 0.06795690399671542,
      "grad_norm": 7.929629325866699,
      "learning_rate": 0.00027961292880098533,
      "loss": 0.8413,
      "step": 19200
    },
    {
      "epoch": 0.06831084620503164,
      "grad_norm": 40.518009185791016,
      "learning_rate": 0.00027950674613849046,
      "loss": 1.0409,
      "step": 19300
    },
    {
      "epoch": 0.06866478841334787,
      "grad_norm": 0.033904410898685455,
      "learning_rate": 0.0002794005634759956,
      "loss": 1.1175,
      "step": 19400
    },
    {
      "epoch": 0.0690187306216641,
      "grad_norm": 0.032807886600494385,
      "learning_rate": 0.00027929438081350077,
      "loss": 0.9012,
      "step": 19500
    },
    {
      "epoch": 0.06937267282998032,
      "grad_norm": 0.004164821468293667,
      "learning_rate": 0.0002791881981510059,
      "loss": 1.0622,
      "step": 19600
    },
    {
      "epoch": 0.06972661503829655,
      "grad_norm": 0.007908297702670097,
      "learning_rate": 0.000279082015488511,
      "loss": 0.9783,
      "step": 19700
    },
    {
      "epoch": 0.07008055724661277,
      "grad_norm": 0.005065699573606253,
      "learning_rate": 0.00027897583282601615,
      "loss": 1.0254,
      "step": 19800
    },
    {
      "epoch": 0.070434499454929,
      "grad_norm": 0.5161617994308472,
      "learning_rate": 0.0002788696501635213,
      "loss": 0.9002,
      "step": 19900
    },
    {
      "epoch": 0.07078844166324523,
      "grad_norm": 31.9924373626709,
      "learning_rate": 0.0002787634675010264,
      "loss": 1.1305,
      "step": 20000
    },
    {
      "epoch": 0.07114238387156145,
      "grad_norm": 3.5483951568603516,
      "learning_rate": 0.00027865728483853153,
      "loss": 0.9476,
      "step": 20100
    },
    {
      "epoch": 0.07149632607987767,
      "grad_norm": 27.938329696655273,
      "learning_rate": 0.00027855110217603666,
      "loss": 0.8339,
      "step": 20200
    },
    {
      "epoch": 0.07185026828819391,
      "grad_norm": 14.975448608398438,
      "learning_rate": 0.00027844491951354184,
      "loss": 0.8437,
      "step": 20300
    },
    {
      "epoch": 0.07220421049651013,
      "grad_norm": 45.74217987060547,
      "learning_rate": 0.0002783387368510469,
      "loss": 0.8669,
      "step": 20400
    },
    {
      "epoch": 0.07255815270482635,
      "grad_norm": 17.89057731628418,
      "learning_rate": 0.0002782325541885521,
      "loss": 0.8642,
      "step": 20500
    },
    {
      "epoch": 0.07291209491314259,
      "grad_norm": 0.06673570722341537,
      "learning_rate": 0.0002781263715260572,
      "loss": 0.9864,
      "step": 20600
    },
    {
      "epoch": 0.07326603712145881,
      "grad_norm": 56.00537872314453,
      "learning_rate": 0.00027802018886356235,
      "loss": 1.006,
      "step": 20700
    },
    {
      "epoch": 0.07361997932977503,
      "grad_norm": 28.72655487060547,
      "learning_rate": 0.0002779140062010675,
      "loss": 1.0696,
      "step": 20800
    },
    {
      "epoch": 0.07397392153809126,
      "grad_norm": 4.461926460266113,
      "learning_rate": 0.0002778078235385726,
      "loss": 0.7976,
      "step": 20900
    },
    {
      "epoch": 0.07432786374640749,
      "grad_norm": 50.48686599731445,
      "learning_rate": 0.00027770164087607773,
      "loss": 0.9144,
      "step": 21000
    },
    {
      "epoch": 0.0746818059547237,
      "grad_norm": 0.4534662365913391,
      "learning_rate": 0.00027759545821358285,
      "loss": 1.0343,
      "step": 21100
    },
    {
      "epoch": 0.07503574816303994,
      "grad_norm": 0.06491657346487045,
      "learning_rate": 0.000277489275551088,
      "loss": 0.9837,
      "step": 21200
    },
    {
      "epoch": 0.07538969037135616,
      "grad_norm": 6.832573890686035,
      "learning_rate": 0.00027738309288859316,
      "loss": 0.6162,
      "step": 21300
    },
    {
      "epoch": 0.07574363257967238,
      "grad_norm": 64.74846649169922,
      "learning_rate": 0.00027727691022609824,
      "loss": 1.2032,
      "step": 21400
    },
    {
      "epoch": 0.07609757478798862,
      "grad_norm": 65.5523452758789,
      "learning_rate": 0.00027717072756360336,
      "loss": 1.1517,
      "step": 21500
    },
    {
      "epoch": 0.07645151699630484,
      "grad_norm": 12.286613464355469,
      "learning_rate": 0.00027706454490110854,
      "loss": 1.2477,
      "step": 21600
    },
    {
      "epoch": 0.07680545920462106,
      "grad_norm": 0.14533065259456635,
      "learning_rate": 0.0002769583622386136,
      "loss": 0.929,
      "step": 21700
    },
    {
      "epoch": 0.0771594014129373,
      "grad_norm": 9.161843299865723,
      "learning_rate": 0.0002768521795761188,
      "loss": 0.7923,
      "step": 21800
    },
    {
      "epoch": 0.07751334362125352,
      "grad_norm": 0.0489131361246109,
      "learning_rate": 0.0002767459969136239,
      "loss": 0.9567,
      "step": 21900
    },
    {
      "epoch": 0.07786728582956975,
      "grad_norm": 0.2978173792362213,
      "learning_rate": 0.00027663981425112905,
      "loss": 1.1555,
      "step": 22000
    },
    {
      "epoch": 0.07822122803788598,
      "grad_norm": 26.898929595947266,
      "learning_rate": 0.0002765336315886342,
      "loss": 0.992,
      "step": 22100
    },
    {
      "epoch": 0.0785751702462022,
      "grad_norm": 29.701818466186523,
      "learning_rate": 0.0002764274489261393,
      "loss": 0.8714,
      "step": 22200
    },
    {
      "epoch": 0.07892911245451843,
      "grad_norm": 34.68513488769531,
      "learning_rate": 0.00027632126626364443,
      "loss": 0.9646,
      "step": 22300
    },
    {
      "epoch": 0.07928305466283465,
      "grad_norm": 64.1881103515625,
      "learning_rate": 0.00027621508360114956,
      "loss": 1.0215,
      "step": 22400
    },
    {
      "epoch": 0.07963699687115088,
      "grad_norm": 9.093976020812988,
      "learning_rate": 0.0002761089009386547,
      "loss": 0.6212,
      "step": 22500
    },
    {
      "epoch": 0.07999093907946711,
      "grad_norm": 0.025010360404849052,
      "learning_rate": 0.00027600271827615987,
      "loss": 0.9159,
      "step": 22600
    },
    {
      "epoch": 0.08034488128778333,
      "grad_norm": 4.19489860534668,
      "learning_rate": 0.000275896535613665,
      "loss": 0.8028,
      "step": 22700
    },
    {
      "epoch": 0.08069882349609955,
      "grad_norm": 55.32488250732422,
      "learning_rate": 0.0002757903529511701,
      "loss": 0.8666,
      "step": 22800
    },
    {
      "epoch": 0.08105276570441579,
      "grad_norm": 1.4037582874298096,
      "learning_rate": 0.00027568417028867525,
      "loss": 1.0224,
      "step": 22900
    },
    {
      "epoch": 0.08140670791273201,
      "grad_norm": 31.708595275878906,
      "learning_rate": 0.0002755779876261804,
      "loss": 0.7451,
      "step": 23000
    },
    {
      "epoch": 0.08176065012104823,
      "grad_norm": 82.19324493408203,
      "learning_rate": 0.0002754718049636855,
      "loss": 0.6714,
      "step": 23100
    },
    {
      "epoch": 0.08211459232936447,
      "grad_norm": 0.2065814733505249,
      "learning_rate": 0.00027536562230119063,
      "loss": 0.9359,
      "step": 23200
    },
    {
      "epoch": 0.08246853453768069,
      "grad_norm": 7.057070255279541,
      "learning_rate": 0.00027525943963869576,
      "loss": 0.8398,
      "step": 23300
    },
    {
      "epoch": 0.08282247674599691,
      "grad_norm": 0.038939468562603,
      "learning_rate": 0.0002751532569762009,
      "loss": 0.6063,
      "step": 23400
    },
    {
      "epoch": 0.08317641895431314,
      "grad_norm": 1.6294053792953491,
      "learning_rate": 0.000275047074313706,
      "loss": 0.7332,
      "step": 23500
    },
    {
      "epoch": 0.08353036116262937,
      "grad_norm": 87.47521209716797,
      "learning_rate": 0.0002749408916512112,
      "loss": 1.0852,
      "step": 23600
    },
    {
      "epoch": 0.08388430337094559,
      "grad_norm": 0.36122074723243713,
      "learning_rate": 0.0002748347089887163,
      "loss": 0.8152,
      "step": 23700
    },
    {
      "epoch": 0.08423824557926182,
      "grad_norm": 0.013461664319038391,
      "learning_rate": 0.00027472852632622145,
      "loss": 1.2289,
      "step": 23800
    },
    {
      "epoch": 0.08459218778757804,
      "grad_norm": 0.006955299060791731,
      "learning_rate": 0.0002746223436637266,
      "loss": 1.0592,
      "step": 23900
    },
    {
      "epoch": 0.08494612999589427,
      "grad_norm": 188.2742156982422,
      "learning_rate": 0.0002745161610012317,
      "loss": 0.9697,
      "step": 24000
    },
    {
      "epoch": 0.0853000722042105,
      "grad_norm": 0.832204282283783,
      "learning_rate": 0.00027440997833873683,
      "loss": 1.0142,
      "step": 24100
    },
    {
      "epoch": 0.08565401441252672,
      "grad_norm": 53.64046096801758,
      "learning_rate": 0.00027430379567624195,
      "loss": 1.0723,
      "step": 24200
    },
    {
      "epoch": 0.08600795662084294,
      "grad_norm": 100.68309783935547,
      "learning_rate": 0.0002741976130137471,
      "loss": 1.1291,
      "step": 24300
    },
    {
      "epoch": 0.08636189882915918,
      "grad_norm": 16.46828842163086,
      "learning_rate": 0.00027409143035125226,
      "loss": 1.108,
      "step": 24400
    },
    {
      "epoch": 0.0867158410374754,
      "grad_norm": 0.07127232104539871,
      "learning_rate": 0.00027398524768875734,
      "loss": 0.8665,
      "step": 24500
    },
    {
      "epoch": 0.08706978324579162,
      "grad_norm": 0.05683160573244095,
      "learning_rate": 0.0002738790650262625,
      "loss": 0.9666,
      "step": 24600
    },
    {
      "epoch": 0.08742372545410786,
      "grad_norm": 0.005310267210006714,
      "learning_rate": 0.00027377288236376764,
      "loss": 0.9704,
      "step": 24700
    },
    {
      "epoch": 0.08777766766242408,
      "grad_norm": 0.0010378226870670915,
      "learning_rate": 0.0002736666997012727,
      "loss": 0.9466,
      "step": 24800
    },
    {
      "epoch": 0.0881316098707403,
      "grad_norm": 0.03394667059183121,
      "learning_rate": 0.0002735605170387779,
      "loss": 0.8697,
      "step": 24900
    },
    {
      "epoch": 0.08848555207905653,
      "grad_norm": 7.554111480712891,
      "learning_rate": 0.000273454334376283,
      "loss": 1.0613,
      "step": 25000
    },
    {
      "epoch": 0.08883949428737276,
      "grad_norm": 12.472132682800293,
      "learning_rate": 0.00027334815171378815,
      "loss": 1.1299,
      "step": 25100
    },
    {
      "epoch": 0.08919343649568899,
      "grad_norm": 0.8771721720695496,
      "learning_rate": 0.0002732419690512933,
      "loss": 0.9345,
      "step": 25200
    },
    {
      "epoch": 0.08954737870400521,
      "grad_norm": 0.20262423157691956,
      "learning_rate": 0.0002731357863887984,
      "loss": 1.0141,
      "step": 25300
    },
    {
      "epoch": 0.08990132091232143,
      "grad_norm": 11.474996566772461,
      "learning_rate": 0.00027302960372630353,
      "loss": 0.7637,
      "step": 25400
    },
    {
      "epoch": 0.09025526312063767,
      "grad_norm": 0.006412324495613575,
      "learning_rate": 0.00027292342106380866,
      "loss": 0.8241,
      "step": 25500
    },
    {
      "epoch": 0.09060920532895389,
      "grad_norm": 0.06449463218450546,
      "learning_rate": 0.0002728172384013138,
      "loss": 0.9458,
      "step": 25600
    },
    {
      "epoch": 0.09096314753727011,
      "grad_norm": 44.39243698120117,
      "learning_rate": 0.00027271105573881897,
      "loss": 0.864,
      "step": 25700
    },
    {
      "epoch": 0.09131708974558635,
      "grad_norm": 47.08360290527344,
      "learning_rate": 0.00027260487307632404,
      "loss": 0.98,
      "step": 25800
    },
    {
      "epoch": 0.09167103195390257,
      "grad_norm": 5.492352485656738,
      "learning_rate": 0.0002724986904138292,
      "loss": 0.929,
      "step": 25900
    },
    {
      "epoch": 0.09202497416221879,
      "grad_norm": 0.007061265408992767,
      "learning_rate": 0.00027239250775133435,
      "loss": 1.053,
      "step": 26000
    },
    {
      "epoch": 0.09237891637053502,
      "grad_norm": 27.044143676757812,
      "learning_rate": 0.0002722863250888395,
      "loss": 0.9044,
      "step": 26100
    },
    {
      "epoch": 0.09273285857885125,
      "grad_norm": 17.14537811279297,
      "learning_rate": 0.0002721801424263446,
      "loss": 1.0843,
      "step": 26200
    },
    {
      "epoch": 0.09308680078716747,
      "grad_norm": 20.693450927734375,
      "learning_rate": 0.00027207395976384973,
      "loss": 0.8066,
      "step": 26300
    },
    {
      "epoch": 0.0934407429954837,
      "grad_norm": 9.930169105529785,
      "learning_rate": 0.00027196777710135486,
      "loss": 1.0256,
      "step": 26400
    },
    {
      "epoch": 0.09379468520379992,
      "grad_norm": 3.5564522743225098,
      "learning_rate": 0.00027186159443886,
      "loss": 0.7053,
      "step": 26500
    },
    {
      "epoch": 0.09414862741211615,
      "grad_norm": 46.2110481262207,
      "learning_rate": 0.0002717554117763651,
      "loss": 0.8171,
      "step": 26600
    },
    {
      "epoch": 0.09450256962043238,
      "grad_norm": 25.580223083496094,
      "learning_rate": 0.0002716492291138703,
      "loss": 0.8794,
      "step": 26700
    },
    {
      "epoch": 0.0948565118287486,
      "grad_norm": 0.7495459318161011,
      "learning_rate": 0.0002715430464513754,
      "loss": 0.8634,
      "step": 26800
    },
    {
      "epoch": 0.09521045403706482,
      "grad_norm": 1.0018008947372437,
      "learning_rate": 0.00027143686378888055,
      "loss": 0.7866,
      "step": 26900
    },
    {
      "epoch": 0.09556439624538106,
      "grad_norm": 12.050156593322754,
      "learning_rate": 0.0002713306811263857,
      "loss": 1.0668,
      "step": 27000
    },
    {
      "epoch": 0.09591833845369728,
      "grad_norm": 22.68415069580078,
      "learning_rate": 0.0002712244984638908,
      "loss": 0.7017,
      "step": 27100
    },
    {
      "epoch": 0.0962722806620135,
      "grad_norm": 1.7102510929107666,
      "learning_rate": 0.0002711183158013959,
      "loss": 0.7305,
      "step": 27200
    },
    {
      "epoch": 0.09662622287032974,
      "grad_norm": 7.826093673706055,
      "learning_rate": 0.00027101213313890105,
      "loss": 0.876,
      "step": 27300
    },
    {
      "epoch": 0.09698016507864596,
      "grad_norm": 18.146638870239258,
      "learning_rate": 0.0002709059504764062,
      "loss": 1.0042,
      "step": 27400
    },
    {
      "epoch": 0.09733410728696218,
      "grad_norm": 42.88251876831055,
      "learning_rate": 0.0002707997678139113,
      "loss": 0.9299,
      "step": 27500
    },
    {
      "epoch": 0.09768804949527841,
      "grad_norm": 1.806411862373352,
      "learning_rate": 0.00027069358515141643,
      "loss": 0.8585,
      "step": 27600
    },
    {
      "epoch": 0.09804199170359464,
      "grad_norm": 29.94402503967285,
      "learning_rate": 0.0002705874024889216,
      "loss": 1.2553,
      "step": 27700
    },
    {
      "epoch": 0.09839593391191086,
      "grad_norm": 45.84809112548828,
      "learning_rate": 0.00027048121982642674,
      "loss": 0.8661,
      "step": 27800
    },
    {
      "epoch": 0.09874987612022709,
      "grad_norm": 0.04631525278091431,
      "learning_rate": 0.0002703750371639318,
      "loss": 0.841,
      "step": 27900
    },
    {
      "epoch": 0.09910381832854331,
      "grad_norm": 56.59980392456055,
      "learning_rate": 0.000270268854501437,
      "loss": 0.9382,
      "step": 28000
    },
    {
      "epoch": 0.09945776053685954,
      "grad_norm": 1.5614511966705322,
      "learning_rate": 0.0002701626718389421,
      "loss": 0.8246,
      "step": 28100
    },
    {
      "epoch": 0.09981170274517577,
      "grad_norm": 1.9112989902496338,
      "learning_rate": 0.00027005648917644725,
      "loss": 0.8244,
      "step": 28200
    },
    {
      "epoch": 0.10016564495349199,
      "grad_norm": 66.12602996826172,
      "learning_rate": 0.0002699503065139524,
      "loss": 1.1106,
      "step": 28300
    },
    {
      "epoch": 0.10051958716180821,
      "grad_norm": 3.9620981216430664,
      "learning_rate": 0.0002698441238514575,
      "loss": 0.7052,
      "step": 28400
    },
    {
      "epoch": 0.10087352937012445,
      "grad_norm": 0.005071914754807949,
      "learning_rate": 0.00026973794118896263,
      "loss": 0.9076,
      "step": 28500
    },
    {
      "epoch": 0.10122747157844067,
      "grad_norm": 0.038996245712041855,
      "learning_rate": 0.00026963175852646776,
      "loss": 0.6856,
      "step": 28600
    },
    {
      "epoch": 0.1015814137867569,
      "grad_norm": 1.288998007774353,
      "learning_rate": 0.0002695255758639729,
      "loss": 0.9998,
      "step": 28700
    },
    {
      "epoch": 0.10193535599507313,
      "grad_norm": 14.694695472717285,
      "learning_rate": 0.00026941939320147807,
      "loss": 0.8864,
      "step": 28800
    },
    {
      "epoch": 0.10228929820338935,
      "grad_norm": 0.016347071155905724,
      "learning_rate": 0.00026931321053898314,
      "loss": 0.8398,
      "step": 28900
    },
    {
      "epoch": 0.10264324041170558,
      "grad_norm": 20.639951705932617,
      "learning_rate": 0.0002692070278764883,
      "loss": 0.9804,
      "step": 29000
    },
    {
      "epoch": 0.1029971826200218,
      "grad_norm": 1.6733394861221313,
      "learning_rate": 0.00026910084521399345,
      "loss": 0.6722,
      "step": 29100
    },
    {
      "epoch": 0.10335112482833803,
      "grad_norm": 0.039993807673454285,
      "learning_rate": 0.0002689946625514986,
      "loss": 1.1706,
      "step": 29200
    },
    {
      "epoch": 0.10370506703665426,
      "grad_norm": 21.58184051513672,
      "learning_rate": 0.0002688884798890037,
      "loss": 0.9381,
      "step": 29300
    },
    {
      "epoch": 0.10405900924497048,
      "grad_norm": 0.17701679468154907,
      "learning_rate": 0.00026878229722650883,
      "loss": 0.7225,
      "step": 29400
    },
    {
      "epoch": 0.1044129514532867,
      "grad_norm": 0.14723412692546844,
      "learning_rate": 0.00026867611456401396,
      "loss": 0.8912,
      "step": 29500
    },
    {
      "epoch": 0.10476689366160294,
      "grad_norm": 0.0021966504864394665,
      "learning_rate": 0.0002685699319015191,
      "loss": 0.9613,
      "step": 29600
    },
    {
      "epoch": 0.10512083586991916,
      "grad_norm": 0.11951707303524017,
      "learning_rate": 0.0002684637492390242,
      "loss": 1.0372,
      "step": 29700
    },
    {
      "epoch": 0.10547477807823538,
      "grad_norm": 1.0516908168792725,
      "learning_rate": 0.0002683575665765294,
      "loss": 0.7425,
      "step": 29800
    },
    {
      "epoch": 0.10582872028655162,
      "grad_norm": 0.041079986840486526,
      "learning_rate": 0.00026825138391403446,
      "loss": 0.9402,
      "step": 29900
    },
    {
      "epoch": 0.10618266249486784,
      "grad_norm": 4.352172374725342,
      "learning_rate": 0.00026814520125153965,
      "loss": 0.846,
      "step": 30000
    },
    {
      "epoch": 0.10653660470318406,
      "grad_norm": 38.240264892578125,
      "learning_rate": 0.00026803901858904477,
      "loss": 1.015,
      "step": 30100
    },
    {
      "epoch": 0.1068905469115003,
      "grad_norm": 47.98465347290039,
      "learning_rate": 0.0002679328359265499,
      "loss": 1.0297,
      "step": 30200
    },
    {
      "epoch": 0.10724448911981652,
      "grad_norm": 0.004168529063463211,
      "learning_rate": 0.000267826653264055,
      "loss": 0.8447,
      "step": 30300
    },
    {
      "epoch": 0.10759843132813274,
      "grad_norm": 0.1272200495004654,
      "learning_rate": 0.00026772047060156015,
      "loss": 1.1231,
      "step": 30400
    },
    {
      "epoch": 0.10795237353644897,
      "grad_norm": 23.27110481262207,
      "learning_rate": 0.0002676142879390653,
      "loss": 0.5975,
      "step": 30500
    },
    {
      "epoch": 0.1083063157447652,
      "grad_norm": 0.050369925796985626,
      "learning_rate": 0.0002675081052765704,
      "loss": 0.7801,
      "step": 30600
    },
    {
      "epoch": 0.10866025795308142,
      "grad_norm": 0.4665485620498657,
      "learning_rate": 0.00026740192261407553,
      "loss": 1.1009,
      "step": 30700
    },
    {
      "epoch": 0.10901420016139765,
      "grad_norm": 0.005569974891841412,
      "learning_rate": 0.0002672957399515807,
      "loss": 0.8486,
      "step": 30800
    },
    {
      "epoch": 0.10936814236971387,
      "grad_norm": 31.152170181274414,
      "learning_rate": 0.0002671895572890858,
      "loss": 0.8215,
      "step": 30900
    },
    {
      "epoch": 0.1097220845780301,
      "grad_norm": 0.36458173394203186,
      "learning_rate": 0.0002670833746265909,
      "loss": 0.8189,
      "step": 31000
    },
    {
      "epoch": 0.11007602678634633,
      "grad_norm": 5.073169708251953,
      "learning_rate": 0.0002669771919640961,
      "loss": 0.9374,
      "step": 31100
    },
    {
      "epoch": 0.11042996899466255,
      "grad_norm": 16.976390838623047,
      "learning_rate": 0.0002668710093016012,
      "loss": 1.0382,
      "step": 31200
    },
    {
      "epoch": 0.11078391120297877,
      "grad_norm": 57.26234436035156,
      "learning_rate": 0.00026676482663910635,
      "loss": 0.7643,
      "step": 31300
    },
    {
      "epoch": 0.111137853411295,
      "grad_norm": 57.85050582885742,
      "learning_rate": 0.0002666586439766115,
      "loss": 0.9633,
      "step": 31400
    },
    {
      "epoch": 0.11149179561961123,
      "grad_norm": 0.04212949797511101,
      "learning_rate": 0.0002665524613141166,
      "loss": 0.5226,
      "step": 31500
    },
    {
      "epoch": 0.11184573782792745,
      "grad_norm": 15.609004020690918,
      "learning_rate": 0.00026644627865162173,
      "loss": 1.1777,
      "step": 31600
    },
    {
      "epoch": 0.11219968003624368,
      "grad_norm": 0.0059874639846384525,
      "learning_rate": 0.00026634009598912686,
      "loss": 0.8391,
      "step": 31700
    },
    {
      "epoch": 0.1125536222445599,
      "grad_norm": 5.167013168334961,
      "learning_rate": 0.000266233913326632,
      "loss": 0.9108,
      "step": 31800
    },
    {
      "epoch": 0.11290756445287614,
      "grad_norm": 16.397642135620117,
      "learning_rate": 0.00026612773066413717,
      "loss": 0.9929,
      "step": 31900
    },
    {
      "epoch": 0.11326150666119236,
      "grad_norm": 52.8481559753418,
      "learning_rate": 0.00026602154800164224,
      "loss": 1.0281,
      "step": 32000
    },
    {
      "epoch": 0.11361544886950858,
      "grad_norm": 15.889848709106445,
      "learning_rate": 0.0002659153653391474,
      "loss": 0.7259,
      "step": 32100
    },
    {
      "epoch": 0.11396939107782482,
      "grad_norm": 60.55693817138672,
      "learning_rate": 0.00026580918267665255,
      "loss": 0.9183,
      "step": 32200
    },
    {
      "epoch": 0.11432333328614104,
      "grad_norm": 12.007347106933594,
      "learning_rate": 0.0002657030000141577,
      "loss": 0.8083,
      "step": 32300
    },
    {
      "epoch": 0.11467727549445726,
      "grad_norm": 14.753996849060059,
      "learning_rate": 0.0002655968173516628,
      "loss": 0.7787,
      "step": 32400
    },
    {
      "epoch": 0.1150312177027735,
      "grad_norm": 16.401269912719727,
      "learning_rate": 0.00026549063468916793,
      "loss": 0.9988,
      "step": 32500
    },
    {
      "epoch": 0.11538515991108972,
      "grad_norm": 1.199244737625122,
      "learning_rate": 0.00026538445202667306,
      "loss": 0.6219,
      "step": 32600
    },
    {
      "epoch": 0.11573910211940594,
      "grad_norm": 0.04184374585747719,
      "learning_rate": 0.0002652782693641782,
      "loss": 0.6297,
      "step": 32700
    },
    {
      "epoch": 0.11609304432772218,
      "grad_norm": 0.45590293407440186,
      "learning_rate": 0.0002651720867016833,
      "loss": 0.867,
      "step": 32800
    },
    {
      "epoch": 0.1164469865360384,
      "grad_norm": 16.576824188232422,
      "learning_rate": 0.0002650659040391885,
      "loss": 0.9066,
      "step": 32900
    },
    {
      "epoch": 0.11680092874435462,
      "grad_norm": 0.2929282784461975,
      "learning_rate": 0.00026495972137669356,
      "loss": 0.9401,
      "step": 33000
    },
    {
      "epoch": 0.11715487095267085,
      "grad_norm": 0.05684174597263336,
      "learning_rate": 0.00026485353871419875,
      "loss": 0.7062,
      "step": 33100
    },
    {
      "epoch": 0.11750881316098707,
      "grad_norm": 0.057285603135824203,
      "learning_rate": 0.00026474735605170387,
      "loss": 0.8325,
      "step": 33200
    },
    {
      "epoch": 0.1178627553693033,
      "grad_norm": 33.03557205200195,
      "learning_rate": 0.00026464117338920894,
      "loss": 0.615,
      "step": 33300
    },
    {
      "epoch": 0.11821669757761953,
      "grad_norm": 38.10950469970703,
      "learning_rate": 0.0002645349907267141,
      "loss": 1.0241,
      "step": 33400
    },
    {
      "epoch": 0.11857063978593575,
      "grad_norm": 74.53234100341797,
      "learning_rate": 0.00026442880806421925,
      "loss": 0.8072,
      "step": 33500
    },
    {
      "epoch": 0.11892458199425197,
      "grad_norm": 0.17428459227085114,
      "learning_rate": 0.0002643226254017244,
      "loss": 0.7604,
      "step": 33600
    },
    {
      "epoch": 0.11927852420256821,
      "grad_norm": 3.628201961517334,
      "learning_rate": 0.0002642164427392295,
      "loss": 0.6275,
      "step": 33700
    },
    {
      "epoch": 0.11963246641088443,
      "grad_norm": 0.11174516379833221,
      "learning_rate": 0.00026411026007673463,
      "loss": 0.7502,
      "step": 33800
    },
    {
      "epoch": 0.11998640861920065,
      "grad_norm": 3.2878143787384033,
      "learning_rate": 0.0002640040774142398,
      "loss": 0.5669,
      "step": 33900
    },
    {
      "epoch": 0.12034035082751689,
      "grad_norm": 101.32809448242188,
      "learning_rate": 0.0002638978947517449,
      "loss": 0.6536,
      "step": 34000
    },
    {
      "epoch": 0.12069429303583311,
      "grad_norm": 4.004289627075195,
      "learning_rate": 0.00026379171208925,
      "loss": 0.5637,
      "step": 34100
    },
    {
      "epoch": 0.12104823524414933,
      "grad_norm": 0.002914929063990712,
      "learning_rate": 0.0002636855294267552,
      "loss": 0.672,
      "step": 34200
    },
    {
      "epoch": 0.12140217745246557,
      "grad_norm": 0.9685130715370178,
      "learning_rate": 0.0002635793467642603,
      "loss": 0.6087,
      "step": 34300
    },
    {
      "epoch": 0.12175611966078179,
      "grad_norm": 1.41073739528656,
      "learning_rate": 0.00026347316410176545,
      "loss": 0.9439,
      "step": 34400
    },
    {
      "epoch": 0.12211006186909801,
      "grad_norm": 0.0232311449944973,
      "learning_rate": 0.0002633669814392706,
      "loss": 0.6945,
      "step": 34500
    },
    {
      "epoch": 0.12246400407741424,
      "grad_norm": 0.28368037939071655,
      "learning_rate": 0.0002632607987767757,
      "loss": 0.5552,
      "step": 34600
    },
    {
      "epoch": 0.12281794628573046,
      "grad_norm": 2.324380397796631,
      "learning_rate": 0.00026315461611428083,
      "loss": 0.5983,
      "step": 34700
    },
    {
      "epoch": 0.12317188849404669,
      "grad_norm": 10.410649299621582,
      "learning_rate": 0.00026304843345178596,
      "loss": 0.7745,
      "step": 34800
    },
    {
      "epoch": 0.12352583070236292,
      "grad_norm": 0.0036447441671043634,
      "learning_rate": 0.0002629422507892911,
      "loss": 0.6916,
      "step": 34900
    },
    {
      "epoch": 0.12387977291067914,
      "grad_norm": 53.12417984008789,
      "learning_rate": 0.0002628360681267962,
      "loss": 0.9512,
      "step": 35000
    },
    {
      "epoch": 0.12423371511899536,
      "grad_norm": 7.981443881988525,
      "learning_rate": 0.00026272988546430134,
      "loss": 0.6767,
      "step": 35100
    },
    {
      "epoch": 0.1245876573273116,
      "grad_norm": 11.73715877532959,
      "learning_rate": 0.0002626237028018065,
      "loss": 0.8893,
      "step": 35200
    },
    {
      "epoch": 0.12494159953562782,
      "grad_norm": 60.32111740112305,
      "learning_rate": 0.00026251752013931165,
      "loss": 0.5411,
      "step": 35300
    },
    {
      "epoch": 0.12529554174394406,
      "grad_norm": 41.935726165771484,
      "learning_rate": 0.0002624113374768168,
      "loss": 0.8537,
      "step": 35400
    },
    {
      "epoch": 0.12564948395226028,
      "grad_norm": 4.041205883026123,
      "learning_rate": 0.0002623051548143219,
      "loss": 0.7606,
      "step": 35500
    },
    {
      "epoch": 0.1260034261605765,
      "grad_norm": 7.964956760406494,
      "learning_rate": 0.00026219897215182703,
      "loss": 0.8684,
      "step": 35600
    },
    {
      "epoch": 0.12635736836889272,
      "grad_norm": 87.49272918701172,
      "learning_rate": 0.00026209278948933216,
      "loss": 1.066,
      "step": 35700
    },
    {
      "epoch": 0.12671131057720894,
      "grad_norm": 0.5807949900627136,
      "learning_rate": 0.0002619866068268373,
      "loss": 0.7518,
      "step": 35800
    },
    {
      "epoch": 0.1270652527855252,
      "grad_norm": 0.10725351423025131,
      "learning_rate": 0.0002618804241643424,
      "loss": 0.7922,
      "step": 35900
    },
    {
      "epoch": 0.1274191949938414,
      "grad_norm": 42.1525993347168,
      "learning_rate": 0.0002617742415018476,
      "loss": 0.6607,
      "step": 36000
    },
    {
      "epoch": 0.12777313720215763,
      "grad_norm": 0.016476046293973923,
      "learning_rate": 0.00026166805883935266,
      "loss": 0.7223,
      "step": 36100
    },
    {
      "epoch": 0.12812707941047385,
      "grad_norm": 0.12246238440275192,
      "learning_rate": 0.00026156187617685784,
      "loss": 0.9172,
      "step": 36200
    },
    {
      "epoch": 0.12848102161879008,
      "grad_norm": 4.334022045135498,
      "learning_rate": 0.00026145569351436297,
      "loss": 0.7048,
      "step": 36300
    },
    {
      "epoch": 0.1288349638271063,
      "grad_norm": 0.9192729592323303,
      "learning_rate": 0.00026134951085186804,
      "loss": 0.7816,
      "step": 36400
    },
    {
      "epoch": 0.12918890603542255,
      "grad_norm": 124.13898468017578,
      "learning_rate": 0.0002612433281893732,
      "loss": 0.8255,
      "step": 36500
    },
    {
      "epoch": 0.12954284824373877,
      "grad_norm": 0.12569187581539154,
      "learning_rate": 0.00026113714552687835,
      "loss": 0.5722,
      "step": 36600
    },
    {
      "epoch": 0.129896790452055,
      "grad_norm": 25.52311897277832,
      "learning_rate": 0.0002610309628643835,
      "loss": 0.7054,
      "step": 36700
    },
    {
      "epoch": 0.1302507326603712,
      "grad_norm": 54.23301696777344,
      "learning_rate": 0.0002609247802018886,
      "loss": 0.8334,
      "step": 36800
    },
    {
      "epoch": 0.13060467486868743,
      "grad_norm": 0.09148281067609787,
      "learning_rate": 0.00026081859753939373,
      "loss": 0.7172,
      "step": 36900
    },
    {
      "epoch": 0.13095861707700368,
      "grad_norm": 0.034164030104875565,
      "learning_rate": 0.0002607124148768989,
      "loss": 0.8578,
      "step": 37000
    },
    {
      "epoch": 0.1313125592853199,
      "grad_norm": 0.49915406107902527,
      "learning_rate": 0.000260606232214404,
      "loss": 0.6611,
      "step": 37100
    },
    {
      "epoch": 0.13166650149363612,
      "grad_norm": 0.0499565452337265,
      "learning_rate": 0.0002605000495519091,
      "loss": 0.6583,
      "step": 37200
    },
    {
      "epoch": 0.13202044370195234,
      "grad_norm": 30.466466903686523,
      "learning_rate": 0.0002603938668894143,
      "loss": 0.8684,
      "step": 37300
    },
    {
      "epoch": 0.13237438591026857,
      "grad_norm": 0.02759012021124363,
      "learning_rate": 0.00026028768422691937,
      "loss": 0.774,
      "step": 37400
    },
    {
      "epoch": 0.1327283281185848,
      "grad_norm": 25.362741470336914,
      "learning_rate": 0.00026018150156442455,
      "loss": 0.8776,
      "step": 37500
    },
    {
      "epoch": 0.13308227032690104,
      "grad_norm": 26.4133358001709,
      "learning_rate": 0.0002600753189019297,
      "loss": 0.7867,
      "step": 37600
    },
    {
      "epoch": 0.13343621253521726,
      "grad_norm": 0.06806546449661255,
      "learning_rate": 0.0002599691362394348,
      "loss": 0.7677,
      "step": 37700
    },
    {
      "epoch": 0.13379015474353348,
      "grad_norm": 0.03779436647891998,
      "learning_rate": 0.00025986295357693993,
      "loss": 0.7397,
      "step": 37800
    },
    {
      "epoch": 0.1341440969518497,
      "grad_norm": 0.004443212412297726,
      "learning_rate": 0.00025975677091444506,
      "loss": 0.3295,
      "step": 37900
    },
    {
      "epoch": 0.13449803916016592,
      "grad_norm": 0.27786561846733093,
      "learning_rate": 0.0002596505882519502,
      "loss": 0.7903,
      "step": 38000
    },
    {
      "epoch": 0.13485198136848214,
      "grad_norm": 0.0026348771061748266,
      "learning_rate": 0.0002595444055894553,
      "loss": 0.5119,
      "step": 38100
    },
    {
      "epoch": 0.1352059235767984,
      "grad_norm": 0.0006205926765687764,
      "learning_rate": 0.00025943822292696044,
      "loss": 0.648,
      "step": 38200
    },
    {
      "epoch": 0.1355598657851146,
      "grad_norm": 55.08979797363281,
      "learning_rate": 0.0002593320402644656,
      "loss": 0.9403,
      "step": 38300
    },
    {
      "epoch": 0.13591380799343084,
      "grad_norm": 0.3046261966228485,
      "learning_rate": 0.00025922585760197075,
      "loss": 0.7361,
      "step": 38400
    },
    {
      "epoch": 0.13626775020174706,
      "grad_norm": 0.0020039184018969536,
      "learning_rate": 0.0002591196749394759,
      "loss": 0.7296,
      "step": 38500
    },
    {
      "epoch": 0.13662169241006328,
      "grad_norm": 0.0046408092603087425,
      "learning_rate": 0.000259013492276981,
      "loss": 0.4749,
      "step": 38600
    },
    {
      "epoch": 0.1369756346183795,
      "grad_norm": 0.06555085629224777,
      "learning_rate": 0.00025890730961448613,
      "loss": 0.6221,
      "step": 38700
    },
    {
      "epoch": 0.13732957682669575,
      "grad_norm": 0.018911050632596016,
      "learning_rate": 0.00025880112695199126,
      "loss": 0.576,
      "step": 38800
    },
    {
      "epoch": 0.13768351903501197,
      "grad_norm": 0.9544603824615479,
      "learning_rate": 0.0002586949442894964,
      "loss": 0.7587,
      "step": 38900
    },
    {
      "epoch": 0.1380374612433282,
      "grad_norm": 0.03619013726711273,
      "learning_rate": 0.0002585887616270015,
      "loss": 0.8229,
      "step": 39000
    },
    {
      "epoch": 0.1383914034516444,
      "grad_norm": 44.37689971923828,
      "learning_rate": 0.00025848257896450664,
      "loss": 1.0323,
      "step": 39100
    },
    {
      "epoch": 0.13874534565996063,
      "grad_norm": 3.503343105316162,
      "learning_rate": 0.00025837639630201176,
      "loss": 0.4538,
      "step": 39200
    },
    {
      "epoch": 0.13909928786827686,
      "grad_norm": 0.02542908303439617,
      "learning_rate": 0.00025827021363951694,
      "loss": 0.6096,
      "step": 39300
    },
    {
      "epoch": 0.1394532300765931,
      "grad_norm": 34.47354507446289,
      "learning_rate": 0.00025816403097702207,
      "loss": 0.7593,
      "step": 39400
    },
    {
      "epoch": 0.13980717228490933,
      "grad_norm": 0.03432901203632355,
      "learning_rate": 0.0002580578483145272,
      "loss": 0.7217,
      "step": 39500
    },
    {
      "epoch": 0.14016111449322555,
      "grad_norm": 0.10025831311941147,
      "learning_rate": 0.0002579516656520323,
      "loss": 0.6299,
      "step": 39600
    },
    {
      "epoch": 0.14051505670154177,
      "grad_norm": 0.0955439954996109,
      "learning_rate": 0.00025784548298953745,
      "loss": 0.8478,
      "step": 39700
    },
    {
      "epoch": 0.140868998909858,
      "grad_norm": 5.736849308013916,
      "learning_rate": 0.0002577393003270426,
      "loss": 0.7211,
      "step": 39800
    },
    {
      "epoch": 0.1412229411181742,
      "grad_norm": 26.575077056884766,
      "learning_rate": 0.0002576331176645477,
      "loss": 0.6678,
      "step": 39900
    },
    {
      "epoch": 0.14157688332649046,
      "grad_norm": 0.31219419836997986,
      "learning_rate": 0.00025752693500205283,
      "loss": 0.6724,
      "step": 40000
    },
    {
      "epoch": 0.14193082553480668,
      "grad_norm": 21.189048767089844,
      "learning_rate": 0.000257420752339558,
      "loss": 0.6782,
      "step": 40100
    },
    {
      "epoch": 0.1422847677431229,
      "grad_norm": 1.9025276899337769,
      "learning_rate": 0.0002573145696770631,
      "loss": 0.6359,
      "step": 40200
    },
    {
      "epoch": 0.14263870995143912,
      "grad_norm": 0.0064227688126266,
      "learning_rate": 0.0002572083870145682,
      "loss": 0.7402,
      "step": 40300
    },
    {
      "epoch": 0.14299265215975535,
      "grad_norm": 0.0041669378988444805,
      "learning_rate": 0.0002571022043520734,
      "loss": 0.5184,
      "step": 40400
    },
    {
      "epoch": 0.1433465943680716,
      "grad_norm": 1.2068450450897217,
      "learning_rate": 0.00025699602168957847,
      "loss": 0.7509,
      "step": 40500
    },
    {
      "epoch": 0.14370053657638782,
      "grad_norm": 29.873153686523438,
      "learning_rate": 0.00025688983902708365,
      "loss": 0.7516,
      "step": 40600
    },
    {
      "epoch": 0.14405447878470404,
      "grad_norm": 20.760225296020508,
      "learning_rate": 0.0002567836563645888,
      "loss": 1.165,
      "step": 40700
    },
    {
      "epoch": 0.14440842099302026,
      "grad_norm": 3.7333242893218994,
      "learning_rate": 0.0002566774737020939,
      "loss": 0.8113,
      "step": 40800
    },
    {
      "epoch": 0.14476236320133648,
      "grad_norm": 88.30436706542969,
      "learning_rate": 0.00025657129103959903,
      "loss": 0.8367,
      "step": 40900
    },
    {
      "epoch": 0.1451163054096527,
      "grad_norm": 55.0876579284668,
      "learning_rate": 0.00025646510837710416,
      "loss": 0.7129,
      "step": 41000
    },
    {
      "epoch": 0.14547024761796895,
      "grad_norm": 0.16958530247211456,
      "learning_rate": 0.0002563589257146093,
      "loss": 0.6622,
      "step": 41100
    },
    {
      "epoch": 0.14582418982628517,
      "grad_norm": 0.012046582996845245,
      "learning_rate": 0.0002562527430521144,
      "loss": 0.6088,
      "step": 41200
    },
    {
      "epoch": 0.1461781320346014,
      "grad_norm": 34.23277282714844,
      "learning_rate": 0.00025614656038961954,
      "loss": 0.9398,
      "step": 41300
    },
    {
      "epoch": 0.14653207424291761,
      "grad_norm": 0.018883807584643364,
      "learning_rate": 0.0002560403777271247,
      "loss": 0.6825,
      "step": 41400
    },
    {
      "epoch": 0.14688601645123384,
      "grad_norm": 0.06428947299718857,
      "learning_rate": 0.0002559341950646298,
      "loss": 0.5387,
      "step": 41500
    },
    {
      "epoch": 0.14723995865955006,
      "grad_norm": 8.689805030822754,
      "learning_rate": 0.000255828012402135,
      "loss": 0.9054,
      "step": 41600
    },
    {
      "epoch": 0.1475939008678663,
      "grad_norm": 0.0012976792640984058,
      "learning_rate": 0.0002557218297396401,
      "loss": 0.7256,
      "step": 41700
    },
    {
      "epoch": 0.14794784307618253,
      "grad_norm": 47.910362243652344,
      "learning_rate": 0.00025561564707714523,
      "loss": 0.3957,
      "step": 41800
    },
    {
      "epoch": 0.14830178528449875,
      "grad_norm": 0.0025047408416867256,
      "learning_rate": 0.00025550946441465035,
      "loss": 0.6262,
      "step": 41900
    },
    {
      "epoch": 0.14865572749281497,
      "grad_norm": 0.051713086664676666,
      "learning_rate": 0.0002554032817521555,
      "loss": 0.843,
      "step": 42000
    },
    {
      "epoch": 0.1490096697011312,
      "grad_norm": 0.042448047548532486,
      "learning_rate": 0.0002552970990896606,
      "loss": 0.7597,
      "step": 42100
    },
    {
      "epoch": 0.1493636119094474,
      "grad_norm": 39.68953323364258,
      "learning_rate": 0.00025519091642716574,
      "loss": 0.7106,
      "step": 42200
    },
    {
      "epoch": 0.14971755411776366,
      "grad_norm": 14.56619930267334,
      "learning_rate": 0.00025508473376467086,
      "loss": 0.6079,
      "step": 42300
    },
    {
      "epoch": 0.15007149632607988,
      "grad_norm": 0.033277276903390884,
      "learning_rate": 0.00025497855110217604,
      "loss": 0.8304,
      "step": 42400
    },
    {
      "epoch": 0.1504254385343961,
      "grad_norm": 0.005222182255238295,
      "learning_rate": 0.00025487236843968117,
      "loss": 0.5329,
      "step": 42500
    },
    {
      "epoch": 0.15077938074271233,
      "grad_norm": 0.01596837304532528,
      "learning_rate": 0.0002547661857771863,
      "loss": 0.6316,
      "step": 42600
    },
    {
      "epoch": 0.15113332295102855,
      "grad_norm": 27.682296752929688,
      "learning_rate": 0.0002546600031146914,
      "loss": 0.6032,
      "step": 42700
    },
    {
      "epoch": 0.15148726515934477,
      "grad_norm": 31.912023544311523,
      "learning_rate": 0.00025455382045219655,
      "loss": 0.461,
      "step": 42800
    },
    {
      "epoch": 0.15184120736766102,
      "grad_norm": 12.224411964416504,
      "learning_rate": 0.0002544476377897017,
      "loss": 1.1997,
      "step": 42900
    },
    {
      "epoch": 0.15219514957597724,
      "grad_norm": 0.10124409943819046,
      "learning_rate": 0.0002543414551272068,
      "loss": 0.5973,
      "step": 43000
    },
    {
      "epoch": 0.15254909178429346,
      "grad_norm": 23.297740936279297,
      "learning_rate": 0.00025423527246471193,
      "loss": 0.7985,
      "step": 43100
    },
    {
      "epoch": 0.15290303399260968,
      "grad_norm": 0.0003970091638620943,
      "learning_rate": 0.00025412908980221706,
      "loss": 0.8804,
      "step": 43200
    },
    {
      "epoch": 0.1532569762009259,
      "grad_norm": 0.2557583451271057,
      "learning_rate": 0.0002540229071397222,
      "loss": 0.6282,
      "step": 43300
    },
    {
      "epoch": 0.15361091840924213,
      "grad_norm": 5.411667823791504,
      "learning_rate": 0.0002539167244772273,
      "loss": 0.7297,
      "step": 43400
    },
    {
      "epoch": 0.15396486061755837,
      "grad_norm": 28.84872817993164,
      "learning_rate": 0.0002538105418147325,
      "loss": 0.7764,
      "step": 43500
    },
    {
      "epoch": 0.1543188028258746,
      "grad_norm": 0.0103608975186944,
      "learning_rate": 0.00025370435915223757,
      "loss": 0.6199,
      "step": 43600
    },
    {
      "epoch": 0.15467274503419082,
      "grad_norm": 0.024033889174461365,
      "learning_rate": 0.00025359817648974275,
      "loss": 0.691,
      "step": 43700
    },
    {
      "epoch": 0.15502668724250704,
      "grad_norm": 6.176700115203857,
      "learning_rate": 0.0002534919938272479,
      "loss": 0.676,
      "step": 43800
    },
    {
      "epoch": 0.15538062945082326,
      "grad_norm": 0.035822559148073196,
      "learning_rate": 0.000253385811164753,
      "loss": 0.7282,
      "step": 43900
    },
    {
      "epoch": 0.1557345716591395,
      "grad_norm": 0.030137525871396065,
      "learning_rate": 0.00025327962850225813,
      "loss": 0.6674,
      "step": 44000
    },
    {
      "epoch": 0.15608851386745573,
      "grad_norm": 20.28973960876465,
      "learning_rate": 0.00025317344583976326,
      "loss": 0.6502,
      "step": 44100
    },
    {
      "epoch": 0.15644245607577195,
      "grad_norm": 1.403366208076477,
      "learning_rate": 0.0002530672631772684,
      "loss": 0.7867,
      "step": 44200
    },
    {
      "epoch": 0.15679639828408817,
      "grad_norm": 11.754390716552734,
      "learning_rate": 0.0002529610805147735,
      "loss": 0.6673,
      "step": 44300
    },
    {
      "epoch": 0.1571503404924044,
      "grad_norm": 0.02717110887169838,
      "learning_rate": 0.00025285489785227864,
      "loss": 0.6011,
      "step": 44400
    },
    {
      "epoch": 0.15750428270072062,
      "grad_norm": 29.6019287109375,
      "learning_rate": 0.0002527487151897838,
      "loss": 0.7599,
      "step": 44500
    },
    {
      "epoch": 0.15785822490903686,
      "grad_norm": 2.6383843421936035,
      "learning_rate": 0.0002526425325272889,
      "loss": 0.8324,
      "step": 44600
    },
    {
      "epoch": 0.1582121671173531,
      "grad_norm": 0.0027415037620812654,
      "learning_rate": 0.0002525363498647941,
      "loss": 0.7211,
      "step": 44700
    },
    {
      "epoch": 0.1585661093256693,
      "grad_norm": 58.753822326660156,
      "learning_rate": 0.0002524301672022992,
      "loss": 0.5639,
      "step": 44800
    },
    {
      "epoch": 0.15892005153398553,
      "grad_norm": 27.768505096435547,
      "learning_rate": 0.00025232398453980433,
      "loss": 0.5869,
      "step": 44900
    },
    {
      "epoch": 0.15927399374230175,
      "grad_norm": 0.0021758046932518482,
      "learning_rate": 0.00025221780187730945,
      "loss": 0.7837,
      "step": 45000
    },
    {
      "epoch": 0.15962793595061797,
      "grad_norm": 9.225341796875,
      "learning_rate": 0.0002521116192148146,
      "loss": 0.7893,
      "step": 45100
    },
    {
      "epoch": 0.15998187815893422,
      "grad_norm": 0.1785644143819809,
      "learning_rate": 0.0002520054365523197,
      "loss": 0.4766,
      "step": 45200
    },
    {
      "epoch": 0.16033582036725044,
      "grad_norm": 0.00347437197342515,
      "learning_rate": 0.00025189925388982484,
      "loss": 0.8519,
      "step": 45300
    },
    {
      "epoch": 0.16068976257556666,
      "grad_norm": 0.00197480502538383,
      "learning_rate": 0.00025179307122732996,
      "loss": 0.6149,
      "step": 45400
    },
    {
      "epoch": 0.16104370478388289,
      "grad_norm": 0.0011577756376937032,
      "learning_rate": 0.00025168688856483514,
      "loss": 0.7151,
      "step": 45500
    },
    {
      "epoch": 0.1613976469921991,
      "grad_norm": 1.6370927095413208,
      "learning_rate": 0.0002515807059023402,
      "loss": 0.7927,
      "step": 45600
    },
    {
      "epoch": 0.16175158920051533,
      "grad_norm": 53.864219665527344,
      "learning_rate": 0.0002514745232398454,
      "loss": 0.9842,
      "step": 45700
    },
    {
      "epoch": 0.16210553140883158,
      "grad_norm": 36.96323013305664,
      "learning_rate": 0.0002513683405773505,
      "loss": 0.5213,
      "step": 45800
    },
    {
      "epoch": 0.1624594736171478,
      "grad_norm": 260.9082336425781,
      "learning_rate": 0.00025126215791485565,
      "loss": 0.7276,
      "step": 45900
    },
    {
      "epoch": 0.16281341582546402,
      "grad_norm": 9.846388816833496,
      "learning_rate": 0.0002511559752523608,
      "loss": 0.8815,
      "step": 46000
    },
    {
      "epoch": 0.16316735803378024,
      "grad_norm": 21.56290054321289,
      "learning_rate": 0.0002510497925898659,
      "loss": 0.635,
      "step": 46100
    },
    {
      "epoch": 0.16352130024209646,
      "grad_norm": 0.12058349698781967,
      "learning_rate": 0.00025094360992737103,
      "loss": 0.9657,
      "step": 46200
    },
    {
      "epoch": 0.16387524245041268,
      "grad_norm": 0.006753317546099424,
      "learning_rate": 0.00025083742726487616,
      "loss": 0.6398,
      "step": 46300
    },
    {
      "epoch": 0.16422918465872893,
      "grad_norm": 75.29236602783203,
      "learning_rate": 0.0002507312446023813,
      "loss": 0.798,
      "step": 46400
    },
    {
      "epoch": 0.16458312686704515,
      "grad_norm": 12.144583702087402,
      "learning_rate": 0.0002506250619398864,
      "loss": 0.6058,
      "step": 46500
    },
    {
      "epoch": 0.16493706907536138,
      "grad_norm": 0.00872951652854681,
      "learning_rate": 0.0002505188792773916,
      "loss": 0.7806,
      "step": 46600
    },
    {
      "epoch": 0.1652910112836776,
      "grad_norm": 47.93341064453125,
      "learning_rate": 0.00025041269661489667,
      "loss": 0.6338,
      "step": 46700
    },
    {
      "epoch": 0.16564495349199382,
      "grad_norm": 4.544069290161133,
      "learning_rate": 0.00025030651395240185,
      "loss": 0.6064,
      "step": 46800
    },
    {
      "epoch": 0.16599889570031004,
      "grad_norm": 114.87174987792969,
      "learning_rate": 0.000250200331289907,
      "loss": 0.7698,
      "step": 46900
    },
    {
      "epoch": 0.1663528379086263,
      "grad_norm": 0.03766665980219841,
      "learning_rate": 0.0002500941486274121,
      "loss": 0.7781,
      "step": 47000
    },
    {
      "epoch": 0.1667067801169425,
      "grad_norm": 32.76826858520508,
      "learning_rate": 0.00024998796596491723,
      "loss": 0.6849,
      "step": 47100
    },
    {
      "epoch": 0.16706072232525873,
      "grad_norm": 29.346342086791992,
      "learning_rate": 0.00024988178330242236,
      "loss": 0.5203,
      "step": 47200
    },
    {
      "epoch": 0.16741466453357495,
      "grad_norm": 0.08478361368179321,
      "learning_rate": 0.0002497756006399275,
      "loss": 0.7811,
      "step": 47300
    },
    {
      "epoch": 0.16776860674189117,
      "grad_norm": 78.58329772949219,
      "learning_rate": 0.0002496694179774326,
      "loss": 0.5728,
      "step": 47400
    },
    {
      "epoch": 0.16812254895020742,
      "grad_norm": 0.007927541621029377,
      "learning_rate": 0.00024956323531493774,
      "loss": 0.5814,
      "step": 47500
    },
    {
      "epoch": 0.16847649115852364,
      "grad_norm": 0.010039475746452808,
      "learning_rate": 0.0002494570526524429,
      "loss": 0.7262,
      "step": 47600
    },
    {
      "epoch": 0.16883043336683987,
      "grad_norm": 0.009838440455496311,
      "learning_rate": 0.000249350869989948,
      "loss": 0.6662,
      "step": 47700
    },
    {
      "epoch": 0.1691843755751561,
      "grad_norm": 0.029637843370437622,
      "learning_rate": 0.0002492446873274532,
      "loss": 0.3369,
      "step": 47800
    },
    {
      "epoch": 0.1695383177834723,
      "grad_norm": 0.04056214913725853,
      "learning_rate": 0.0002491385046649583,
      "loss": 0.868,
      "step": 47900
    },
    {
      "epoch": 0.16989225999178853,
      "grad_norm": 0.12268264591693878,
      "learning_rate": 0.0002490323220024634,
      "loss": 0.5141,
      "step": 48000
    },
    {
      "epoch": 0.17024620220010478,
      "grad_norm": 0.1307104378938675,
      "learning_rate": 0.00024892613933996855,
      "loss": 0.6565,
      "step": 48100
    },
    {
      "epoch": 0.170600144408421,
      "grad_norm": 0.4444679319858551,
      "learning_rate": 0.0002488199566774737,
      "loss": 0.7586,
      "step": 48200
    },
    {
      "epoch": 0.17095408661673722,
      "grad_norm": 14.802724838256836,
      "learning_rate": 0.0002487137740149788,
      "loss": 0.8379,
      "step": 48300
    },
    {
      "epoch": 0.17130802882505344,
      "grad_norm": 39.75431442260742,
      "learning_rate": 0.00024860759135248394,
      "loss": 0.5698,
      "step": 48400
    },
    {
      "epoch": 0.17166197103336966,
      "grad_norm": 0.05133102089166641,
      "learning_rate": 0.00024850140868998906,
      "loss": 0.6103,
      "step": 48500
    },
    {
      "epoch": 0.17201591324168589,
      "grad_norm": 0.7378095388412476,
      "learning_rate": 0.00024839522602749424,
      "loss": 1.0112,
      "step": 48600
    },
    {
      "epoch": 0.17236985545000214,
      "grad_norm": 0.0018586126388981938,
      "learning_rate": 0.0002482890433649993,
      "loss": 0.616,
      "step": 48700
    },
    {
      "epoch": 0.17272379765831836,
      "grad_norm": 0.011207600124180317,
      "learning_rate": 0.0002481828607025045,
      "loss": 0.7281,
      "step": 48800
    },
    {
      "epoch": 0.17307773986663458,
      "grad_norm": 0.021314434707164764,
      "learning_rate": 0.0002480766780400096,
      "loss": 0.5414,
      "step": 48900
    },
    {
      "epoch": 0.1734316820749508,
      "grad_norm": 16.481046676635742,
      "learning_rate": 0.0002479704953775147,
      "loss": 0.6637,
      "step": 49000
    },
    {
      "epoch": 0.17378562428326702,
      "grad_norm": 0.38004806637763977,
      "learning_rate": 0.0002478643127150199,
      "loss": 0.5389,
      "step": 49100
    },
    {
      "epoch": 0.17413956649158324,
      "grad_norm": 0.01784871146082878,
      "learning_rate": 0.000247758130052525,
      "loss": 0.6067,
      "step": 49200
    },
    {
      "epoch": 0.1744935086998995,
      "grad_norm": 16.399858474731445,
      "learning_rate": 0.00024765194739003013,
      "loss": 0.656,
      "step": 49300
    },
    {
      "epoch": 0.1748474509082157,
      "grad_norm": 0.002148872474208474,
      "learning_rate": 0.00024754576472753526,
      "loss": 0.6811,
      "step": 49400
    },
    {
      "epoch": 0.17520139311653193,
      "grad_norm": 0.019459398463368416,
      "learning_rate": 0.0002474395820650404,
      "loss": 0.3512,
      "step": 49500
    },
    {
      "epoch": 0.17555533532484816,
      "grad_norm": 0.029044954106211662,
      "learning_rate": 0.0002473333994025455,
      "loss": 0.9182,
      "step": 49600
    },
    {
      "epoch": 0.17590927753316438,
      "grad_norm": 36.55784225463867,
      "learning_rate": 0.00024722721674005064,
      "loss": 0.5504,
      "step": 49700
    },
    {
      "epoch": 0.1762632197414806,
      "grad_norm": 0.001223058090545237,
      "learning_rate": 0.00024712103407755577,
      "loss": 0.6528,
      "step": 49800
    },
    {
      "epoch": 0.17661716194979685,
      "grad_norm": 0.07835223525762558,
      "learning_rate": 0.00024701485141506095,
      "loss": 0.4135,
      "step": 49900
    },
    {
      "epoch": 0.17697110415811307,
      "grad_norm": 33.394287109375,
      "learning_rate": 0.0002469086687525661,
      "loss": 0.8836,
      "step": 50000
    },
    {
      "epoch": 0.1773250463664293,
      "grad_norm": 28.18423843383789,
      "learning_rate": 0.0002468024860900712,
      "loss": 0.9282,
      "step": 50100
    },
    {
      "epoch": 0.1776789885747455,
      "grad_norm": 1.6709798574447632,
      "learning_rate": 0.00024669630342757633,
      "loss": 0.5233,
      "step": 50200
    },
    {
      "epoch": 0.17803293078306173,
      "grad_norm": 51.66534423828125,
      "learning_rate": 0.00024659012076508146,
      "loss": 0.7486,
      "step": 50300
    },
    {
      "epoch": 0.17838687299137798,
      "grad_norm": 0.022905955091118813,
      "learning_rate": 0.0002464839381025866,
      "loss": 0.7918,
      "step": 50400
    },
    {
      "epoch": 0.1787408151996942,
      "grad_norm": 28.958919525146484,
      "learning_rate": 0.0002463777554400917,
      "loss": 0.6386,
      "step": 50500
    },
    {
      "epoch": 0.17909475740801042,
      "grad_norm": 27.61915397644043,
      "learning_rate": 0.00024627157277759684,
      "loss": 0.5455,
      "step": 50600
    },
    {
      "epoch": 0.17944869961632665,
      "grad_norm": 9.652910232543945,
      "learning_rate": 0.00024616539011510196,
      "loss": 0.6248,
      "step": 50700
    },
    {
      "epoch": 0.17980264182464287,
      "grad_norm": 0.00016032658459153026,
      "learning_rate": 0.0002460592074526071,
      "loss": 0.7715,
      "step": 50800
    },
    {
      "epoch": 0.1801565840329591,
      "grad_norm": 51.485740661621094,
      "learning_rate": 0.00024595302479011227,
      "loss": 0.6312,
      "step": 50900
    },
    {
      "epoch": 0.18051052624127534,
      "grad_norm": 0.8240976333618164,
      "learning_rate": 0.0002458468421276174,
      "loss": 0.6605,
      "step": 51000
    },
    {
      "epoch": 0.18086446844959156,
      "grad_norm": 5.912894248962402,
      "learning_rate": 0.0002457406594651225,
      "loss": 0.7525,
      "step": 51100
    },
    {
      "epoch": 0.18121841065790778,
      "grad_norm": 106.22941589355469,
      "learning_rate": 0.00024563447680262765,
      "loss": 0.6344,
      "step": 51200
    },
    {
      "epoch": 0.181572352866224,
      "grad_norm": 0.08876483887434006,
      "learning_rate": 0.0002455282941401328,
      "loss": 0.8435,
      "step": 51300
    },
    {
      "epoch": 0.18192629507454022,
      "grad_norm": 34.98408889770508,
      "learning_rate": 0.0002454221114776379,
      "loss": 0.5718,
      "step": 51400
    },
    {
      "epoch": 0.18228023728285644,
      "grad_norm": 0.04174148291349411,
      "learning_rate": 0.00024531592881514303,
      "loss": 0.6087,
      "step": 51500
    },
    {
      "epoch": 0.1826341794911727,
      "grad_norm": 9.829266548156738,
      "learning_rate": 0.00024520974615264816,
      "loss": 0.6884,
      "step": 51600
    },
    {
      "epoch": 0.18298812169948891,
      "grad_norm": 97.04524993896484,
      "learning_rate": 0.00024510356349015334,
      "loss": 0.6029,
      "step": 51700
    },
    {
      "epoch": 0.18334206390780514,
      "grad_norm": 0.11874611675739288,
      "learning_rate": 0.0002449973808276584,
      "loss": 0.678,
      "step": 51800
    },
    {
      "epoch": 0.18369600611612136,
      "grad_norm": 69.36714935302734,
      "learning_rate": 0.0002448911981651636,
      "loss": 0.7923,
      "step": 51900
    },
    {
      "epoch": 0.18404994832443758,
      "grad_norm": 0.0009711026796139777,
      "learning_rate": 0.0002447850155026687,
      "loss": 0.7143,
      "step": 52000
    },
    {
      "epoch": 0.1844038905327538,
      "grad_norm": 0.0284415353089571,
      "learning_rate": 0.0002446788328401738,
      "loss": 0.6105,
      "step": 52100
    },
    {
      "epoch": 0.18475783274107005,
      "grad_norm": 1.5152456760406494,
      "learning_rate": 0.000244572650177679,
      "loss": 0.6044,
      "step": 52200
    },
    {
      "epoch": 0.18511177494938627,
      "grad_norm": 0.006316325627267361,
      "learning_rate": 0.0002444664675151841,
      "loss": 0.589,
      "step": 52300
    },
    {
      "epoch": 0.1854657171577025,
      "grad_norm": 28.284791946411133,
      "learning_rate": 0.00024436028485268923,
      "loss": 0.4418,
      "step": 52400
    },
    {
      "epoch": 0.1858196593660187,
      "grad_norm": 24.20500373840332,
      "learning_rate": 0.00024425410219019436,
      "loss": 0.6876,
      "step": 52500
    },
    {
      "epoch": 0.18617360157433493,
      "grad_norm": 39.4619140625,
      "learning_rate": 0.0002441479195276995,
      "loss": 0.536,
      "step": 52600
    },
    {
      "epoch": 0.18652754378265116,
      "grad_norm": 0.009991252794861794,
      "learning_rate": 0.00024404173686520464,
      "loss": 0.6785,
      "step": 52700
    },
    {
      "epoch": 0.1868814859909674,
      "grad_norm": 0.0018206741660833359,
      "learning_rate": 0.00024393555420270974,
      "loss": 0.5776,
      "step": 52800
    },
    {
      "epoch": 0.18723542819928363,
      "grad_norm": 0.0012952997349202633,
      "learning_rate": 0.0002438293715402149,
      "loss": 0.7707,
      "step": 52900
    },
    {
      "epoch": 0.18758937040759985,
      "grad_norm": 0.001108196098357439,
      "learning_rate": 0.00024372318887772002,
      "loss": 0.6953,
      "step": 53000
    },
    {
      "epoch": 0.18794331261591607,
      "grad_norm": 42.75584030151367,
      "learning_rate": 0.00024361700621522515,
      "loss": 0.8052,
      "step": 53100
    },
    {
      "epoch": 0.1882972548242323,
      "grad_norm": 30.422643661499023,
      "learning_rate": 0.00024351082355273027,
      "loss": 0.5324,
      "step": 53200
    },
    {
      "epoch": 0.1886511970325485,
      "grad_norm": 0.20822738111019135,
      "learning_rate": 0.00024340464089023543,
      "loss": 0.898,
      "step": 53300
    },
    {
      "epoch": 0.18900513924086476,
      "grad_norm": 0.8909003734588623,
      "learning_rate": 0.00024329845822774056,
      "loss": 0.5672,
      "step": 53400
    },
    {
      "epoch": 0.18935908144918098,
      "grad_norm": 3.8476502895355225,
      "learning_rate": 0.00024319227556524568,
      "loss": 0.7351,
      "step": 53500
    },
    {
      "epoch": 0.1897130236574972,
      "grad_norm": 41.6547737121582,
      "learning_rate": 0.0002430860929027508,
      "loss": 0.6188,
      "step": 53600
    },
    {
      "epoch": 0.19006696586581343,
      "grad_norm": 0.022409046068787575,
      "learning_rate": 0.00024297991024025596,
      "loss": 0.6351,
      "step": 53700
    },
    {
      "epoch": 0.19042090807412965,
      "grad_norm": 0.08597921580076218,
      "learning_rate": 0.00024287372757776106,
      "loss": 0.5327,
      "step": 53800
    },
    {
      "epoch": 0.1907748502824459,
      "grad_norm": 95.5647201538086,
      "learning_rate": 0.00024276754491526622,
      "loss": 0.7722,
      "step": 53900
    },
    {
      "epoch": 0.19112879249076212,
      "grad_norm": 0.0014487368753179908,
      "learning_rate": 0.00024266136225277135,
      "loss": 0.5847,
      "step": 54000
    },
    {
      "epoch": 0.19148273469907834,
      "grad_norm": 147.2389373779297,
      "learning_rate": 0.0002425551795902765,
      "loss": 0.6458,
      "step": 54100
    },
    {
      "epoch": 0.19183667690739456,
      "grad_norm": 0.6598365306854248,
      "learning_rate": 0.0002424489969277816,
      "loss": 0.6636,
      "step": 54200
    },
    {
      "epoch": 0.19219061911571078,
      "grad_norm": 0.26067790389060974,
      "learning_rate": 0.00024234281426528675,
      "loss": 0.5832,
      "step": 54300
    },
    {
      "epoch": 0.192544561324027,
      "grad_norm": 0.07632215321063995,
      "learning_rate": 0.00024223663160279188,
      "loss": 0.8031,
      "step": 54400
    },
    {
      "epoch": 0.19289850353234325,
      "grad_norm": 0.0027953393291682005,
      "learning_rate": 0.000242130448940297,
      "loss": 0.642,
      "step": 54500
    },
    {
      "epoch": 0.19325244574065947,
      "grad_norm": 16.159631729125977,
      "learning_rate": 0.00024202426627780213,
      "loss": 0.7326,
      "step": 54600
    },
    {
      "epoch": 0.1936063879489757,
      "grad_norm": 0.5427790880203247,
      "learning_rate": 0.0002419180836153073,
      "loss": 0.5234,
      "step": 54700
    },
    {
      "epoch": 0.19396033015729192,
      "grad_norm": 42.45887756347656,
      "learning_rate": 0.0002418119009528124,
      "loss": 0.6654,
      "step": 54800
    },
    {
      "epoch": 0.19431427236560814,
      "grad_norm": 0.0012695861514657736,
      "learning_rate": 0.00024170571829031754,
      "loss": 0.8372,
      "step": 54900
    },
    {
      "epoch": 0.19466821457392436,
      "grad_norm": 0.3425418734550476,
      "learning_rate": 0.00024159953562782267,
      "loss": 0.5951,
      "step": 55000
    },
    {
      "epoch": 0.1950221567822406,
      "grad_norm": 12.282039642333984,
      "learning_rate": 0.00024149335296532782,
      "loss": 0.4417,
      "step": 55100
    },
    {
      "epoch": 0.19537609899055683,
      "grad_norm": 0.012095840647816658,
      "learning_rate": 0.00024138717030283292,
      "loss": 0.7793,
      "step": 55200
    },
    {
      "epoch": 0.19573004119887305,
      "grad_norm": 0.0010515169706195593,
      "learning_rate": 0.00024128098764033805,
      "loss": 0.7615,
      "step": 55300
    },
    {
      "epoch": 0.19608398340718927,
      "grad_norm": 0.04763033986091614,
      "learning_rate": 0.0002411748049778432,
      "loss": 0.5191,
      "step": 55400
    },
    {
      "epoch": 0.1964379256155055,
      "grad_norm": 0.010414720512926579,
      "learning_rate": 0.0002410686223153483,
      "loss": 0.4381,
      "step": 55500
    },
    {
      "epoch": 0.19679186782382171,
      "grad_norm": 0.0029427548870444298,
      "learning_rate": 0.00024096243965285346,
      "loss": 0.6122,
      "step": 55600
    },
    {
      "epoch": 0.19714581003213796,
      "grad_norm": 0.02455226331949234,
      "learning_rate": 0.00024085625699035859,
      "loss": 0.681,
      "step": 55700
    },
    {
      "epoch": 0.19749975224045418,
      "grad_norm": 0.004535209853202105,
      "learning_rate": 0.00024075007432786374,
      "loss": 0.6141,
      "step": 55800
    },
    {
      "epoch": 0.1978536944487704,
      "grad_norm": 0.5456475019454956,
      "learning_rate": 0.00024064389166536884,
      "loss": 0.5532,
      "step": 55900
    },
    {
      "epoch": 0.19820763665708663,
      "grad_norm": 0.25506457686424255,
      "learning_rate": 0.000240537709002874,
      "loss": 0.4171,
      "step": 56000
    },
    {
      "epoch": 0.19856157886540285,
      "grad_norm": 0.0018890254432335496,
      "learning_rate": 0.00024043152634037912,
      "loss": 0.6053,
      "step": 56100
    },
    {
      "epoch": 0.19891552107371907,
      "grad_norm": 82.57660675048828,
      "learning_rate": 0.00024032534367788425,
      "loss": 0.5857,
      "step": 56200
    },
    {
      "epoch": 0.19926946328203532,
      "grad_norm": 0.050279710441827774,
      "learning_rate": 0.00024021916101538937,
      "loss": 0.573,
      "step": 56300
    },
    {
      "epoch": 0.19962340549035154,
      "grad_norm": 11.907794952392578,
      "learning_rate": 0.00024011297835289453,
      "loss": 0.6929,
      "step": 56400
    },
    {
      "epoch": 0.19997734769866776,
      "grad_norm": 0.0908779576420784,
      "learning_rate": 0.00024000679569039966,
      "loss": 0.4982,
      "step": 56500
    },
    {
      "epoch": 0.20033128990698398,
      "grad_norm": 0.0022973623126745224,
      "learning_rate": 0.00023990061302790478,
      "loss": 0.5148,
      "step": 56600
    },
    {
      "epoch": 0.2006852321153002,
      "grad_norm": 19.77754020690918,
      "learning_rate": 0.0002397944303654099,
      "loss": 0.5312,
      "step": 56700
    },
    {
      "epoch": 0.20103917432361643,
      "grad_norm": 0.04234253242611885,
      "learning_rate": 0.00023968824770291506,
      "loss": 0.6539,
      "step": 56800
    },
    {
      "epoch": 0.20139311653193268,
      "grad_norm": 1.05698561668396,
      "learning_rate": 0.00023958206504042016,
      "loss": 0.744,
      "step": 56900
    },
    {
      "epoch": 0.2017470587402489,
      "grad_norm": 0.8616833090782166,
      "learning_rate": 0.00023947588237792532,
      "loss": 0.5431,
      "step": 57000
    },
    {
      "epoch": 0.20210100094856512,
      "grad_norm": 0.014497307129204273,
      "learning_rate": 0.00023936969971543044,
      "loss": 0.5056,
      "step": 57100
    },
    {
      "epoch": 0.20245494315688134,
      "grad_norm": 73.64208984375,
      "learning_rate": 0.00023926351705293557,
      "loss": 0.5914,
      "step": 57200
    },
    {
      "epoch": 0.20280888536519756,
      "grad_norm": 0.01626821607351303,
      "learning_rate": 0.0002391573343904407,
      "loss": 0.478,
      "step": 57300
    },
    {
      "epoch": 0.2031628275735138,
      "grad_norm": 0.11304982751607895,
      "learning_rate": 0.00023905115172794585,
      "loss": 0.6367,
      "step": 57400
    },
    {
      "epoch": 0.20351676978183003,
      "grad_norm": 0.00742335245013237,
      "learning_rate": 0.00023894496906545098,
      "loss": 0.6925,
      "step": 57500
    },
    {
      "epoch": 0.20387071199014625,
      "grad_norm": 0.005893389228731394,
      "learning_rate": 0.0002388387864029561,
      "loss": 0.7629,
      "step": 57600
    },
    {
      "epoch": 0.20422465419846247,
      "grad_norm": 0.016157403588294983,
      "learning_rate": 0.00023873260374046123,
      "loss": 0.6851,
      "step": 57700
    },
    {
      "epoch": 0.2045785964067787,
      "grad_norm": 0.07022003084421158,
      "learning_rate": 0.0002386264210779664,
      "loss": 0.5322,
      "step": 57800
    },
    {
      "epoch": 0.20493253861509492,
      "grad_norm": 16.785030364990234,
      "learning_rate": 0.0002385202384154715,
      "loss": 0.5406,
      "step": 57900
    },
    {
      "epoch": 0.20528648082341117,
      "grad_norm": 10.924997329711914,
      "learning_rate": 0.00023841405575297664,
      "loss": 0.6624,
      "step": 58000
    },
    {
      "epoch": 0.2056404230317274,
      "grad_norm": 1.054756760597229,
      "learning_rate": 0.00023830787309048177,
      "loss": 0.6665,
      "step": 58100
    },
    {
      "epoch": 0.2059943652400436,
      "grad_norm": 0.012695015408098698,
      "learning_rate": 0.00023820169042798692,
      "loss": 0.5855,
      "step": 58200
    },
    {
      "epoch": 0.20634830744835983,
      "grad_norm": 0.40368300676345825,
      "learning_rate": 0.00023809550776549202,
      "loss": 0.5029,
      "step": 58300
    },
    {
      "epoch": 0.20670224965667605,
      "grad_norm": 0.0007679344853386283,
      "learning_rate": 0.00023798932510299718,
      "loss": 0.589,
      "step": 58400
    },
    {
      "epoch": 0.20705619186499227,
      "grad_norm": 0.07127822190523148,
      "learning_rate": 0.0002378831424405023,
      "loss": 0.4059,
      "step": 58500
    },
    {
      "epoch": 0.20741013407330852,
      "grad_norm": 98.28783416748047,
      "learning_rate": 0.0002377769597780074,
      "loss": 0.5361,
      "step": 58600
    },
    {
      "epoch": 0.20776407628162474,
      "grad_norm": 0.0019998501520603895,
      "learning_rate": 0.00023767077711551256,
      "loss": 0.5579,
      "step": 58700
    },
    {
      "epoch": 0.20811801848994096,
      "grad_norm": 41.453311920166016,
      "learning_rate": 0.00023756459445301769,
      "loss": 0.5881,
      "step": 58800
    },
    {
      "epoch": 0.20847196069825719,
      "grad_norm": 58.43058776855469,
      "learning_rate": 0.0002374584117905228,
      "loss": 0.7134,
      "step": 58900
    },
    {
      "epoch": 0.2088259029065734,
      "grad_norm": 0.049378521740436554,
      "learning_rate": 0.00023735222912802794,
      "loss": 0.6777,
      "step": 59000
    },
    {
      "epoch": 0.20917984511488963,
      "grad_norm": 0.0009024893515743315,
      "learning_rate": 0.0002372460464655331,
      "loss": 0.504,
      "step": 59100
    },
    {
      "epoch": 0.20953378732320588,
      "grad_norm": 0.802407443523407,
      "learning_rate": 0.00023713986380303822,
      "loss": 0.5065,
      "step": 59200
    },
    {
      "epoch": 0.2098877295315221,
      "grad_norm": 22.52256202697754,
      "learning_rate": 0.00023703368114054335,
      "loss": 0.5117,
      "step": 59300
    },
    {
      "epoch": 0.21024167173983832,
      "grad_norm": 0.021256379783153534,
      "learning_rate": 0.00023692749847804847,
      "loss": 0.7268,
      "step": 59400
    },
    {
      "epoch": 0.21059561394815454,
      "grad_norm": 0.3797862231731415,
      "learning_rate": 0.00023682131581555363,
      "loss": 0.585,
      "step": 59500
    },
    {
      "epoch": 0.21094955615647076,
      "grad_norm": 59.999996185302734,
      "learning_rate": 0.00023671513315305873,
      "loss": 0.6938,
      "step": 59600
    },
    {
      "epoch": 0.21130349836478698,
      "grad_norm": 0.023646388202905655,
      "learning_rate": 0.00023660895049056388,
      "loss": 0.7679,
      "step": 59700
    },
    {
      "epoch": 0.21165744057310323,
      "grad_norm": 0.021082986146211624,
      "learning_rate": 0.000236502767828069,
      "loss": 0.4887,
      "step": 59800
    },
    {
      "epoch": 0.21201138278141946,
      "grad_norm": 0.39226117730140686,
      "learning_rate": 0.00023639658516557416,
      "loss": 0.4642,
      "step": 59900
    },
    {
      "epoch": 0.21236532498973568,
      "grad_norm": 0.6232686638832092,
      "learning_rate": 0.00023629040250307926,
      "loss": 0.7604,
      "step": 60000
    },
    {
      "epoch": 0.2127192671980519,
      "grad_norm": 0.043643102049827576,
      "learning_rate": 0.00023618421984058442,
      "loss": 0.6616,
      "step": 60100
    },
    {
      "epoch": 0.21307320940636812,
      "grad_norm": 77.71270751953125,
      "learning_rate": 0.00023607803717808954,
      "loss": 0.6293,
      "step": 60200
    },
    {
      "epoch": 0.21342715161468434,
      "grad_norm": 0.12072785198688507,
      "learning_rate": 0.00023597185451559467,
      "loss": 0.6884,
      "step": 60300
    },
    {
      "epoch": 0.2137810938230006,
      "grad_norm": 55.812294006347656,
      "learning_rate": 0.0002358656718530998,
      "loss": 0.8071,
      "step": 60400
    },
    {
      "epoch": 0.2141350360313168,
      "grad_norm": 0.07566922158002853,
      "learning_rate": 0.00023575948919060495,
      "loss": 0.816,
      "step": 60500
    },
    {
      "epoch": 0.21448897823963303,
      "grad_norm": 6.4715728759765625,
      "learning_rate": 0.00023565330652811005,
      "loss": 0.6834,
      "step": 60600
    },
    {
      "epoch": 0.21484292044794925,
      "grad_norm": 4.4308617361821234e-05,
      "learning_rate": 0.0002355471238656152,
      "loss": 0.7014,
      "step": 60700
    },
    {
      "epoch": 0.21519686265626548,
      "grad_norm": 0.003134550992399454,
      "learning_rate": 0.00023544094120312033,
      "loss": 0.4883,
      "step": 60800
    },
    {
      "epoch": 0.21555080486458172,
      "grad_norm": 0.0062415325082838535,
      "learning_rate": 0.0002353347585406255,
      "loss": 0.7452,
      "step": 60900
    },
    {
      "epoch": 0.21590474707289795,
      "grad_norm": 65.1856918334961,
      "learning_rate": 0.0002352285758781306,
      "loss": 0.8525,
      "step": 61000
    },
    {
      "epoch": 0.21625868928121417,
      "grad_norm": 0.0026942146942019463,
      "learning_rate": 0.00023512239321563574,
      "loss": 0.4488,
      "step": 61100
    },
    {
      "epoch": 0.2166126314895304,
      "grad_norm": 0.0516231507062912,
      "learning_rate": 0.00023501621055314087,
      "loss": 0.5739,
      "step": 61200
    },
    {
      "epoch": 0.2169665736978466,
      "grad_norm": 26.45957374572754,
      "learning_rate": 0.00023491002789064597,
      "loss": 0.4649,
      "step": 61300
    },
    {
      "epoch": 0.21732051590616283,
      "grad_norm": 0.0009546856745146215,
      "learning_rate": 0.00023480384522815112,
      "loss": 0.7396,
      "step": 61400
    },
    {
      "epoch": 0.21767445811447908,
      "grad_norm": 0.011234255507588387,
      "learning_rate": 0.00023469766256565628,
      "loss": 0.5015,
      "step": 61500
    },
    {
      "epoch": 0.2180284003227953,
      "grad_norm": 41.50663757324219,
      "learning_rate": 0.0002345914799031614,
      "loss": 0.741,
      "step": 61600
    },
    {
      "epoch": 0.21838234253111152,
      "grad_norm": 8.493412971496582,
      "learning_rate": 0.0002344852972406665,
      "loss": 0.4316,
      "step": 61700
    },
    {
      "epoch": 0.21873628473942774,
      "grad_norm": 50.55502700805664,
      "learning_rate": 0.00023437911457817166,
      "loss": 0.6387,
      "step": 61800
    },
    {
      "epoch": 0.21909022694774397,
      "grad_norm": 33.03248596191406,
      "learning_rate": 0.00023427293191567678,
      "loss": 0.7474,
      "step": 61900
    },
    {
      "epoch": 0.2194441691560602,
      "grad_norm": 0.022378889843821526,
      "learning_rate": 0.0002341667492531819,
      "loss": 0.6914,
      "step": 62000
    },
    {
      "epoch": 0.21979811136437644,
      "grad_norm": 0.9923698902130127,
      "learning_rate": 0.00023406056659068704,
      "loss": 0.613,
      "step": 62100
    },
    {
      "epoch": 0.22015205357269266,
      "grad_norm": 1.4865244626998901,
      "learning_rate": 0.0002339543839281922,
      "loss": 0.6851,
      "step": 62200
    },
    {
      "epoch": 0.22050599578100888,
      "grad_norm": 0.02700992114841938,
      "learning_rate": 0.00023384820126569732,
      "loss": 0.7552,
      "step": 62300
    },
    {
      "epoch": 0.2208599379893251,
      "grad_norm": 87.35425567626953,
      "learning_rate": 0.00023374201860320245,
      "loss": 0.5111,
      "step": 62400
    },
    {
      "epoch": 0.22121388019764132,
      "grad_norm": 0.0003373320505488664,
      "learning_rate": 0.00023363583594070757,
      "loss": 0.5104,
      "step": 62500
    },
    {
      "epoch": 0.22156782240595754,
      "grad_norm": 12.376291275024414,
      "learning_rate": 0.00023352965327821273,
      "loss": 0.5801,
      "step": 62600
    },
    {
      "epoch": 0.2219217646142738,
      "grad_norm": 0.0013802079483866692,
      "learning_rate": 0.00023342347061571783,
      "loss": 0.6335,
      "step": 62700
    },
    {
      "epoch": 0.22227570682259,
      "grad_norm": 0.004245092160999775,
      "learning_rate": 0.00023331728795322298,
      "loss": 0.5358,
      "step": 62800
    },
    {
      "epoch": 0.22262964903090623,
      "grad_norm": 0.0012535196729004383,
      "learning_rate": 0.0002332111052907281,
      "loss": 0.7001,
      "step": 62900
    },
    {
      "epoch": 0.22298359123922246,
      "grad_norm": 0.06673852354288101,
      "learning_rate": 0.00023310492262823324,
      "loss": 0.9824,
      "step": 63000
    },
    {
      "epoch": 0.22333753344753868,
      "grad_norm": 131.25860595703125,
      "learning_rate": 0.00023299873996573836,
      "loss": 0.6498,
      "step": 63100
    },
    {
      "epoch": 0.2236914756558549,
      "grad_norm": 0.1278112679719925,
      "learning_rate": 0.00023289255730324352,
      "loss": 0.6769,
      "step": 63200
    },
    {
      "epoch": 0.22404541786417115,
      "grad_norm": 0.0242126677185297,
      "learning_rate": 0.00023278637464074864,
      "loss": 0.5203,
      "step": 63300
    },
    {
      "epoch": 0.22439936007248737,
      "grad_norm": 47.30770492553711,
      "learning_rate": 0.00023268019197825377,
      "loss": 0.5889,
      "step": 63400
    },
    {
      "epoch": 0.2247533022808036,
      "grad_norm": 0.12148036807775497,
      "learning_rate": 0.0002325740093157589,
      "loss": 0.517,
      "step": 63500
    },
    {
      "epoch": 0.2251072444891198,
      "grad_norm": 0.0011545508168637753,
      "learning_rate": 0.00023246782665326405,
      "loss": 0.6873,
      "step": 63600
    },
    {
      "epoch": 0.22546118669743603,
      "grad_norm": 0.02129247598350048,
      "learning_rate": 0.00023236164399076915,
      "loss": 0.5822,
      "step": 63700
    },
    {
      "epoch": 0.22581512890575228,
      "grad_norm": 0.0006344378343783319,
      "learning_rate": 0.0002322554613282743,
      "loss": 0.474,
      "step": 63800
    },
    {
      "epoch": 0.2261690711140685,
      "grad_norm": 0.04947756603360176,
      "learning_rate": 0.00023214927866577943,
      "loss": 0.4731,
      "step": 63900
    },
    {
      "epoch": 0.22652301332238473,
      "grad_norm": 0.019044578075408936,
      "learning_rate": 0.0002320430960032846,
      "loss": 0.6297,
      "step": 64000
    },
    {
      "epoch": 0.22687695553070095,
      "grad_norm": 1.8533638715744019,
      "learning_rate": 0.0002319369133407897,
      "loss": 0.6271,
      "step": 64100
    },
    {
      "epoch": 0.22723089773901717,
      "grad_norm": 0.01317841187119484,
      "learning_rate": 0.00023183073067829484,
      "loss": 0.6008,
      "step": 64200
    },
    {
      "epoch": 0.2275848399473334,
      "grad_norm": 0.05902509763836861,
      "learning_rate": 0.00023172454801579997,
      "loss": 0.5056,
      "step": 64300
    },
    {
      "epoch": 0.22793878215564964,
      "grad_norm": 0.04395325109362602,
      "learning_rate": 0.00023161836535330507,
      "loss": 0.5941,
      "step": 64400
    },
    {
      "epoch": 0.22829272436396586,
      "grad_norm": 0.005787978880107403,
      "learning_rate": 0.00023151218269081022,
      "loss": 0.628,
      "step": 64500
    },
    {
      "epoch": 0.22864666657228208,
      "grad_norm": 27.21927833557129,
      "learning_rate": 0.00023140600002831538,
      "loss": 0.8001,
      "step": 64600
    },
    {
      "epoch": 0.2290006087805983,
      "grad_norm": 0.01538899540901184,
      "learning_rate": 0.00023129981736582048,
      "loss": 0.6019,
      "step": 64700
    },
    {
      "epoch": 0.22935455098891452,
      "grad_norm": 1.3970266580581665,
      "learning_rate": 0.0002311936347033256,
      "loss": 0.4902,
      "step": 64800
    },
    {
      "epoch": 0.22970849319723075,
      "grad_norm": 0.03136025741696358,
      "learning_rate": 0.00023108745204083076,
      "loss": 0.4999,
      "step": 64900
    },
    {
      "epoch": 0.230062435405547,
      "grad_norm": 0.07360921800136566,
      "learning_rate": 0.0002309812693783359,
      "loss": 0.5824,
      "step": 65000
    },
    {
      "epoch": 0.23041637761386322,
      "grad_norm": 0.0056391265243291855,
      "learning_rate": 0.000230875086715841,
      "loss": 0.422,
      "step": 65100
    },
    {
      "epoch": 0.23077031982217944,
      "grad_norm": 13.844613075256348,
      "learning_rate": 0.00023076890405334614,
      "loss": 0.5405,
      "step": 65200
    },
    {
      "epoch": 0.23112426203049566,
      "grad_norm": 65.76300811767578,
      "learning_rate": 0.0002306627213908513,
      "loss": 0.4462,
      "step": 65300
    },
    {
      "epoch": 0.23147820423881188,
      "grad_norm": 0.199738547205925,
      "learning_rate": 0.0002305565387283564,
      "loss": 0.4605,
      "step": 65400
    },
    {
      "epoch": 0.2318321464471281,
      "grad_norm": 0.0016276179812848568,
      "learning_rate": 0.00023045035606586155,
      "loss": 0.5007,
      "step": 65500
    },
    {
      "epoch": 0.23218608865544435,
      "grad_norm": 0.0034294200595468283,
      "learning_rate": 0.00023034417340336667,
      "loss": 0.8364,
      "step": 65600
    },
    {
      "epoch": 0.23254003086376057,
      "grad_norm": 21.91857147216797,
      "learning_rate": 0.00023023799074087183,
      "loss": 0.6592,
      "step": 65700
    },
    {
      "epoch": 0.2328939730720768,
      "grad_norm": 0.003627269295975566,
      "learning_rate": 0.00023013180807837693,
      "loss": 0.3982,
      "step": 65800
    },
    {
      "epoch": 0.23324791528039301,
      "grad_norm": 0.003498760284855962,
      "learning_rate": 0.00023002562541588208,
      "loss": 0.4513,
      "step": 65900
    },
    {
      "epoch": 0.23360185748870924,
      "grad_norm": 0.002888878807425499,
      "learning_rate": 0.0002299194427533872,
      "loss": 0.3776,
      "step": 66000
    },
    {
      "epoch": 0.23395579969702546,
      "grad_norm": 2.5405640602111816,
      "learning_rate": 0.00022981326009089234,
      "loss": 0.7162,
      "step": 66100
    },
    {
      "epoch": 0.2343097419053417,
      "grad_norm": 10.37153434753418,
      "learning_rate": 0.00022970707742839746,
      "loss": 0.6323,
      "step": 66200
    },
    {
      "epoch": 0.23466368411365793,
      "grad_norm": 0.06039111688733101,
      "learning_rate": 0.00022960089476590262,
      "loss": 0.5243,
      "step": 66300
    },
    {
      "epoch": 0.23501762632197415,
      "grad_norm": 0.002463160315528512,
      "learning_rate": 0.00022949471210340774,
      "loss": 0.571,
      "step": 66400
    },
    {
      "epoch": 0.23537156853029037,
      "grad_norm": 0.0032260222360491753,
      "learning_rate": 0.00022938852944091287,
      "loss": 0.4355,
      "step": 66500
    },
    {
      "epoch": 0.2357255107386066,
      "grad_norm": 0.0013817882863804698,
      "learning_rate": 0.000229282346778418,
      "loss": 0.7258,
      "step": 66600
    },
    {
      "epoch": 0.2360794529469228,
      "grad_norm": 0.012854141183197498,
      "learning_rate": 0.00022917616411592315,
      "loss": 0.5478,
      "step": 66700
    },
    {
      "epoch": 0.23643339515523906,
      "grad_norm": 40.666629791259766,
      "learning_rate": 0.00022906998145342825,
      "loss": 0.4687,
      "step": 66800
    },
    {
      "epoch": 0.23678733736355528,
      "grad_norm": 0.0006915736012160778,
      "learning_rate": 0.0002289637987909334,
      "loss": 0.6292,
      "step": 66900
    },
    {
      "epoch": 0.2371412795718715,
      "grad_norm": 0.13302549719810486,
      "learning_rate": 0.00022885761612843853,
      "loss": 0.508,
      "step": 67000
    },
    {
      "epoch": 0.23749522178018773,
      "grad_norm": 0.0053052157163619995,
      "learning_rate": 0.00022875143346594363,
      "loss": 0.8784,
      "step": 67100
    },
    {
      "epoch": 0.23784916398850395,
      "grad_norm": 17.933557510375977,
      "learning_rate": 0.0002286452508034488,
      "loss": 0.8511,
      "step": 67200
    },
    {
      "epoch": 0.2382031061968202,
      "grad_norm": 0.3951636254787445,
      "learning_rate": 0.00022853906814095394,
      "loss": 0.4888,
      "step": 67300
    },
    {
      "epoch": 0.23855704840513642,
      "grad_norm": 83.28270721435547,
      "learning_rate": 0.00022843288547845907,
      "loss": 0.6044,
      "step": 67400
    },
    {
      "epoch": 0.23891099061345264,
      "grad_norm": 31.227014541625977,
      "learning_rate": 0.00022832670281596417,
      "loss": 0.6415,
      "step": 67500
    },
    {
      "epoch": 0.23926493282176886,
      "grad_norm": 1.402417778968811,
      "learning_rate": 0.00022822052015346932,
      "loss": 0.5359,
      "step": 67600
    },
    {
      "epoch": 0.23961887503008508,
      "grad_norm": 0.0017812406877055764,
      "learning_rate": 0.00022811433749097448,
      "loss": 0.3966,
      "step": 67700
    },
    {
      "epoch": 0.2399728172384013,
      "grad_norm": 0.030247271060943604,
      "learning_rate": 0.00022800815482847958,
      "loss": 0.4047,
      "step": 67800
    },
    {
      "epoch": 0.24032675944671755,
      "grad_norm": 0.0009784637950360775,
      "learning_rate": 0.0002279019721659847,
      "loss": 0.3948,
      "step": 67900
    },
    {
      "epoch": 0.24068070165503377,
      "grad_norm": 0.004583772737532854,
      "learning_rate": 0.00022779578950348986,
      "loss": 0.4327,
      "step": 68000
    },
    {
      "epoch": 0.24103464386335,
      "grad_norm": 0.036776330322027206,
      "learning_rate": 0.000227689606840995,
      "loss": 0.6523,
      "step": 68100
    },
    {
      "epoch": 0.24138858607166622,
      "grad_norm": 0.005093471612781286,
      "learning_rate": 0.0002275834241785001,
      "loss": 0.6381,
      "step": 68200
    },
    {
      "epoch": 0.24174252827998244,
      "grad_norm": 0.018497833982110023,
      "learning_rate": 0.00022747724151600524,
      "loss": 0.5704,
      "step": 68300
    },
    {
      "epoch": 0.24209647048829866,
      "grad_norm": 2.4035565853118896,
      "learning_rate": 0.0002273710588535104,
      "loss": 0.7439,
      "step": 68400
    },
    {
      "epoch": 0.2424504126966149,
      "grad_norm": 0.010150866582989693,
      "learning_rate": 0.0002272648761910155,
      "loss": 0.7586,
      "step": 68500
    },
    {
      "epoch": 0.24280435490493113,
      "grad_norm": 36.42326736450195,
      "learning_rate": 0.00022715869352852065,
      "loss": 0.5733,
      "step": 68600
    },
    {
      "epoch": 0.24315829711324735,
      "grad_norm": 0.09588570892810822,
      "learning_rate": 0.00022705251086602577,
      "loss": 0.5974,
      "step": 68700
    },
    {
      "epoch": 0.24351223932156357,
      "grad_norm": 0.46386075019836426,
      "learning_rate": 0.0002269463282035309,
      "loss": 0.6439,
      "step": 68800
    },
    {
      "epoch": 0.2438661815298798,
      "grad_norm": 7.792166233062744,
      "learning_rate": 0.00022684014554103603,
      "loss": 0.4401,
      "step": 68900
    },
    {
      "epoch": 0.24422012373819602,
      "grad_norm": 16.782928466796875,
      "learning_rate": 0.00022673396287854118,
      "loss": 0.5571,
      "step": 69000
    },
    {
      "epoch": 0.24457406594651226,
      "grad_norm": 0.03939970210194588,
      "learning_rate": 0.0002266277802160463,
      "loss": 0.5429,
      "step": 69100
    },
    {
      "epoch": 0.24492800815482849,
      "grad_norm": 20.859708786010742,
      "learning_rate": 0.00022652159755355144,
      "loss": 0.3315,
      "step": 69200
    },
    {
      "epoch": 0.2452819503631447,
      "grad_norm": 0.00019168120343238115,
      "learning_rate": 0.00022641541489105656,
      "loss": 0.8574,
      "step": 69300
    },
    {
      "epoch": 0.24563589257146093,
      "grad_norm": 0.012550211511552334,
      "learning_rate": 0.00022630923222856172,
      "loss": 0.4895,
      "step": 69400
    },
    {
      "epoch": 0.24598983477977715,
      "grad_norm": 0.020156726241111755,
      "learning_rate": 0.00022620304956606682,
      "loss": 0.4321,
      "step": 69500
    },
    {
      "epoch": 0.24634377698809337,
      "grad_norm": 51.08183670043945,
      "learning_rate": 0.00022609686690357197,
      "loss": 0.6665,
      "step": 69600
    },
    {
      "epoch": 0.24669771919640962,
      "grad_norm": 0.056832823902368546,
      "learning_rate": 0.0002259906842410771,
      "loss": 0.5578,
      "step": 69700
    },
    {
      "epoch": 0.24705166140472584,
      "grad_norm": 45.00947952270508,
      "learning_rate": 0.00022588450157858225,
      "loss": 0.4837,
      "step": 69800
    },
    {
      "epoch": 0.24740560361304206,
      "grad_norm": 102.06136322021484,
      "learning_rate": 0.00022577831891608735,
      "loss": 0.6101,
      "step": 69900
    },
    {
      "epoch": 0.24775954582135828,
      "grad_norm": 66.20899200439453,
      "learning_rate": 0.0002256721362535925,
      "loss": 0.8322,
      "step": 70000
    },
    {
      "epoch": 0.2481134880296745,
      "grad_norm": 0.06309425085783005,
      "learning_rate": 0.00022556595359109763,
      "loss": 0.5294,
      "step": 70100
    },
    {
      "epoch": 0.24846743023799073,
      "grad_norm": 0.00029336303123272955,
      "learning_rate": 0.00022545977092860273,
      "loss": 0.4713,
      "step": 70200
    },
    {
      "epoch": 0.24882137244630698,
      "grad_norm": 42.21486282348633,
      "learning_rate": 0.00022535358826610789,
      "loss": 0.5807,
      "step": 70300
    },
    {
      "epoch": 0.2491753146546232,
      "grad_norm": 33.86411666870117,
      "learning_rate": 0.00022524740560361304,
      "loss": 0.5648,
      "step": 70400
    },
    {
      "epoch": 0.24952925686293942,
      "grad_norm": 41.10194396972656,
      "learning_rate": 0.00022514122294111814,
      "loss": 0.387,
      "step": 70500
    },
    {
      "epoch": 0.24988319907125564,
      "grad_norm": 2.7319934368133545,
      "learning_rate": 0.00022503504027862327,
      "loss": 0.3426,
      "step": 70600
    },
    {
      "epoch": 0.25023714127957186,
      "grad_norm": 0.00032501426176168025,
      "learning_rate": 0.00022492885761612842,
      "loss": 0.7721,
      "step": 70700
    },
    {
      "epoch": 0.2505910834878881,
      "grad_norm": 3.2708323001861572,
      "learning_rate": 0.00022482267495363358,
      "loss": 0.5582,
      "step": 70800
    },
    {
      "epoch": 0.2509450256962043,
      "grad_norm": 5.0880022048950195,
      "learning_rate": 0.00022471649229113868,
      "loss": 0.5108,
      "step": 70900
    },
    {
      "epoch": 0.25129896790452055,
      "grad_norm": 0.0019316986436024308,
      "learning_rate": 0.0002246103096286438,
      "loss": 0.4184,
      "step": 71000
    },
    {
      "epoch": 0.2516529101128368,
      "grad_norm": 0.2098011076450348,
      "learning_rate": 0.00022450412696614896,
      "loss": 0.2697,
      "step": 71100
    },
    {
      "epoch": 0.252006852321153,
      "grad_norm": 0.04678136110305786,
      "learning_rate": 0.00022439794430365406,
      "loss": 0.8063,
      "step": 71200
    },
    {
      "epoch": 0.25236079452946925,
      "grad_norm": 0.6238464117050171,
      "learning_rate": 0.0002242917616411592,
      "loss": 0.8,
      "step": 71300
    },
    {
      "epoch": 0.25271473673778544,
      "grad_norm": 0.08936027437448502,
      "learning_rate": 0.00022418557897866434,
      "loss": 0.4576,
      "step": 71400
    },
    {
      "epoch": 0.2530686789461017,
      "grad_norm": 3.0651872158050537,
      "learning_rate": 0.0002240793963161695,
      "loss": 0.613,
      "step": 71500
    },
    {
      "epoch": 0.2534226211544179,
      "grad_norm": 0.016391359269618988,
      "learning_rate": 0.0002239732136536746,
      "loss": 0.5842,
      "step": 71600
    },
    {
      "epoch": 0.25377656336273413,
      "grad_norm": 7.650646686553955,
      "learning_rate": 0.00022386703099117975,
      "loss": 0.6898,
      "step": 71700
    },
    {
      "epoch": 0.2541305055710504,
      "grad_norm": 0.0009385911398567259,
      "learning_rate": 0.00022376084832868487,
      "loss": 0.5152,
      "step": 71800
    },
    {
      "epoch": 0.2544844477793666,
      "grad_norm": 32.067440032958984,
      "learning_rate": 0.00022365466566619,
      "loss": 0.6056,
      "step": 71900
    },
    {
      "epoch": 0.2548383899876828,
      "grad_norm": 0.011000525206327438,
      "learning_rate": 0.00022354848300369513,
      "loss": 0.6388,
      "step": 72000
    },
    {
      "epoch": 0.255192332195999,
      "grad_norm": 0.0375247448682785,
      "learning_rate": 0.00022344230034120028,
      "loss": 0.589,
      "step": 72100
    },
    {
      "epoch": 0.25554627440431527,
      "grad_norm": 0.023969819769263268,
      "learning_rate": 0.0002233361176787054,
      "loss": 0.7462,
      "step": 72200
    },
    {
      "epoch": 0.2559002166126315,
      "grad_norm": 0.01265735737979412,
      "learning_rate": 0.00022322993501621053,
      "loss": 0.7332,
      "step": 72300
    },
    {
      "epoch": 0.2562541588209477,
      "grad_norm": 0.07890637218952179,
      "learning_rate": 0.00022312375235371566,
      "loss": 0.4535,
      "step": 72400
    },
    {
      "epoch": 0.25660810102926396,
      "grad_norm": 0.004419080447405577,
      "learning_rate": 0.00022301756969122082,
      "loss": 0.646,
      "step": 72500
    },
    {
      "epoch": 0.25696204323758015,
      "grad_norm": 2.408046007156372,
      "learning_rate": 0.00022291138702872592,
      "loss": 0.5607,
      "step": 72600
    },
    {
      "epoch": 0.2573159854458964,
      "grad_norm": 0.01611856371164322,
      "learning_rate": 0.00022280520436623107,
      "loss": 0.4092,
      "step": 72700
    },
    {
      "epoch": 0.2576699276542126,
      "grad_norm": 0.005606411024928093,
      "learning_rate": 0.0002226990217037362,
      "loss": 0.5856,
      "step": 72800
    },
    {
      "epoch": 0.25802386986252884,
      "grad_norm": 0.02384222485125065,
      "learning_rate": 0.00022259283904124132,
      "loss": 0.7988,
      "step": 72900
    },
    {
      "epoch": 0.2583778120708451,
      "grad_norm": 0.000709413958247751,
      "learning_rate": 0.00022248665637874645,
      "loss": 0.3998,
      "step": 73000
    },
    {
      "epoch": 0.2587317542791613,
      "grad_norm": 0.008253461681306362,
      "learning_rate": 0.0002223804737162516,
      "loss": 0.7132,
      "step": 73100
    },
    {
      "epoch": 0.25908569648747753,
      "grad_norm": 0.002733228961005807,
      "learning_rate": 0.00022227429105375673,
      "loss": 0.3557,
      "step": 73200
    },
    {
      "epoch": 0.25943963869579373,
      "grad_norm": 41.89241027832031,
      "learning_rate": 0.00022216810839126186,
      "loss": 0.5555,
      "step": 73300
    },
    {
      "epoch": 0.25979358090411,
      "grad_norm": 0.35539117455482483,
      "learning_rate": 0.00022206192572876699,
      "loss": 0.5597,
      "step": 73400
    },
    {
      "epoch": 0.2601475231124262,
      "grad_norm": 0.003937071654945612,
      "learning_rate": 0.00022195574306627214,
      "loss": 0.5612,
      "step": 73500
    },
    {
      "epoch": 0.2605014653207424,
      "grad_norm": 0.0017457337817177176,
      "learning_rate": 0.00022184956040377724,
      "loss": 0.5925,
      "step": 73600
    },
    {
      "epoch": 0.26085540752905867,
      "grad_norm": 0.02176280878484249,
      "learning_rate": 0.00022174337774128237,
      "loss": 0.5441,
      "step": 73700
    },
    {
      "epoch": 0.26120934973737486,
      "grad_norm": 3.6740212440490723,
      "learning_rate": 0.00022163719507878752,
      "loss": 0.4071,
      "step": 73800
    },
    {
      "epoch": 0.2615632919456911,
      "grad_norm": 0.001858164556324482,
      "learning_rate": 0.00022153101241629268,
      "loss": 0.5273,
      "step": 73900
    },
    {
      "epoch": 0.26191723415400736,
      "grad_norm": 0.0070975301787257195,
      "learning_rate": 0.00022142482975379777,
      "loss": 0.3036,
      "step": 74000
    },
    {
      "epoch": 0.26227117636232355,
      "grad_norm": 45.792137145996094,
      "learning_rate": 0.0002213186470913029,
      "loss": 0.5866,
      "step": 74100
    },
    {
      "epoch": 0.2626251185706398,
      "grad_norm": 1.1109352111816406,
      "learning_rate": 0.00022121246442880806,
      "loss": 0.7757,
      "step": 74200
    },
    {
      "epoch": 0.262979060778956,
      "grad_norm": 0.0059360419400036335,
      "learning_rate": 0.00022110628176631316,
      "loss": 0.4793,
      "step": 74300
    },
    {
      "epoch": 0.26333300298727225,
      "grad_norm": 0.0055282264947891235,
      "learning_rate": 0.0002210000991038183,
      "loss": 0.4571,
      "step": 74400
    },
    {
      "epoch": 0.26368694519558844,
      "grad_norm": 0.00012933253310620785,
      "learning_rate": 0.00022089391644132344,
      "loss": 0.5733,
      "step": 74500
    },
    {
      "epoch": 0.2640408874039047,
      "grad_norm": 0.0006334063946269453,
      "learning_rate": 0.00022078773377882856,
      "loss": 0.4851,
      "step": 74600
    },
    {
      "epoch": 0.26439482961222094,
      "grad_norm": 0.1850070357322693,
      "learning_rate": 0.0002206815511163337,
      "loss": 0.5198,
      "step": 74700
    },
    {
      "epoch": 0.26474877182053713,
      "grad_norm": 0.0015473469393327832,
      "learning_rate": 0.00022057536845383885,
      "loss": 0.3706,
      "step": 74800
    },
    {
      "epoch": 0.2651027140288534,
      "grad_norm": 0.09538562595844269,
      "learning_rate": 0.00022046918579134397,
      "loss": 0.7315,
      "step": 74900
    },
    {
      "epoch": 0.2654566562371696,
      "grad_norm": 44.302345275878906,
      "learning_rate": 0.0002203630031288491,
      "loss": 0.4489,
      "step": 75000
    },
    {
      "epoch": 0.2658105984454858,
      "grad_norm": 7.363826751708984,
      "learning_rate": 0.00022025682046635423,
      "loss": 0.461,
      "step": 75100
    },
    {
      "epoch": 0.2661645406538021,
      "grad_norm": 0.41374433040618896,
      "learning_rate": 0.00022015063780385938,
      "loss": 0.4777,
      "step": 75200
    },
    {
      "epoch": 0.26651848286211827,
      "grad_norm": 0.002379446057602763,
      "learning_rate": 0.00022004445514136448,
      "loss": 0.5811,
      "step": 75300
    },
    {
      "epoch": 0.2668724250704345,
      "grad_norm": 0.05983249098062515,
      "learning_rate": 0.00021993827247886963,
      "loss": 0.527,
      "step": 75400
    },
    {
      "epoch": 0.2672263672787507,
      "grad_norm": 31.092788696289062,
      "learning_rate": 0.00021983208981637476,
      "loss": 0.6316,
      "step": 75500
    },
    {
      "epoch": 0.26758030948706696,
      "grad_norm": 38.144378662109375,
      "learning_rate": 0.00021972590715387992,
      "loss": 0.8277,
      "step": 75600
    },
    {
      "epoch": 0.26793425169538315,
      "grad_norm": 53.68874740600586,
      "learning_rate": 0.00021961972449138502,
      "loss": 0.4811,
      "step": 75700
    },
    {
      "epoch": 0.2682881939036994,
      "grad_norm": 0.002004300244152546,
      "learning_rate": 0.00021951354182889017,
      "loss": 0.4828,
      "step": 75800
    },
    {
      "epoch": 0.26864213611201565,
      "grad_norm": 0.07686828821897507,
      "learning_rate": 0.0002194073591663953,
      "loss": 0.6124,
      "step": 75900
    },
    {
      "epoch": 0.26899607832033184,
      "grad_norm": 103.57546997070312,
      "learning_rate": 0.00021930117650390042,
      "loss": 0.5058,
      "step": 76000
    },
    {
      "epoch": 0.2693500205286481,
      "grad_norm": 0.048691846430301666,
      "learning_rate": 0.00021919499384140555,
      "loss": 0.7016,
      "step": 76100
    },
    {
      "epoch": 0.2697039627369643,
      "grad_norm": 0.024375230073928833,
      "learning_rate": 0.0002190888111789107,
      "loss": 0.6789,
      "step": 76200
    },
    {
      "epoch": 0.27005790494528054,
      "grad_norm": 3.5205510357627645e-05,
      "learning_rate": 0.00021898262851641583,
      "loss": 0.5627,
      "step": 76300
    },
    {
      "epoch": 0.2704118471535968,
      "grad_norm": 0.0005593812675215304,
      "learning_rate": 0.00021887644585392096,
      "loss": 0.3821,
      "step": 76400
    },
    {
      "epoch": 0.270765789361913,
      "grad_norm": 0.01336153969168663,
      "learning_rate": 0.00021877026319142609,
      "loss": 0.5385,
      "step": 76500
    },
    {
      "epoch": 0.2711197315702292,
      "grad_norm": 0.2281973510980606,
      "learning_rate": 0.00021866408052893124,
      "loss": 0.3968,
      "step": 76600
    },
    {
      "epoch": 0.2714736737785454,
      "grad_norm": 5.967747688293457,
      "learning_rate": 0.00021855789786643634,
      "loss": 0.4556,
      "step": 76700
    },
    {
      "epoch": 0.27182761598686167,
      "grad_norm": 7.567358261439949e-05,
      "learning_rate": 0.00021845171520394147,
      "loss": 0.6667,
      "step": 76800
    },
    {
      "epoch": 0.27218155819517786,
      "grad_norm": 0.006308100186288357,
      "learning_rate": 0.00021834553254144662,
      "loss": 0.5981,
      "step": 76900
    },
    {
      "epoch": 0.2725355004034941,
      "grad_norm": 0.0060990783385932446,
      "learning_rate": 0.00021823934987895172,
      "loss": 0.4766,
      "step": 77000
    },
    {
      "epoch": 0.27288944261181036,
      "grad_norm": 47.66979217529297,
      "learning_rate": 0.00021813316721645687,
      "loss": 0.6725,
      "step": 77100
    },
    {
      "epoch": 0.27324338482012656,
      "grad_norm": 0.7159374952316284,
      "learning_rate": 0.000218026984553962,
      "loss": 0.6101,
      "step": 77200
    },
    {
      "epoch": 0.2735973270284428,
      "grad_norm": 0.0011152835795655847,
      "learning_rate": 0.00021792080189146716,
      "loss": 0.4968,
      "step": 77300
    },
    {
      "epoch": 0.273951269236759,
      "grad_norm": 0.003654401283711195,
      "learning_rate": 0.00021781461922897226,
      "loss": 0.549,
      "step": 77400
    },
    {
      "epoch": 0.27430521144507525,
      "grad_norm": 0.0008362227235920727,
      "learning_rate": 0.0002177084365664774,
      "loss": 0.4034,
      "step": 77500
    },
    {
      "epoch": 0.2746591536533915,
      "grad_norm": 0.4472920000553131,
      "learning_rate": 0.00021760225390398254,
      "loss": 0.6866,
      "step": 77600
    },
    {
      "epoch": 0.2750130958617077,
      "grad_norm": 2.991166114807129,
      "learning_rate": 0.00021749607124148766,
      "loss": 0.5653,
      "step": 77700
    },
    {
      "epoch": 0.27536703807002394,
      "grad_norm": 0.0012113304110243917,
      "learning_rate": 0.0002173898885789928,
      "loss": 0.6552,
      "step": 77800
    },
    {
      "epoch": 0.27572098027834013,
      "grad_norm": 0.0006360775441862643,
      "learning_rate": 0.00021728370591649794,
      "loss": 0.6185,
      "step": 77900
    },
    {
      "epoch": 0.2760749224866564,
      "grad_norm": 0.000723906559869647,
      "learning_rate": 0.00021717752325400307,
      "loss": 0.6271,
      "step": 78000
    },
    {
      "epoch": 0.27642886469497263,
      "grad_norm": 84.03152465820312,
      "learning_rate": 0.0002170713405915082,
      "loss": 0.7414,
      "step": 78100
    },
    {
      "epoch": 0.2767828069032888,
      "grad_norm": 0.6155434846878052,
      "learning_rate": 0.00021696515792901333,
      "loss": 0.4091,
      "step": 78200
    },
    {
      "epoch": 0.2771367491116051,
      "grad_norm": 61.67657470703125,
      "learning_rate": 0.00021685897526651848,
      "loss": 0.8308,
      "step": 78300
    },
    {
      "epoch": 0.27749069131992127,
      "grad_norm": 0.0015484782634302974,
      "learning_rate": 0.00021675279260402358,
      "loss": 0.5078,
      "step": 78400
    },
    {
      "epoch": 0.2778446335282375,
      "grad_norm": 10.129175186157227,
      "learning_rate": 0.00021664660994152873,
      "loss": 0.5372,
      "step": 78500
    },
    {
      "epoch": 0.2781985757365537,
      "grad_norm": 0.00013455591397359967,
      "learning_rate": 0.00021654042727903386,
      "loss": 0.5108,
      "step": 78600
    },
    {
      "epoch": 0.27855251794486996,
      "grad_norm": 0.010557359084486961,
      "learning_rate": 0.000216434244616539,
      "loss": 0.4783,
      "step": 78700
    },
    {
      "epoch": 0.2789064601531862,
      "grad_norm": 0.00030983862234279513,
      "learning_rate": 0.00021632806195404411,
      "loss": 0.4913,
      "step": 78800
    },
    {
      "epoch": 0.2792604023615024,
      "grad_norm": 0.006316937040537596,
      "learning_rate": 0.00021622187929154927,
      "loss": 0.5066,
      "step": 78900
    },
    {
      "epoch": 0.27961434456981865,
      "grad_norm": 0.058432478457689285,
      "learning_rate": 0.0002161156966290544,
      "loss": 0.5836,
      "step": 79000
    },
    {
      "epoch": 0.27996828677813484,
      "grad_norm": 32.192726135253906,
      "learning_rate": 0.00021600951396655952,
      "loss": 0.2532,
      "step": 79100
    },
    {
      "epoch": 0.2803222289864511,
      "grad_norm": 30.36431312561035,
      "learning_rate": 0.00021590333130406465,
      "loss": 0.4624,
      "step": 79200
    },
    {
      "epoch": 0.28067617119476734,
      "grad_norm": 0.007326848339289427,
      "learning_rate": 0.0002157971486415698,
      "loss": 0.5276,
      "step": 79300
    },
    {
      "epoch": 0.28103011340308354,
      "grad_norm": 19.281917572021484,
      "learning_rate": 0.0002156909659790749,
      "loss": 0.4952,
      "step": 79400
    },
    {
      "epoch": 0.2813840556113998,
      "grad_norm": 66.64662170410156,
      "learning_rate": 0.00021558478331658006,
      "loss": 0.799,
      "step": 79500
    },
    {
      "epoch": 0.281737997819716,
      "grad_norm": 87.2091293334961,
      "learning_rate": 0.00021547860065408519,
      "loss": 0.593,
      "step": 79600
    },
    {
      "epoch": 0.28209194002803223,
      "grad_norm": 0.005845528095960617,
      "learning_rate": 0.00021537241799159034,
      "loss": 0.3845,
      "step": 79700
    },
    {
      "epoch": 0.2824458822363484,
      "grad_norm": 0.0005331406719051301,
      "learning_rate": 0.00021526623532909544,
      "loss": 0.307,
      "step": 79800
    },
    {
      "epoch": 0.28279982444466467,
      "grad_norm": 0.0027792463079094887,
      "learning_rate": 0.0002151600526666006,
      "loss": 0.5525,
      "step": 79900
    },
    {
      "epoch": 0.2831537666529809,
      "grad_norm": 0.0007405968499369919,
      "learning_rate": 0.00021505387000410572,
      "loss": 0.4196,
      "step": 80000
    },
    {
      "epoch": 0.2835077088612971,
      "grad_norm": 0.01913599483668804,
      "learning_rate": 0.00021494768734161082,
      "loss": 0.3731,
      "step": 80100
    },
    {
      "epoch": 0.28386165106961336,
      "grad_norm": 0.02207254059612751,
      "learning_rate": 0.00021484150467911597,
      "loss": 0.6692,
      "step": 80200
    },
    {
      "epoch": 0.28421559327792956,
      "grad_norm": 0.0363372266292572,
      "learning_rate": 0.0002147353220166211,
      "loss": 0.3029,
      "step": 80300
    },
    {
      "epoch": 0.2845695354862458,
      "grad_norm": 0.16672448813915253,
      "learning_rate": 0.00021462913935412626,
      "loss": 0.3589,
      "step": 80400
    },
    {
      "epoch": 0.28492347769456206,
      "grad_norm": 3.8038322925567627,
      "learning_rate": 0.00021452295669163136,
      "loss": 0.5543,
      "step": 80500
    },
    {
      "epoch": 0.28527741990287825,
      "grad_norm": 0.00090587924933061,
      "learning_rate": 0.0002144167740291365,
      "loss": 0.5108,
      "step": 80600
    },
    {
      "epoch": 0.2856313621111945,
      "grad_norm": 0.004614041652530432,
      "learning_rate": 0.00021431059136664164,
      "loss": 0.556,
      "step": 80700
    },
    {
      "epoch": 0.2859853043195107,
      "grad_norm": 16.02202796936035,
      "learning_rate": 0.00021420440870414676,
      "loss": 0.4342,
      "step": 80800
    },
    {
      "epoch": 0.28633924652782694,
      "grad_norm": 0.035441186279058456,
      "learning_rate": 0.0002140982260416519,
      "loss": 0.3752,
      "step": 80900
    },
    {
      "epoch": 0.2866931887361432,
      "grad_norm": 0.07163670659065247,
      "learning_rate": 0.00021399204337915704,
      "loss": 0.5795,
      "step": 81000
    },
    {
      "epoch": 0.2870471309444594,
      "grad_norm": 0.001736646518111229,
      "learning_rate": 0.00021388586071666214,
      "loss": 0.4296,
      "step": 81100
    },
    {
      "epoch": 0.28740107315277563,
      "grad_norm": 0.879366934299469,
      "learning_rate": 0.0002137796780541673,
      "loss": 0.631,
      "step": 81200
    },
    {
      "epoch": 0.2877550153610918,
      "grad_norm": 3.433742046356201,
      "learning_rate": 0.00021367349539167243,
      "loss": 0.6715,
      "step": 81300
    },
    {
      "epoch": 0.2881089575694081,
      "grad_norm": 24.422618865966797,
      "learning_rate": 0.00021356731272917758,
      "loss": 0.5481,
      "step": 81400
    },
    {
      "epoch": 0.28846289977772427,
      "grad_norm": 0.11658604443073273,
      "learning_rate": 0.00021346113006668268,
      "loss": 0.5245,
      "step": 81500
    },
    {
      "epoch": 0.2888168419860405,
      "grad_norm": 0.00017150412895716727,
      "learning_rate": 0.00021335494740418783,
      "loss": 0.6617,
      "step": 81600
    },
    {
      "epoch": 0.28917078419435677,
      "grad_norm": 0.06883813440799713,
      "learning_rate": 0.00021324876474169296,
      "loss": 0.6244,
      "step": 81700
    },
    {
      "epoch": 0.28952472640267296,
      "grad_norm": 0.016613714396953583,
      "learning_rate": 0.0002131425820791981,
      "loss": 0.4776,
      "step": 81800
    },
    {
      "epoch": 0.2898786686109892,
      "grad_norm": 0.2171647846698761,
      "learning_rate": 0.00021303639941670321,
      "loss": 0.5363,
      "step": 81900
    },
    {
      "epoch": 0.2902326108193054,
      "grad_norm": 99.09957122802734,
      "learning_rate": 0.00021293021675420837,
      "loss": 0.3307,
      "step": 82000
    },
    {
      "epoch": 0.29058655302762165,
      "grad_norm": 0.3393919765949249,
      "learning_rate": 0.0002128240340917135,
      "loss": 0.5339,
      "step": 82100
    },
    {
      "epoch": 0.2909404952359379,
      "grad_norm": 0.00547815952450037,
      "learning_rate": 0.00021271785142921862,
      "loss": 0.4336,
      "step": 82200
    },
    {
      "epoch": 0.2912944374442541,
      "grad_norm": 0.0002501690178178251,
      "learning_rate": 0.00021261166876672375,
      "loss": 0.471,
      "step": 82300
    },
    {
      "epoch": 0.29164837965257034,
      "grad_norm": 0.004066321067512035,
      "learning_rate": 0.0002125054861042289,
      "loss": 0.5489,
      "step": 82400
    },
    {
      "epoch": 0.29200232186088654,
      "grad_norm": 0.011726540513336658,
      "learning_rate": 0.000212399303441734,
      "loss": 0.443,
      "step": 82500
    },
    {
      "epoch": 0.2923562640692028,
      "grad_norm": 0.013870825991034508,
      "learning_rate": 0.00021229312077923916,
      "loss": 0.6119,
      "step": 82600
    },
    {
      "epoch": 0.292710206277519,
      "grad_norm": 0.003368845907971263,
      "learning_rate": 0.00021218693811674428,
      "loss": 0.5859,
      "step": 82700
    },
    {
      "epoch": 0.29306414848583523,
      "grad_norm": 0.03936193138360977,
      "learning_rate": 0.00021208075545424938,
      "loss": 0.6595,
      "step": 82800
    },
    {
      "epoch": 0.2934180906941515,
      "grad_norm": 0.0009708348079584539,
      "learning_rate": 0.00021197457279175454,
      "loss": 0.573,
      "step": 82900
    },
    {
      "epoch": 0.2937720329024677,
      "grad_norm": 0.0042193434201180935,
      "learning_rate": 0.0002118683901292597,
      "loss": 0.399,
      "step": 83000
    },
    {
      "epoch": 0.2941259751107839,
      "grad_norm": 1.1210867166519165,
      "learning_rate": 0.00021176220746676482,
      "loss": 0.4892,
      "step": 83100
    },
    {
      "epoch": 0.2944799173191001,
      "grad_norm": 22.863862991333008,
      "learning_rate": 0.00021165602480426992,
      "loss": 0.4491,
      "step": 83200
    },
    {
      "epoch": 0.29483385952741636,
      "grad_norm": 51.24999237060547,
      "learning_rate": 0.00021154984214177507,
      "loss": 0.4685,
      "step": 83300
    },
    {
      "epoch": 0.2951878017357326,
      "grad_norm": 1.6144822835922241,
      "learning_rate": 0.0002114436594792802,
      "loss": 0.5095,
      "step": 83400
    },
    {
      "epoch": 0.2955417439440488,
      "grad_norm": 0.00029625091701745987,
      "learning_rate": 0.00021133747681678533,
      "loss": 0.3242,
      "step": 83500
    },
    {
      "epoch": 0.29589568615236506,
      "grad_norm": 0.0039874594658613205,
      "learning_rate": 0.00021123129415429045,
      "loss": 0.6992,
      "step": 83600
    },
    {
      "epoch": 0.29624962836068125,
      "grad_norm": 0.3766278326511383,
      "learning_rate": 0.0002111251114917956,
      "loss": 0.3222,
      "step": 83700
    },
    {
      "epoch": 0.2966035705689975,
      "grad_norm": 0.17274250090122223,
      "learning_rate": 0.00021101892882930074,
      "loss": 0.7533,
      "step": 83800
    },
    {
      "epoch": 0.29695751277731375,
      "grad_norm": 0.005524106789380312,
      "learning_rate": 0.00021091274616680586,
      "loss": 0.625,
      "step": 83900
    },
    {
      "epoch": 0.29731145498562994,
      "grad_norm": 0.0016033037099987268,
      "learning_rate": 0.000210806563504311,
      "loss": 0.4756,
      "step": 84000
    },
    {
      "epoch": 0.2976653971939462,
      "grad_norm": 129.57373046875,
      "learning_rate": 0.00021070038084181614,
      "loss": 0.8199,
      "step": 84100
    },
    {
      "epoch": 0.2980193394022624,
      "grad_norm": 0.0029792042914777994,
      "learning_rate": 0.00021059419817932124,
      "loss": 0.7184,
      "step": 84200
    },
    {
      "epoch": 0.29837328161057863,
      "grad_norm": 34.56376647949219,
      "learning_rate": 0.0002104880155168264,
      "loss": 0.4077,
      "step": 84300
    },
    {
      "epoch": 0.2987272238188948,
      "grad_norm": 0.011323941871523857,
      "learning_rate": 0.00021038183285433152,
      "loss": 0.4558,
      "step": 84400
    },
    {
      "epoch": 0.2990811660272111,
      "grad_norm": 1.9177170991897583,
      "learning_rate": 0.00021027565019183665,
      "loss": 0.3705,
      "step": 84500
    },
    {
      "epoch": 0.2994351082355273,
      "grad_norm": 9.942028045654297,
      "learning_rate": 0.00021016946752934178,
      "loss": 0.6028,
      "step": 84600
    },
    {
      "epoch": 0.2997890504438435,
      "grad_norm": 0.1508915275335312,
      "learning_rate": 0.00021006328486684693,
      "loss": 0.6769,
      "step": 84700
    },
    {
      "epoch": 0.30014299265215977,
      "grad_norm": 0.012989984825253487,
      "learning_rate": 0.00020995710220435206,
      "loss": 0.5353,
      "step": 84800
    },
    {
      "epoch": 0.30049693486047596,
      "grad_norm": 30.039236068725586,
      "learning_rate": 0.0002098509195418572,
      "loss": 0.6697,
      "step": 84900
    },
    {
      "epoch": 0.3008508770687922,
      "grad_norm": 0.08807296305894852,
      "learning_rate": 0.00020974473687936231,
      "loss": 0.2845,
      "step": 85000
    },
    {
      "epoch": 0.30120481927710846,
      "grad_norm": 79.34669494628906,
      "learning_rate": 0.00020963855421686747,
      "loss": 0.4814,
      "step": 85100
    },
    {
      "epoch": 0.30155876148542465,
      "grad_norm": 0.0465250238776207,
      "learning_rate": 0.00020953237155437257,
      "loss": 0.3332,
      "step": 85200
    },
    {
      "epoch": 0.3019127036937409,
      "grad_norm": 12.882299423217773,
      "learning_rate": 0.00020942618889187772,
      "loss": 0.5641,
      "step": 85300
    },
    {
      "epoch": 0.3022666459020571,
      "grad_norm": 0.06227657571434975,
      "learning_rate": 0.00020932000622938285,
      "loss": 0.5695,
      "step": 85400
    },
    {
      "epoch": 0.30262058811037335,
      "grad_norm": 0.08644769340753555,
      "learning_rate": 0.000209213823566888,
      "loss": 0.4117,
      "step": 85500
    },
    {
      "epoch": 0.30297453031868954,
      "grad_norm": 0.005827262531965971,
      "learning_rate": 0.0002091076409043931,
      "loss": 0.3404,
      "step": 85600
    },
    {
      "epoch": 0.3033284725270058,
      "grad_norm": 0.006961371749639511,
      "learning_rate": 0.00020900145824189826,
      "loss": 0.4208,
      "step": 85700
    },
    {
      "epoch": 0.30368241473532204,
      "grad_norm": 0.009460206143558025,
      "learning_rate": 0.00020889527557940338,
      "loss": 0.2873,
      "step": 85800
    },
    {
      "epoch": 0.30403635694363823,
      "grad_norm": 0.0007366419886238873,
      "learning_rate": 0.00020878909291690848,
      "loss": 0.4597,
      "step": 85900
    },
    {
      "epoch": 0.3043902991519545,
      "grad_norm": 0.00035873588058166206,
      "learning_rate": 0.00020868291025441364,
      "loss": 0.369,
      "step": 86000
    },
    {
      "epoch": 0.3047442413602707,
      "grad_norm": 0.004843196365982294,
      "learning_rate": 0.0002085767275919188,
      "loss": 0.4024,
      "step": 86100
    },
    {
      "epoch": 0.3050981835685869,
      "grad_norm": 0.0021692232694476843,
      "learning_rate": 0.00020847054492942392,
      "loss": 0.4128,
      "step": 86200
    },
    {
      "epoch": 0.30545212577690317,
      "grad_norm": 0.08596684038639069,
      "learning_rate": 0.00020836436226692902,
      "loss": 0.4037,
      "step": 86300
    },
    {
      "epoch": 0.30580606798521937,
      "grad_norm": 2.2871108055114746,
      "learning_rate": 0.00020825817960443417,
      "loss": 0.6157,
      "step": 86400
    },
    {
      "epoch": 0.3061600101935356,
      "grad_norm": 1.5202181339263916,
      "learning_rate": 0.00020815199694193933,
      "loss": 0.3936,
      "step": 86500
    },
    {
      "epoch": 0.3065139524018518,
      "grad_norm": 25.238008499145508,
      "learning_rate": 0.00020804581427944443,
      "loss": 0.3799,
      "step": 86600
    },
    {
      "epoch": 0.30686789461016806,
      "grad_norm": 0.1571120023727417,
      "learning_rate": 0.00020793963161694955,
      "loss": 0.391,
      "step": 86700
    },
    {
      "epoch": 0.30722183681848425,
      "grad_norm": 26.97327995300293,
      "learning_rate": 0.0002078334489544547,
      "loss": 0.7729,
      "step": 86800
    },
    {
      "epoch": 0.3075757790268005,
      "grad_norm": 0.0035560503602027893,
      "learning_rate": 0.0002077272662919598,
      "loss": 0.5884,
      "step": 86900
    },
    {
      "epoch": 0.30792972123511675,
      "grad_norm": 0.000361075799446553,
      "learning_rate": 0.00020762108362946496,
      "loss": 0.3831,
      "step": 87000
    },
    {
      "epoch": 0.30828366344343294,
      "grad_norm": 3.8052117824554443,
      "learning_rate": 0.0002075149009669701,
      "loss": 0.3951,
      "step": 87100
    },
    {
      "epoch": 0.3086376056517492,
      "grad_norm": 0.0005414470215328038,
      "learning_rate": 0.00020740871830447524,
      "loss": 0.8435,
      "step": 87200
    },
    {
      "epoch": 0.3089915478600654,
      "grad_norm": 0.00032211269717663527,
      "learning_rate": 0.00020730253564198034,
      "loss": 0.2466,
      "step": 87300
    },
    {
      "epoch": 0.30934549006838163,
      "grad_norm": 0.02183152362704277,
      "learning_rate": 0.0002071963529794855,
      "loss": 0.4633,
      "step": 87400
    },
    {
      "epoch": 0.3096994322766979,
      "grad_norm": 0.0009940792806446552,
      "learning_rate": 0.00020709017031699062,
      "loss": 0.4839,
      "step": 87500
    },
    {
      "epoch": 0.3100533744850141,
      "grad_norm": 0.0018730100709944963,
      "learning_rate": 0.00020698398765449575,
      "loss": 0.3661,
      "step": 87600
    },
    {
      "epoch": 0.3104073166933303,
      "grad_norm": 0.07509302347898483,
      "learning_rate": 0.00020687780499200088,
      "loss": 0.5411,
      "step": 87700
    },
    {
      "epoch": 0.3107612589016465,
      "grad_norm": 87.43438720703125,
      "learning_rate": 0.00020677162232950603,
      "loss": 0.5738,
      "step": 87800
    },
    {
      "epoch": 0.31111520110996277,
      "grad_norm": 7.133511066436768,
      "learning_rate": 0.00020666543966701116,
      "loss": 0.5601,
      "step": 87900
    },
    {
      "epoch": 0.311469143318279,
      "grad_norm": 8.841109275817871,
      "learning_rate": 0.0002065592570045163,
      "loss": 0.462,
      "step": 88000
    },
    {
      "epoch": 0.3118230855265952,
      "grad_norm": 0.0007105479598976672,
      "learning_rate": 0.00020645307434202141,
      "loss": 0.5721,
      "step": 88100
    },
    {
      "epoch": 0.31217702773491146,
      "grad_norm": 26.7828311920166,
      "learning_rate": 0.00020634689167952657,
      "loss": 0.456,
      "step": 88200
    },
    {
      "epoch": 0.31253096994322765,
      "grad_norm": 12.04302978515625,
      "learning_rate": 0.00020624070901703167,
      "loss": 0.409,
      "step": 88300
    },
    {
      "epoch": 0.3128849121515439,
      "grad_norm": 2.5912468433380127,
      "learning_rate": 0.00020613452635453682,
      "loss": 0.5058,
      "step": 88400
    },
    {
      "epoch": 0.3132388543598601,
      "grad_norm": 24.579402923583984,
      "learning_rate": 0.00020602834369204195,
      "loss": 0.4125,
      "step": 88500
    },
    {
      "epoch": 0.31359279656817635,
      "grad_norm": 0.06004372611641884,
      "learning_rate": 0.00020592216102954705,
      "loss": 0.4763,
      "step": 88600
    },
    {
      "epoch": 0.3139467387764926,
      "grad_norm": 0.00024244286760222167,
      "learning_rate": 0.0002058159783670522,
      "loss": 0.4687,
      "step": 88700
    },
    {
      "epoch": 0.3143006809848088,
      "grad_norm": 0.9188334345817566,
      "learning_rate": 0.00020570979570455736,
      "loss": 0.6121,
      "step": 88800
    },
    {
      "epoch": 0.31465462319312504,
      "grad_norm": 0.15927846729755402,
      "learning_rate": 0.00020560361304206248,
      "loss": 0.5804,
      "step": 88900
    },
    {
      "epoch": 0.31500856540144123,
      "grad_norm": 33.022212982177734,
      "learning_rate": 0.00020549743037956758,
      "loss": 0.2878,
      "step": 89000
    },
    {
      "epoch": 0.3153625076097575,
      "grad_norm": 97.90675354003906,
      "learning_rate": 0.00020539124771707274,
      "loss": 0.3852,
      "step": 89100
    },
    {
      "epoch": 0.31571644981807373,
      "grad_norm": 0.001303751254454255,
      "learning_rate": 0.0002052850650545779,
      "loss": 0.5105,
      "step": 89200
    },
    {
      "epoch": 0.3160703920263899,
      "grad_norm": 0.045637957751750946,
      "learning_rate": 0.000205178882392083,
      "loss": 0.5378,
      "step": 89300
    },
    {
      "epoch": 0.3164243342347062,
      "grad_norm": 0.0008978534024208784,
      "learning_rate": 0.00020507269972958812,
      "loss": 0.3694,
      "step": 89400
    },
    {
      "epoch": 0.31677827644302237,
      "grad_norm": 69.89029693603516,
      "learning_rate": 0.00020496651706709327,
      "loss": 0.6752,
      "step": 89500
    },
    {
      "epoch": 0.3171322186513386,
      "grad_norm": 8.935966491699219,
      "learning_rate": 0.00020486033440459843,
      "loss": 0.7064,
      "step": 89600
    },
    {
      "epoch": 0.3174861608596548,
      "grad_norm": 0.20006006956100464,
      "learning_rate": 0.00020475415174210353,
      "loss": 0.3428,
      "step": 89700
    },
    {
      "epoch": 0.31784010306797106,
      "grad_norm": 2.398658275604248,
      "learning_rate": 0.00020464796907960865,
      "loss": 0.2765,
      "step": 89800
    },
    {
      "epoch": 0.3181940452762873,
      "grad_norm": 0.0026793673168867826,
      "learning_rate": 0.0002045417864171138,
      "loss": 0.3733,
      "step": 89900
    },
    {
      "epoch": 0.3185479874846035,
      "grad_norm": 5.444052219390869,
      "learning_rate": 0.0002044356037546189,
      "loss": 0.5,
      "step": 90000
    },
    {
      "epoch": 0.31890192969291975,
      "grad_norm": 0.0008637946448288858,
      "learning_rate": 0.00020432942109212406,
      "loss": 0.3337,
      "step": 90100
    },
    {
      "epoch": 0.31925587190123594,
      "grad_norm": 0.058259718120098114,
      "learning_rate": 0.0002042232384296292,
      "loss": 0.6415,
      "step": 90200
    },
    {
      "epoch": 0.3196098141095522,
      "grad_norm": 0.008032217621803284,
      "learning_rate": 0.00020411705576713434,
      "loss": 0.4172,
      "step": 90300
    },
    {
      "epoch": 0.31996375631786844,
      "grad_norm": 86.98816680908203,
      "learning_rate": 0.00020401087310463944,
      "loss": 0.6324,
      "step": 90400
    },
    {
      "epoch": 0.32031769852618464,
      "grad_norm": 0.0011197569547221065,
      "learning_rate": 0.0002039046904421446,
      "loss": 0.5134,
      "step": 90500
    },
    {
      "epoch": 0.3206716407345009,
      "grad_norm": 53.82942581176758,
      "learning_rate": 0.00020379850777964972,
      "loss": 0.4411,
      "step": 90600
    },
    {
      "epoch": 0.3210255829428171,
      "grad_norm": 0.0016394301783293486,
      "learning_rate": 0.00020369232511715485,
      "loss": 0.2372,
      "step": 90700
    },
    {
      "epoch": 0.3213795251511333,
      "grad_norm": 0.02723991870880127,
      "learning_rate": 0.00020358614245465998,
      "loss": 0.3844,
      "step": 90800
    },
    {
      "epoch": 0.3217334673594496,
      "grad_norm": 29.725034713745117,
      "learning_rate": 0.00020347995979216513,
      "loss": 0.5835,
      "step": 90900
    },
    {
      "epoch": 0.32208740956776577,
      "grad_norm": 0.0017157795373350382,
      "learning_rate": 0.00020337377712967023,
      "loss": 0.344,
      "step": 91000
    },
    {
      "epoch": 0.322441351776082,
      "grad_norm": 38.37425994873047,
      "learning_rate": 0.00020326759446717539,
      "loss": 0.8084,
      "step": 91100
    },
    {
      "epoch": 0.3227952939843982,
      "grad_norm": 0.27756184339523315,
      "learning_rate": 0.0002031614118046805,
      "loss": 0.5667,
      "step": 91200
    },
    {
      "epoch": 0.32314923619271446,
      "grad_norm": 0.00033951381919905543,
      "learning_rate": 0.00020305522914218567,
      "loss": 0.2583,
      "step": 91300
    },
    {
      "epoch": 0.32350317840103066,
      "grad_norm": 0.027525924146175385,
      "learning_rate": 0.00020294904647969077,
      "loss": 0.6263,
      "step": 91400
    },
    {
      "epoch": 0.3238571206093469,
      "grad_norm": 0.00031774284434504807,
      "learning_rate": 0.00020284286381719592,
      "loss": 0.5953,
      "step": 91500
    },
    {
      "epoch": 0.32421106281766315,
      "grad_norm": 0.000801755755674094,
      "learning_rate": 0.00020273668115470105,
      "loss": 0.3605,
      "step": 91600
    },
    {
      "epoch": 0.32456500502597935,
      "grad_norm": 0.00038067129207774997,
      "learning_rate": 0.00020263049849220615,
      "loss": 0.432,
      "step": 91700
    },
    {
      "epoch": 0.3249189472342956,
      "grad_norm": 46.58652114868164,
      "learning_rate": 0.0002025243158297113,
      "loss": 0.6308,
      "step": 91800
    },
    {
      "epoch": 0.3252728894426118,
      "grad_norm": 0.009719625115394592,
      "learning_rate": 0.00020241813316721646,
      "loss": 0.4725,
      "step": 91900
    },
    {
      "epoch": 0.32562683165092804,
      "grad_norm": 5.358495712280273,
      "learning_rate": 0.00020231195050472158,
      "loss": 0.492,
      "step": 92000
    },
    {
      "epoch": 0.3259807738592443,
      "grad_norm": 0.005418631713837385,
      "learning_rate": 0.00020220576784222668,
      "loss": 0.3664,
      "step": 92100
    },
    {
      "epoch": 0.3263347160675605,
      "grad_norm": 6.212271213531494,
      "learning_rate": 0.00020209958517973184,
      "loss": 0.441,
      "step": 92200
    },
    {
      "epoch": 0.32668865827587673,
      "grad_norm": 2.1320364475250244,
      "learning_rate": 0.000201993402517237,
      "loss": 0.3769,
      "step": 92300
    },
    {
      "epoch": 0.3270426004841929,
      "grad_norm": 0.0003894322144333273,
      "learning_rate": 0.0002018872198547421,
      "loss": 0.571,
      "step": 92400
    },
    {
      "epoch": 0.3273965426925092,
      "grad_norm": 14.631438255310059,
      "learning_rate": 0.00020178103719224722,
      "loss": 0.6275,
      "step": 92500
    },
    {
      "epoch": 0.32775048490082537,
      "grad_norm": 0.00025925287627615035,
      "learning_rate": 0.00020167485452975237,
      "loss": 0.4167,
      "step": 92600
    },
    {
      "epoch": 0.3281044271091416,
      "grad_norm": 66.025146484375,
      "learning_rate": 0.00020156867186725747,
      "loss": 0.659,
      "step": 92700
    },
    {
      "epoch": 0.32845836931745787,
      "grad_norm": 0.008759630843997002,
      "learning_rate": 0.00020146248920476263,
      "loss": 0.3286,
      "step": 92800
    },
    {
      "epoch": 0.32881231152577406,
      "grad_norm": 0.002283697947859764,
      "learning_rate": 0.00020135630654226775,
      "loss": 0.3657,
      "step": 92900
    },
    {
      "epoch": 0.3291662537340903,
      "grad_norm": 1.611616611480713,
      "learning_rate": 0.0002012501238797729,
      "loss": 0.6447,
      "step": 93000
    },
    {
      "epoch": 0.3295201959424065,
      "grad_norm": 6.1592817306518555,
      "learning_rate": 0.000201143941217278,
      "loss": 0.4757,
      "step": 93100
    },
    {
      "epoch": 0.32987413815072275,
      "grad_norm": 0.010807673446834087,
      "learning_rate": 0.00020103775855478316,
      "loss": 0.3868,
      "step": 93200
    },
    {
      "epoch": 0.330228080359039,
      "grad_norm": 0.1767604798078537,
      "learning_rate": 0.0002009315758922883,
      "loss": 0.4542,
      "step": 93300
    },
    {
      "epoch": 0.3305820225673552,
      "grad_norm": 0.022902240976691246,
      "learning_rate": 0.00020082539322979342,
      "loss": 0.4938,
      "step": 93400
    },
    {
      "epoch": 0.33093596477567144,
      "grad_norm": 4.018898010253906,
      "learning_rate": 0.00020071921056729854,
      "loss": 0.4642,
      "step": 93500
    },
    {
      "epoch": 0.33128990698398764,
      "grad_norm": 0.001766013097949326,
      "learning_rate": 0.0002006130279048037,
      "loss": 0.5315,
      "step": 93600
    },
    {
      "epoch": 0.3316438491923039,
      "grad_norm": 0.00045788398711010814,
      "learning_rate": 0.00020050684524230882,
      "loss": 0.5729,
      "step": 93700
    },
    {
      "epoch": 0.3319977914006201,
      "grad_norm": 10.098040580749512,
      "learning_rate": 0.00020040066257981395,
      "loss": 0.4253,
      "step": 93800
    },
    {
      "epoch": 0.33235173360893633,
      "grad_norm": 0.0022279706317931414,
      "learning_rate": 0.00020029447991731908,
      "loss": 0.6582,
      "step": 93900
    },
    {
      "epoch": 0.3327056758172526,
      "grad_norm": 0.010355494916439056,
      "learning_rate": 0.00020018829725482423,
      "loss": 0.3939,
      "step": 94000
    },
    {
      "epoch": 0.33305961802556877,
      "grad_norm": 0.0064200954511761665,
      "learning_rate": 0.00020008211459232933,
      "loss": 0.2745,
      "step": 94100
    },
    {
      "epoch": 0.333413560233885,
      "grad_norm": 0.00067698466591537,
      "learning_rate": 0.00019997593192983449,
      "loss": 0.5437,
      "step": 94200
    },
    {
      "epoch": 0.3337675024422012,
      "grad_norm": 0.002064367989078164,
      "learning_rate": 0.0001998697492673396,
      "loss": 0.359,
      "step": 94300
    },
    {
      "epoch": 0.33412144465051746,
      "grad_norm": 0.00012008793419227004,
      "learning_rate": 0.00019976356660484474,
      "loss": 0.5625,
      "step": 94400
    },
    {
      "epoch": 0.3344753868588337,
      "grad_norm": 82.06367492675781,
      "learning_rate": 0.00019965738394234987,
      "loss": 0.4859,
      "step": 94500
    },
    {
      "epoch": 0.3348293290671499,
      "grad_norm": 0.0006819402915425599,
      "learning_rate": 0.00019955120127985502,
      "loss": 0.4526,
      "step": 94600
    },
    {
      "epoch": 0.33518327127546615,
      "grad_norm": 0.0014431574381887913,
      "learning_rate": 0.00019944501861736015,
      "loss": 0.5689,
      "step": 94700
    },
    {
      "epoch": 0.33553721348378235,
      "grad_norm": 0.450916051864624,
      "learning_rate": 0.00019933883595486525,
      "loss": 0.3638,
      "step": 94800
    },
    {
      "epoch": 0.3358911556920986,
      "grad_norm": 0.004718041978776455,
      "learning_rate": 0.0001992326532923704,
      "loss": 0.4582,
      "step": 94900
    },
    {
      "epoch": 0.33624509790041485,
      "grad_norm": 0.007026315201073885,
      "learning_rate": 0.00019912647062987556,
      "loss": 0.6197,
      "step": 95000
    },
    {
      "epoch": 0.33659904010873104,
      "grad_norm": 20.445127487182617,
      "learning_rate": 0.00019902028796738066,
      "loss": 0.5145,
      "step": 95100
    },
    {
      "epoch": 0.3369529823170473,
      "grad_norm": 4.640829086303711,
      "learning_rate": 0.00019891410530488578,
      "loss": 0.4163,
      "step": 95200
    },
    {
      "epoch": 0.3373069245253635,
      "grad_norm": 0.019704433158040047,
      "learning_rate": 0.00019880792264239094,
      "loss": 0.4613,
      "step": 95300
    },
    {
      "epoch": 0.33766086673367973,
      "grad_norm": 0.0012554172426462173,
      "learning_rate": 0.0001987017399798961,
      "loss": 0.3872,
      "step": 95400
    },
    {
      "epoch": 0.3380148089419959,
      "grad_norm": 0.7367091178894043,
      "learning_rate": 0.0001985955573174012,
      "loss": 0.6608,
      "step": 95500
    },
    {
      "epoch": 0.3383687511503122,
      "grad_norm": 0.11151973158121109,
      "learning_rate": 0.00019848937465490632,
      "loss": 0.3286,
      "step": 95600
    },
    {
      "epoch": 0.3387226933586284,
      "grad_norm": 0.044714346528053284,
      "learning_rate": 0.00019838319199241147,
      "loss": 0.4514,
      "step": 95700
    },
    {
      "epoch": 0.3390766355669446,
      "grad_norm": 0.05641728267073631,
      "learning_rate": 0.00019827700932991657,
      "loss": 0.5728,
      "step": 95800
    },
    {
      "epoch": 0.33943057777526087,
      "grad_norm": 0.0001295714027946815,
      "learning_rate": 0.00019817082666742173,
      "loss": 0.6847,
      "step": 95900
    },
    {
      "epoch": 0.33978451998357706,
      "grad_norm": 27.967971801757812,
      "learning_rate": 0.00019806464400492685,
      "loss": 0.2689,
      "step": 96000
    },
    {
      "epoch": 0.3401384621918933,
      "grad_norm": 0.00024610539549030364,
      "learning_rate": 0.000197958461342432,
      "loss": 0.4559,
      "step": 96100
    },
    {
      "epoch": 0.34049240440020956,
      "grad_norm": 0.0004889754927717149,
      "learning_rate": 0.0001978522786799371,
      "loss": 0.4783,
      "step": 96200
    },
    {
      "epoch": 0.34084634660852575,
      "grad_norm": 13.783347129821777,
      "learning_rate": 0.00019774609601744226,
      "loss": 0.4282,
      "step": 96300
    },
    {
      "epoch": 0.341200288816842,
      "grad_norm": 0.0009451705846004188,
      "learning_rate": 0.0001976399133549474,
      "loss": 0.6308,
      "step": 96400
    },
    {
      "epoch": 0.3415542310251582,
      "grad_norm": 1.156911015510559,
      "learning_rate": 0.00019753373069245252,
      "loss": 0.2706,
      "step": 96500
    },
    {
      "epoch": 0.34190817323347444,
      "grad_norm": 0.00026673098909668624,
      "learning_rate": 0.00019742754802995764,
      "loss": 0.3242,
      "step": 96600
    },
    {
      "epoch": 0.34226211544179064,
      "grad_norm": 30.259967803955078,
      "learning_rate": 0.0001973213653674628,
      "loss": 0.2677,
      "step": 96700
    },
    {
      "epoch": 0.3426160576501069,
      "grad_norm": 0.24424998462200165,
      "learning_rate": 0.0001972151827049679,
      "loss": 0.5307,
      "step": 96800
    },
    {
      "epoch": 0.34296999985842314,
      "grad_norm": 0.00215453770942986,
      "learning_rate": 0.00019710900004247305,
      "loss": 0.3569,
      "step": 96900
    },
    {
      "epoch": 0.34332394206673933,
      "grad_norm": 0.0007457747124135494,
      "learning_rate": 0.00019700281737997818,
      "loss": 0.3764,
      "step": 97000
    },
    {
      "epoch": 0.3436778842750556,
      "grad_norm": 0.0001993565383600071,
      "learning_rate": 0.00019689663471748333,
      "loss": 0.4766,
      "step": 97100
    },
    {
      "epoch": 0.34403182648337177,
      "grad_norm": 0.01615445502102375,
      "learning_rate": 0.00019679045205498843,
      "loss": 0.4776,
      "step": 97200
    },
    {
      "epoch": 0.344385768691688,
      "grad_norm": 0.018729260191321373,
      "learning_rate": 0.00019668426939249359,
      "loss": 0.4235,
      "step": 97300
    },
    {
      "epoch": 0.34473971090000427,
      "grad_norm": 0.003594816429540515,
      "learning_rate": 0.0001965780867299987,
      "loss": 0.5109,
      "step": 97400
    },
    {
      "epoch": 0.34509365310832046,
      "grad_norm": 10.429831504821777,
      "learning_rate": 0.00019647190406750384,
      "loss": 0.397,
      "step": 97500
    },
    {
      "epoch": 0.3454475953166367,
      "grad_norm": 0.11674519628286362,
      "learning_rate": 0.00019636572140500897,
      "loss": 0.6007,
      "step": 97600
    },
    {
      "epoch": 0.3458015375249529,
      "grad_norm": 45.174278259277344,
      "learning_rate": 0.00019625953874251412,
      "loss": 0.3036,
      "step": 97700
    },
    {
      "epoch": 0.34615547973326916,
      "grad_norm": 11.908125877380371,
      "learning_rate": 0.00019615335608001925,
      "loss": 0.4642,
      "step": 97800
    },
    {
      "epoch": 0.3465094219415854,
      "grad_norm": 0.004534800536930561,
      "learning_rate": 0.00019604717341752437,
      "loss": 0.4192,
      "step": 97900
    },
    {
      "epoch": 0.3468633641499016,
      "grad_norm": 0.030315963551402092,
      "learning_rate": 0.0001959409907550295,
      "loss": 0.3441,
      "step": 98000
    },
    {
      "epoch": 0.34721730635821785,
      "grad_norm": 0.045651741325855255,
      "learning_rate": 0.00019583480809253466,
      "loss": 0.4652,
      "step": 98100
    },
    {
      "epoch": 0.34757124856653404,
      "grad_norm": 0.010045632719993591,
      "learning_rate": 0.00019572862543003976,
      "loss": 0.5993,
      "step": 98200
    },
    {
      "epoch": 0.3479251907748503,
      "grad_norm": 0.006764690391719341,
      "learning_rate": 0.00019562244276754488,
      "loss": 0.3403,
      "step": 98300
    },
    {
      "epoch": 0.3482791329831665,
      "grad_norm": 0.01898247003555298,
      "learning_rate": 0.00019551626010505004,
      "loss": 0.4363,
      "step": 98400
    },
    {
      "epoch": 0.34863307519148273,
      "grad_norm": 0.0006448924541473389,
      "learning_rate": 0.00019541007744255514,
      "loss": 0.5005,
      "step": 98500
    },
    {
      "epoch": 0.348987017399799,
      "grad_norm": 3.381462738616392e-05,
      "learning_rate": 0.0001953038947800603,
      "loss": 0.4373,
      "step": 98600
    },
    {
      "epoch": 0.3493409596081152,
      "grad_norm": 1.344719648361206,
      "learning_rate": 0.00019519771211756542,
      "loss": 0.4317,
      "step": 98700
    },
    {
      "epoch": 0.3496949018164314,
      "grad_norm": 0.0478852204978466,
      "learning_rate": 0.00019509152945507057,
      "loss": 0.3875,
      "step": 98800
    },
    {
      "epoch": 0.3500488440247476,
      "grad_norm": 33.10993957519531,
      "learning_rate": 0.00019498534679257567,
      "loss": 0.3212,
      "step": 98900
    },
    {
      "epoch": 0.35040278623306387,
      "grad_norm": 0.0076875532977283,
      "learning_rate": 0.00019487916413008083,
      "loss": 0.426,
      "step": 99000
    },
    {
      "epoch": 0.3507567284413801,
      "grad_norm": 0.0010352047393098474,
      "learning_rate": 0.00019477298146758595,
      "loss": 0.4699,
      "step": 99100
    },
    {
      "epoch": 0.3511106706496963,
      "grad_norm": 0.03846851363778114,
      "learning_rate": 0.00019466679880509108,
      "loss": 0.5266,
      "step": 99200
    },
    {
      "epoch": 0.35146461285801256,
      "grad_norm": 0.007336963899433613,
      "learning_rate": 0.0001945606161425962,
      "loss": 0.5732,
      "step": 99300
    },
    {
      "epoch": 0.35181855506632875,
      "grad_norm": 5.1328125,
      "learning_rate": 0.00019445443348010136,
      "loss": 0.5351,
      "step": 99400
    },
    {
      "epoch": 0.352172497274645,
      "grad_norm": 0.00023748783860355616,
      "learning_rate": 0.0001943482508176065,
      "loss": 0.4385,
      "step": 99500
    },
    {
      "epoch": 0.3525264394829612,
      "grad_norm": 0.009158974513411522,
      "learning_rate": 0.00019424206815511161,
      "loss": 0.4061,
      "step": 99600
    },
    {
      "epoch": 0.35288038169127744,
      "grad_norm": 1.1820509433746338,
      "learning_rate": 0.00019413588549261674,
      "loss": 0.5366,
      "step": 99700
    },
    {
      "epoch": 0.3532343238995937,
      "grad_norm": 0.02230796590447426,
      "learning_rate": 0.0001940297028301219,
      "loss": 0.355,
      "step": 99800
    },
    {
      "epoch": 0.3535882661079099,
      "grad_norm": 0.07459902763366699,
      "learning_rate": 0.000193923520167627,
      "loss": 0.3808,
      "step": 99900
    },
    {
      "epoch": 0.35394220831622614,
      "grad_norm": 0.0033049718476831913,
      "learning_rate": 0.00019381733750513215,
      "loss": 0.4037,
      "step": 100000
    },
    {
      "epoch": 0.35429615052454233,
      "grad_norm": 0.0005235238349996507,
      "learning_rate": 0.00019371115484263728,
      "loss": 0.5818,
      "step": 100100
    },
    {
      "epoch": 0.3546500927328586,
      "grad_norm": 0.0020955868531018496,
      "learning_rate": 0.00019360497218014243,
      "loss": 0.414,
      "step": 100200
    },
    {
      "epoch": 0.35500403494117483,
      "grad_norm": 0.0014749838737770915,
      "learning_rate": 0.00019349878951764753,
      "loss": 0.4369,
      "step": 100300
    },
    {
      "epoch": 0.355357977149491,
      "grad_norm": 0.003548850305378437,
      "learning_rate": 0.00019339260685515269,
      "loss": 0.1385,
      "step": 100400
    },
    {
      "epoch": 0.35571191935780727,
      "grad_norm": 0.9619706273078918,
      "learning_rate": 0.0001932864241926578,
      "loss": 0.3661,
      "step": 100500
    },
    {
      "epoch": 0.35606586156612346,
      "grad_norm": 1.6787320375442505,
      "learning_rate": 0.00019318024153016294,
      "loss": 0.4358,
      "step": 100600
    },
    {
      "epoch": 0.3564198037744397,
      "grad_norm": 0.007589868735522032,
      "learning_rate": 0.00019307405886766807,
      "loss": 0.4891,
      "step": 100700
    },
    {
      "epoch": 0.35677374598275596,
      "grad_norm": 7.831006951164454e-05,
      "learning_rate": 0.00019296787620517322,
      "loss": 0.6148,
      "step": 100800
    },
    {
      "epoch": 0.35712768819107216,
      "grad_norm": 0.047638390213251114,
      "learning_rate": 0.00019286169354267832,
      "loss": 0.3055,
      "step": 100900
    },
    {
      "epoch": 0.3574816303993884,
      "grad_norm": 0.3529464602470398,
      "learning_rate": 0.00019275551088018347,
      "loss": 0.5599,
      "step": 101000
    },
    {
      "epoch": 0.3578355726077046,
      "grad_norm": 0.006593294441699982,
      "learning_rate": 0.0001926493282176886,
      "loss": 0.5074,
      "step": 101100
    },
    {
      "epoch": 0.35818951481602085,
      "grad_norm": 4.253584384918213,
      "learning_rate": 0.00019254314555519376,
      "loss": 0.5971,
      "step": 101200
    },
    {
      "epoch": 0.35854345702433704,
      "grad_norm": 0.0022826751228421926,
      "learning_rate": 0.00019243696289269886,
      "loss": 0.4013,
      "step": 101300
    },
    {
      "epoch": 0.3588973992326533,
      "grad_norm": 0.004513528663665056,
      "learning_rate": 0.00019233078023020398,
      "loss": 0.4164,
      "step": 101400
    },
    {
      "epoch": 0.35925134144096954,
      "grad_norm": 2.13153338432312,
      "learning_rate": 0.00019222459756770914,
      "loss": 0.4268,
      "step": 101500
    },
    {
      "epoch": 0.35960528364928573,
      "grad_norm": 27.132652282714844,
      "learning_rate": 0.00019211841490521424,
      "loss": 0.4822,
      "step": 101600
    },
    {
      "epoch": 0.359959225857602,
      "grad_norm": 109.50152587890625,
      "learning_rate": 0.0001920122322427194,
      "loss": 0.4839,
      "step": 101700
    },
    {
      "epoch": 0.3603131680659182,
      "grad_norm": 0.005534731782972813,
      "learning_rate": 0.00019190604958022452,
      "loss": 0.5091,
      "step": 101800
    },
    {
      "epoch": 0.3606671102742344,
      "grad_norm": 0.002932875184342265,
      "learning_rate": 0.00019179986691772967,
      "loss": 0.3214,
      "step": 101900
    },
    {
      "epoch": 0.3610210524825507,
      "grad_norm": 0.2107335925102234,
      "learning_rate": 0.00019169368425523477,
      "loss": 0.4086,
      "step": 102000
    },
    {
      "epoch": 0.36137499469086687,
      "grad_norm": 0.07896485179662704,
      "learning_rate": 0.00019158750159273993,
      "loss": 0.4437,
      "step": 102100
    },
    {
      "epoch": 0.3617289368991831,
      "grad_norm": 73.72564697265625,
      "learning_rate": 0.00019148131893024505,
      "loss": 0.4443,
      "step": 102200
    },
    {
      "epoch": 0.3620828791074993,
      "grad_norm": 0.003753039753064513,
      "learning_rate": 0.00019137513626775018,
      "loss": 0.6296,
      "step": 102300
    },
    {
      "epoch": 0.36243682131581556,
      "grad_norm": 0.0014412406599149108,
      "learning_rate": 0.0001912689536052553,
      "loss": 0.2512,
      "step": 102400
    },
    {
      "epoch": 0.36279076352413175,
      "grad_norm": 0.0029653606470674276,
      "learning_rate": 0.00019116277094276046,
      "loss": 0.51,
      "step": 102500
    },
    {
      "epoch": 0.363144705732448,
      "grad_norm": 16.876859664916992,
      "learning_rate": 0.00019105658828026556,
      "loss": 0.5676,
      "step": 102600
    },
    {
      "epoch": 0.36349864794076425,
      "grad_norm": 33.98918533325195,
      "learning_rate": 0.00019095040561777071,
      "loss": 0.3125,
      "step": 102700
    },
    {
      "epoch": 0.36385259014908045,
      "grad_norm": 0.005223563872277737,
      "learning_rate": 0.00019084422295527584,
      "loss": 0.2586,
      "step": 102800
    },
    {
      "epoch": 0.3642065323573967,
      "grad_norm": 0.001219758065417409,
      "learning_rate": 0.000190738040292781,
      "loss": 0.4842,
      "step": 102900
    },
    {
      "epoch": 0.3645604745657129,
      "grad_norm": 0.0012902222806587815,
      "learning_rate": 0.0001906318576302861,
      "loss": 0.2197,
      "step": 103000
    },
    {
      "epoch": 0.36491441677402914,
      "grad_norm": 0.0625343918800354,
      "learning_rate": 0.00019052567496779125,
      "loss": 0.7514,
      "step": 103100
    },
    {
      "epoch": 0.3652683589823454,
      "grad_norm": 17.801300048828125,
      "learning_rate": 0.00019041949230529638,
      "loss": 0.4828,
      "step": 103200
    },
    {
      "epoch": 0.3656223011906616,
      "grad_norm": 0.001202102517709136,
      "learning_rate": 0.0001903133096428015,
      "loss": 0.5588,
      "step": 103300
    },
    {
      "epoch": 0.36597624339897783,
      "grad_norm": 0.0004308359930291772,
      "learning_rate": 0.00019020712698030663,
      "loss": 0.496,
      "step": 103400
    },
    {
      "epoch": 0.366330185607294,
      "grad_norm": 37.83311462402344,
      "learning_rate": 0.00019010094431781178,
      "loss": 0.3293,
      "step": 103500
    },
    {
      "epoch": 0.3666841278156103,
      "grad_norm": 0.00014220924640540034,
      "learning_rate": 0.0001899947616553169,
      "loss": 0.3025,
      "step": 103600
    },
    {
      "epoch": 0.36703807002392647,
      "grad_norm": 0.005057744216173887,
      "learning_rate": 0.00018988857899282204,
      "loss": 0.4487,
      "step": 103700
    },
    {
      "epoch": 0.3673920122322427,
      "grad_norm": 0.01755637675523758,
      "learning_rate": 0.00018978239633032717,
      "loss": 0.5612,
      "step": 103800
    },
    {
      "epoch": 0.36774595444055896,
      "grad_norm": 25.93537712097168,
      "learning_rate": 0.00018967621366783232,
      "loss": 0.5634,
      "step": 103900
    },
    {
      "epoch": 0.36809989664887516,
      "grad_norm": 53.84065628051758,
      "learning_rate": 0.00018957003100533742,
      "loss": 0.2773,
      "step": 104000
    },
    {
      "epoch": 0.3684538388571914,
      "grad_norm": 0.5873176455497742,
      "learning_rate": 0.00018946384834284257,
      "loss": 0.5524,
      "step": 104100
    },
    {
      "epoch": 0.3688077810655076,
      "grad_norm": 0.008592495694756508,
      "learning_rate": 0.0001893576656803477,
      "loss": 0.5361,
      "step": 104200
    },
    {
      "epoch": 0.36916172327382385,
      "grad_norm": 58.97052764892578,
      "learning_rate": 0.0001892514830178528,
      "loss": 0.3798,
      "step": 104300
    },
    {
      "epoch": 0.3695156654821401,
      "grad_norm": 0.1599302738904953,
      "learning_rate": 0.00018914530035535795,
      "loss": 0.5079,
      "step": 104400
    },
    {
      "epoch": 0.3698696076904563,
      "grad_norm": 0.002654127310961485,
      "learning_rate": 0.0001890391176928631,
      "loss": 0.5199,
      "step": 104500
    },
    {
      "epoch": 0.37022354989877254,
      "grad_norm": 0.1159871444106102,
      "learning_rate": 0.00018893293503036824,
      "loss": 0.1419,
      "step": 104600
    },
    {
      "epoch": 0.37057749210708874,
      "grad_norm": 0.0013257693499326706,
      "learning_rate": 0.00018882675236787334,
      "loss": 0.561,
      "step": 104700
    },
    {
      "epoch": 0.370931434315405,
      "grad_norm": 0.11944516748189926,
      "learning_rate": 0.0001887205697053785,
      "loss": 0.5484,
      "step": 104800
    },
    {
      "epoch": 0.37128537652372123,
      "grad_norm": 0.005228110589087009,
      "learning_rate": 0.00018861438704288362,
      "loss": 0.4279,
      "step": 104900
    },
    {
      "epoch": 0.3716393187320374,
      "grad_norm": 0.01377524621784687,
      "learning_rate": 0.00018850820438038874,
      "loss": 0.4428,
      "step": 105000
    },
    {
      "epoch": 0.3719932609403537,
      "grad_norm": 0.1061965599656105,
      "learning_rate": 0.00018840202171789387,
      "loss": 0.4616,
      "step": 105100
    },
    {
      "epoch": 0.37234720314866987,
      "grad_norm": 32.13261032104492,
      "learning_rate": 0.00018829583905539903,
      "loss": 0.3462,
      "step": 105200
    },
    {
      "epoch": 0.3727011453569861,
      "grad_norm": 0.05976620316505432,
      "learning_rate": 0.00018818965639290415,
      "loss": 0.3598,
      "step": 105300
    },
    {
      "epoch": 0.3730550875653023,
      "grad_norm": 7.496638774871826,
      "learning_rate": 0.00018808347373040928,
      "loss": 0.3634,
      "step": 105400
    },
    {
      "epoch": 0.37340902977361856,
      "grad_norm": 0.013925150968134403,
      "learning_rate": 0.0001879772910679144,
      "loss": 0.3823,
      "step": 105500
    },
    {
      "epoch": 0.3737629719819348,
      "grad_norm": 0.04078619182109833,
      "learning_rate": 0.00018787110840541956,
      "loss": 0.2805,
      "step": 105600
    },
    {
      "epoch": 0.374116914190251,
      "grad_norm": 2.0608925819396973,
      "learning_rate": 0.00018776492574292466,
      "loss": 0.3809,
      "step": 105700
    },
    {
      "epoch": 0.37447085639856725,
      "grad_norm": 0.024679597467184067,
      "learning_rate": 0.00018765874308042981,
      "loss": 0.4468,
      "step": 105800
    },
    {
      "epoch": 0.37482479860688345,
      "grad_norm": 0.00037112663267180324,
      "learning_rate": 0.00018755256041793494,
      "loss": 0.4562,
      "step": 105900
    },
    {
      "epoch": 0.3751787408151997,
      "grad_norm": 0.001196765573695302,
      "learning_rate": 0.0001874463777554401,
      "loss": 0.6278,
      "step": 106000
    },
    {
      "epoch": 0.37553268302351595,
      "grad_norm": 12.248783111572266,
      "learning_rate": 0.0001873401950929452,
      "loss": 0.4814,
      "step": 106100
    },
    {
      "epoch": 0.37588662523183214,
      "grad_norm": 0.00974240805953741,
      "learning_rate": 0.00018723401243045035,
      "loss": 0.3968,
      "step": 106200
    },
    {
      "epoch": 0.3762405674401484,
      "grad_norm": 0.0007179128006100655,
      "learning_rate": 0.00018712782976795548,
      "loss": 0.3149,
      "step": 106300
    },
    {
      "epoch": 0.3765945096484646,
      "grad_norm": 124.5879135131836,
      "learning_rate": 0.0001870216471054606,
      "loss": 0.3713,
      "step": 106400
    },
    {
      "epoch": 0.37694845185678083,
      "grad_norm": 0.0027415556833148003,
      "learning_rate": 0.00018691546444296573,
      "loss": 0.6659,
      "step": 106500
    },
    {
      "epoch": 0.377302394065097,
      "grad_norm": 59.49862289428711,
      "learning_rate": 0.00018680928178047088,
      "loss": 0.3406,
      "step": 106600
    },
    {
      "epoch": 0.3776563362734133,
      "grad_norm": 0.0427040234208107,
      "learning_rate": 0.00018670309911797598,
      "loss": 0.4342,
      "step": 106700
    },
    {
      "epoch": 0.3780102784817295,
      "grad_norm": 95.84465789794922,
      "learning_rate": 0.00018659691645548114,
      "loss": 0.4317,
      "step": 106800
    },
    {
      "epoch": 0.3783642206900457,
      "grad_norm": 0.045938558876514435,
      "learning_rate": 0.00018649073379298627,
      "loss": 0.643,
      "step": 106900
    },
    {
      "epoch": 0.37871816289836197,
      "grad_norm": 1.013120174407959,
      "learning_rate": 0.00018638455113049142,
      "loss": 0.5424,
      "step": 107000
    },
    {
      "epoch": 0.37907210510667816,
      "grad_norm": 67.88285827636719,
      "learning_rate": 0.00018627836846799652,
      "loss": 0.2313,
      "step": 107100
    },
    {
      "epoch": 0.3794260473149944,
      "grad_norm": 0.13609248399734497,
      "learning_rate": 0.00018617218580550167,
      "loss": 0.4965,
      "step": 107200
    },
    {
      "epoch": 0.37977998952331066,
      "grad_norm": 0.0006888614152558148,
      "learning_rate": 0.0001860660031430068,
      "loss": 0.2785,
      "step": 107300
    },
    {
      "epoch": 0.38013393173162685,
      "grad_norm": 0.000500769354403019,
      "learning_rate": 0.0001859598204805119,
      "loss": 0.6058,
      "step": 107400
    },
    {
      "epoch": 0.3804878739399431,
      "grad_norm": 1.4506802558898926,
      "learning_rate": 0.00018585363781801705,
      "loss": 0.4249,
      "step": 107500
    },
    {
      "epoch": 0.3808418161482593,
      "grad_norm": 0.13765372335910797,
      "learning_rate": 0.0001857474551555222,
      "loss": 0.3609,
      "step": 107600
    },
    {
      "epoch": 0.38119575835657554,
      "grad_norm": 0.04281753674149513,
      "learning_rate": 0.00018564127249302734,
      "loss": 0.5312,
      "step": 107700
    },
    {
      "epoch": 0.3815497005648918,
      "grad_norm": 0.004272662103176117,
      "learning_rate": 0.00018553508983053244,
      "loss": 0.6191,
      "step": 107800
    },
    {
      "epoch": 0.381903642773208,
      "grad_norm": 0.001152220880612731,
      "learning_rate": 0.0001854289071680376,
      "loss": 0.5787,
      "step": 107900
    },
    {
      "epoch": 0.38225758498152423,
      "grad_norm": 0.0028657743241637945,
      "learning_rate": 0.00018532272450554274,
      "loss": 0.4634,
      "step": 108000
    },
    {
      "epoch": 0.38261152718984043,
      "grad_norm": 0.0008001064416021109,
      "learning_rate": 0.00018521654184304784,
      "loss": 0.5386,
      "step": 108100
    },
    {
      "epoch": 0.3829654693981567,
      "grad_norm": 0.01999436318874359,
      "learning_rate": 0.00018511035918055297,
      "loss": 0.476,
      "step": 108200
    },
    {
      "epoch": 0.38331941160647287,
      "grad_norm": 16.18714714050293,
      "learning_rate": 0.00018500417651805812,
      "loss": 0.4492,
      "step": 108300
    },
    {
      "epoch": 0.3836733538147891,
      "grad_norm": 0.016681509092450142,
      "learning_rate": 0.00018489799385556322,
      "loss": 0.4575,
      "step": 108400
    },
    {
      "epoch": 0.38402729602310537,
      "grad_norm": 0.00031426004716195166,
      "learning_rate": 0.00018479181119306838,
      "loss": 0.4435,
      "step": 108500
    },
    {
      "epoch": 0.38438123823142156,
      "grad_norm": 51.28889083862305,
      "learning_rate": 0.0001846856285305735,
      "loss": 0.4432,
      "step": 108600
    },
    {
      "epoch": 0.3847351804397378,
      "grad_norm": 0.0025085709057748318,
      "learning_rate": 0.00018457944586807866,
      "loss": 0.377,
      "step": 108700
    },
    {
      "epoch": 0.385089122648054,
      "grad_norm": 0.012239156290888786,
      "learning_rate": 0.00018447326320558376,
      "loss": 0.3436,
      "step": 108800
    },
    {
      "epoch": 0.38544306485637025,
      "grad_norm": 0.3200022876262665,
      "learning_rate": 0.00018436708054308891,
      "loss": 0.3539,
      "step": 108900
    },
    {
      "epoch": 0.3857970070646865,
      "grad_norm": 0.000144878780702129,
      "learning_rate": 0.00018426089788059404,
      "loss": 0.3153,
      "step": 109000
    },
    {
      "epoch": 0.3861509492730027,
      "grad_norm": 1.0570980310440063,
      "learning_rate": 0.00018415471521809917,
      "loss": 0.3552,
      "step": 109100
    },
    {
      "epoch": 0.38650489148131895,
      "grad_norm": 0.029592756181955338,
      "learning_rate": 0.0001840485325556043,
      "loss": 0.3206,
      "step": 109200
    },
    {
      "epoch": 0.38685883368963514,
      "grad_norm": 0.05255153402686119,
      "learning_rate": 0.00018394234989310945,
      "loss": 0.265,
      "step": 109300
    },
    {
      "epoch": 0.3872127758979514,
      "grad_norm": 0.016697917133569717,
      "learning_rate": 0.00018383616723061458,
      "loss": 0.3421,
      "step": 109400
    },
    {
      "epoch": 0.3875667181062676,
      "grad_norm": 0.04554728791117668,
      "learning_rate": 0.0001837299845681197,
      "loss": 0.5326,
      "step": 109500
    },
    {
      "epoch": 0.38792066031458383,
      "grad_norm": 0.0028281796257942915,
      "learning_rate": 0.00018362380190562483,
      "loss": 0.4276,
      "step": 109600
    },
    {
      "epoch": 0.3882746025229001,
      "grad_norm": 0.0010690463241189718,
      "learning_rate": 0.00018351761924312998,
      "loss": 0.2904,
      "step": 109700
    },
    {
      "epoch": 0.3886285447312163,
      "grad_norm": 0.004735868889838457,
      "learning_rate": 0.00018341143658063508,
      "loss": 0.4181,
      "step": 109800
    },
    {
      "epoch": 0.3889824869395325,
      "grad_norm": 0.004535992629826069,
      "learning_rate": 0.00018330525391814024,
      "loss": 0.5317,
      "step": 109900
    },
    {
      "epoch": 0.3893364291478487,
      "grad_norm": 0.025107605382800102,
      "learning_rate": 0.00018319907125564536,
      "loss": 0.2929,
      "step": 110000
    },
    {
      "epoch": 0.38969037135616497,
      "grad_norm": 0.0017239806475117803,
      "learning_rate": 0.00018309288859315052,
      "loss": 0.5278,
      "step": 110100
    },
    {
      "epoch": 0.3900443135644812,
      "grad_norm": 0.021319301798939705,
      "learning_rate": 0.00018298670593065562,
      "loss": 0.3108,
      "step": 110200
    },
    {
      "epoch": 0.3903982557727974,
      "grad_norm": 6.280332088470459,
      "learning_rate": 0.00018288052326816077,
      "loss": 0.4958,
      "step": 110300
    },
    {
      "epoch": 0.39075219798111366,
      "grad_norm": 0.02332872711122036,
      "learning_rate": 0.0001827743406056659,
      "loss": 0.33,
      "step": 110400
    },
    {
      "epoch": 0.39110614018942985,
      "grad_norm": 22.8580322265625,
      "learning_rate": 0.000182668157943171,
      "loss": 0.2648,
      "step": 110500
    },
    {
      "epoch": 0.3914600823977461,
      "grad_norm": 0.008780324831604958,
      "learning_rate": 0.00018256197528067615,
      "loss": 0.5026,
      "step": 110600
    },
    {
      "epoch": 0.3918140246060623,
      "grad_norm": 0.0003521266335155815,
      "learning_rate": 0.0001824557926181813,
      "loss": 0.3254,
      "step": 110700
    },
    {
      "epoch": 0.39216796681437854,
      "grad_norm": 0.38464000821113586,
      "learning_rate": 0.0001823496099556864,
      "loss": 0.4987,
      "step": 110800
    },
    {
      "epoch": 0.3925219090226948,
      "grad_norm": 0.0005870671011507511,
      "learning_rate": 0.00018224342729319154,
      "loss": 0.5401,
      "step": 110900
    },
    {
      "epoch": 0.392875851231011,
      "grad_norm": 0.0005376478657126427,
      "learning_rate": 0.0001821372446306967,
      "loss": 0.4656,
      "step": 111000
    },
    {
      "epoch": 0.39322979343932724,
      "grad_norm": 0.06893102824687958,
      "learning_rate": 0.00018203106196820184,
      "loss": 0.4003,
      "step": 111100
    },
    {
      "epoch": 0.39358373564764343,
      "grad_norm": 0.048727937042713165,
      "learning_rate": 0.00018192487930570694,
      "loss": 0.2669,
      "step": 111200
    },
    {
      "epoch": 0.3939376778559597,
      "grad_norm": 0.010502955876290798,
      "learning_rate": 0.00018181869664321207,
      "loss": 0.406,
      "step": 111300
    },
    {
      "epoch": 0.3942916200642759,
      "grad_norm": 0.00018114267732016742,
      "learning_rate": 0.00018171251398071722,
      "loss": 0.4398,
      "step": 111400
    },
    {
      "epoch": 0.3946455622725921,
      "grad_norm": 19.38096046447754,
      "learning_rate": 0.00018160633131822232,
      "loss": 0.5105,
      "step": 111500
    },
    {
      "epoch": 0.39499950448090837,
      "grad_norm": 0.2574065327644348,
      "learning_rate": 0.00018150014865572748,
      "loss": 0.5037,
      "step": 111600
    },
    {
      "epoch": 0.39535344668922456,
      "grad_norm": 0.00032129930332303047,
      "learning_rate": 0.0001813939659932326,
      "loss": 0.4375,
      "step": 111700
    },
    {
      "epoch": 0.3957073888975408,
      "grad_norm": 0.07600153237581253,
      "learning_rate": 0.00018128778333073776,
      "loss": 0.2708,
      "step": 111800
    },
    {
      "epoch": 0.39606133110585706,
      "grad_norm": 0.0008832072489894927,
      "learning_rate": 0.00018118160066824286,
      "loss": 0.5856,
      "step": 111900
    },
    {
      "epoch": 0.39641527331417326,
      "grad_norm": 53.49614715576172,
      "learning_rate": 0.000181075418005748,
      "loss": 0.5239,
      "step": 112000
    },
    {
      "epoch": 0.3967692155224895,
      "grad_norm": 1.3707655668258667,
      "learning_rate": 0.00018096923534325314,
      "loss": 0.2858,
      "step": 112100
    },
    {
      "epoch": 0.3971231577308057,
      "grad_norm": 0.0007876601302996278,
      "learning_rate": 0.00018086305268075827,
      "loss": 0.4346,
      "step": 112200
    },
    {
      "epoch": 0.39747709993912195,
      "grad_norm": 3.7860283851623535,
      "learning_rate": 0.0001807568700182634,
      "loss": 0.5205,
      "step": 112300
    },
    {
      "epoch": 0.39783104214743814,
      "grad_norm": 33.092613220214844,
      "learning_rate": 0.00018065068735576855,
      "loss": 0.5693,
      "step": 112400
    },
    {
      "epoch": 0.3981849843557544,
      "grad_norm": 0.07232469320297241,
      "learning_rate": 0.00018054450469327365,
      "loss": 0.4515,
      "step": 112500
    },
    {
      "epoch": 0.39853892656407064,
      "grad_norm": 0.004305677954107523,
      "learning_rate": 0.0001804383220307788,
      "loss": 0.3929,
      "step": 112600
    },
    {
      "epoch": 0.39889286877238683,
      "grad_norm": 0.000368569977581501,
      "learning_rate": 0.00018033213936828393,
      "loss": 0.355,
      "step": 112700
    },
    {
      "epoch": 0.3992468109807031,
      "grad_norm": 66.04598999023438,
      "learning_rate": 0.00018022595670578908,
      "loss": 0.5191,
      "step": 112800
    },
    {
      "epoch": 0.3996007531890193,
      "grad_norm": 0.04472880810499191,
      "learning_rate": 0.00018011977404329418,
      "loss": 0.3784,
      "step": 112900
    },
    {
      "epoch": 0.3999546953973355,
      "grad_norm": 0.001682584872469306,
      "learning_rate": 0.00018001359138079934,
      "loss": 0.2465,
      "step": 113000
    },
    {
      "epoch": 0.4003086376056518,
      "grad_norm": 0.000262698158621788,
      "learning_rate": 0.00017990740871830446,
      "loss": 0.3259,
      "step": 113100
    },
    {
      "epoch": 0.40066257981396797,
      "grad_norm": 0.0019234888022765517,
      "learning_rate": 0.00017980122605580956,
      "loss": 0.486,
      "step": 113200
    },
    {
      "epoch": 0.4010165220222842,
      "grad_norm": 0.00017134458175860345,
      "learning_rate": 0.00017969504339331472,
      "loss": 0.1784,
      "step": 113300
    },
    {
      "epoch": 0.4013704642306004,
      "grad_norm": 2.2795708179473877,
      "learning_rate": 0.00017958886073081987,
      "loss": 0.3161,
      "step": 113400
    },
    {
      "epoch": 0.40172440643891666,
      "grad_norm": 0.004386072978377342,
      "learning_rate": 0.000179482678068325,
      "loss": 0.2822,
      "step": 113500
    },
    {
      "epoch": 0.40207834864723285,
      "grad_norm": 0.005924169439822435,
      "learning_rate": 0.0001793764954058301,
      "loss": 0.1984,
      "step": 113600
    },
    {
      "epoch": 0.4024322908555491,
      "grad_norm": 0.00014342600479722023,
      "learning_rate": 0.00017927031274333525,
      "loss": 0.3563,
      "step": 113700
    },
    {
      "epoch": 0.40278623306386535,
      "grad_norm": 0.0037111053243279457,
      "learning_rate": 0.0001791641300808404,
      "loss": 0.2538,
      "step": 113800
    },
    {
      "epoch": 0.40314017527218154,
      "grad_norm": 0.03837452456355095,
      "learning_rate": 0.0001790579474183455,
      "loss": 0.6231,
      "step": 113900
    },
    {
      "epoch": 0.4034941174804978,
      "grad_norm": 0.09938058257102966,
      "learning_rate": 0.00017895176475585063,
      "loss": 0.3629,
      "step": 114000
    },
    {
      "epoch": 0.403848059688814,
      "grad_norm": 8.523372650146484,
      "learning_rate": 0.0001788455820933558,
      "loss": 0.5393,
      "step": 114100
    },
    {
      "epoch": 0.40420200189713024,
      "grad_norm": 0.009187678806483746,
      "learning_rate": 0.0001787393994308609,
      "loss": 0.4673,
      "step": 114200
    },
    {
      "epoch": 0.4045559441054465,
      "grad_norm": 0.04003890976309776,
      "learning_rate": 0.00017863321676836604,
      "loss": 0.3193,
      "step": 114300
    },
    {
      "epoch": 0.4049098863137627,
      "grad_norm": 0.0729435384273529,
      "learning_rate": 0.00017852703410587117,
      "loss": 0.2491,
      "step": 114400
    },
    {
      "epoch": 0.40526382852207893,
      "grad_norm": 0.00181145453825593,
      "learning_rate": 0.00017842085144337632,
      "loss": 0.3096,
      "step": 114500
    },
    {
      "epoch": 0.4056177707303951,
      "grad_norm": 114.71125793457031,
      "learning_rate": 0.00017831466878088142,
      "loss": 0.4542,
      "step": 114600
    },
    {
      "epoch": 0.40597171293871137,
      "grad_norm": 0.0014293811982497573,
      "learning_rate": 0.00017820848611838658,
      "loss": 0.5378,
      "step": 114700
    },
    {
      "epoch": 0.4063256551470276,
      "grad_norm": 0.021083656698465347,
      "learning_rate": 0.0001781023034558917,
      "loss": 0.4395,
      "step": 114800
    },
    {
      "epoch": 0.4066795973553438,
      "grad_norm": 0.003272381843999028,
      "learning_rate": 0.00017799612079339683,
      "loss": 0.5337,
      "step": 114900
    },
    {
      "epoch": 0.40703353956366006,
      "grad_norm": 0.001582846394740045,
      "learning_rate": 0.00017788993813090196,
      "loss": 0.4708,
      "step": 115000
    },
    {
      "epoch": 0.40738748177197626,
      "grad_norm": 0.43893182277679443,
      "learning_rate": 0.0001777837554684071,
      "loss": 0.552,
      "step": 115100
    },
    {
      "epoch": 0.4077414239802925,
      "grad_norm": 69.58409118652344,
      "learning_rate": 0.00017767757280591224,
      "loss": 0.4462,
      "step": 115200
    },
    {
      "epoch": 0.4080953661886087,
      "grad_norm": 0.1939355880022049,
      "learning_rate": 0.00017757139014341737,
      "loss": 0.2758,
      "step": 115300
    },
    {
      "epoch": 0.40844930839692495,
      "grad_norm": 0.0045469156466424465,
      "learning_rate": 0.0001774652074809225,
      "loss": 0.2316,
      "step": 115400
    },
    {
      "epoch": 0.4088032506052412,
      "grad_norm": 0.001637987676076591,
      "learning_rate": 0.00017735902481842765,
      "loss": 0.3564,
      "step": 115500
    },
    {
      "epoch": 0.4091571928135574,
      "grad_norm": 0.000922633393201977,
      "learning_rate": 0.00017725284215593275,
      "loss": 0.3514,
      "step": 115600
    },
    {
      "epoch": 0.40951113502187364,
      "grad_norm": 35.711185455322266,
      "learning_rate": 0.0001771466594934379,
      "loss": 0.3674,
      "step": 115700
    },
    {
      "epoch": 0.40986507723018983,
      "grad_norm": 0.0012362306006252766,
      "learning_rate": 0.00017704047683094303,
      "loss": 0.5068,
      "step": 115800
    },
    {
      "epoch": 0.4102190194385061,
      "grad_norm": 62.68331527709961,
      "learning_rate": 0.00017693429416844818,
      "loss": 0.5306,
      "step": 115900
    },
    {
      "epoch": 0.41057296164682233,
      "grad_norm": 0.1991034597158432,
      "learning_rate": 0.00017682811150595328,
      "loss": 0.2285,
      "step": 116000
    },
    {
      "epoch": 0.4109269038551385,
      "grad_norm": 0.0002780851209536195,
      "learning_rate": 0.00017672192884345844,
      "loss": 0.3189,
      "step": 116100
    },
    {
      "epoch": 0.4112808460634548,
      "grad_norm": 0.00014598859706893563,
      "learning_rate": 0.00017661574618096356,
      "loss": 0.3527,
      "step": 116200
    },
    {
      "epoch": 0.41163478827177097,
      "grad_norm": 163.73199462890625,
      "learning_rate": 0.00017650956351846866,
      "loss": 0.489,
      "step": 116300
    },
    {
      "epoch": 0.4119887304800872,
      "grad_norm": 0.006341095548123121,
      "learning_rate": 0.00017640338085597382,
      "loss": 0.2589,
      "step": 116400
    },
    {
      "epoch": 0.4123426726884034,
      "grad_norm": 0.00036739828647114336,
      "learning_rate": 0.00017629719819347897,
      "loss": 0.6049,
      "step": 116500
    },
    {
      "epoch": 0.41269661489671966,
      "grad_norm": 30.78715705871582,
      "learning_rate": 0.00017619101553098407,
      "loss": 0.5145,
      "step": 116600
    },
    {
      "epoch": 0.4130505571050359,
      "grad_norm": 0.00025534513406455517,
      "learning_rate": 0.0001760848328684892,
      "loss": 0.2675,
      "step": 116700
    },
    {
      "epoch": 0.4134044993133521,
      "grad_norm": 93.2374038696289,
      "learning_rate": 0.00017597865020599435,
      "loss": 0.4548,
      "step": 116800
    },
    {
      "epoch": 0.41375844152166835,
      "grad_norm": 0.0014745040098205209,
      "learning_rate": 0.0001758724675434995,
      "loss": 0.5093,
      "step": 116900
    },
    {
      "epoch": 0.41411238372998455,
      "grad_norm": 0.030192626640200615,
      "learning_rate": 0.0001757662848810046,
      "loss": 0.3376,
      "step": 117000
    },
    {
      "epoch": 0.4144663259383008,
      "grad_norm": 0.5106778740882874,
      "learning_rate": 0.00017566010221850973,
      "loss": 0.5121,
      "step": 117100
    },
    {
      "epoch": 0.41482026814661704,
      "grad_norm": 8.849395751953125,
      "learning_rate": 0.0001755539195560149,
      "loss": 0.4175,
      "step": 117200
    },
    {
      "epoch": 0.41517421035493324,
      "grad_norm": 0.023859674111008644,
      "learning_rate": 0.00017544773689352,
      "loss": 0.2505,
      "step": 117300
    },
    {
      "epoch": 0.4155281525632495,
      "grad_norm": 22.090822219848633,
      "learning_rate": 0.00017534155423102514,
      "loss": 0.5333,
      "step": 117400
    },
    {
      "epoch": 0.4158820947715657,
      "grad_norm": 0.004611338023096323,
      "learning_rate": 0.00017523537156853027,
      "loss": 0.4126,
      "step": 117500
    },
    {
      "epoch": 0.41623603697988193,
      "grad_norm": 9.760564804077148,
      "learning_rate": 0.00017512918890603542,
      "loss": 0.3524,
      "step": 117600
    },
    {
      "epoch": 0.4165899791881982,
      "grad_norm": 0.001722149783745408,
      "learning_rate": 0.00017502300624354052,
      "loss": 0.287,
      "step": 117700
    },
    {
      "epoch": 0.41694392139651437,
      "grad_norm": 131.9387969970703,
      "learning_rate": 0.00017491682358104568,
      "loss": 0.2302,
      "step": 117800
    },
    {
      "epoch": 0.4172978636048306,
      "grad_norm": 8.044792175292969,
      "learning_rate": 0.0001748106409185508,
      "loss": 0.5088,
      "step": 117900
    },
    {
      "epoch": 0.4176518058131468,
      "grad_norm": 0.0036503891460597515,
      "learning_rate": 0.00017470445825605593,
      "loss": 0.3084,
      "step": 118000
    },
    {
      "epoch": 0.41800574802146306,
      "grad_norm": 0.009504147805273533,
      "learning_rate": 0.00017459827559356106,
      "loss": 0.2934,
      "step": 118100
    },
    {
      "epoch": 0.41835969022977926,
      "grad_norm": 0.0014004219556227326,
      "learning_rate": 0.0001744920929310662,
      "loss": 0.4637,
      "step": 118200
    },
    {
      "epoch": 0.4187136324380955,
      "grad_norm": 0.03585392236709595,
      "learning_rate": 0.0001743859102685713,
      "loss": 0.3835,
      "step": 118300
    },
    {
      "epoch": 0.41906757464641176,
      "grad_norm": 0.0004882137000095099,
      "learning_rate": 0.00017427972760607647,
      "loss": 0.333,
      "step": 118400
    },
    {
      "epoch": 0.41942151685472795,
      "grad_norm": 0.004894638899713755,
      "learning_rate": 0.0001741735449435816,
      "loss": 0.3339,
      "step": 118500
    },
    {
      "epoch": 0.4197754590630442,
      "grad_norm": 88.6510009765625,
      "learning_rate": 0.00017406736228108675,
      "loss": 0.4791,
      "step": 118600
    },
    {
      "epoch": 0.4201294012713604,
      "grad_norm": 0.028083432465791702,
      "learning_rate": 0.00017396117961859185,
      "loss": 0.2656,
      "step": 118700
    },
    {
      "epoch": 0.42048334347967664,
      "grad_norm": 0.0006513296393677592,
      "learning_rate": 0.000173854996956097,
      "loss": 0.254,
      "step": 118800
    },
    {
      "epoch": 0.4208372856879929,
      "grad_norm": 0.05726177617907524,
      "learning_rate": 0.00017374881429360213,
      "loss": 0.3225,
      "step": 118900
    },
    {
      "epoch": 0.4211912278963091,
      "grad_norm": 0.0012635213788598776,
      "learning_rate": 0.00017364263163110726,
      "loss": 0.3238,
      "step": 119000
    },
    {
      "epoch": 0.42154517010462533,
      "grad_norm": 0.0019405442290008068,
      "learning_rate": 0.00017353644896861238,
      "loss": 0.3047,
      "step": 119100
    },
    {
      "epoch": 0.4218991123129415,
      "grad_norm": 0.08317021280527115,
      "learning_rate": 0.00017343026630611754,
      "loss": 0.2818,
      "step": 119200
    },
    {
      "epoch": 0.4222530545212578,
      "grad_norm": 0.04011426120996475,
      "learning_rate": 0.00017332408364362266,
      "loss": 0.4475,
      "step": 119300
    },
    {
      "epoch": 0.42260699672957397,
      "grad_norm": 0.0015651745488867164,
      "learning_rate": 0.0001732179009811278,
      "loss": 0.2159,
      "step": 119400
    },
    {
      "epoch": 0.4229609389378902,
      "grad_norm": 0.00018850239575840533,
      "learning_rate": 0.00017311171831863292,
      "loss": 0.261,
      "step": 119500
    },
    {
      "epoch": 0.42331488114620647,
      "grad_norm": 0.014060952700674534,
      "learning_rate": 0.00017300553565613807,
      "loss": 0.2455,
      "step": 119600
    },
    {
      "epoch": 0.42366882335452266,
      "grad_norm": 61.757049560546875,
      "learning_rate": 0.00017289935299364317,
      "loss": 0.3119,
      "step": 119700
    },
    {
      "epoch": 0.4240227655628389,
      "grad_norm": 14.111300468444824,
      "learning_rate": 0.0001727931703311483,
      "loss": 0.415,
      "step": 119800
    },
    {
      "epoch": 0.4243767077711551,
      "grad_norm": 0.41110289096832275,
      "learning_rate": 0.00017268698766865345,
      "loss": 0.383,
      "step": 119900
    },
    {
      "epoch": 0.42473064997947135,
      "grad_norm": 15.226280212402344,
      "learning_rate": 0.0001725808050061586,
      "loss": 0.2529,
      "step": 120000
    },
    {
      "epoch": 0.4250845921877876,
      "grad_norm": 0.07809939235448837,
      "learning_rate": 0.0001724746223436637,
      "loss": 0.434,
      "step": 120100
    },
    {
      "epoch": 0.4254385343961038,
      "grad_norm": 6.853774070739746,
      "learning_rate": 0.00017236843968116883,
      "loss": 0.4963,
      "step": 120200
    },
    {
      "epoch": 0.42579247660442004,
      "grad_norm": 0.000403218058636412,
      "learning_rate": 0.000172262257018674,
      "loss": 0.29,
      "step": 120300
    },
    {
      "epoch": 0.42614641881273624,
      "grad_norm": 0.0015645502135157585,
      "learning_rate": 0.0001721560743561791,
      "loss": 0.4308,
      "step": 120400
    },
    {
      "epoch": 0.4265003610210525,
      "grad_norm": 0.0015045847976580262,
      "learning_rate": 0.00017204989169368424,
      "loss": 0.2994,
      "step": 120500
    },
    {
      "epoch": 0.4268543032293687,
      "grad_norm": 0.0008363049710169435,
      "learning_rate": 0.00017194370903118937,
      "loss": 0.4826,
      "step": 120600
    },
    {
      "epoch": 0.42720824543768493,
      "grad_norm": 0.0003549297107383609,
      "learning_rate": 0.0001718375263686945,
      "loss": 0.4243,
      "step": 120700
    },
    {
      "epoch": 0.4275621876460012,
      "grad_norm": 0.04398646950721741,
      "learning_rate": 0.00017173134370619962,
      "loss": 0.4247,
      "step": 120800
    },
    {
      "epoch": 0.4279161298543174,
      "grad_norm": 0.025905344635248184,
      "learning_rate": 0.00017162516104370478,
      "loss": 0.3534,
      "step": 120900
    },
    {
      "epoch": 0.4282700720626336,
      "grad_norm": 40.51395797729492,
      "learning_rate": 0.0001715189783812099,
      "loss": 0.4779,
      "step": 121000
    },
    {
      "epoch": 0.4286240142709498,
      "grad_norm": 0.0001513625174993649,
      "learning_rate": 0.00017141279571871503,
      "loss": 0.3802,
      "step": 121100
    },
    {
      "epoch": 0.42897795647926606,
      "grad_norm": 0.0006813268410041928,
      "learning_rate": 0.00017130661305622016,
      "loss": 0.4575,
      "step": 121200
    },
    {
      "epoch": 0.4293318986875823,
      "grad_norm": 0.0002685281215235591,
      "learning_rate": 0.0001712004303937253,
      "loss": 0.5044,
      "step": 121300
    },
    {
      "epoch": 0.4296858408958985,
      "grad_norm": 0.007111358921974897,
      "learning_rate": 0.0001710942477312304,
      "loss": 0.5323,
      "step": 121400
    },
    {
      "epoch": 0.43003978310421476,
      "grad_norm": 0.020568542182445526,
      "learning_rate": 0.00017098806506873557,
      "loss": 0.6401,
      "step": 121500
    },
    {
      "epoch": 0.43039372531253095,
      "grad_norm": 0.0019606733694672585,
      "learning_rate": 0.0001708818824062407,
      "loss": 0.3577,
      "step": 121600
    },
    {
      "epoch": 0.4307476675208472,
      "grad_norm": 84.61051940917969,
      "learning_rate": 0.00017077569974374585,
      "loss": 0.392,
      "step": 121700
    },
    {
      "epoch": 0.43110160972916345,
      "grad_norm": 0.0006567912641912699,
      "learning_rate": 0.00017066951708125095,
      "loss": 0.337,
      "step": 121800
    },
    {
      "epoch": 0.43145555193747964,
      "grad_norm": 31.037084579467773,
      "learning_rate": 0.0001705633344187561,
      "loss": 0.4315,
      "step": 121900
    },
    {
      "epoch": 0.4318094941457959,
      "grad_norm": 84.1902847290039,
      "learning_rate": 0.00017045715175626123,
      "loss": 0.5356,
      "step": 122000
    },
    {
      "epoch": 0.4321634363541121,
      "grad_norm": 0.018842000514268875,
      "learning_rate": 0.00017035096909376636,
      "loss": 0.2157,
      "step": 122100
    },
    {
      "epoch": 0.43251737856242833,
      "grad_norm": 0.0008347660768777132,
      "learning_rate": 0.00017024478643127148,
      "loss": 0.2424,
      "step": 122200
    },
    {
      "epoch": 0.4328713207707445,
      "grad_norm": 0.0025559996720403433,
      "learning_rate": 0.00017013860376877664,
      "loss": 0.5185,
      "step": 122300
    },
    {
      "epoch": 0.4332252629790608,
      "grad_norm": 0.003280577715486288,
      "learning_rate": 0.00017003242110628174,
      "loss": 0.4319,
      "step": 122400
    },
    {
      "epoch": 0.433579205187377,
      "grad_norm": 0.00010350902448408306,
      "learning_rate": 0.0001699262384437869,
      "loss": 0.3028,
      "step": 122500
    },
    {
      "epoch": 0.4339331473956932,
      "grad_norm": 0.05414055287837982,
      "learning_rate": 0.00016982005578129202,
      "loss": 0.415,
      "step": 122600
    },
    {
      "epoch": 0.43428708960400947,
      "grad_norm": 0.13031604886054993,
      "learning_rate": 0.00016971387311879717,
      "loss": 0.3041,
      "step": 122700
    },
    {
      "epoch": 0.43464103181232566,
      "grad_norm": 22.701318740844727,
      "learning_rate": 0.00016960769045630227,
      "loss": 0.48,
      "step": 122800
    },
    {
      "epoch": 0.4349949740206419,
      "grad_norm": 0.0015034980606287718,
      "learning_rate": 0.0001695015077938074,
      "loss": 0.3719,
      "step": 122900
    },
    {
      "epoch": 0.43534891622895816,
      "grad_norm": 6.795718669891357,
      "learning_rate": 0.00016939532513131255,
      "loss": 0.3753,
      "step": 123000
    },
    {
      "epoch": 0.43570285843727435,
      "grad_norm": 0.590488612651825,
      "learning_rate": 0.00016928914246881765,
      "loss": 0.3941,
      "step": 123100
    },
    {
      "epoch": 0.4360568006455906,
      "grad_norm": 0.0017526261508464813,
      "learning_rate": 0.0001691829598063228,
      "loss": 0.3598,
      "step": 123200
    },
    {
      "epoch": 0.4364107428539068,
      "grad_norm": 0.001986169721931219,
      "learning_rate": 0.00016907677714382793,
      "loss": 0.586,
      "step": 123300
    },
    {
      "epoch": 0.43676468506222305,
      "grad_norm": 0.13648973405361176,
      "learning_rate": 0.0001689705944813331,
      "loss": 0.3107,
      "step": 123400
    },
    {
      "epoch": 0.43711862727053924,
      "grad_norm": 0.013867201283574104,
      "learning_rate": 0.0001688644118188382,
      "loss": 0.4538,
      "step": 123500
    },
    {
      "epoch": 0.4374725694788555,
      "grad_norm": 0.0007069053826853633,
      "learning_rate": 0.00016875822915634334,
      "loss": 0.3066,
      "step": 123600
    },
    {
      "epoch": 0.43782651168717174,
      "grad_norm": 0.0021088712383061647,
      "learning_rate": 0.00016865204649384847,
      "loss": 0.4345,
      "step": 123700
    },
    {
      "epoch": 0.43818045389548793,
      "grad_norm": 0.004662761930376291,
      "learning_rate": 0.0001685458638313536,
      "loss": 0.4176,
      "step": 123800
    },
    {
      "epoch": 0.4385343961038042,
      "grad_norm": 0.001522615784779191,
      "learning_rate": 0.00016843968116885872,
      "loss": 0.2418,
      "step": 123900
    },
    {
      "epoch": 0.4388883383121204,
      "grad_norm": 0.03625858575105667,
      "learning_rate": 0.00016833349850636388,
      "loss": 0.526,
      "step": 124000
    },
    {
      "epoch": 0.4392422805204366,
      "grad_norm": 0.0003870763466693461,
      "learning_rate": 0.000168227315843869,
      "loss": 0.281,
      "step": 124100
    },
    {
      "epoch": 0.4395962227287529,
      "grad_norm": 0.10938356071710587,
      "learning_rate": 0.00016812113318137413,
      "loss": 0.5361,
      "step": 124200
    },
    {
      "epoch": 0.43995016493706907,
      "grad_norm": 0.00031535467132925987,
      "learning_rate": 0.00016801495051887926,
      "loss": 0.2814,
      "step": 124300
    },
    {
      "epoch": 0.4403041071453853,
      "grad_norm": 4.854906364926137e-05,
      "learning_rate": 0.0001679087678563844,
      "loss": 0.3487,
      "step": 124400
    },
    {
      "epoch": 0.4406580493537015,
      "grad_norm": 0.013396586291491985,
      "learning_rate": 0.0001678025851938895,
      "loss": 0.3986,
      "step": 124500
    },
    {
      "epoch": 0.44101199156201776,
      "grad_norm": 0.0027332801837474108,
      "learning_rate": 0.00016769640253139467,
      "loss": 0.3166,
      "step": 124600
    },
    {
      "epoch": 0.441365933770334,
      "grad_norm": 0.001952769118361175,
      "learning_rate": 0.0001675902198688998,
      "loss": 0.301,
      "step": 124700
    },
    {
      "epoch": 0.4417198759786502,
      "grad_norm": 0.039488423615694046,
      "learning_rate": 0.00016748403720640492,
      "loss": 0.4731,
      "step": 124800
    },
    {
      "epoch": 0.44207381818696645,
      "grad_norm": 0.009766051545739174,
      "learning_rate": 0.00016737785454391005,
      "loss": 0.3031,
      "step": 124900
    },
    {
      "epoch": 0.44242776039528264,
      "grad_norm": 0.002424001693725586,
      "learning_rate": 0.0001672716718814152,
      "loss": 0.516,
      "step": 125000
    },
    {
      "epoch": 0.4427817026035989,
      "grad_norm": 0.0005420633242465556,
      "learning_rate": 0.00016716548921892033,
      "loss": 0.448,
      "step": 125100
    },
    {
      "epoch": 0.4431356448119151,
      "grad_norm": 0.0012458114651963115,
      "learning_rate": 0.00016705930655642545,
      "loss": 0.5234,
      "step": 125200
    },
    {
      "epoch": 0.44348958702023134,
      "grad_norm": 0.0029335219878703356,
      "learning_rate": 0.00016695312389393058,
      "loss": 0.441,
      "step": 125300
    },
    {
      "epoch": 0.4438435292285476,
      "grad_norm": 0.0011766740353778005,
      "learning_rate": 0.00016684694123143574,
      "loss": 0.4258,
      "step": 125400
    },
    {
      "epoch": 0.4441974714368638,
      "grad_norm": 0.0007220967672765255,
      "learning_rate": 0.00016674075856894084,
      "loss": 0.4087,
      "step": 125500
    },
    {
      "epoch": 0.44455141364518,
      "grad_norm": 0.03504214808344841,
      "learning_rate": 0.000166634575906446,
      "loss": 0.3742,
      "step": 125600
    },
    {
      "epoch": 0.4449053558534962,
      "grad_norm": 0.005420558620244265,
      "learning_rate": 0.00016652839324395112,
      "loss": 0.3753,
      "step": 125700
    },
    {
      "epoch": 0.44525929806181247,
      "grad_norm": 33.82738494873047,
      "learning_rate": 0.00016642221058145627,
      "loss": 0.5288,
      "step": 125800
    },
    {
      "epoch": 0.4456132402701287,
      "grad_norm": 0.0010648824973031878,
      "learning_rate": 0.00016631602791896137,
      "loss": 0.2938,
      "step": 125900
    },
    {
      "epoch": 0.4459671824784449,
      "grad_norm": 0.0007512267911806703,
      "learning_rate": 0.00016620984525646653,
      "loss": 0.3867,
      "step": 126000
    },
    {
      "epoch": 0.44632112468676116,
      "grad_norm": 0.00010042668145615608,
      "learning_rate": 0.00016610366259397165,
      "loss": 0.3951,
      "step": 126100
    },
    {
      "epoch": 0.44667506689507736,
      "grad_norm": 0.41508054733276367,
      "learning_rate": 0.00016599747993147675,
      "loss": 0.3059,
      "step": 126200
    },
    {
      "epoch": 0.4470290091033936,
      "grad_norm": 0.017887694761157036,
      "learning_rate": 0.0001658912972689819,
      "loss": 0.3083,
      "step": 126300
    },
    {
      "epoch": 0.4473829513117098,
      "grad_norm": 0.0001384417264489457,
      "learning_rate": 0.00016578511460648703,
      "loss": 0.5827,
      "step": 126400
    },
    {
      "epoch": 0.44773689352002605,
      "grad_norm": 0.001193953095935285,
      "learning_rate": 0.00016567893194399216,
      "loss": 0.5274,
      "step": 126500
    },
    {
      "epoch": 0.4480908357283423,
      "grad_norm": 15.541926383972168,
      "learning_rate": 0.0001655727492814973,
      "loss": 0.3496,
      "step": 126600
    },
    {
      "epoch": 0.4484447779366585,
      "grad_norm": 68.42987823486328,
      "learning_rate": 0.00016546656661900244,
      "loss": 0.4547,
      "step": 126700
    },
    {
      "epoch": 0.44879872014497474,
      "grad_norm": 0.026276715099811554,
      "learning_rate": 0.00016536038395650757,
      "loss": 0.4049,
      "step": 126800
    },
    {
      "epoch": 0.44915266235329093,
      "grad_norm": 0.017596552148461342,
      "learning_rate": 0.0001652542012940127,
      "loss": 0.433,
      "step": 126900
    },
    {
      "epoch": 0.4495066045616072,
      "grad_norm": 0.03788014128804207,
      "learning_rate": 0.00016514801863151782,
      "loss": 0.3925,
      "step": 127000
    },
    {
      "epoch": 0.44986054676992343,
      "grad_norm": 123.572021484375,
      "learning_rate": 0.00016504183596902298,
      "loss": 0.3922,
      "step": 127100
    },
    {
      "epoch": 0.4502144889782396,
      "grad_norm": 0.40698254108428955,
      "learning_rate": 0.00016493565330652808,
      "loss": 0.3158,
      "step": 127200
    },
    {
      "epoch": 0.4505684311865559,
      "grad_norm": 0.017475759610533714,
      "learning_rate": 0.00016482947064403323,
      "loss": 0.4224,
      "step": 127300
    },
    {
      "epoch": 0.45092237339487207,
      "grad_norm": 0.0027214970905333757,
      "learning_rate": 0.00016472328798153836,
      "loss": 0.4117,
      "step": 127400
    },
    {
      "epoch": 0.4512763156031883,
      "grad_norm": 0.0013627924490720034,
      "learning_rate": 0.0001646171053190435,
      "loss": 0.3679,
      "step": 127500
    },
    {
      "epoch": 0.45163025781150457,
      "grad_norm": 56.80171203613281,
      "learning_rate": 0.0001645109226565486,
      "loss": 0.3333,
      "step": 127600
    },
    {
      "epoch": 0.45198420001982076,
      "grad_norm": 0.00722738029435277,
      "learning_rate": 0.00016440473999405377,
      "loss": 0.227,
      "step": 127700
    },
    {
      "epoch": 0.452338142228137,
      "grad_norm": 0.0030379141680896282,
      "learning_rate": 0.0001642985573315589,
      "loss": 0.4443,
      "step": 127800
    },
    {
      "epoch": 0.4526920844364532,
      "grad_norm": 0.06873321533203125,
      "learning_rate": 0.00016419237466906402,
      "loss": 0.1504,
      "step": 127900
    },
    {
      "epoch": 0.45304602664476945,
      "grad_norm": 4.920862197875977,
      "learning_rate": 0.00016408619200656915,
      "loss": 0.3886,
      "step": 128000
    },
    {
      "epoch": 0.45339996885308564,
      "grad_norm": 17.969175338745117,
      "learning_rate": 0.0001639800093440743,
      "loss": 0.233,
      "step": 128100
    },
    {
      "epoch": 0.4537539110614019,
      "grad_norm": 0.0021118212025612593,
      "learning_rate": 0.0001638738266815794,
      "loss": 0.3052,
      "step": 128200
    },
    {
      "epoch": 0.45410785326971814,
      "grad_norm": 0.006179771386086941,
      "learning_rate": 0.00016376764401908455,
      "loss": 0.399,
      "step": 128300
    },
    {
      "epoch": 0.45446179547803434,
      "grad_norm": 31.063264846801758,
      "learning_rate": 0.00016366146135658968,
      "loss": 0.3183,
      "step": 128400
    },
    {
      "epoch": 0.4548157376863506,
      "grad_norm": 0.00024966002092696726,
      "learning_rate": 0.00016355527869409484,
      "loss": 0.323,
      "step": 128500
    },
    {
      "epoch": 0.4551696798946668,
      "grad_norm": 9.028554916381836,
      "learning_rate": 0.00016344909603159994,
      "loss": 0.403,
      "step": 128600
    },
    {
      "epoch": 0.45552362210298303,
      "grad_norm": 0.0533665306866169,
      "learning_rate": 0.0001633429133691051,
      "loss": 0.4195,
      "step": 128700
    },
    {
      "epoch": 0.4558775643112993,
      "grad_norm": 0.004691832698881626,
      "learning_rate": 0.00016323673070661022,
      "loss": 0.2313,
      "step": 128800
    },
    {
      "epoch": 0.45623150651961547,
      "grad_norm": 1.3656736612319946,
      "learning_rate": 0.00016313054804411532,
      "loss": 0.5721,
      "step": 128900
    },
    {
      "epoch": 0.4565854487279317,
      "grad_norm": 0.03062422387301922,
      "learning_rate": 0.00016302436538162047,
      "loss": 0.4126,
      "step": 129000
    },
    {
      "epoch": 0.4569393909362479,
      "grad_norm": 0.018571259453892708,
      "learning_rate": 0.00016291818271912562,
      "loss": 0.4481,
      "step": 129100
    },
    {
      "epoch": 0.45729333314456416,
      "grad_norm": 0.0031067458912730217,
      "learning_rate": 0.00016281200005663075,
      "loss": 0.3974,
      "step": 129200
    },
    {
      "epoch": 0.45764727535288036,
      "grad_norm": 49.607383728027344,
      "learning_rate": 0.00016270581739413585,
      "loss": 0.2755,
      "step": 129300
    },
    {
      "epoch": 0.4580012175611966,
      "grad_norm": 0.7891022562980652,
      "learning_rate": 0.000162599634731641,
      "loss": 0.2463,
      "step": 129400
    },
    {
      "epoch": 0.45835515976951285,
      "grad_norm": 0.6186698079109192,
      "learning_rate": 0.00016249345206914613,
      "loss": 0.2505,
      "step": 129500
    },
    {
      "epoch": 0.45870910197782905,
      "grad_norm": 0.007351053413003683,
      "learning_rate": 0.00016238726940665126,
      "loss": 0.6585,
      "step": 129600
    },
    {
      "epoch": 0.4590630441861453,
      "grad_norm": 0.001393411890603602,
      "learning_rate": 0.0001622810867441564,
      "loss": 0.2802,
      "step": 129700
    },
    {
      "epoch": 0.4594169863944615,
      "grad_norm": 0.0005241560284048319,
      "learning_rate": 0.00016217490408166154,
      "loss": 0.2963,
      "step": 129800
    },
    {
      "epoch": 0.45977092860277774,
      "grad_norm": 0.0137933986261487,
      "learning_rate": 0.00016206872141916667,
      "loss": 0.3426,
      "step": 129900
    },
    {
      "epoch": 0.460124870811094,
      "grad_norm": 0.00036474698572419584,
      "learning_rate": 0.0001619625387566718,
      "loss": 0.3661,
      "step": 130000
    },
    {
      "epoch": 0.4604788130194102,
      "grad_norm": 2.963860273361206,
      "learning_rate": 0.00016185635609417692,
      "loss": 0.2651,
      "step": 130100
    },
    {
      "epoch": 0.46083275522772643,
      "grad_norm": 0.00040178000926971436,
      "learning_rate": 0.00016175017343168208,
      "loss": 0.4093,
      "step": 130200
    },
    {
      "epoch": 0.4611866974360426,
      "grad_norm": 0.018600452691316605,
      "learning_rate": 0.00016164399076918718,
      "loss": 0.3595,
      "step": 130300
    },
    {
      "epoch": 0.4615406396443589,
      "grad_norm": 0.0002266382216475904,
      "learning_rate": 0.00016153780810669233,
      "loss": 0.2268,
      "step": 130400
    },
    {
      "epoch": 0.46189458185267507,
      "grad_norm": 64.01014709472656,
      "learning_rate": 0.00016143162544419746,
      "loss": 0.466,
      "step": 130500
    },
    {
      "epoch": 0.4622485240609913,
      "grad_norm": 0.0031827620696276426,
      "learning_rate": 0.00016132544278170258,
      "loss": 0.3747,
      "step": 130600
    },
    {
      "epoch": 0.46260246626930757,
      "grad_norm": 0.05036916956305504,
      "learning_rate": 0.0001612192601192077,
      "loss": 0.3356,
      "step": 130700
    },
    {
      "epoch": 0.46295640847762376,
      "grad_norm": 0.6726358532905579,
      "learning_rate": 0.00016111307745671286,
      "loss": 0.1989,
      "step": 130800
    },
    {
      "epoch": 0.46331035068594,
      "grad_norm": 0.10111358761787415,
      "learning_rate": 0.000161006894794218,
      "loss": 0.3182,
      "step": 130900
    },
    {
      "epoch": 0.4636642928942562,
      "grad_norm": 0.00010054433369077742,
      "learning_rate": 0.00016090071213172312,
      "loss": 0.3007,
      "step": 131000
    },
    {
      "epoch": 0.46401823510257245,
      "grad_norm": 0.0018896005349233747,
      "learning_rate": 0.00016079452946922825,
      "loss": 0.1947,
      "step": 131100
    },
    {
      "epoch": 0.4643721773108887,
      "grad_norm": 0.00036187973455525935,
      "learning_rate": 0.0001606883468067334,
      "loss": 0.3141,
      "step": 131200
    },
    {
      "epoch": 0.4647261195192049,
      "grad_norm": 0.0010561251547187567,
      "learning_rate": 0.0001605821641442385,
      "loss": 0.2556,
      "step": 131300
    },
    {
      "epoch": 0.46508006172752114,
      "grad_norm": 0.004103172570466995,
      "learning_rate": 0.00016047598148174365,
      "loss": 0.2669,
      "step": 131400
    },
    {
      "epoch": 0.46543400393583734,
      "grad_norm": 0.028295865282416344,
      "learning_rate": 0.00016036979881924878,
      "loss": 0.2569,
      "step": 131500
    },
    {
      "epoch": 0.4657879461441536,
      "grad_norm": 0.9899498820304871,
      "learning_rate": 0.00016026361615675394,
      "loss": 0.3788,
      "step": 131600
    },
    {
      "epoch": 0.46614188835246984,
      "grad_norm": 14.192044258117676,
      "learning_rate": 0.00016015743349425904,
      "loss": 0.4291,
      "step": 131700
    },
    {
      "epoch": 0.46649583056078603,
      "grad_norm": 0.0004037294420413673,
      "learning_rate": 0.0001600512508317642,
      "loss": 0.3605,
      "step": 131800
    },
    {
      "epoch": 0.4668497727691023,
      "grad_norm": 31.557498931884766,
      "learning_rate": 0.00015994506816926932,
      "loss": 0.1813,
      "step": 131900
    },
    {
      "epoch": 0.46720371497741847,
      "grad_norm": 0.00023654414690099657,
      "learning_rate": 0.00015983888550677442,
      "loss": 0.3014,
      "step": 132000
    },
    {
      "epoch": 0.4675576571857347,
      "grad_norm": 1.208106279373169,
      "learning_rate": 0.00015973270284427957,
      "loss": 0.2516,
      "step": 132100
    },
    {
      "epoch": 0.4679115993940509,
      "grad_norm": 9.301146507263184,
      "learning_rate": 0.00015962652018178472,
      "loss": 0.2103,
      "step": 132200
    },
    {
      "epoch": 0.46826554160236716,
      "grad_norm": 0.0132752051576972,
      "learning_rate": 0.00015952033751928982,
      "loss": 0.3317,
      "step": 132300
    },
    {
      "epoch": 0.4686194838106834,
      "grad_norm": 0.007855209521949291,
      "learning_rate": 0.00015941415485679495,
      "loss": 0.2976,
      "step": 132400
    },
    {
      "epoch": 0.4689734260189996,
      "grad_norm": 0.009656408801674843,
      "learning_rate": 0.0001593079721943001,
      "loss": 0.3537,
      "step": 132500
    },
    {
      "epoch": 0.46932736822731586,
      "grad_norm": 0.008998815901577473,
      "learning_rate": 0.00015920178953180526,
      "loss": 0.5821,
      "step": 132600
    },
    {
      "epoch": 0.46968131043563205,
      "grad_norm": 0.04814349487423897,
      "learning_rate": 0.00015909560686931036,
      "loss": 0.2731,
      "step": 132700
    },
    {
      "epoch": 0.4700352526439483,
      "grad_norm": 0.007742264308035374,
      "learning_rate": 0.00015898942420681549,
      "loss": 0.3871,
      "step": 132800
    },
    {
      "epoch": 0.47038919485226455,
      "grad_norm": 0.005995805840939283,
      "learning_rate": 0.00015888324154432064,
      "loss": 0.4641,
      "step": 132900
    },
    {
      "epoch": 0.47074313706058074,
      "grad_norm": 1.2938828468322754,
      "learning_rate": 0.00015877705888182574,
      "loss": 0.3602,
      "step": 133000
    },
    {
      "epoch": 0.471097079268897,
      "grad_norm": 0.08212719112634659,
      "learning_rate": 0.0001586708762193309,
      "loss": 0.3371,
      "step": 133100
    },
    {
      "epoch": 0.4714510214772132,
      "grad_norm": 0.03037545271217823,
      "learning_rate": 0.00015856469355683602,
      "loss": 0.1405,
      "step": 133200
    },
    {
      "epoch": 0.47180496368552943,
      "grad_norm": 0.004782915581017733,
      "learning_rate": 0.00015845851089434118,
      "loss": 0.2787,
      "step": 133300
    },
    {
      "epoch": 0.4721589058938456,
      "grad_norm": 61.563446044921875,
      "learning_rate": 0.00015835232823184628,
      "loss": 0.2629,
      "step": 133400
    },
    {
      "epoch": 0.4725128481021619,
      "grad_norm": 0.05406413599848747,
      "learning_rate": 0.00015824614556935143,
      "loss": 0.3425,
      "step": 133500
    },
    {
      "epoch": 0.4728667903104781,
      "grad_norm": 0.009026836603879929,
      "learning_rate": 0.00015813996290685656,
      "loss": 0.3775,
      "step": 133600
    },
    {
      "epoch": 0.4732207325187943,
      "grad_norm": 0.0024082039017230272,
      "learning_rate": 0.00015803378024436168,
      "loss": 0.3956,
      "step": 133700
    },
    {
      "epoch": 0.47357467472711057,
      "grad_norm": 0.006472037173807621,
      "learning_rate": 0.0001579275975818668,
      "loss": 0.3806,
      "step": 133800
    },
    {
      "epoch": 0.47392861693542676,
      "grad_norm": 46.85919189453125,
      "learning_rate": 0.00015782141491937196,
      "loss": 0.4083,
      "step": 133900
    },
    {
      "epoch": 0.474282559143743,
      "grad_norm": 0.004700032528489828,
      "learning_rate": 0.0001577152322568771,
      "loss": 0.3986,
      "step": 134000
    },
    {
      "epoch": 0.47463650135205926,
      "grad_norm": 0.0503331795334816,
      "learning_rate": 0.00015760904959438222,
      "loss": 0.3118,
      "step": 134100
    },
    {
      "epoch": 0.47499044356037545,
      "grad_norm": 19.59756851196289,
      "learning_rate": 0.00015750286693188735,
      "loss": 0.3238,
      "step": 134200
    },
    {
      "epoch": 0.4753443857686917,
      "grad_norm": 70.75485229492188,
      "learning_rate": 0.0001573966842693925,
      "loss": 0.3632,
      "step": 134300
    },
    {
      "epoch": 0.4756983279770079,
      "grad_norm": 0.0003622135263867676,
      "learning_rate": 0.0001572905016068976,
      "loss": 0.2908,
      "step": 134400
    },
    {
      "epoch": 0.47605227018532414,
      "grad_norm": 2.2293758392333984,
      "learning_rate": 0.00015718431894440275,
      "loss": 0.2972,
      "step": 134500
    },
    {
      "epoch": 0.4764062123936404,
      "grad_norm": 0.0011708278907462955,
      "learning_rate": 0.00015707813628190788,
      "loss": 0.2909,
      "step": 134600
    },
    {
      "epoch": 0.4767601546019566,
      "grad_norm": 109.32562255859375,
      "learning_rate": 0.00015697195361941298,
      "loss": 0.4192,
      "step": 134700
    },
    {
      "epoch": 0.47711409681027284,
      "grad_norm": 0.0013484290102496743,
      "learning_rate": 0.00015686577095691813,
      "loss": 0.3351,
      "step": 134800
    },
    {
      "epoch": 0.47746803901858903,
      "grad_norm": 0.014992326498031616,
      "learning_rate": 0.0001567595882944233,
      "loss": 0.3699,
      "step": 134900
    },
    {
      "epoch": 0.4778219812269053,
      "grad_norm": 0.00017809105338528752,
      "learning_rate": 0.00015665340563192842,
      "loss": 0.2956,
      "step": 135000
    },
    {
      "epoch": 0.4781759234352215,
      "grad_norm": 0.05981632322072983,
      "learning_rate": 0.00015654722296943352,
      "loss": 0.3164,
      "step": 135100
    },
    {
      "epoch": 0.4785298656435377,
      "grad_norm": 0.5510818958282471,
      "learning_rate": 0.00015644104030693867,
      "loss": 0.2142,
      "step": 135200
    },
    {
      "epoch": 0.47888380785185397,
      "grad_norm": 26.4432315826416,
      "learning_rate": 0.00015633485764444382,
      "loss": 0.2962,
      "step": 135300
    },
    {
      "epoch": 0.47923775006017016,
      "grad_norm": 0.509357750415802,
      "learning_rate": 0.00015622867498194892,
      "loss": 0.4772,
      "step": 135400
    },
    {
      "epoch": 0.4795916922684864,
      "grad_norm": 0.45216700434684753,
      "learning_rate": 0.00015612249231945405,
      "loss": 0.4407,
      "step": 135500
    },
    {
      "epoch": 0.4799456344768026,
      "grad_norm": 0.0017040884122252464,
      "learning_rate": 0.0001560163096569592,
      "loss": 0.39,
      "step": 135600
    },
    {
      "epoch": 0.48029957668511886,
      "grad_norm": 0.04533383995294571,
      "learning_rate": 0.00015591012699446436,
      "loss": 0.4326,
      "step": 135700
    },
    {
      "epoch": 0.4806535188934351,
      "grad_norm": 0.0001581776887178421,
      "learning_rate": 0.00015580394433196946,
      "loss": 0.4586,
      "step": 135800
    },
    {
      "epoch": 0.4810074611017513,
      "grad_norm": 0.0033358328510075808,
      "learning_rate": 0.00015569776166947459,
      "loss": 0.2445,
      "step": 135900
    },
    {
      "epoch": 0.48136140331006755,
      "grad_norm": 32.800880432128906,
      "learning_rate": 0.00015559157900697974,
      "loss": 0.5169,
      "step": 136000
    },
    {
      "epoch": 0.48171534551838374,
      "grad_norm": 3.487830638885498,
      "learning_rate": 0.00015548539634448484,
      "loss": 0.3724,
      "step": 136100
    },
    {
      "epoch": 0.4820692877267,
      "grad_norm": 0.11333850026130676,
      "learning_rate": 0.00015537921368199,
      "loss": 0.6215,
      "step": 136200
    },
    {
      "epoch": 0.4824232299350162,
      "grad_norm": 20.715547561645508,
      "learning_rate": 0.00015527303101949512,
      "loss": 0.3544,
      "step": 136300
    },
    {
      "epoch": 0.48277717214333243,
      "grad_norm": 20.01791000366211,
      "learning_rate": 0.00015516684835700025,
      "loss": 0.2873,
      "step": 136400
    },
    {
      "epoch": 0.4831311143516487,
      "grad_norm": 18.633132934570312,
      "learning_rate": 0.00015506066569450537,
      "loss": 0.4034,
      "step": 136500
    },
    {
      "epoch": 0.4834850565599649,
      "grad_norm": 0.03706157207489014,
      "learning_rate": 0.00015495448303201053,
      "loss": 0.3516,
      "step": 136600
    },
    {
      "epoch": 0.4838389987682811,
      "grad_norm": 0.007039338815957308,
      "learning_rate": 0.00015484830036951566,
      "loss": 0.237,
      "step": 136700
    },
    {
      "epoch": 0.4841929409765973,
      "grad_norm": 0.0006637162296101451,
      "learning_rate": 0.00015474211770702078,
      "loss": 0.2436,
      "step": 136800
    },
    {
      "epoch": 0.48454688318491357,
      "grad_norm": 0.00016755297838244587,
      "learning_rate": 0.0001546359350445259,
      "loss": 0.3582,
      "step": 136900
    },
    {
      "epoch": 0.4849008253932298,
      "grad_norm": 1.3888490200042725,
      "learning_rate": 0.00015452975238203106,
      "loss": 0.3485,
      "step": 137000
    },
    {
      "epoch": 0.485254767601546,
      "grad_norm": 0.00029843690572306514,
      "learning_rate": 0.00015442356971953616,
      "loss": 0.0721,
      "step": 137100
    },
    {
      "epoch": 0.48560870980986226,
      "grad_norm": 0.02156670019030571,
      "learning_rate": 0.00015431738705704132,
      "loss": 0.2639,
      "step": 137200
    },
    {
      "epoch": 0.48596265201817845,
      "grad_norm": 0.0010482484940439463,
      "learning_rate": 0.00015421120439454645,
      "loss": 0.4722,
      "step": 137300
    },
    {
      "epoch": 0.4863165942264947,
      "grad_norm": 0.026267366483807564,
      "learning_rate": 0.0001541050217320516,
      "loss": 0.3698,
      "step": 137400
    },
    {
      "epoch": 0.4866705364348109,
      "grad_norm": 0.008635909296572208,
      "learning_rate": 0.0001539988390695567,
      "loss": 0.1992,
      "step": 137500
    },
    {
      "epoch": 0.48702447864312715,
      "grad_norm": 0.0019734555389732122,
      "learning_rate": 0.00015389265640706185,
      "loss": 0.3722,
      "step": 137600
    },
    {
      "epoch": 0.4873784208514434,
      "grad_norm": 0.00011410761362640187,
      "learning_rate": 0.00015378647374456698,
      "loss": 0.295,
      "step": 137700
    },
    {
      "epoch": 0.4877323630597596,
      "grad_norm": 7.872322021285072e-05,
      "learning_rate": 0.00015368029108207208,
      "loss": 0.3067,
      "step": 137800
    },
    {
      "epoch": 0.48808630526807584,
      "grad_norm": 21.91559410095215,
      "learning_rate": 0.00015357410841957723,
      "loss": 0.4086,
      "step": 137900
    },
    {
      "epoch": 0.48844024747639203,
      "grad_norm": 42.904361724853516,
      "learning_rate": 0.0001534679257570824,
      "loss": 0.2403,
      "step": 138000
    },
    {
      "epoch": 0.4887941896847083,
      "grad_norm": 13.468207359313965,
      "learning_rate": 0.0001533617430945875,
      "loss": 0.2997,
      "step": 138100
    },
    {
      "epoch": 0.48914813189302453,
      "grad_norm": 0.051535945385694504,
      "learning_rate": 0.00015325556043209262,
      "loss": 0.334,
      "step": 138200
    },
    {
      "epoch": 0.4895020741013407,
      "grad_norm": 0.0003281892859376967,
      "learning_rate": 0.00015314937776959777,
      "loss": 0.1907,
      "step": 138300
    },
    {
      "epoch": 0.48985601630965697,
      "grad_norm": 17.503192901611328,
      "learning_rate": 0.00015304319510710292,
      "loss": 0.3248,
      "step": 138400
    },
    {
      "epoch": 0.49020995851797317,
      "grad_norm": 0.006039150524884462,
      "learning_rate": 0.00015293701244460802,
      "loss": 0.3055,
      "step": 138500
    },
    {
      "epoch": 0.4905639007262894,
      "grad_norm": 0.026435673236846924,
      "learning_rate": 0.00015283082978211315,
      "loss": 0.1909,
      "step": 138600
    },
    {
      "epoch": 0.49091784293460566,
      "grad_norm": 0.0006384566659107804,
      "learning_rate": 0.0001527246471196183,
      "loss": 0.1845,
      "step": 138700
    },
    {
      "epoch": 0.49127178514292186,
      "grad_norm": 0.004883905407041311,
      "learning_rate": 0.0001526184644571234,
      "loss": 0.6001,
      "step": 138800
    },
    {
      "epoch": 0.4916257273512381,
      "grad_norm": 1.3326191902160645,
      "learning_rate": 0.00015251228179462856,
      "loss": 0.3061,
      "step": 138900
    },
    {
      "epoch": 0.4919796695595543,
      "grad_norm": 0.007343944627791643,
      "learning_rate": 0.00015240609913213369,
      "loss": 0.3997,
      "step": 139000
    },
    {
      "epoch": 0.49233361176787055,
      "grad_norm": 2.167741537094116,
      "learning_rate": 0.00015229991646963884,
      "loss": 0.374,
      "step": 139100
    },
    {
      "epoch": 0.49268755397618674,
      "grad_norm": 0.022274810820817947,
      "learning_rate": 0.00015219373380714394,
      "loss": 0.4006,
      "step": 139200
    },
    {
      "epoch": 0.493041496184503,
      "grad_norm": 14.71191120147705,
      "learning_rate": 0.0001520875511446491,
      "loss": 0.4838,
      "step": 139300
    },
    {
      "epoch": 0.49339543839281924,
      "grad_norm": 21.760229110717773,
      "learning_rate": 0.00015198136848215422,
      "loss": 0.3346,
      "step": 139400
    },
    {
      "epoch": 0.49374938060113543,
      "grad_norm": 0.05318216606974602,
      "learning_rate": 0.00015187518581965935,
      "loss": 0.2684,
      "step": 139500
    },
    {
      "epoch": 0.4941033228094517,
      "grad_norm": 0.0006659928476437926,
      "learning_rate": 0.00015176900315716447,
      "loss": 0.3252,
      "step": 139600
    },
    {
      "epoch": 0.4944572650177679,
      "grad_norm": 4.927140235900879,
      "learning_rate": 0.00015166282049466963,
      "loss": 0.3807,
      "step": 139700
    },
    {
      "epoch": 0.4948112072260841,
      "grad_norm": 0.00014005550474394113,
      "learning_rate": 0.00015155663783217476,
      "loss": 0.3432,
      "step": 139800
    },
    {
      "epoch": 0.4951651494344004,
      "grad_norm": 7.562499523162842,
      "learning_rate": 0.00015145045516967988,
      "loss": 0.4965,
      "step": 139900
    },
    {
      "epoch": 0.49551909164271657,
      "grad_norm": 0.008013779297471046,
      "learning_rate": 0.000151344272507185,
      "loss": 0.3214,
      "step": 140000
    },
    {
      "epoch": 0.4958730338510328,
      "grad_norm": 5.580400466918945,
      "learning_rate": 0.00015123808984469016,
      "loss": 0.402,
      "step": 140100
    },
    {
      "epoch": 0.496226976059349,
      "grad_norm": 36.05997848510742,
      "learning_rate": 0.00015113190718219526,
      "loss": 0.3271,
      "step": 140200
    },
    {
      "epoch": 0.49658091826766526,
      "grad_norm": 18.25318717956543,
      "learning_rate": 0.00015102572451970042,
      "loss": 0.2518,
      "step": 140300
    },
    {
      "epoch": 0.49693486047598145,
      "grad_norm": 50.728851318359375,
      "learning_rate": 0.00015091954185720554,
      "loss": 0.41,
      "step": 140400
    },
    {
      "epoch": 0.4972888026842977,
      "grad_norm": 0.3367927074432373,
      "learning_rate": 0.00015081335919471067,
      "loss": 0.3976,
      "step": 140500
    },
    {
      "epoch": 0.49764274489261395,
      "grad_norm": 0.003242569975554943,
      "learning_rate": 0.0001507071765322158,
      "loss": 0.3251,
      "step": 140600
    },
    {
      "epoch": 0.49799668710093015,
      "grad_norm": 0.0025579212233424187,
      "learning_rate": 0.00015060099386972095,
      "loss": 0.3961,
      "step": 140700
    },
    {
      "epoch": 0.4983506293092464,
      "grad_norm": 8.877049549482763e-05,
      "learning_rate": 0.00015049481120722608,
      "loss": 0.3351,
      "step": 140800
    },
    {
      "epoch": 0.4987045715175626,
      "grad_norm": 17.82809066772461,
      "learning_rate": 0.0001503886285447312,
      "loss": 0.3431,
      "step": 140900
    },
    {
      "epoch": 0.49905851372587884,
      "grad_norm": 0.025657838210463524,
      "learning_rate": 0.00015028244588223633,
      "loss": 0.3459,
      "step": 141000
    },
    {
      "epoch": 0.4994124559341951,
      "grad_norm": 0.0004106948908884078,
      "learning_rate": 0.0001501762632197415,
      "loss": 0.2256,
      "step": 141100
    },
    {
      "epoch": 0.4997663981425113,
      "grad_norm": 12.18494987487793,
      "learning_rate": 0.0001500700805572466,
      "loss": 0.2931,
      "step": 141200
    },
    {
      "epoch": 0.5001203403508275,
      "grad_norm": 0.002776885172352195,
      "learning_rate": 0.00014996389789475171,
      "loss": 0.3665,
      "step": 141300
    },
    {
      "epoch": 0.5004742825591437,
      "grad_norm": 0.000984273967333138,
      "learning_rate": 0.00014985771523225687,
      "loss": 0.4815,
      "step": 141400
    },
    {
      "epoch": 0.5008282247674599,
      "grad_norm": 0.04492281377315521,
      "learning_rate": 0.000149751532569762,
      "loss": 0.23,
      "step": 141500
    },
    {
      "epoch": 0.5011821669757762,
      "grad_norm": 0.0001147434304584749,
      "learning_rate": 0.00014964534990726712,
      "loss": 0.427,
      "step": 141600
    },
    {
      "epoch": 0.5015361091840924,
      "grad_norm": 0.0050025684759020805,
      "learning_rate": 0.00014953916724477225,
      "loss": 0.2491,
      "step": 141700
    },
    {
      "epoch": 0.5018900513924086,
      "grad_norm": 17.19159507751465,
      "learning_rate": 0.00014943298458227738,
      "loss": 0.3399,
      "step": 141800
    },
    {
      "epoch": 0.5022439936007249,
      "grad_norm": 0.11870469152927399,
      "learning_rate": 0.00014932680191978253,
      "loss": 0.3993,
      "step": 141900
    },
    {
      "epoch": 0.5025979358090411,
      "grad_norm": 0.025409840047359467,
      "learning_rate": 0.00014922061925728766,
      "loss": 0.3396,
      "step": 142000
    },
    {
      "epoch": 0.5029518780173573,
      "grad_norm": 0.0025565761607140303,
      "learning_rate": 0.00014911443659479279,
      "loss": 0.327,
      "step": 142100
    },
    {
      "epoch": 0.5033058202256736,
      "grad_norm": 0.014999634586274624,
      "learning_rate": 0.0001490082539322979,
      "loss": 0.3835,
      "step": 142200
    },
    {
      "epoch": 0.5036597624339898,
      "grad_norm": 81.85242462158203,
      "learning_rate": 0.00014890207126980304,
      "loss": 0.3315,
      "step": 142300
    },
    {
      "epoch": 0.504013704642306,
      "grad_norm": 0.00018296056077815592,
      "learning_rate": 0.0001487958886073082,
      "loss": 0.4536,
      "step": 142400
    },
    {
      "epoch": 0.5043676468506222,
      "grad_norm": 0.07117115706205368,
      "learning_rate": 0.00014868970594481332,
      "loss": 0.2957,
      "step": 142500
    },
    {
      "epoch": 0.5047215890589385,
      "grad_norm": 24.026283264160156,
      "learning_rate": 0.00014858352328231845,
      "loss": 0.3971,
      "step": 142600
    },
    {
      "epoch": 0.5050755312672547,
      "grad_norm": 0.012988509610295296,
      "learning_rate": 0.00014847734061982357,
      "loss": 0.3204,
      "step": 142700
    },
    {
      "epoch": 0.5054294734755709,
      "grad_norm": 0.001135221216827631,
      "learning_rate": 0.0001483711579573287,
      "loss": 0.3755,
      "step": 142800
    },
    {
      "epoch": 0.5057834156838872,
      "grad_norm": 0.0038426427636295557,
      "learning_rate": 0.00014826497529483386,
      "loss": 0.2387,
      "step": 142900
    },
    {
      "epoch": 0.5061373578922034,
      "grad_norm": 0.004199059214442968,
      "learning_rate": 0.00014815879263233898,
      "loss": 0.2597,
      "step": 143000
    },
    {
      "epoch": 0.5064913001005196,
      "grad_norm": 0.2375722974538803,
      "learning_rate": 0.0001480526099698441,
      "loss": 0.2891,
      "step": 143100
    },
    {
      "epoch": 0.5068452423088358,
      "grad_norm": 0.6084772944450378,
      "learning_rate": 0.00014794642730734924,
      "loss": 0.3595,
      "step": 143200
    },
    {
      "epoch": 0.5071991845171521,
      "grad_norm": 36.329593658447266,
      "learning_rate": 0.0001478402446448544,
      "loss": 0.273,
      "step": 143300
    },
    {
      "epoch": 0.5075531267254683,
      "grad_norm": 0.0002451998007018119,
      "learning_rate": 0.00014773406198235952,
      "loss": 0.3949,
      "step": 143400
    },
    {
      "epoch": 0.5079070689337845,
      "grad_norm": 0.0001825980725698173,
      "learning_rate": 0.00014762787931986464,
      "loss": 0.4056,
      "step": 143500
    },
    {
      "epoch": 0.5082610111421008,
      "grad_norm": 7.361876964569092,
      "learning_rate": 0.00014752169665736977,
      "loss": 0.2625,
      "step": 143600
    },
    {
      "epoch": 0.508614953350417,
      "grad_norm": 9.331559704151005e-05,
      "learning_rate": 0.0001474155139948749,
      "loss": 0.5471,
      "step": 143700
    },
    {
      "epoch": 0.5089688955587331,
      "grad_norm": 0.0003190656425431371,
      "learning_rate": 0.00014730933133238005,
      "loss": 0.1283,
      "step": 143800
    },
    {
      "epoch": 0.5093228377670493,
      "grad_norm": 0.011564650572836399,
      "learning_rate": 0.00014720314866988518,
      "loss": 0.3525,
      "step": 143900
    },
    {
      "epoch": 0.5096767799753656,
      "grad_norm": 25.518409729003906,
      "learning_rate": 0.0001470969660073903,
      "loss": 0.4955,
      "step": 144000
    },
    {
      "epoch": 0.5100307221836818,
      "grad_norm": 0.0026720319874584675,
      "learning_rate": 0.00014699078334489543,
      "loss": 0.2933,
      "step": 144100
    },
    {
      "epoch": 0.510384664391998,
      "grad_norm": 0.047819238156080246,
      "learning_rate": 0.00014688460068240056,
      "loss": 0.2944,
      "step": 144200
    },
    {
      "epoch": 0.5107386066003143,
      "grad_norm": 0.019408181309700012,
      "learning_rate": 0.00014677841801990571,
      "loss": 0.2775,
      "step": 144300
    },
    {
      "epoch": 0.5110925488086305,
      "grad_norm": 0.03435729071497917,
      "learning_rate": 0.00014667223535741081,
      "loss": 0.3529,
      "step": 144400
    },
    {
      "epoch": 0.5114464910169467,
      "grad_norm": 0.04034920036792755,
      "learning_rate": 0.00014656605269491594,
      "loss": 0.4004,
      "step": 144500
    },
    {
      "epoch": 0.511800433225263,
      "grad_norm": 0.0007400690810754895,
      "learning_rate": 0.0001464598700324211,
      "loss": 0.2996,
      "step": 144600
    },
    {
      "epoch": 0.5121543754335792,
      "grad_norm": 0.0010132259922102094,
      "learning_rate": 0.00014635368736992622,
      "loss": 0.3791,
      "step": 144700
    },
    {
      "epoch": 0.5125083176418954,
      "grad_norm": 4.2504379962338135e-05,
      "learning_rate": 0.00014624750470743135,
      "loss": 0.4685,
      "step": 144800
    },
    {
      "epoch": 0.5128622598502116,
      "grad_norm": 0.0013769271317869425,
      "learning_rate": 0.00014614132204493648,
      "loss": 0.3026,
      "step": 144900
    },
    {
      "epoch": 0.5132162020585279,
      "grad_norm": 0.3395538926124573,
      "learning_rate": 0.00014603513938244163,
      "loss": 0.559,
      "step": 145000
    },
    {
      "epoch": 0.5135701442668441,
      "grad_norm": 54.80721664428711,
      "learning_rate": 0.00014592895671994676,
      "loss": 0.3829,
      "step": 145100
    },
    {
      "epoch": 0.5139240864751603,
      "grad_norm": 0.3029990792274475,
      "learning_rate": 0.00014582277405745188,
      "loss": 0.2426,
      "step": 145200
    },
    {
      "epoch": 0.5142780286834766,
      "grad_norm": 0.0003814368392340839,
      "learning_rate": 0.000145716591394957,
      "loss": 0.2765,
      "step": 145300
    },
    {
      "epoch": 0.5146319708917928,
      "grad_norm": 0.020125191658735275,
      "learning_rate": 0.00014561040873246214,
      "loss": 0.4443,
      "step": 145400
    },
    {
      "epoch": 0.514985913100109,
      "grad_norm": 0.0010866179363802075,
      "learning_rate": 0.0001455042260699673,
      "loss": 0.3667,
      "step": 145500
    },
    {
      "epoch": 0.5153398553084252,
      "grad_norm": 0.004966847598552704,
      "learning_rate": 0.00014539804340747242,
      "loss": 0.2651,
      "step": 145600
    },
    {
      "epoch": 0.5156937975167415,
      "grad_norm": 0.00032093835761770606,
      "learning_rate": 0.00014529186074497755,
      "loss": 0.2239,
      "step": 145700
    },
    {
      "epoch": 0.5160477397250577,
      "grad_norm": 0.004529470112174749,
      "learning_rate": 0.00014518567808248267,
      "loss": 0.2669,
      "step": 145800
    },
    {
      "epoch": 0.5164016819333739,
      "grad_norm": 16.87831687927246,
      "learning_rate": 0.0001450794954199878,
      "loss": 0.2543,
      "step": 145900
    },
    {
      "epoch": 0.5167556241416902,
      "grad_norm": 0.32678717374801636,
      "learning_rate": 0.00014497331275749295,
      "loss": 0.3286,
      "step": 146000
    },
    {
      "epoch": 0.5171095663500064,
      "grad_norm": 0.0010116000194102526,
      "learning_rate": 0.00014486713009499808,
      "loss": 0.569,
      "step": 146100
    },
    {
      "epoch": 0.5174635085583226,
      "grad_norm": 0.052041586488485336,
      "learning_rate": 0.0001447609474325032,
      "loss": 0.36,
      "step": 146200
    },
    {
      "epoch": 0.5178174507666389,
      "grad_norm": 0.0003805088344961405,
      "learning_rate": 0.00014465476477000834,
      "loss": 0.1866,
      "step": 146300
    },
    {
      "epoch": 0.5181713929749551,
      "grad_norm": 0.0483073927462101,
      "learning_rate": 0.00014454858210751346,
      "loss": 0.2975,
      "step": 146400
    },
    {
      "epoch": 0.5185253351832713,
      "grad_norm": 0.012200524099171162,
      "learning_rate": 0.00014444239944501862,
      "loss": 0.3958,
      "step": 146500
    },
    {
      "epoch": 0.5188792773915875,
      "grad_norm": 0.008058540523052216,
      "learning_rate": 0.00014433621678252374,
      "loss": 0.4004,
      "step": 146600
    },
    {
      "epoch": 0.5192332195999038,
      "grad_norm": 6.188900470733643,
      "learning_rate": 0.00014423003412002887,
      "loss": 0.4601,
      "step": 146700
    },
    {
      "epoch": 0.51958716180822,
      "grad_norm": 0.0020912191830575466,
      "learning_rate": 0.000144123851457534,
      "loss": 0.5023,
      "step": 146800
    },
    {
      "epoch": 0.5199411040165361,
      "grad_norm": 0.0030626330990344286,
      "learning_rate": 0.00014401766879503912,
      "loss": 0.3244,
      "step": 146900
    },
    {
      "epoch": 0.5202950462248525,
      "grad_norm": 0.2994672656059265,
      "learning_rate": 0.00014391148613254428,
      "loss": 0.6761,
      "step": 147000
    },
    {
      "epoch": 0.5206489884331686,
      "grad_norm": 41.04188919067383,
      "learning_rate": 0.0001438053034700494,
      "loss": 0.3466,
      "step": 147100
    },
    {
      "epoch": 0.5210029306414848,
      "grad_norm": 1.4336408639792353e-05,
      "learning_rate": 0.00014369912080755453,
      "loss": 0.5173,
      "step": 147200
    },
    {
      "epoch": 0.521356872849801,
      "grad_norm": 0.0006960247410461307,
      "learning_rate": 0.00014359293814505966,
      "loss": 0.3368,
      "step": 147300
    },
    {
      "epoch": 0.5217108150581173,
      "grad_norm": 0.7433618307113647,
      "learning_rate": 0.00014348675548256481,
      "loss": 0.3019,
      "step": 147400
    },
    {
      "epoch": 0.5220647572664335,
      "grad_norm": 0.007528694812208414,
      "learning_rate": 0.00014338057282006994,
      "loss": 0.249,
      "step": 147500
    },
    {
      "epoch": 0.5224186994747497,
      "grad_norm": 0.032573822885751724,
      "learning_rate": 0.00014327439015757504,
      "loss": 0.2848,
      "step": 147600
    },
    {
      "epoch": 0.522772641683066,
      "grad_norm": 0.004713478963822126,
      "learning_rate": 0.0001431682074950802,
      "loss": 0.25,
      "step": 147700
    },
    {
      "epoch": 0.5231265838913822,
      "grad_norm": 48.17216873168945,
      "learning_rate": 0.00014306202483258532,
      "loss": 0.4493,
      "step": 147800
    },
    {
      "epoch": 0.5234805260996984,
      "grad_norm": 0.13271720707416534,
      "learning_rate": 0.00014295584217009045,
      "loss": 0.3365,
      "step": 147900
    },
    {
      "epoch": 0.5238344683080147,
      "grad_norm": 0.007231591735035181,
      "learning_rate": 0.00014284965950759558,
      "loss": 0.1844,
      "step": 148000
    },
    {
      "epoch": 0.5241884105163309,
      "grad_norm": 0.05539791285991669,
      "learning_rate": 0.0001427434768451007,
      "loss": 0.3498,
      "step": 148100
    },
    {
      "epoch": 0.5245423527246471,
      "grad_norm": 0.3066464960575104,
      "learning_rate": 0.00014263729418260586,
      "loss": 0.2548,
      "step": 148200
    },
    {
      "epoch": 0.5248962949329633,
      "grad_norm": 0.03617653250694275,
      "learning_rate": 0.00014253111152011098,
      "loss": 0.3236,
      "step": 148300
    },
    {
      "epoch": 0.5252502371412796,
      "grad_norm": 0.0057815080508589745,
      "learning_rate": 0.0001424249288576161,
      "loss": 0.2346,
      "step": 148400
    },
    {
      "epoch": 0.5256041793495958,
      "grad_norm": 0.9969614744186401,
      "learning_rate": 0.00014231874619512124,
      "loss": 0.4521,
      "step": 148500
    },
    {
      "epoch": 0.525958121557912,
      "grad_norm": 0.0007312497473321855,
      "learning_rate": 0.00014221256353262637,
      "loss": 0.365,
      "step": 148600
    },
    {
      "epoch": 0.5263120637662283,
      "grad_norm": 41.739097595214844,
      "learning_rate": 0.00014210638087013152,
      "loss": 0.3216,
      "step": 148700
    },
    {
      "epoch": 0.5266660059745445,
      "grad_norm": 0.0010452682618051767,
      "learning_rate": 0.00014200019820763665,
      "loss": 0.3288,
      "step": 148800
    },
    {
      "epoch": 0.5270199481828607,
      "grad_norm": 0.0008346611284650862,
      "learning_rate": 0.00014189401554514177,
      "loss": 0.1609,
      "step": 148900
    },
    {
      "epoch": 0.5273738903911769,
      "grad_norm": 0.00392491277307272,
      "learning_rate": 0.0001417878328826469,
      "loss": 0.3692,
      "step": 149000
    },
    {
      "epoch": 0.5277278325994932,
      "grad_norm": 1.4789916276931763,
      "learning_rate": 0.00014168165022015205,
      "loss": 0.2274,
      "step": 149100
    },
    {
      "epoch": 0.5280817748078094,
      "grad_norm": 0.0003778580576181412,
      "learning_rate": 0.00014157546755765718,
      "loss": 0.3028,
      "step": 149200
    },
    {
      "epoch": 0.5284357170161256,
      "grad_norm": 0.0011064596474170685,
      "learning_rate": 0.0001414692848951623,
      "loss": 0.2621,
      "step": 149300
    },
    {
      "epoch": 0.5287896592244419,
      "grad_norm": 47.831241607666016,
      "learning_rate": 0.00014136310223266744,
      "loss": 0.5255,
      "step": 149400
    },
    {
      "epoch": 0.5291436014327581,
      "grad_norm": 0.00018024191376753151,
      "learning_rate": 0.00014125691957017256,
      "loss": 0.2704,
      "step": 149500
    },
    {
      "epoch": 0.5294975436410743,
      "grad_norm": 0.0001044181699398905,
      "learning_rate": 0.00014115073690767772,
      "loss": 0.2598,
      "step": 149600
    },
    {
      "epoch": 0.5298514858493905,
      "grad_norm": 0.030864395201206207,
      "learning_rate": 0.00014104455424518284,
      "loss": 0.29,
      "step": 149700
    },
    {
      "epoch": 0.5302054280577068,
      "grad_norm": 0.12271931767463684,
      "learning_rate": 0.00014093837158268797,
      "loss": 0.3436,
      "step": 149800
    },
    {
      "epoch": 0.530559370266023,
      "grad_norm": 0.00024631890119053423,
      "learning_rate": 0.0001408321889201931,
      "loss": 0.28,
      "step": 149900
    },
    {
      "epoch": 0.5309133124743391,
      "grad_norm": 0.0001222057908307761,
      "learning_rate": 0.00014072600625769822,
      "loss": 0.4666,
      "step": 150000
    },
    {
      "epoch": 0.5312672546826555,
      "grad_norm": 60.096622467041016,
      "learning_rate": 0.00014061982359520338,
      "loss": 0.2624,
      "step": 150100
    },
    {
      "epoch": 0.5316211968909716,
      "grad_norm": 0.03221751004457474,
      "learning_rate": 0.0001405136409327085,
      "loss": 0.3499,
      "step": 150200
    },
    {
      "epoch": 0.5319751390992878,
      "grad_norm": 0.016536252573132515,
      "learning_rate": 0.00014040745827021363,
      "loss": 0.2033,
      "step": 150300
    },
    {
      "epoch": 0.5323290813076041,
      "grad_norm": 0.0003071252431254834,
      "learning_rate": 0.00014030127560771876,
      "loss": 0.2204,
      "step": 150400
    },
    {
      "epoch": 0.5326830235159203,
      "grad_norm": 0.14606302976608276,
      "learning_rate": 0.0001401950929452239,
      "loss": 0.2481,
      "step": 150500
    },
    {
      "epoch": 0.5330369657242365,
      "grad_norm": 7.655479566892609e-05,
      "learning_rate": 0.00014008891028272904,
      "loss": 0.3457,
      "step": 150600
    },
    {
      "epoch": 0.5333909079325527,
      "grad_norm": 6.883208698127419e-05,
      "learning_rate": 0.00013998272762023414,
      "loss": 0.3093,
      "step": 150700
    },
    {
      "epoch": 0.533744850140869,
      "grad_norm": 0.3525581359863281,
      "learning_rate": 0.0001398765449577393,
      "loss": 0.2194,
      "step": 150800
    },
    {
      "epoch": 0.5340987923491852,
      "grad_norm": 212.0054473876953,
      "learning_rate": 0.00013977036229524442,
      "loss": 0.3272,
      "step": 150900
    },
    {
      "epoch": 0.5344527345575014,
      "grad_norm": 0.0021757918875664473,
      "learning_rate": 0.00013966417963274955,
      "loss": 0.3888,
      "step": 151000
    },
    {
      "epoch": 0.5348066767658177,
      "grad_norm": 0.002484355354681611,
      "learning_rate": 0.00013955799697025468,
      "loss": 0.2496,
      "step": 151100
    },
    {
      "epoch": 0.5351606189741339,
      "grad_norm": 0.014755102805793285,
      "learning_rate": 0.0001394518143077598,
      "loss": 0.3697,
      "step": 151200
    },
    {
      "epoch": 0.5355145611824501,
      "grad_norm": 0.0027204379439353943,
      "learning_rate": 0.00013934563164526496,
      "loss": 0.2788,
      "step": 151300
    },
    {
      "epoch": 0.5358685033907663,
      "grad_norm": 29.425769805908203,
      "learning_rate": 0.00013923944898277008,
      "loss": 0.405,
      "step": 151400
    },
    {
      "epoch": 0.5362224455990826,
      "grad_norm": 84.65825653076172,
      "learning_rate": 0.0001391332663202752,
      "loss": 0.4318,
      "step": 151500
    },
    {
      "epoch": 0.5365763878073988,
      "grad_norm": 0.08385151624679565,
      "learning_rate": 0.00013902708365778034,
      "loss": 0.2468,
      "step": 151600
    },
    {
      "epoch": 0.536930330015715,
      "grad_norm": 0.0011976626701653004,
      "learning_rate": 0.00013892090099528546,
      "loss": 0.2391,
      "step": 151700
    },
    {
      "epoch": 0.5372842722240313,
      "grad_norm": 0.0007570473244413733,
      "learning_rate": 0.00013881471833279062,
      "loss": 0.5158,
      "step": 151800
    },
    {
      "epoch": 0.5376382144323475,
      "grad_norm": 0.0013363453326746821,
      "learning_rate": 0.00013870853567029575,
      "loss": 0.2303,
      "step": 151900
    },
    {
      "epoch": 0.5379921566406637,
      "grad_norm": 0.00023019025684334338,
      "learning_rate": 0.00013860235300780087,
      "loss": 0.3405,
      "step": 152000
    },
    {
      "epoch": 0.53834609884898,
      "grad_norm": 0.00016891579434741288,
      "learning_rate": 0.000138496170345306,
      "loss": 0.6027,
      "step": 152100
    },
    {
      "epoch": 0.5387000410572962,
      "grad_norm": 72.81297302246094,
      "learning_rate": 0.00013838998768281113,
      "loss": 0.3546,
      "step": 152200
    },
    {
      "epoch": 0.5390539832656124,
      "grad_norm": 1.9095107316970825,
      "learning_rate": 0.00013828380502031628,
      "loss": 0.3577,
      "step": 152300
    },
    {
      "epoch": 0.5394079254739286,
      "grad_norm": 0.00025053531862795353,
      "learning_rate": 0.0001381776223578214,
      "loss": 0.3892,
      "step": 152400
    },
    {
      "epoch": 0.5397618676822449,
      "grad_norm": 0.005705397110432386,
      "learning_rate": 0.00013807143969532654,
      "loss": 0.1598,
      "step": 152500
    },
    {
      "epoch": 0.5401158098905611,
      "grad_norm": 0.032343048602342606,
      "learning_rate": 0.00013796525703283166,
      "loss": 0.3906,
      "step": 152600
    },
    {
      "epoch": 0.5404697520988773,
      "grad_norm": 0.0006701996317133307,
      "learning_rate": 0.0001378590743703368,
      "loss": 0.2414,
      "step": 152700
    },
    {
      "epoch": 0.5408236943071936,
      "grad_norm": 0.010081013664603233,
      "learning_rate": 0.00013775289170784194,
      "loss": 0.367,
      "step": 152800
    },
    {
      "epoch": 0.5411776365155098,
      "grad_norm": 0.0032931233290582895,
      "learning_rate": 0.00013764670904534707,
      "loss": 0.162,
      "step": 152900
    },
    {
      "epoch": 0.541531578723826,
      "grad_norm": 0.002440332667902112,
      "learning_rate": 0.0001375405263828522,
      "loss": 0.5024,
      "step": 153000
    },
    {
      "epoch": 0.5418855209321422,
      "grad_norm": 0.09762558341026306,
      "learning_rate": 0.00013743434372035732,
      "loss": 0.2907,
      "step": 153100
    },
    {
      "epoch": 0.5422394631404585,
      "grad_norm": 68.66604614257812,
      "learning_rate": 0.00013732816105786248,
      "loss": 0.2944,
      "step": 153200
    },
    {
      "epoch": 0.5425934053487746,
      "grad_norm": 0.0005047146696597338,
      "learning_rate": 0.0001372219783953676,
      "loss": 0.3572,
      "step": 153300
    },
    {
      "epoch": 0.5429473475570908,
      "grad_norm": 0.002147745108231902,
      "learning_rate": 0.0001371157957328727,
      "loss": 0.3846,
      "step": 153400
    },
    {
      "epoch": 0.5433012897654071,
      "grad_norm": 0.00010017125168815255,
      "learning_rate": 0.00013700961307037786,
      "loss": 0.2122,
      "step": 153500
    },
    {
      "epoch": 0.5436552319737233,
      "grad_norm": 0.0014192581875249743,
      "learning_rate": 0.00013690343040788299,
      "loss": 0.1834,
      "step": 153600
    },
    {
      "epoch": 0.5440091741820395,
      "grad_norm": 0.09179441630840302,
      "learning_rate": 0.00013679724774538814,
      "loss": 0.2873,
      "step": 153700
    },
    {
      "epoch": 0.5443631163903557,
      "grad_norm": 0.08266142755746841,
      "learning_rate": 0.00013669106508289324,
      "loss": 0.3762,
      "step": 153800
    },
    {
      "epoch": 0.544717058598672,
      "grad_norm": 0.004993661306798458,
      "learning_rate": 0.00013658488242039837,
      "loss": 0.4164,
      "step": 153900
    },
    {
      "epoch": 0.5450710008069882,
      "grad_norm": 0.05969098210334778,
      "learning_rate": 0.00013647869975790352,
      "loss": 0.2391,
      "step": 154000
    },
    {
      "epoch": 0.5454249430153044,
      "grad_norm": 0.06148568540811539,
      "learning_rate": 0.00013637251709540865,
      "loss": 0.1536,
      "step": 154100
    },
    {
      "epoch": 0.5457788852236207,
      "grad_norm": 0.0837991014122963,
      "learning_rate": 0.00013626633443291378,
      "loss": 0.4433,
      "step": 154200
    },
    {
      "epoch": 0.5461328274319369,
      "grad_norm": 0.0004910906427539885,
      "learning_rate": 0.0001361601517704189,
      "loss": 0.2787,
      "step": 154300
    },
    {
      "epoch": 0.5464867696402531,
      "grad_norm": 0.01370271760970354,
      "learning_rate": 0.00013605396910792403,
      "loss": 0.3717,
      "step": 154400
    },
    {
      "epoch": 0.5468407118485694,
      "grad_norm": 0.0007291835499927402,
      "learning_rate": 0.00013594778644542918,
      "loss": 0.276,
      "step": 154500
    },
    {
      "epoch": 0.5471946540568856,
      "grad_norm": 0.004093229304999113,
      "learning_rate": 0.0001358416037829343,
      "loss": 0.2389,
      "step": 154600
    },
    {
      "epoch": 0.5475485962652018,
      "grad_norm": 0.000743549782782793,
      "learning_rate": 0.00013573542112043944,
      "loss": 0.4049,
      "step": 154700
    },
    {
      "epoch": 0.547902538473518,
      "grad_norm": 0.0006198465125635266,
      "learning_rate": 0.00013562923845794456,
      "loss": 0.3792,
      "step": 154800
    },
    {
      "epoch": 0.5482564806818343,
      "grad_norm": 0.15283608436584473,
      "learning_rate": 0.00013552305579544972,
      "loss": 0.2159,
      "step": 154900
    },
    {
      "epoch": 0.5486104228901505,
      "grad_norm": 37.86817932128906,
      "learning_rate": 0.00013541687313295485,
      "loss": 0.2326,
      "step": 155000
    },
    {
      "epoch": 0.5489643650984667,
      "grad_norm": 0.1556432545185089,
      "learning_rate": 0.00013531069047045997,
      "loss": 0.3047,
      "step": 155100
    },
    {
      "epoch": 0.549318307306783,
      "grad_norm": 0.8847606182098389,
      "learning_rate": 0.0001352045078079651,
      "loss": 0.4737,
      "step": 155200
    },
    {
      "epoch": 0.5496722495150992,
      "grad_norm": 0.864137589931488,
      "learning_rate": 0.00013509832514547023,
      "loss": 0.3904,
      "step": 155300
    },
    {
      "epoch": 0.5500261917234154,
      "grad_norm": 0.004253356717526913,
      "learning_rate": 0.00013499214248297538,
      "loss": 0.3279,
      "step": 155400
    },
    {
      "epoch": 0.5503801339317316,
      "grad_norm": 14.022339820861816,
      "learning_rate": 0.0001348859598204805,
      "loss": 0.558,
      "step": 155500
    },
    {
      "epoch": 0.5507340761400479,
      "grad_norm": 0.0016959032509475946,
      "learning_rate": 0.00013477977715798563,
      "loss": 0.2444,
      "step": 155600
    },
    {
      "epoch": 0.5510880183483641,
      "grad_norm": 0.027893604710698128,
      "learning_rate": 0.00013467359449549076,
      "loss": 0.4588,
      "step": 155700
    },
    {
      "epoch": 0.5514419605566803,
      "grad_norm": 7.570319652557373,
      "learning_rate": 0.0001345674118329959,
      "loss": 0.383,
      "step": 155800
    },
    {
      "epoch": 0.5517959027649966,
      "grad_norm": 0.0042258454486727715,
      "learning_rate": 0.00013446122917050104,
      "loss": 0.3438,
      "step": 155900
    },
    {
      "epoch": 0.5521498449733128,
      "grad_norm": 2.0699062588391826e-05,
      "learning_rate": 0.00013435504650800617,
      "loss": 0.2716,
      "step": 156000
    },
    {
      "epoch": 0.552503787181629,
      "grad_norm": 0.019592564553022385,
      "learning_rate": 0.0001342488638455113,
      "loss": 0.2093,
      "step": 156100
    },
    {
      "epoch": 0.5528577293899453,
      "grad_norm": 0.0004886601818725467,
      "learning_rate": 0.00013414268118301642,
      "loss": 0.3953,
      "step": 156200
    },
    {
      "epoch": 0.5532116715982615,
      "grad_norm": 0.0006096099386923015,
      "learning_rate": 0.00013403649852052155,
      "loss": 0.2032,
      "step": 156300
    },
    {
      "epoch": 0.5535656138065776,
      "grad_norm": 0.061874862760305405,
      "learning_rate": 0.0001339303158580267,
      "loss": 0.3808,
      "step": 156400
    },
    {
      "epoch": 0.5539195560148938,
      "grad_norm": 0.13940918445587158,
      "learning_rate": 0.00013382413319553183,
      "loss": 0.332,
      "step": 156500
    },
    {
      "epoch": 0.5542734982232101,
      "grad_norm": 0.00013106992992106825,
      "learning_rate": 0.00013371795053303696,
      "loss": 0.3944,
      "step": 156600
    },
    {
      "epoch": 0.5546274404315263,
      "grad_norm": 0.0001458141632610932,
      "learning_rate": 0.00013361176787054209,
      "loss": 0.2318,
      "step": 156700
    },
    {
      "epoch": 0.5549813826398425,
      "grad_norm": 0.013540597632527351,
      "learning_rate": 0.0001335055852080472,
      "loss": 0.2535,
      "step": 156800
    },
    {
      "epoch": 0.5553353248481588,
      "grad_norm": 11.914315223693848,
      "learning_rate": 0.00013339940254555234,
      "loss": 0.3218,
      "step": 156900
    },
    {
      "epoch": 0.555689267056475,
      "grad_norm": 0.013224762864410877,
      "learning_rate": 0.00013329321988305747,
      "loss": 0.2357,
      "step": 157000
    },
    {
      "epoch": 0.5560432092647912,
      "grad_norm": 0.0028254801873117685,
      "learning_rate": 0.00013318703722056262,
      "loss": 0.1871,
      "step": 157100
    },
    {
      "epoch": 0.5563971514731074,
      "grad_norm": 1.350197672843933,
      "learning_rate": 0.00013308085455806775,
      "loss": 0.2206,
      "step": 157200
    },
    {
      "epoch": 0.5567510936814237,
      "grad_norm": 0.024128438904881477,
      "learning_rate": 0.00013297467189557287,
      "loss": 0.2883,
      "step": 157300
    },
    {
      "epoch": 0.5571050358897399,
      "grad_norm": 0.00016592658357694745,
      "learning_rate": 0.000132868489233078,
      "loss": 0.3455,
      "step": 157400
    },
    {
      "epoch": 0.5574589780980561,
      "grad_norm": 0.003129335353150964,
      "learning_rate": 0.00013276230657058313,
      "loss": 0.3954,
      "step": 157500
    },
    {
      "epoch": 0.5578129203063724,
      "grad_norm": 0.08732234686613083,
      "learning_rate": 0.00013265612390808828,
      "loss": 0.4013,
      "step": 157600
    },
    {
      "epoch": 0.5581668625146886,
      "grad_norm": 3.976539373397827,
      "learning_rate": 0.0001325499412455934,
      "loss": 0.3107,
      "step": 157700
    },
    {
      "epoch": 0.5585208047230048,
      "grad_norm": 0.0005821883678436279,
      "learning_rate": 0.00013244375858309854,
      "loss": 0.1488,
      "step": 157800
    },
    {
      "epoch": 0.5588747469313211,
      "grad_norm": 8.578905544709414e-05,
      "learning_rate": 0.00013233757592060366,
      "loss": 0.441,
      "step": 157900
    },
    {
      "epoch": 0.5592286891396373,
      "grad_norm": 0.0008478199597448111,
      "learning_rate": 0.0001322313932581088,
      "loss": 0.3083,
      "step": 158000
    },
    {
      "epoch": 0.5595826313479535,
      "grad_norm": 0.0048212832771241665,
      "learning_rate": 0.00013212521059561395,
      "loss": 0.2686,
      "step": 158100
    },
    {
      "epoch": 0.5599365735562697,
      "grad_norm": 0.5720701217651367,
      "learning_rate": 0.00013201902793311907,
      "loss": 0.3787,
      "step": 158200
    },
    {
      "epoch": 0.560290515764586,
      "grad_norm": 0.0036068668123334646,
      "learning_rate": 0.0001319128452706242,
      "loss": 0.2513,
      "step": 158300
    },
    {
      "epoch": 0.5606444579729022,
      "grad_norm": 0.0018472668016329408,
      "learning_rate": 0.00013180666260812933,
      "loss": 0.4028,
      "step": 158400
    },
    {
      "epoch": 0.5609984001812184,
      "grad_norm": 0.0012405472807586193,
      "learning_rate": 0.00013170047994563445,
      "loss": 0.4022,
      "step": 158500
    },
    {
      "epoch": 0.5613523423895347,
      "grad_norm": 4.2570300138322636e-05,
      "learning_rate": 0.0001315942972831396,
      "loss": 0.2614,
      "step": 158600
    },
    {
      "epoch": 0.5617062845978509,
      "grad_norm": 0.000341012142598629,
      "learning_rate": 0.00013148811462064473,
      "loss": 0.324,
      "step": 158700
    },
    {
      "epoch": 0.5620602268061671,
      "grad_norm": 0.0002812921884469688,
      "learning_rate": 0.00013138193195814986,
      "loss": 0.4231,
      "step": 158800
    },
    {
      "epoch": 0.5624141690144833,
      "grad_norm": 0.21336199343204498,
      "learning_rate": 0.000131275749295655,
      "loss": 0.1342,
      "step": 158900
    },
    {
      "epoch": 0.5627681112227996,
      "grad_norm": 0.05391711741685867,
      "learning_rate": 0.00013116956663316014,
      "loss": 0.1953,
      "step": 159000
    },
    {
      "epoch": 0.5631220534311158,
      "grad_norm": 0.06421391665935516,
      "learning_rate": 0.00013106338397066527,
      "loss": 0.2961,
      "step": 159100
    },
    {
      "epoch": 0.563475995639432,
      "grad_norm": 0.0008096778765320778,
      "learning_rate": 0.0001309572013081704,
      "loss": 0.1139,
      "step": 159200
    },
    {
      "epoch": 0.5638299378477483,
      "grad_norm": 0.0004045145760755986,
      "learning_rate": 0.00013085101864567552,
      "loss": 0.3802,
      "step": 159300
    },
    {
      "epoch": 0.5641838800560645,
      "grad_norm": 0.019443994387984276,
      "learning_rate": 0.00013074483598318065,
      "loss": 0.2857,
      "step": 159400
    },
    {
      "epoch": 0.5645378222643807,
      "grad_norm": 0.008878680877387524,
      "learning_rate": 0.0001306386533206858,
      "loss": 0.3929,
      "step": 159500
    },
    {
      "epoch": 0.5648917644726968,
      "grad_norm": 0.0010016602464020252,
      "learning_rate": 0.00013053247065819093,
      "loss": 0.4005,
      "step": 159600
    },
    {
      "epoch": 0.5652457066810131,
      "grad_norm": 0.0038124879356473684,
      "learning_rate": 0.00013042628799569603,
      "loss": 0.1982,
      "step": 159700
    },
    {
      "epoch": 0.5655996488893293,
      "grad_norm": 0.00043305050348863006,
      "learning_rate": 0.00013032010533320119,
      "loss": 0.222,
      "step": 159800
    },
    {
      "epoch": 0.5659535910976455,
      "grad_norm": 0.24525338411331177,
      "learning_rate": 0.0001302139226707063,
      "loss": 0.1645,
      "step": 159900
    },
    {
      "epoch": 0.5663075333059618,
      "grad_norm": 0.0006993935676291585,
      "learning_rate": 0.00013010774000821144,
      "loss": 0.3863,
      "step": 160000
    },
    {
      "epoch": 0.566661475514278,
      "grad_norm": 47.62461471557617,
      "learning_rate": 0.00013000155734571657,
      "loss": 0.3371,
      "step": 160100
    },
    {
      "epoch": 0.5670154177225942,
      "grad_norm": 0.0005928955506533384,
      "learning_rate": 0.00012989537468322172,
      "loss": 0.4335,
      "step": 160200
    },
    {
      "epoch": 0.5673693599309105,
      "grad_norm": 0.009074991568922997,
      "learning_rate": 0.00012978919202072685,
      "loss": 0.3137,
      "step": 160300
    },
    {
      "epoch": 0.5677233021392267,
      "grad_norm": 0.0021381108090281487,
      "learning_rate": 0.00012968300935823197,
      "loss": 0.3966,
      "step": 160400
    },
    {
      "epoch": 0.5680772443475429,
      "grad_norm": 0.0620691180229187,
      "learning_rate": 0.0001295768266957371,
      "loss": 0.21,
      "step": 160500
    },
    {
      "epoch": 0.5684311865558591,
      "grad_norm": 1.877120018005371,
      "learning_rate": 0.00012947064403324223,
      "loss": 0.233,
      "step": 160600
    },
    {
      "epoch": 0.5687851287641754,
      "grad_norm": 0.022502904757857323,
      "learning_rate": 0.00012936446137074738,
      "loss": 0.1392,
      "step": 160700
    },
    {
      "epoch": 0.5691390709724916,
      "grad_norm": 23.082178115844727,
      "learning_rate": 0.0001292582787082525,
      "loss": 0.4256,
      "step": 160800
    },
    {
      "epoch": 0.5694930131808078,
      "grad_norm": 0.0007206187583506107,
      "learning_rate": 0.00012915209604575764,
      "loss": 0.3766,
      "step": 160900
    },
    {
      "epoch": 0.5698469553891241,
      "grad_norm": 0.0031058783642947674,
      "learning_rate": 0.00012904591338326276,
      "loss": 0.2933,
      "step": 161000
    },
    {
      "epoch": 0.5702008975974403,
      "grad_norm": 5.066650192020461e-05,
      "learning_rate": 0.0001289397307207679,
      "loss": 0.2517,
      "step": 161100
    },
    {
      "epoch": 0.5705548398057565,
      "grad_norm": 17.476097106933594,
      "learning_rate": 0.00012883354805827304,
      "loss": 0.2854,
      "step": 161200
    },
    {
      "epoch": 0.5709087820140727,
      "grad_norm": 0.0004449182015378028,
      "learning_rate": 0.00012872736539577817,
      "loss": 0.2721,
      "step": 161300
    },
    {
      "epoch": 0.571262724222389,
      "grad_norm": 0.0032623924780637026,
      "learning_rate": 0.0001286211827332833,
      "loss": 0.5032,
      "step": 161400
    },
    {
      "epoch": 0.5716166664307052,
      "grad_norm": 0.0016204322455450892,
      "learning_rate": 0.00012851500007078843,
      "loss": 0.4264,
      "step": 161500
    },
    {
      "epoch": 0.5719706086390214,
      "grad_norm": 58.05134201049805,
      "learning_rate": 0.00012840881740829355,
      "loss": 0.4997,
      "step": 161600
    },
    {
      "epoch": 0.5723245508473377,
      "grad_norm": 0.00039386245771311224,
      "learning_rate": 0.0001283026347457987,
      "loss": 0.3205,
      "step": 161700
    },
    {
      "epoch": 0.5726784930556539,
      "grad_norm": 86.29100799560547,
      "learning_rate": 0.00012819645208330383,
      "loss": 0.5258,
      "step": 161800
    },
    {
      "epoch": 0.5730324352639701,
      "grad_norm": 0.002440741052851081,
      "learning_rate": 0.00012809026942080896,
      "loss": 0.406,
      "step": 161900
    },
    {
      "epoch": 0.5733863774722864,
      "grad_norm": 0.003436428727582097,
      "learning_rate": 0.0001279840867583141,
      "loss": 0.215,
      "step": 162000
    },
    {
      "epoch": 0.5737403196806026,
      "grad_norm": 0.00040687600267119706,
      "learning_rate": 0.00012787790409581921,
      "loss": 0.1869,
      "step": 162100
    },
    {
      "epoch": 0.5740942618889188,
      "grad_norm": 53.65892028808594,
      "learning_rate": 0.00012777172143332437,
      "loss": 0.2091,
      "step": 162200
    },
    {
      "epoch": 0.574448204097235,
      "grad_norm": 0.0002556252875365317,
      "learning_rate": 0.0001276655387708295,
      "loss": 0.3765,
      "step": 162300
    },
    {
      "epoch": 0.5748021463055513,
      "grad_norm": 0.0002333003794774413,
      "learning_rate": 0.00012755935610833462,
      "loss": 0.2446,
      "step": 162400
    },
    {
      "epoch": 0.5751560885138675,
      "grad_norm": 0.032335396856069565,
      "learning_rate": 0.00012745317344583975,
      "loss": 0.1868,
      "step": 162500
    },
    {
      "epoch": 0.5755100307221837,
      "grad_norm": 5.09469500684645e-05,
      "learning_rate": 0.00012734699078334488,
      "loss": 0.2871,
      "step": 162600
    },
    {
      "epoch": 0.5758639729305,
      "grad_norm": 0.0040388875640928745,
      "learning_rate": 0.00012724080812085003,
      "loss": 0.2604,
      "step": 162700
    },
    {
      "epoch": 0.5762179151388162,
      "grad_norm": 104.798828125,
      "learning_rate": 0.00012713462545835513,
      "loss": 0.4263,
      "step": 162800
    },
    {
      "epoch": 0.5765718573471323,
      "grad_norm": 35.14812088012695,
      "learning_rate": 0.00012702844279586029,
      "loss": 0.3355,
      "step": 162900
    },
    {
      "epoch": 0.5769257995554485,
      "grad_norm": 0.0023135230876505375,
      "learning_rate": 0.0001269222601333654,
      "loss": 0.3357,
      "step": 163000
    },
    {
      "epoch": 0.5772797417637648,
      "grad_norm": 0.019220909103751183,
      "learning_rate": 0.00012681607747087057,
      "loss": 0.399,
      "step": 163100
    },
    {
      "epoch": 0.577633683972081,
      "grad_norm": 0.08441314101219177,
      "learning_rate": 0.00012670989480837567,
      "loss": 0.3405,
      "step": 163200
    },
    {
      "epoch": 0.5779876261803972,
      "grad_norm": 0.0001504582614870742,
      "learning_rate": 0.0001266037121458808,
      "loss": 0.1246,
      "step": 163300
    },
    {
      "epoch": 0.5783415683887135,
      "grad_norm": 7.976074994076043e-05,
      "learning_rate": 0.00012649752948338595,
      "loss": 0.3957,
      "step": 163400
    },
    {
      "epoch": 0.5786955105970297,
      "grad_norm": 115.06658172607422,
      "learning_rate": 0.00012639134682089107,
      "loss": 0.3893,
      "step": 163500
    },
    {
      "epoch": 0.5790494528053459,
      "grad_norm": 0.00019395913113839924,
      "learning_rate": 0.0001262851641583962,
      "loss": 0.4127,
      "step": 163600
    },
    {
      "epoch": 0.5794033950136621,
      "grad_norm": 0.0009797862730920315,
      "learning_rate": 0.00012617898149590133,
      "loss": 0.4665,
      "step": 163700
    },
    {
      "epoch": 0.5797573372219784,
      "grad_norm": 0.009385377168655396,
      "learning_rate": 0.00012607279883340646,
      "loss": 0.2602,
      "step": 163800
    },
    {
      "epoch": 0.5801112794302946,
      "grad_norm": 0.00042959474376402795,
      "learning_rate": 0.0001259666161709116,
      "loss": 0.2314,
      "step": 163900
    },
    {
      "epoch": 0.5804652216386108,
      "grad_norm": 0.0012668352574110031,
      "learning_rate": 0.00012586043350841674,
      "loss": 0.3186,
      "step": 164000
    },
    {
      "epoch": 0.5808191638469271,
      "grad_norm": 0.0014760058838874102,
      "learning_rate": 0.00012575425084592186,
      "loss": 0.2725,
      "step": 164100
    },
    {
      "epoch": 0.5811731060552433,
      "grad_norm": 0.05843788757920265,
      "learning_rate": 0.000125648068183427,
      "loss": 0.2667,
      "step": 164200
    },
    {
      "epoch": 0.5815270482635595,
      "grad_norm": 0.0001530882582301274,
      "learning_rate": 0.00012554188552093214,
      "loss": 0.3944,
      "step": 164300
    },
    {
      "epoch": 0.5818809904718758,
      "grad_norm": 0.002302404958754778,
      "learning_rate": 0.00012543570285843727,
      "loss": 0.3828,
      "step": 164400
    },
    {
      "epoch": 0.582234932680192,
      "grad_norm": 3.5730419158935547,
      "learning_rate": 0.0001253295201959424,
      "loss": 0.1875,
      "step": 164500
    },
    {
      "epoch": 0.5825888748885082,
      "grad_norm": 0.0001094777908292599,
      "learning_rate": 0.00012522333753344753,
      "loss": 0.3525,
      "step": 164600
    },
    {
      "epoch": 0.5829428170968244,
      "grad_norm": 0.014704044908285141,
      "learning_rate": 0.00012511715487095265,
      "loss": 0.2707,
      "step": 164700
    },
    {
      "epoch": 0.5832967593051407,
      "grad_norm": 0.0034852835815399885,
      "learning_rate": 0.0001250109722084578,
      "loss": 0.2425,
      "step": 164800
    },
    {
      "epoch": 0.5836507015134569,
      "grad_norm": 0.03632556274533272,
      "learning_rate": 0.00012490478954596293,
      "loss": 0.3458,
      "step": 164900
    },
    {
      "epoch": 0.5840046437217731,
      "grad_norm": 0.01439952477812767,
      "learning_rate": 0.00012479860688346806,
      "loss": 0.3268,
      "step": 165000
    },
    {
      "epoch": 0.5843585859300894,
      "grad_norm": 0.5857570767402649,
      "learning_rate": 0.0001246924242209732,
      "loss": 0.1001,
      "step": 165100
    },
    {
      "epoch": 0.5847125281384056,
      "grad_norm": 0.25058045983314514,
      "learning_rate": 0.00012458624155847831,
      "loss": 0.4429,
      "step": 165200
    },
    {
      "epoch": 0.5850664703467218,
      "grad_norm": 0.6823555827140808,
      "learning_rate": 0.00012448005889598347,
      "loss": 0.3182,
      "step": 165300
    },
    {
      "epoch": 0.585420412555038,
      "grad_norm": 0.013071631081402302,
      "learning_rate": 0.0001243738762334886,
      "loss": 0.3929,
      "step": 165400
    },
    {
      "epoch": 0.5857743547633543,
      "grad_norm": 0.02845560573041439,
      "learning_rate": 0.00012426769357099372,
      "loss": 0.2713,
      "step": 165500
    },
    {
      "epoch": 0.5861282969716705,
      "grad_norm": 9.769936561584473,
      "learning_rate": 0.00012416151090849885,
      "loss": 0.2234,
      "step": 165600
    },
    {
      "epoch": 0.5864822391799867,
      "grad_norm": 3.420125961303711,
      "learning_rate": 0.00012405532824600398,
      "loss": 0.1711,
      "step": 165700
    },
    {
      "epoch": 0.586836181388303,
      "grad_norm": 16.897539138793945,
      "learning_rate": 0.00012394914558350913,
      "loss": 0.2227,
      "step": 165800
    },
    {
      "epoch": 0.5871901235966192,
      "grad_norm": 0.002094609895721078,
      "learning_rate": 0.00012384296292101423,
      "loss": 0.15,
      "step": 165900
    },
    {
      "epoch": 0.5875440658049353,
      "grad_norm": 8.54030704498291,
      "learning_rate": 0.00012373678025851938,
      "loss": 0.3479,
      "step": 166000
    },
    {
      "epoch": 0.5878980080132516,
      "grad_norm": 4.014587466372177e-05,
      "learning_rate": 0.0001236305975960245,
      "loss": 0.2707,
      "step": 166100
    },
    {
      "epoch": 0.5882519502215678,
      "grad_norm": 0.000935908465180546,
      "learning_rate": 0.00012352441493352964,
      "loss": 0.1546,
      "step": 166200
    },
    {
      "epoch": 0.588605892429884,
      "grad_norm": 7.132421160349622e-05,
      "learning_rate": 0.00012341823227103477,
      "loss": 0.2702,
      "step": 166300
    },
    {
      "epoch": 0.5889598346382002,
      "grad_norm": 0.3028908967971802,
      "learning_rate": 0.0001233120496085399,
      "loss": 0.2558,
      "step": 166400
    },
    {
      "epoch": 0.5893137768465165,
      "grad_norm": 42.083282470703125,
      "learning_rate": 0.00012320586694604505,
      "loss": 0.2658,
      "step": 166500
    },
    {
      "epoch": 0.5896677190548327,
      "grad_norm": 9.118854904954787e-06,
      "learning_rate": 0.00012309968428355017,
      "loss": 0.3048,
      "step": 166600
    },
    {
      "epoch": 0.5900216612631489,
      "grad_norm": 0.006089723203331232,
      "learning_rate": 0.0001229935016210553,
      "loss": 0.361,
      "step": 166700
    },
    {
      "epoch": 0.5903756034714652,
      "grad_norm": 0.004112991504371166,
      "learning_rate": 0.00012288731895856043,
      "loss": 0.3239,
      "step": 166800
    },
    {
      "epoch": 0.5907295456797814,
      "grad_norm": 0.00031083772773854434,
      "learning_rate": 0.00012278113629606555,
      "loss": 0.1641,
      "step": 166900
    },
    {
      "epoch": 0.5910834878880976,
      "grad_norm": 0.14696119725704193,
      "learning_rate": 0.0001226749536335707,
      "loss": 0.43,
      "step": 167000
    },
    {
      "epoch": 0.5914374300964138,
      "grad_norm": 0.004101978614926338,
      "learning_rate": 0.00012256877097107584,
      "loss": 0.224,
      "step": 167100
    },
    {
      "epoch": 0.5917913723047301,
      "grad_norm": 0.0209491103887558,
      "learning_rate": 0.00012246258830858096,
      "loss": 0.1477,
      "step": 167200
    },
    {
      "epoch": 0.5921453145130463,
      "grad_norm": 0.009013243950903416,
      "learning_rate": 0.0001223564056460861,
      "loss": 0.3918,
      "step": 167300
    },
    {
      "epoch": 0.5924992567213625,
      "grad_norm": 0.011261737905442715,
      "learning_rate": 0.00012225022298359122,
      "loss": 0.3338,
      "step": 167400
    },
    {
      "epoch": 0.5928531989296788,
      "grad_norm": 0.001461472944356501,
      "learning_rate": 0.00012214404032109637,
      "loss": 0.24,
      "step": 167500
    },
    {
      "epoch": 0.593207141137995,
      "grad_norm": 0.0017281685722991824,
      "learning_rate": 0.00012203785765860148,
      "loss": 0.2952,
      "step": 167600
    },
    {
      "epoch": 0.5935610833463112,
      "grad_norm": 0.0001978647051146254,
      "learning_rate": 0.00012193167499610664,
      "loss": 0.2844,
      "step": 167700
    },
    {
      "epoch": 0.5939150255546275,
      "grad_norm": 17.491687774658203,
      "learning_rate": 0.00012182549233361175,
      "loss": 0.2075,
      "step": 167800
    },
    {
      "epoch": 0.5942689677629437,
      "grad_norm": 0.0017765569500625134,
      "learning_rate": 0.00012171930967111688,
      "loss": 0.3946,
      "step": 167900
    },
    {
      "epoch": 0.5946229099712599,
      "grad_norm": 3.567612648010254,
      "learning_rate": 0.00012161312700862202,
      "loss": 0.4022,
      "step": 168000
    },
    {
      "epoch": 0.5949768521795761,
      "grad_norm": 0.1942697912454605,
      "learning_rate": 0.00012150694434612715,
      "loss": 0.3643,
      "step": 168100
    },
    {
      "epoch": 0.5953307943878924,
      "grad_norm": 0.0011740154586732388,
      "learning_rate": 0.00012140076168363229,
      "loss": 0.4175,
      "step": 168200
    },
    {
      "epoch": 0.5956847365962086,
      "grad_norm": 0.16074661910533905,
      "learning_rate": 0.00012129457902113741,
      "loss": 0.1875,
      "step": 168300
    },
    {
      "epoch": 0.5960386788045248,
      "grad_norm": 8.397983037866652e-05,
      "learning_rate": 0.00012118839635864254,
      "loss": 0.3744,
      "step": 168400
    },
    {
      "epoch": 0.5963926210128411,
      "grad_norm": 0.00035529525484889746,
      "learning_rate": 0.00012108221369614768,
      "loss": 0.3374,
      "step": 168500
    },
    {
      "epoch": 0.5967465632211573,
      "grad_norm": 16.97090721130371,
      "learning_rate": 0.00012097603103365281,
      "loss": 0.1302,
      "step": 168600
    },
    {
      "epoch": 0.5971005054294735,
      "grad_norm": 0.23497039079666138,
      "learning_rate": 0.00012086984837115795,
      "loss": 0.2904,
      "step": 168700
    },
    {
      "epoch": 0.5974544476377897,
      "grad_norm": 21.047107696533203,
      "learning_rate": 0.00012076366570866308,
      "loss": 0.1757,
      "step": 168800
    },
    {
      "epoch": 0.597808389846106,
      "grad_norm": 0.0007735867984592915,
      "learning_rate": 0.00012065748304616822,
      "loss": 0.4206,
      "step": 168900
    },
    {
      "epoch": 0.5981623320544222,
      "grad_norm": 0.00018900033319368958,
      "learning_rate": 0.00012055130038367334,
      "loss": 0.2562,
      "step": 169000
    },
    {
      "epoch": 0.5985162742627383,
      "grad_norm": 0.053550299257040024,
      "learning_rate": 0.00012044511772117847,
      "loss": 0.398,
      "step": 169100
    },
    {
      "epoch": 0.5988702164710547,
      "grad_norm": 0.00030475840321742,
      "learning_rate": 0.00012033893505868361,
      "loss": 0.2487,
      "step": 169200
    },
    {
      "epoch": 0.5992241586793708,
      "grad_norm": 0.000880255363881588,
      "learning_rate": 0.00012023275239618874,
      "loss": 0.4002,
      "step": 169300
    },
    {
      "epoch": 0.599578100887687,
      "grad_norm": 44.01853942871094,
      "learning_rate": 0.00012012656973369388,
      "loss": 0.3124,
      "step": 169400
    },
    {
      "epoch": 0.5999320430960032,
      "grad_norm": 0.0003668208373710513,
      "learning_rate": 0.000120020387071199,
      "loss": 0.3545,
      "step": 169500
    },
    {
      "epoch": 0.6002859853043195,
      "grad_norm": 0.0001677889667917043,
      "learning_rate": 0.00011991420440870413,
      "loss": 0.2532,
      "step": 169600
    },
    {
      "epoch": 0.6006399275126357,
      "grad_norm": 18.352445602416992,
      "learning_rate": 0.00011980802174620927,
      "loss": 0.2309,
      "step": 169700
    },
    {
      "epoch": 0.6009938697209519,
      "grad_norm": 0.0018277165945619345,
      "learning_rate": 0.0001197018390837144,
      "loss": 0.3064,
      "step": 169800
    },
    {
      "epoch": 0.6013478119292682,
      "grad_norm": 2.5460445880889893,
      "learning_rate": 0.00011959565642121954,
      "loss": 0.2548,
      "step": 169900
    },
    {
      "epoch": 0.6017017541375844,
      "grad_norm": 0.0007542670937255025,
      "learning_rate": 0.00011948947375872467,
      "loss": 0.1687,
      "step": 170000
    },
    {
      "epoch": 0.6020556963459006,
      "grad_norm": 0.08593782782554626,
      "learning_rate": 0.00011938329109622981,
      "loss": 0.2725,
      "step": 170100
    },
    {
      "epoch": 0.6024096385542169,
      "grad_norm": 0.000848646042868495,
      "learning_rate": 0.00011927710843373494,
      "loss": 0.3077,
      "step": 170200
    },
    {
      "epoch": 0.6027635807625331,
      "grad_norm": 4.537283420562744,
      "learning_rate": 0.00011917092577124005,
      "loss": 0.3012,
      "step": 170300
    },
    {
      "epoch": 0.6031175229708493,
      "grad_norm": 3.8612568378448486,
      "learning_rate": 0.0001190647431087452,
      "loss": 0.2784,
      "step": 170400
    },
    {
      "epoch": 0.6034714651791655,
      "grad_norm": 0.010104550048708916,
      "learning_rate": 0.00011895856044625032,
      "loss": 0.1839,
      "step": 170500
    },
    {
      "epoch": 0.6038254073874818,
      "grad_norm": 0.01611441746354103,
      "learning_rate": 0.00011885237778375547,
      "loss": 0.2154,
      "step": 170600
    },
    {
      "epoch": 0.604179349595798,
      "grad_norm": 0.0005631400854326785,
      "learning_rate": 0.00011874619512126058,
      "loss": 0.3493,
      "step": 170700
    },
    {
      "epoch": 0.6045332918041142,
      "grad_norm": 0.06504174321889877,
      "learning_rate": 0.00011864001245876571,
      "loss": 0.2336,
      "step": 170800
    },
    {
      "epoch": 0.6048872340124305,
      "grad_norm": 0.09483174979686737,
      "learning_rate": 0.00011853382979627085,
      "loss": 0.3653,
      "step": 170900
    },
    {
      "epoch": 0.6052411762207467,
      "grad_norm": 0.004286467097699642,
      "learning_rate": 0.00011842764713377598,
      "loss": 0.4005,
      "step": 171000
    },
    {
      "epoch": 0.6055951184290629,
      "grad_norm": 0.00012084834452252835,
      "learning_rate": 0.00011832146447128112,
      "loss": 0.2996,
      "step": 171100
    },
    {
      "epoch": 0.6059490606373791,
      "grad_norm": 0.0008703188505023718,
      "learning_rate": 0.00011821528180878625,
      "loss": 0.3242,
      "step": 171200
    },
    {
      "epoch": 0.6063030028456954,
      "grad_norm": 0.1959550529718399,
      "learning_rate": 0.00011810909914629137,
      "loss": 0.3759,
      "step": 171300
    },
    {
      "epoch": 0.6066569450540116,
      "grad_norm": 0.010442912578582764,
      "learning_rate": 0.00011800291648379651,
      "loss": 0.1972,
      "step": 171400
    },
    {
      "epoch": 0.6070108872623278,
      "grad_norm": 0.0016024868236854672,
      "learning_rate": 0.00011789673382130164,
      "loss": 0.2379,
      "step": 171500
    },
    {
      "epoch": 0.6073648294706441,
      "grad_norm": 0.0004102512903045863,
      "learning_rate": 0.00011779055115880678,
      "loss": 0.2589,
      "step": 171600
    },
    {
      "epoch": 0.6077187716789603,
      "grad_norm": 0.00027439987752586603,
      "learning_rate": 0.00011768436849631191,
      "loss": 0.2536,
      "step": 171700
    },
    {
      "epoch": 0.6080727138872765,
      "grad_norm": 0.0018568740924820304,
      "learning_rate": 0.00011757818583381705,
      "loss": 0.1469,
      "step": 171800
    },
    {
      "epoch": 0.6084266560955928,
      "grad_norm": 0.001246382831595838,
      "learning_rate": 0.00011747200317132218,
      "loss": 0.2625,
      "step": 171900
    },
    {
      "epoch": 0.608780598303909,
      "grad_norm": 0.00013903780200053006,
      "learning_rate": 0.0001173658205088273,
      "loss": 0.3833,
      "step": 172000
    },
    {
      "epoch": 0.6091345405122252,
      "grad_norm": 0.0003067846701014787,
      "learning_rate": 0.00011725963784633244,
      "loss": 0.0563,
      "step": 172100
    },
    {
      "epoch": 0.6094884827205413,
      "grad_norm": 0.003789792535826564,
      "learning_rate": 0.00011715345518383757,
      "loss": 0.4403,
      "step": 172200
    },
    {
      "epoch": 0.6098424249288577,
      "grad_norm": 0.05966721475124359,
      "learning_rate": 0.00011704727252134271,
      "loss": 0.3131,
      "step": 172300
    },
    {
      "epoch": 0.6101963671371738,
      "grad_norm": 1.5578657388687134,
      "learning_rate": 0.00011694108985884784,
      "loss": 0.2954,
      "step": 172400
    },
    {
      "epoch": 0.61055030934549,
      "grad_norm": 4.453505971468985e-05,
      "learning_rate": 0.00011683490719635296,
      "loss": 0.3319,
      "step": 172500
    },
    {
      "epoch": 0.6109042515538063,
      "grad_norm": 0.005519811995327473,
      "learning_rate": 0.0001167287245338581,
      "loss": 0.3937,
      "step": 172600
    },
    {
      "epoch": 0.6112581937621225,
      "grad_norm": 0.0279240719974041,
      "learning_rate": 0.00011662254187136323,
      "loss": 0.283,
      "step": 172700
    },
    {
      "epoch": 0.6116121359704387,
      "grad_norm": 0.002993575530126691,
      "learning_rate": 0.00011651635920886837,
      "loss": 0.2994,
      "step": 172800
    },
    {
      "epoch": 0.6119660781787549,
      "grad_norm": 0.0005081566050648689,
      "learning_rate": 0.0001164101765463735,
      "loss": 0.1844,
      "step": 172900
    },
    {
      "epoch": 0.6123200203870712,
      "grad_norm": 0.00030234127189032733,
      "learning_rate": 0.00011630399388387864,
      "loss": 0.1383,
      "step": 173000
    },
    {
      "epoch": 0.6126739625953874,
      "grad_norm": 0.019033385440707207,
      "learning_rate": 0.00011619781122138377,
      "loss": 0.4351,
      "step": 173100
    },
    {
      "epoch": 0.6130279048037036,
      "grad_norm": 0.0012344084680080414,
      "learning_rate": 0.0001160916285588889,
      "loss": 0.2602,
      "step": 173200
    },
    {
      "epoch": 0.6133818470120199,
      "grad_norm": 0.00023788452381268144,
      "learning_rate": 0.00011598544589639404,
      "loss": 0.317,
      "step": 173300
    },
    {
      "epoch": 0.6137357892203361,
      "grad_norm": 0.010689952410757542,
      "learning_rate": 0.00011587926323389916,
      "loss": 0.3328,
      "step": 173400
    },
    {
      "epoch": 0.6140897314286523,
      "grad_norm": 0.0016991287702694535,
      "learning_rate": 0.0001157730805714043,
      "loss": 0.1765,
      "step": 173500
    },
    {
      "epoch": 0.6144436736369685,
      "grad_norm": 0.0048422012478113174,
      "learning_rate": 0.00011566689790890942,
      "loss": 0.3591,
      "step": 173600
    },
    {
      "epoch": 0.6147976158452848,
      "grad_norm": 62.14545440673828,
      "learning_rate": 0.00011556071524641454,
      "loss": 0.2022,
      "step": 173700
    },
    {
      "epoch": 0.615151558053601,
      "grad_norm": 0.0008399393409490585,
      "learning_rate": 0.00011545453258391968,
      "loss": 0.4629,
      "step": 173800
    },
    {
      "epoch": 0.6155055002619172,
      "grad_norm": 0.0004219201800879091,
      "learning_rate": 0.00011534834992142481,
      "loss": 0.2406,
      "step": 173900
    },
    {
      "epoch": 0.6158594424702335,
      "grad_norm": 148.59349060058594,
      "learning_rate": 0.00011524216725892995,
      "loss": 0.2944,
      "step": 174000
    },
    {
      "epoch": 0.6162133846785497,
      "grad_norm": 0.01686900295317173,
      "learning_rate": 0.00011513598459643508,
      "loss": 0.3868,
      "step": 174100
    },
    {
      "epoch": 0.6165673268868659,
      "grad_norm": 0.007399337366223335,
      "learning_rate": 0.00011502980193394022,
      "loss": 0.3744,
      "step": 174200
    },
    {
      "epoch": 0.6169212690951822,
      "grad_norm": 66.61783599853516,
      "learning_rate": 0.00011492361927144535,
      "loss": 0.4935,
      "step": 174300
    },
    {
      "epoch": 0.6172752113034984,
      "grad_norm": 0.021093623712658882,
      "learning_rate": 0.00011481743660895047,
      "loss": 0.3009,
      "step": 174400
    },
    {
      "epoch": 0.6176291535118146,
      "grad_norm": 0.6198359131813049,
      "learning_rate": 0.00011471125394645561,
      "loss": 0.2665,
      "step": 174500
    },
    {
      "epoch": 0.6179830957201308,
      "grad_norm": 0.04896489530801773,
      "learning_rate": 0.00011460507128396074,
      "loss": 0.4317,
      "step": 174600
    },
    {
      "epoch": 0.6183370379284471,
      "grad_norm": 0.00015087785141076893,
      "learning_rate": 0.00011449888862146588,
      "loss": 0.3931,
      "step": 174700
    },
    {
      "epoch": 0.6186909801367633,
      "grad_norm": 0.012301267124712467,
      "learning_rate": 0.00011439270595897101,
      "loss": 0.1955,
      "step": 174800
    },
    {
      "epoch": 0.6190449223450795,
      "grad_norm": 0.00044424697989597917,
      "learning_rate": 0.00011428652329647613,
      "loss": 0.2712,
      "step": 174900
    },
    {
      "epoch": 0.6193988645533958,
      "grad_norm": 0.002778675640001893,
      "learning_rate": 0.00011418034063398128,
      "loss": 0.264,
      "step": 175000
    },
    {
      "epoch": 0.619752806761712,
      "grad_norm": 0.002625187858939171,
      "learning_rate": 0.0001140741579714864,
      "loss": 0.1845,
      "step": 175100
    },
    {
      "epoch": 0.6201067489700282,
      "grad_norm": 11.678531646728516,
      "learning_rate": 0.00011396797530899154,
      "loss": 0.4754,
      "step": 175200
    },
    {
      "epoch": 0.6204606911783443,
      "grad_norm": 0.00034890591632574797,
      "learning_rate": 0.00011386179264649667,
      "loss": 0.318,
      "step": 175300
    },
    {
      "epoch": 0.6208146333866607,
      "grad_norm": 0.061912499368190765,
      "learning_rate": 0.0001137556099840018,
      "loss": 0.2832,
      "step": 175400
    },
    {
      "epoch": 0.6211685755949768,
      "grad_norm": 8.58188868733123e-05,
      "learning_rate": 0.00011364942732150694,
      "loss": 0.075,
      "step": 175500
    },
    {
      "epoch": 0.621522517803293,
      "grad_norm": 0.05147671699523926,
      "learning_rate": 0.00011354324465901206,
      "loss": 0.3422,
      "step": 175600
    },
    {
      "epoch": 0.6218764600116093,
      "grad_norm": 0.0034616319462656975,
      "learning_rate": 0.0001134370619965172,
      "loss": 0.2552,
      "step": 175700
    },
    {
      "epoch": 0.6222304022199255,
      "grad_norm": 0.002235042629763484,
      "learning_rate": 0.00011333087933402233,
      "loss": 0.1905,
      "step": 175800
    },
    {
      "epoch": 0.6225843444282417,
      "grad_norm": 0.012005730532109737,
      "learning_rate": 0.00011322469667152747,
      "loss": 0.244,
      "step": 175900
    },
    {
      "epoch": 0.622938286636558,
      "grad_norm": 3.7275984287261963,
      "learning_rate": 0.0001131185140090326,
      "loss": 0.2551,
      "step": 176000
    },
    {
      "epoch": 0.6232922288448742,
      "grad_norm": 0.004324924200773239,
      "learning_rate": 0.00011301233134653773,
      "loss": 0.348,
      "step": 176100
    },
    {
      "epoch": 0.6236461710531904,
      "grad_norm": 0.00023480449453927577,
      "learning_rate": 0.00011290614868404287,
      "loss": 0.3105,
      "step": 176200
    },
    {
      "epoch": 0.6240001132615066,
      "grad_norm": 0.06515062600374222,
      "learning_rate": 0.000112799966021548,
      "loss": 0.2164,
      "step": 176300
    },
    {
      "epoch": 0.6243540554698229,
      "grad_norm": 0.00206768698990345,
      "learning_rate": 0.00011269378335905313,
      "loss": 0.3091,
      "step": 176400
    },
    {
      "epoch": 0.6247079976781391,
      "grad_norm": 0.442810982465744,
      "learning_rate": 0.00011258760069655826,
      "loss": 0.1731,
      "step": 176500
    },
    {
      "epoch": 0.6250619398864553,
      "grad_norm": 0.004726666025817394,
      "learning_rate": 0.00011248141803406338,
      "loss": 0.2529,
      "step": 176600
    },
    {
      "epoch": 0.6254158820947716,
      "grad_norm": 93.73685455322266,
      "learning_rate": 0.00011237523537156853,
      "loss": 0.2389,
      "step": 176700
    },
    {
      "epoch": 0.6257698243030878,
      "grad_norm": 5.2093946578679606e-05,
      "learning_rate": 0.00011226905270907364,
      "loss": 0.2983,
      "step": 176800
    },
    {
      "epoch": 0.626123766511404,
      "grad_norm": 0.0002733947476372123,
      "learning_rate": 0.00011216287004657878,
      "loss": 0.2766,
      "step": 176900
    },
    {
      "epoch": 0.6264777087197202,
      "grad_norm": 0.9117742776870728,
      "learning_rate": 0.00011205668738408391,
      "loss": 0.184,
      "step": 177000
    },
    {
      "epoch": 0.6268316509280365,
      "grad_norm": 0.0052248816937208176,
      "learning_rate": 0.00011195050472158905,
      "loss": 0.423,
      "step": 177100
    },
    {
      "epoch": 0.6271855931363527,
      "grad_norm": 0.8232775926589966,
      "learning_rate": 0.00011184432205909418,
      "loss": 0.3581,
      "step": 177200
    },
    {
      "epoch": 0.6275395353446689,
      "grad_norm": 0.25984570384025574,
      "learning_rate": 0.0001117381393965993,
      "loss": 0.345,
      "step": 177300
    },
    {
      "epoch": 0.6278934775529852,
      "grad_norm": 0.009521636180579662,
      "learning_rate": 0.00011163195673410445,
      "loss": 0.3996,
      "step": 177400
    },
    {
      "epoch": 0.6282474197613014,
      "grad_norm": 0.012068679556250572,
      "learning_rate": 0.00011152577407160957,
      "loss": 0.2464,
      "step": 177500
    },
    {
      "epoch": 0.6286013619696176,
      "grad_norm": 0.08196254819631577,
      "learning_rate": 0.00011141959140911471,
      "loss": 0.4216,
      "step": 177600
    },
    {
      "epoch": 0.6289553041779338,
      "grad_norm": 0.013244636356830597,
      "learning_rate": 0.00011131340874661984,
      "loss": 0.3114,
      "step": 177700
    },
    {
      "epoch": 0.6293092463862501,
      "grad_norm": 0.0004038230690639466,
      "learning_rate": 0.00011120722608412497,
      "loss": 0.263,
      "step": 177800
    },
    {
      "epoch": 0.6296631885945663,
      "grad_norm": 0.0006272779428400099,
      "learning_rate": 0.00011110104342163011,
      "loss": 0.2807,
      "step": 177900
    },
    {
      "epoch": 0.6300171308028825,
      "grad_norm": 8.721996307373047,
      "learning_rate": 0.00011099486075913523,
      "loss": 0.1566,
      "step": 178000
    },
    {
      "epoch": 0.6303710730111988,
      "grad_norm": 0.34424933791160583,
      "learning_rate": 0.00011088867809664038,
      "loss": 0.2173,
      "step": 178100
    },
    {
      "epoch": 0.630725015219515,
      "grad_norm": 6.796455272706226e-05,
      "learning_rate": 0.0001107824954341455,
      "loss": 0.2132,
      "step": 178200
    },
    {
      "epoch": 0.6310789574278312,
      "grad_norm": 15.749792098999023,
      "learning_rate": 0.00011067631277165063,
      "loss": 0.3765,
      "step": 178300
    },
    {
      "epoch": 0.6314328996361475,
      "grad_norm": 68.42402648925781,
      "learning_rate": 0.00011057013010915577,
      "loss": 0.3822,
      "step": 178400
    },
    {
      "epoch": 0.6317868418444637,
      "grad_norm": 31.62430191040039,
      "learning_rate": 0.0001104639474466609,
      "loss": 0.4956,
      "step": 178500
    },
    {
      "epoch": 0.6321407840527798,
      "grad_norm": 0.0006521931500174105,
      "learning_rate": 0.00011035776478416604,
      "loss": 0.1978,
      "step": 178600
    },
    {
      "epoch": 0.632494726261096,
      "grad_norm": 0.004331324715167284,
      "learning_rate": 0.00011025158212167116,
      "loss": 0.2428,
      "step": 178700
    },
    {
      "epoch": 0.6328486684694123,
      "grad_norm": 0.00023338125902228057,
      "learning_rate": 0.0001101453994591763,
      "loss": 0.2923,
      "step": 178800
    },
    {
      "epoch": 0.6332026106777285,
      "grad_norm": 0.5951911807060242,
      "learning_rate": 0.00011003921679668143,
      "loss": 0.337,
      "step": 178900
    },
    {
      "epoch": 0.6335565528860447,
      "grad_norm": 1.3168220520019531,
      "learning_rate": 0.00010993303413418656,
      "loss": 0.2342,
      "step": 179000
    },
    {
      "epoch": 0.633910495094361,
      "grad_norm": 6.513140397146344e-05,
      "learning_rate": 0.0001098268514716917,
      "loss": 0.4176,
      "step": 179100
    },
    {
      "epoch": 0.6342644373026772,
      "grad_norm": 0.000244154070969671,
      "learning_rate": 0.00010972066880919683,
      "loss": 0.5522,
      "step": 179200
    },
    {
      "epoch": 0.6346183795109934,
      "grad_norm": 0.00032045101397670805,
      "learning_rate": 0.00010961448614670197,
      "loss": 0.2397,
      "step": 179300
    },
    {
      "epoch": 0.6349723217193096,
      "grad_norm": 0.0006160698249004781,
      "learning_rate": 0.0001095083034842071,
      "loss": 0.2495,
      "step": 179400
    },
    {
      "epoch": 0.6353262639276259,
      "grad_norm": 26.289201736450195,
      "learning_rate": 0.00010940212082171221,
      "loss": 0.1483,
      "step": 179500
    },
    {
      "epoch": 0.6356802061359421,
      "grad_norm": 0.0015526257921010256,
      "learning_rate": 0.00010929593815921736,
      "loss": 0.2645,
      "step": 179600
    },
    {
      "epoch": 0.6360341483442583,
      "grad_norm": 10.945293426513672,
      "learning_rate": 0.00010918975549672247,
      "loss": 0.3272,
      "step": 179700
    },
    {
      "epoch": 0.6363880905525746,
      "grad_norm": 0.00021797213412355632,
      "learning_rate": 0.00010908357283422763,
      "loss": 0.1852,
      "step": 179800
    },
    {
      "epoch": 0.6367420327608908,
      "grad_norm": 0.01709829457104206,
      "learning_rate": 0.00010897739017173274,
      "loss": 0.495,
      "step": 179900
    },
    {
      "epoch": 0.637095974969207,
      "grad_norm": 9.200700151268393e-05,
      "learning_rate": 0.0001088712075092379,
      "loss": 0.1239,
      "step": 180000
    },
    {
      "epoch": 0.6374499171775233,
      "grad_norm": 0.00038731162203475833,
      "learning_rate": 0.00010876502484674301,
      "loss": 0.2096,
      "step": 180100
    },
    {
      "epoch": 0.6378038593858395,
      "grad_norm": 0.48006609082221985,
      "learning_rate": 0.00010865884218424814,
      "loss": 0.1612,
      "step": 180200
    },
    {
      "epoch": 0.6381578015941557,
      "grad_norm": 0.0025218811351805925,
      "learning_rate": 0.00010855265952175328,
      "loss": 0.29,
      "step": 180300
    },
    {
      "epoch": 0.6385117438024719,
      "grad_norm": 0.002889868337661028,
      "learning_rate": 0.0001084464768592584,
      "loss": 0.2993,
      "step": 180400
    },
    {
      "epoch": 0.6388656860107882,
      "grad_norm": 0.0007285219035111368,
      "learning_rate": 0.00010834029419676354,
      "loss": 0.1926,
      "step": 180500
    },
    {
      "epoch": 0.6392196282191044,
      "grad_norm": 0.000164230921654962,
      "learning_rate": 0.00010823411153426867,
      "loss": 0.3391,
      "step": 180600
    },
    {
      "epoch": 0.6395735704274206,
      "grad_norm": 10.210373878479004,
      "learning_rate": 0.0001081279288717738,
      "loss": 0.2927,
      "step": 180700
    },
    {
      "epoch": 0.6399275126357369,
      "grad_norm": 0.0007941501680761576,
      "learning_rate": 0.00010802174620927894,
      "loss": 0.2217,
      "step": 180800
    },
    {
      "epoch": 0.6402814548440531,
      "grad_norm": 0.2510702610015869,
      "learning_rate": 0.00010791556354678407,
      "loss": 0.1486,
      "step": 180900
    },
    {
      "epoch": 0.6406353970523693,
      "grad_norm": 0.00017763013602234423,
      "learning_rate": 0.00010780938088428921,
      "loss": 0.2968,
      "step": 181000
    },
    {
      "epoch": 0.6409893392606855,
      "grad_norm": 0.3648669123649597,
      "learning_rate": 0.00010770319822179433,
      "loss": 0.2375,
      "step": 181100
    },
    {
      "epoch": 0.6413432814690018,
      "grad_norm": 0.00037799295387230814,
      "learning_rate": 0.00010759701555929946,
      "loss": 0.3204,
      "step": 181200
    },
    {
      "epoch": 0.641697223677318,
      "grad_norm": 0.0003945862699765712,
      "learning_rate": 0.0001074908328968046,
      "loss": 0.3041,
      "step": 181300
    },
    {
      "epoch": 0.6420511658856342,
      "grad_norm": 0.0001197449309984222,
      "learning_rate": 0.00010738465023430973,
      "loss": 0.2333,
      "step": 181400
    },
    {
      "epoch": 0.6424051080939505,
      "grad_norm": 0.0007370267994701862,
      "learning_rate": 0.00010727846757181487,
      "loss": 0.259,
      "step": 181500
    },
    {
      "epoch": 0.6427590503022667,
      "grad_norm": 0.002256210893392563,
      "learning_rate": 0.00010717228490932,
      "loss": 0.4305,
      "step": 181600
    },
    {
      "epoch": 0.6431129925105828,
      "grad_norm": 0.0012570054968819022,
      "learning_rate": 0.00010706610224682514,
      "loss": 0.1398,
      "step": 181700
    },
    {
      "epoch": 0.6434669347188992,
      "grad_norm": 0.009159339591860771,
      "learning_rate": 0.00010695991958433026,
      "loss": 0.3381,
      "step": 181800
    },
    {
      "epoch": 0.6438208769272153,
      "grad_norm": 61.00651168823242,
      "learning_rate": 0.00010685373692183539,
      "loss": 0.2866,
      "step": 181900
    },
    {
      "epoch": 0.6441748191355315,
      "grad_norm": 0.003991526551544666,
      "learning_rate": 0.00010674755425934053,
      "loss": 0.1912,
      "step": 182000
    },
    {
      "epoch": 0.6445287613438477,
      "grad_norm": 0.00012666918337345123,
      "learning_rate": 0.00010664137159684566,
      "loss": 0.2409,
      "step": 182100
    },
    {
      "epoch": 0.644882703552164,
      "grad_norm": 0.00017401653167326003,
      "learning_rate": 0.0001065351889343508,
      "loss": 0.1665,
      "step": 182200
    },
    {
      "epoch": 0.6452366457604802,
      "grad_norm": 1.884135127067566,
      "learning_rate": 0.00010642900627185593,
      "loss": 0.2196,
      "step": 182300
    },
    {
      "epoch": 0.6455905879687964,
      "grad_norm": 1.5884379148483276,
      "learning_rate": 0.00010632282360936105,
      "loss": 0.2919,
      "step": 182400
    },
    {
      "epoch": 0.6459445301771127,
      "grad_norm": 0.0003057966532651335,
      "learning_rate": 0.0001062166409468662,
      "loss": 0.4438,
      "step": 182500
    },
    {
      "epoch": 0.6462984723854289,
      "grad_norm": 0.00026049057487398386,
      "learning_rate": 0.00010611045828437131,
      "loss": 0.2304,
      "step": 182600
    },
    {
      "epoch": 0.6466524145937451,
      "grad_norm": 0.006750942207872868,
      "learning_rate": 0.00010600427562187646,
      "loss": 0.2723,
      "step": 182700
    },
    {
      "epoch": 0.6470063568020613,
      "grad_norm": 0.000372898590285331,
      "learning_rate": 0.00010589809295938157,
      "loss": 0.2516,
      "step": 182800
    },
    {
      "epoch": 0.6473602990103776,
      "grad_norm": 9.226272231899202e-05,
      "learning_rate": 0.00010579191029688673,
      "loss": 0.2434,
      "step": 182900
    },
    {
      "epoch": 0.6477142412186938,
      "grad_norm": 0.0019152745371684432,
      "learning_rate": 0.00010568572763439184,
      "loss": 0.1943,
      "step": 183000
    },
    {
      "epoch": 0.64806818342701,
      "grad_norm": 0.0032054262701421976,
      "learning_rate": 0.00010557954497189697,
      "loss": 0.2196,
      "step": 183100
    },
    {
      "epoch": 0.6484221256353263,
      "grad_norm": 0.0015297448262572289,
      "learning_rate": 0.00010547336230940211,
      "loss": 0.2887,
      "step": 183200
    },
    {
      "epoch": 0.6487760678436425,
      "grad_norm": 0.0023035085760056973,
      "learning_rate": 0.00010536717964690724,
      "loss": 0.3319,
      "step": 183300
    },
    {
      "epoch": 0.6491300100519587,
      "grad_norm": 0.0033646689262241125,
      "learning_rate": 0.00010526099698441238,
      "loss": 0.1362,
      "step": 183400
    },
    {
      "epoch": 0.6494839522602749,
      "grad_norm": 0.010070037096738815,
      "learning_rate": 0.0001051548143219175,
      "loss": 0.1919,
      "step": 183500
    },
    {
      "epoch": 0.6498378944685912,
      "grad_norm": 0.06341830641031265,
      "learning_rate": 0.00010504863165942263,
      "loss": 0.3247,
      "step": 183600
    },
    {
      "epoch": 0.6501918366769074,
      "grad_norm": 0.003515901044011116,
      "learning_rate": 0.00010494244899692777,
      "loss": 0.2754,
      "step": 183700
    },
    {
      "epoch": 0.6505457788852236,
      "grad_norm": 85.28102111816406,
      "learning_rate": 0.0001048362663344329,
      "loss": 0.2189,
      "step": 183800
    },
    {
      "epoch": 0.6508997210935399,
      "grad_norm": 0.022373126819729805,
      "learning_rate": 0.00010473008367193804,
      "loss": 0.1634,
      "step": 183900
    },
    {
      "epoch": 0.6512536633018561,
      "grad_norm": 0.019531290978193283,
      "learning_rate": 0.00010462390100944317,
      "loss": 0.2424,
      "step": 184000
    },
    {
      "epoch": 0.6516076055101723,
      "grad_norm": 0.0036809942685067654,
      "learning_rate": 0.0001045177183469483,
      "loss": 0.2498,
      "step": 184100
    },
    {
      "epoch": 0.6519615477184886,
      "grad_norm": 0.0030080676078796387,
      "learning_rate": 0.00010441153568445343,
      "loss": 0.3035,
      "step": 184200
    },
    {
      "epoch": 0.6523154899268048,
      "grad_norm": 0.0019591280724853277,
      "learning_rate": 0.00010430535302195856,
      "loss": 0.3076,
      "step": 184300
    },
    {
      "epoch": 0.652669432135121,
      "grad_norm": 0.20371770858764648,
      "learning_rate": 0.0001041991703594637,
      "loss": 0.2737,
      "step": 184400
    },
    {
      "epoch": 0.6530233743434372,
      "grad_norm": 0.007367484737187624,
      "learning_rate": 0.00010409298769696883,
      "loss": 0.2873,
      "step": 184500
    },
    {
      "epoch": 0.6533773165517535,
      "grad_norm": 55.854705810546875,
      "learning_rate": 0.00010398680503447397,
      "loss": 0.3638,
      "step": 184600
    },
    {
      "epoch": 0.6537312587600697,
      "grad_norm": 0.2144746482372284,
      "learning_rate": 0.0001038806223719791,
      "loss": 0.3488,
      "step": 184700
    },
    {
      "epoch": 0.6540852009683858,
      "grad_norm": 8.477124356431887e-05,
      "learning_rate": 0.00010377443970948422,
      "loss": 0.2239,
      "step": 184800
    },
    {
      "epoch": 0.6544391431767022,
      "grad_norm": 0.000621544721070677,
      "learning_rate": 0.00010366825704698936,
      "loss": 0.1498,
      "step": 184900
    },
    {
      "epoch": 0.6547930853850183,
      "grad_norm": 0.00030382987461052835,
      "learning_rate": 0.00010356207438449449,
      "loss": 0.2886,
      "step": 185000
    },
    {
      "epoch": 0.6551470275933345,
      "grad_norm": 8.43688685563393e-05,
      "learning_rate": 0.00010345589172199963,
      "loss": 0.4467,
      "step": 185100
    },
    {
      "epoch": 0.6555009698016507,
      "grad_norm": 0.02938286028802395,
      "learning_rate": 0.00010334970905950476,
      "loss": 0.3455,
      "step": 185200
    },
    {
      "epoch": 0.655854912009967,
      "grad_norm": 17.000268936157227,
      "learning_rate": 0.00010324352639700988,
      "loss": 0.1418,
      "step": 185300
    },
    {
      "epoch": 0.6562088542182832,
      "grad_norm": 0.0005483353743329644,
      "learning_rate": 0.00010313734373451503,
      "loss": 0.1156,
      "step": 185400
    },
    {
      "epoch": 0.6565627964265994,
      "grad_norm": 0.003603101707994938,
      "learning_rate": 0.00010303116107202015,
      "loss": 0.3082,
      "step": 185500
    },
    {
      "epoch": 0.6569167386349157,
      "grad_norm": 0.002953025745227933,
      "learning_rate": 0.00010292497840952529,
      "loss": 0.3788,
      "step": 185600
    },
    {
      "epoch": 0.6572706808432319,
      "grad_norm": 28.925052642822266,
      "learning_rate": 0.00010281879574703042,
      "loss": 0.2054,
      "step": 185700
    },
    {
      "epoch": 0.6576246230515481,
      "grad_norm": 5.390224396251142e-05,
      "learning_rate": 0.00010271261308453556,
      "loss": 0.3014,
      "step": 185800
    },
    {
      "epoch": 0.6579785652598644,
      "grad_norm": 12.993224143981934,
      "learning_rate": 0.00010260643042204067,
      "loss": 0.2328,
      "step": 185900
    },
    {
      "epoch": 0.6583325074681806,
      "grad_norm": 0.00025490266853012145,
      "learning_rate": 0.0001025002477595458,
      "loss": 0.3435,
      "step": 186000
    },
    {
      "epoch": 0.6586864496764968,
      "grad_norm": 0.011600878089666367,
      "learning_rate": 0.00010239406509705094,
      "loss": 0.3112,
      "step": 186100
    },
    {
      "epoch": 0.659040391884813,
      "grad_norm": 0.0007041693315841258,
      "learning_rate": 0.00010228788243455607,
      "loss": 0.1877,
      "step": 186200
    },
    {
      "epoch": 0.6593943340931293,
      "grad_norm": 2.9325779905775562e-05,
      "learning_rate": 0.00010218169977206121,
      "loss": 0.1934,
      "step": 186300
    },
    {
      "epoch": 0.6597482763014455,
      "grad_norm": 0.014352734200656414,
      "learning_rate": 0.00010207551710956634,
      "loss": 0.1818,
      "step": 186400
    },
    {
      "epoch": 0.6601022185097617,
      "grad_norm": 0.0015896501718088984,
      "learning_rate": 0.00010196933444707146,
      "loss": 0.191,
      "step": 186500
    },
    {
      "epoch": 0.660456160718078,
      "grad_norm": 0.0005087489844299853,
      "learning_rate": 0.0001018631517845766,
      "loss": 0.4503,
      "step": 186600
    },
    {
      "epoch": 0.6608101029263942,
      "grad_norm": 0.008219613693654537,
      "learning_rate": 0.00010175696912208173,
      "loss": 0.228,
      "step": 186700
    },
    {
      "epoch": 0.6611640451347104,
      "grad_norm": 22.7480525970459,
      "learning_rate": 0.00010165078645958687,
      "loss": 0.2776,
      "step": 186800
    },
    {
      "epoch": 0.6615179873430266,
      "grad_norm": 8.216926653403789e-05,
      "learning_rate": 0.000101544603797092,
      "loss": 0.224,
      "step": 186900
    },
    {
      "epoch": 0.6618719295513429,
      "grad_norm": 0.00196401821449399,
      "learning_rate": 0.00010143842113459714,
      "loss": 0.2897,
      "step": 187000
    },
    {
      "epoch": 0.6622258717596591,
      "grad_norm": 0.004564512055367231,
      "learning_rate": 0.00010133223847210227,
      "loss": 0.2445,
      "step": 187100
    },
    {
      "epoch": 0.6625798139679753,
      "grad_norm": 9.15982891456224e-05,
      "learning_rate": 0.00010122605580960739,
      "loss": 0.2513,
      "step": 187200
    },
    {
      "epoch": 0.6629337561762916,
      "grad_norm": 0.0017842076485976577,
      "learning_rate": 0.00010111987314711253,
      "loss": 0.2771,
      "step": 187300
    },
    {
      "epoch": 0.6632876983846078,
      "grad_norm": 9.778343519428745e-05,
      "learning_rate": 0.00010101369048461766,
      "loss": 0.4999,
      "step": 187400
    },
    {
      "epoch": 0.663641640592924,
      "grad_norm": 20.206926345825195,
      "learning_rate": 0.0001009075078221228,
      "loss": 0.1519,
      "step": 187500
    },
    {
      "epoch": 0.6639955828012402,
      "grad_norm": 12.828245162963867,
      "learning_rate": 0.00010080132515962793,
      "loss": 0.1815,
      "step": 187600
    },
    {
      "epoch": 0.6643495250095565,
      "grad_norm": 0.0005050048930570483,
      "learning_rate": 0.00010069514249713305,
      "loss": 0.1313,
      "step": 187700
    },
    {
      "epoch": 0.6647034672178727,
      "grad_norm": 0.08574919402599335,
      "learning_rate": 0.0001005889598346382,
      "loss": 0.2832,
      "step": 187800
    },
    {
      "epoch": 0.6650574094261889,
      "grad_norm": 0.0008294479339383543,
      "learning_rate": 0.00010048277717214332,
      "loss": 0.2839,
      "step": 187900
    },
    {
      "epoch": 0.6654113516345052,
      "grad_norm": 0.0031995882745832205,
      "learning_rate": 0.00010037659450964846,
      "loss": 0.4245,
      "step": 188000
    },
    {
      "epoch": 0.6657652938428213,
      "grad_norm": 0.0022330889478325844,
      "learning_rate": 0.00010027041184715359,
      "loss": 0.2622,
      "step": 188100
    },
    {
      "epoch": 0.6661192360511375,
      "grad_norm": 8.293439168483019e-05,
      "learning_rate": 0.00010016422918465872,
      "loss": 0.2688,
      "step": 188200
    },
    {
      "epoch": 0.6664731782594538,
      "grad_norm": 82.450927734375,
      "learning_rate": 0.00010005804652216386,
      "loss": 0.3182,
      "step": 188300
    },
    {
      "epoch": 0.66682712046777,
      "grad_norm": 0.003377897199243307,
      "learning_rate": 9.995186385966898e-05,
      "loss": 0.282,
      "step": 188400
    },
    {
      "epoch": 0.6671810626760862,
      "grad_norm": 0.0041867452673614025,
      "learning_rate": 9.984568119717413e-05,
      "loss": 0.227,
      "step": 188500
    },
    {
      "epoch": 0.6675350048844024,
      "grad_norm": 0.001010715845040977,
      "learning_rate": 9.973949853467925e-05,
      "loss": 0.2681,
      "step": 188600
    },
    {
      "epoch": 0.6678889470927187,
      "grad_norm": 0.0011596877593547106,
      "learning_rate": 9.963331587218439e-05,
      "loss": 0.2553,
      "step": 188700
    },
    {
      "epoch": 0.6682428893010349,
      "grad_norm": 0.0017722239717841148,
      "learning_rate": 9.952713320968952e-05,
      "loss": 0.2288,
      "step": 188800
    },
    {
      "epoch": 0.6685968315093511,
      "grad_norm": 0.0012692665914073586,
      "learning_rate": 9.942095054719463e-05,
      "loss": 0.1923,
      "step": 188900
    },
    {
      "epoch": 0.6689507737176674,
      "grad_norm": 0.0628691092133522,
      "learning_rate": 9.931476788469979e-05,
      "loss": 0.1974,
      "step": 189000
    },
    {
      "epoch": 0.6693047159259836,
      "grad_norm": 0.5564727187156677,
      "learning_rate": 9.92085852222049e-05,
      "loss": 0.3442,
      "step": 189100
    },
    {
      "epoch": 0.6696586581342998,
      "grad_norm": 0.0037764504086226225,
      "learning_rate": 9.910240255971005e-05,
      "loss": 0.2643,
      "step": 189200
    },
    {
      "epoch": 0.670012600342616,
      "grad_norm": 0.008034400641918182,
      "learning_rate": 9.899621989721517e-05,
      "loss": 0.3548,
      "step": 189300
    },
    {
      "epoch": 0.6703665425509323,
      "grad_norm": 1.3533165454864502,
      "learning_rate": 9.88900372347203e-05,
      "loss": 0.2119,
      "step": 189400
    },
    {
      "epoch": 0.6707204847592485,
      "grad_norm": 0.009873222559690475,
      "learning_rate": 9.878385457222544e-05,
      "loss": 0.4248,
      "step": 189500
    },
    {
      "epoch": 0.6710744269675647,
      "grad_norm": 0.04435025900602341,
      "learning_rate": 9.867767190973056e-05,
      "loss": 0.2667,
      "step": 189600
    },
    {
      "epoch": 0.671428369175881,
      "grad_norm": 0.001086053205654025,
      "learning_rate": 9.85714892472357e-05,
      "loss": 0.3082,
      "step": 189700
    },
    {
      "epoch": 0.6717823113841972,
      "grad_norm": 0.006994348485022783,
      "learning_rate": 9.846530658474083e-05,
      "loss": 0.3973,
      "step": 189800
    },
    {
      "epoch": 0.6721362535925134,
      "grad_norm": 0.06642832607030869,
      "learning_rate": 9.835912392224597e-05,
      "loss": 0.363,
      "step": 189900
    },
    {
      "epoch": 0.6724901958008297,
      "grad_norm": 55.21269989013672,
      "learning_rate": 9.82529412597511e-05,
      "loss": 0.2032,
      "step": 190000
    },
    {
      "epoch": 0.6728441380091459,
      "grad_norm": 0.03798074275255203,
      "learning_rate": 9.814675859725622e-05,
      "loss": 0.2674,
      "step": 190100
    },
    {
      "epoch": 0.6731980802174621,
      "grad_norm": 0.01672937721014023,
      "learning_rate": 9.804057593476137e-05,
      "loss": 0.1829,
      "step": 190200
    },
    {
      "epoch": 0.6735520224257783,
      "grad_norm": 0.00027075380785390735,
      "learning_rate": 9.793439327226649e-05,
      "loss": 0.3869,
      "step": 190300
    },
    {
      "epoch": 0.6739059646340946,
      "grad_norm": 36.18632888793945,
      "learning_rate": 9.782821060977163e-05,
      "loss": 0.3919,
      "step": 190400
    },
    {
      "epoch": 0.6742599068424108,
      "grad_norm": 0.0023902629036456347,
      "learning_rate": 9.772202794727676e-05,
      "loss": 0.2308,
      "step": 190500
    },
    {
      "epoch": 0.674613849050727,
      "grad_norm": 0.004745993763208389,
      "learning_rate": 9.761584528478189e-05,
      "loss": 0.2243,
      "step": 190600
    },
    {
      "epoch": 0.6749677912590433,
      "grad_norm": 0.0018392509082332253,
      "learning_rate": 9.750966262228703e-05,
      "loss": 0.2981,
      "step": 190700
    },
    {
      "epoch": 0.6753217334673595,
      "grad_norm": 0.0014820483047515154,
      "learning_rate": 9.740347995979215e-05,
      "loss": 0.169,
      "step": 190800
    },
    {
      "epoch": 0.6756756756756757,
      "grad_norm": 0.34073105454444885,
      "learning_rate": 9.72972972972973e-05,
      "loss": 0.2928,
      "step": 190900
    },
    {
      "epoch": 0.6760296178839919,
      "grad_norm": 32.37798309326172,
      "learning_rate": 9.719111463480242e-05,
      "loss": 0.4788,
      "step": 191000
    },
    {
      "epoch": 0.6763835600923082,
      "grad_norm": 0.0071701486594974995,
      "learning_rate": 9.708493197230756e-05,
      "loss": 0.2823,
      "step": 191100
    },
    {
      "epoch": 0.6767375023006243,
      "grad_norm": 50.976898193359375,
      "learning_rate": 9.697874930981269e-05,
      "loss": 0.24,
      "step": 191200
    },
    {
      "epoch": 0.6770914445089405,
      "grad_norm": 0.029482664540410042,
      "learning_rate": 9.687256664731782e-05,
      "loss": 0.2393,
      "step": 191300
    },
    {
      "epoch": 0.6774453867172568,
      "grad_norm": 0.046291522681713104,
      "learning_rate": 9.676638398482296e-05,
      "loss": 0.3574,
      "step": 191400
    },
    {
      "epoch": 0.677799328925573,
      "grad_norm": 0.0003591250570025295,
      "learning_rate": 9.666020132232808e-05,
      "loss": 0.1269,
      "step": 191500
    },
    {
      "epoch": 0.6781532711338892,
      "grad_norm": 18.194534301757812,
      "learning_rate": 9.655401865983322e-05,
      "loss": 0.3794,
      "step": 191600
    },
    {
      "epoch": 0.6785072133422055,
      "grad_norm": 0.0726270079612732,
      "learning_rate": 9.644783599733835e-05,
      "loss": 0.2272,
      "step": 191700
    },
    {
      "epoch": 0.6788611555505217,
      "grad_norm": 0.00020178276463411748,
      "learning_rate": 9.634165333484347e-05,
      "loss": 0.6517,
      "step": 191800
    },
    {
      "epoch": 0.6792150977588379,
      "grad_norm": 0.15292523801326752,
      "learning_rate": 9.623547067234862e-05,
      "loss": 0.1436,
      "step": 191900
    },
    {
      "epoch": 0.6795690399671541,
      "grad_norm": 0.0013673924840986729,
      "learning_rate": 9.612928800985373e-05,
      "loss": 0.1403,
      "step": 192000
    },
    {
      "epoch": 0.6799229821754704,
      "grad_norm": 0.0003932446416001767,
      "learning_rate": 9.602310534735889e-05,
      "loss": 0.1696,
      "step": 192100
    },
    {
      "epoch": 0.6802769243837866,
      "grad_norm": 34.16385269165039,
      "learning_rate": 9.5916922684864e-05,
      "loss": 0.3223,
      "step": 192200
    },
    {
      "epoch": 0.6806308665921028,
      "grad_norm": 0.33474764227867126,
      "learning_rate": 9.581074002236913e-05,
      "loss": 0.2455,
      "step": 192300
    },
    {
      "epoch": 0.6809848088004191,
      "grad_norm": 0.00013010464317630976,
      "learning_rate": 9.570455735987427e-05,
      "loss": 0.181,
      "step": 192400
    },
    {
      "epoch": 0.6813387510087353,
      "grad_norm": 0.00013065226085018367,
      "learning_rate": 9.55983746973794e-05,
      "loss": 0.265,
      "step": 192500
    },
    {
      "epoch": 0.6816926932170515,
      "grad_norm": 0.00634736567735672,
      "learning_rate": 9.549219203488454e-05,
      "loss": 0.0847,
      "step": 192600
    },
    {
      "epoch": 0.6820466354253677,
      "grad_norm": 0.002523854374885559,
      "learning_rate": 9.538600937238966e-05,
      "loss": 0.3677,
      "step": 192700
    },
    {
      "epoch": 0.682400577633684,
      "grad_norm": 0.00028111765277571976,
      "learning_rate": 9.52798267098948e-05,
      "loss": 0.1192,
      "step": 192800
    },
    {
      "epoch": 0.6827545198420002,
      "grad_norm": 0.00033063720911741257,
      "learning_rate": 9.517364404739993e-05,
      "loss": 0.2556,
      "step": 192900
    },
    {
      "epoch": 0.6831084620503164,
      "grad_norm": 0.0037081455811858177,
      "learning_rate": 9.506746138490506e-05,
      "loss": 0.2641,
      "step": 193000
    },
    {
      "epoch": 0.6834624042586327,
      "grad_norm": 0.23532195389270782,
      "learning_rate": 9.49612787224102e-05,
      "loss": 0.3207,
      "step": 193100
    },
    {
      "epoch": 0.6838163464669489,
      "grad_norm": 0.00301129138097167,
      "learning_rate": 9.485509605991532e-05,
      "loss": 0.2143,
      "step": 193200
    },
    {
      "epoch": 0.6841702886752651,
      "grad_norm": 0.0008533395593985915,
      "learning_rate": 9.474891339742046e-05,
      "loss": 0.1564,
      "step": 193300
    },
    {
      "epoch": 0.6845242308835813,
      "grad_norm": 0.00664127804338932,
      "learning_rate": 9.464273073492559e-05,
      "loss": 0.3041,
      "step": 193400
    },
    {
      "epoch": 0.6848781730918976,
      "grad_norm": 0.0002653513802215457,
      "learning_rate": 9.453654807243072e-05,
      "loss": 0.4408,
      "step": 193500
    },
    {
      "epoch": 0.6852321153002138,
      "grad_norm": 0.00477770296856761,
      "learning_rate": 9.443036540993586e-05,
      "loss": 0.4563,
      "step": 193600
    },
    {
      "epoch": 0.68558605750853,
      "grad_norm": 0.0003456194244790822,
      "learning_rate": 9.432418274744099e-05,
      "loss": 0.0686,
      "step": 193700
    },
    {
      "epoch": 0.6859399997168463,
      "grad_norm": 93.06956481933594,
      "learning_rate": 9.421800008494613e-05,
      "loss": 0.3363,
      "step": 193800
    },
    {
      "epoch": 0.6862939419251625,
      "grad_norm": 0.0018949456280097365,
      "learning_rate": 9.411181742245125e-05,
      "loss": 0.1603,
      "step": 193900
    },
    {
      "epoch": 0.6866478841334787,
      "grad_norm": 17.98063850402832,
      "learning_rate": 9.40056347599564e-05,
      "loss": 0.1838,
      "step": 194000
    },
    {
      "epoch": 0.687001826341795,
      "grad_norm": 4.263315200805664,
      "learning_rate": 9.389945209746152e-05,
      "loss": 0.3415,
      "step": 194100
    },
    {
      "epoch": 0.6873557685501112,
      "grad_norm": 0.007608375977724791,
      "learning_rate": 9.379326943496665e-05,
      "loss": 0.3769,
      "step": 194200
    },
    {
      "epoch": 0.6877097107584274,
      "grad_norm": 0.0004946698900312185,
      "learning_rate": 9.368708677247179e-05,
      "loss": 0.1534,
      "step": 194300
    },
    {
      "epoch": 0.6880636529667435,
      "grad_norm": 0.0006228742422536016,
      "learning_rate": 9.358090410997692e-05,
      "loss": 0.2802,
      "step": 194400
    },
    {
      "epoch": 0.6884175951750598,
      "grad_norm": 0.01082155853509903,
      "learning_rate": 9.347472144748206e-05,
      "loss": 0.2901,
      "step": 194500
    },
    {
      "epoch": 0.688771537383376,
      "grad_norm": 0.007087044883519411,
      "learning_rate": 9.336853878498718e-05,
      "loss": 0.2284,
      "step": 194600
    },
    {
      "epoch": 0.6891254795916922,
      "grad_norm": 0.0005083183641545475,
      "learning_rate": 9.326235612249231e-05,
      "loss": 0.1961,
      "step": 194700
    },
    {
      "epoch": 0.6894794218000085,
      "grad_norm": 0.0004262541770003736,
      "learning_rate": 9.315617345999745e-05,
      "loss": 0.2088,
      "step": 194800
    },
    {
      "epoch": 0.6898333640083247,
      "grad_norm": 0.000559193198569119,
      "learning_rate": 9.304999079750258e-05,
      "loss": 0.2807,
      "step": 194900
    },
    {
      "epoch": 0.6901873062166409,
      "grad_norm": 33.73904037475586,
      "learning_rate": 9.294380813500772e-05,
      "loss": 0.3446,
      "step": 195000
    },
    {
      "epoch": 0.6905412484249571,
      "grad_norm": 16.455341339111328,
      "learning_rate": 9.283762547251283e-05,
      "loss": 0.2946,
      "step": 195100
    },
    {
      "epoch": 0.6908951906332734,
      "grad_norm": 0.015101075172424316,
      "learning_rate": 9.273144281001796e-05,
      "loss": 0.2041,
      "step": 195200
    },
    {
      "epoch": 0.6912491328415896,
      "grad_norm": 46.9390869140625,
      "learning_rate": 9.26252601475231e-05,
      "loss": 0.3613,
      "step": 195300
    },
    {
      "epoch": 0.6916030750499058,
      "grad_norm": 0.031250737607479095,
      "learning_rate": 9.251907748502823e-05,
      "loss": 0.4402,
      "step": 195400
    },
    {
      "epoch": 0.6919570172582221,
      "grad_norm": 0.0005934244836680591,
      "learning_rate": 9.241289482253337e-05,
      "loss": 0.1371,
      "step": 195500
    },
    {
      "epoch": 0.6923109594665383,
      "grad_norm": 0.0002576820261310786,
      "learning_rate": 9.23067121600385e-05,
      "loss": 0.0607,
      "step": 195600
    },
    {
      "epoch": 0.6926649016748545,
      "grad_norm": 0.004434580449014902,
      "learning_rate": 9.220052949754363e-05,
      "loss": 0.282,
      "step": 195700
    },
    {
      "epoch": 0.6930188438831708,
      "grad_norm": 0.000268304196652025,
      "learning_rate": 9.209434683504876e-05,
      "loss": 0.2223,
      "step": 195800
    },
    {
      "epoch": 0.693372786091487,
      "grad_norm": 0.012623433023691177,
      "learning_rate": 9.198816417255389e-05,
      "loss": 0.1214,
      "step": 195900
    },
    {
      "epoch": 0.6937267282998032,
      "grad_norm": 0.002067705150693655,
      "learning_rate": 9.188198151005903e-05,
      "loss": 0.2376,
      "step": 196000
    },
    {
      "epoch": 0.6940806705081194,
      "grad_norm": 0.0012965495698153973,
      "learning_rate": 9.177579884756416e-05,
      "loss": 0.3819,
      "step": 196100
    },
    {
      "epoch": 0.6944346127164357,
      "grad_norm": 34.50071716308594,
      "learning_rate": 9.16696161850693e-05,
      "loss": 0.1899,
      "step": 196200
    },
    {
      "epoch": 0.6947885549247519,
      "grad_norm": 0.07035639882087708,
      "learning_rate": 9.156343352257442e-05,
      "loss": 0.127,
      "step": 196300
    },
    {
      "epoch": 0.6951424971330681,
      "grad_norm": 0.01296608429402113,
      "learning_rate": 9.145725086007955e-05,
      "loss": 0.2375,
      "step": 196400
    },
    {
      "epoch": 0.6954964393413844,
      "grad_norm": 16.85318946838379,
      "learning_rate": 9.135106819758469e-05,
      "loss": 0.3769,
      "step": 196500
    },
    {
      "epoch": 0.6958503815497006,
      "grad_norm": 0.0025036572478711605,
      "learning_rate": 9.124488553508982e-05,
      "loss": 0.2947,
      "step": 196600
    },
    {
      "epoch": 0.6962043237580168,
      "grad_norm": 0.044049471616744995,
      "learning_rate": 9.113870287259496e-05,
      "loss": 0.2771,
      "step": 196700
    },
    {
      "epoch": 0.696558265966333,
      "grad_norm": 0.0030539778526872396,
      "learning_rate": 9.103252021010009e-05,
      "loss": 0.3569,
      "step": 196800
    },
    {
      "epoch": 0.6969122081746493,
      "grad_norm": 2.887782335281372,
      "learning_rate": 9.092633754760523e-05,
      "loss": 0.2789,
      "step": 196900
    },
    {
      "epoch": 0.6972661503829655,
      "grad_norm": 0.016591878607869148,
      "learning_rate": 9.082015488511035e-05,
      "loss": 0.3079,
      "step": 197000
    },
    {
      "epoch": 0.6976200925912817,
      "grad_norm": 0.00010996893252013251,
      "learning_rate": 9.071397222261548e-05,
      "loss": 0.1819,
      "step": 197100
    },
    {
      "epoch": 0.697974034799598,
      "grad_norm": 0.01280361320823431,
      "learning_rate": 9.060778956012062e-05,
      "loss": 0.2951,
      "step": 197200
    },
    {
      "epoch": 0.6983279770079142,
      "grad_norm": 0.1630173772573471,
      "learning_rate": 9.050160689762575e-05,
      "loss": 0.2375,
      "step": 197300
    },
    {
      "epoch": 0.6986819192162304,
      "grad_norm": 0.01587500236928463,
      "learning_rate": 9.039542423513089e-05,
      "loss": 0.3378,
      "step": 197400
    },
    {
      "epoch": 0.6990358614245465,
      "grad_norm": 0.0578685961663723,
      "learning_rate": 9.028924157263602e-05,
      "loss": 0.2506,
      "step": 197500
    },
    {
      "epoch": 0.6993898036328628,
      "grad_norm": 0.0006471314118243754,
      "learning_rate": 9.018305891014114e-05,
      "loss": 0.1535,
      "step": 197600
    },
    {
      "epoch": 0.699743745841179,
      "grad_norm": 0.32236069440841675,
      "learning_rate": 9.007687624764628e-05,
      "loss": 0.2279,
      "step": 197700
    },
    {
      "epoch": 0.7000976880494952,
      "grad_norm": 18.765499114990234,
      "learning_rate": 8.997069358515141e-05,
      "loss": 0.1911,
      "step": 197800
    },
    {
      "epoch": 0.7004516302578115,
      "grad_norm": 0.02663063444197178,
      "learning_rate": 8.986451092265655e-05,
      "loss": 0.0986,
      "step": 197900
    },
    {
      "epoch": 0.7008055724661277,
      "grad_norm": 0.005335270427167416,
      "learning_rate": 8.975832826016168e-05,
      "loss": 0.2629,
      "step": 198000
    },
    {
      "epoch": 0.7011595146744439,
      "grad_norm": 0.0001487065019318834,
      "learning_rate": 8.965214559766679e-05,
      "loss": 0.0926,
      "step": 198100
    },
    {
      "epoch": 0.7015134568827602,
      "grad_norm": 0.0009711019811220467,
      "learning_rate": 8.954596293517195e-05,
      "loss": 0.1806,
      "step": 198200
    },
    {
      "epoch": 0.7018673990910764,
      "grad_norm": 0.00043793945224024355,
      "learning_rate": 8.943978027267706e-05,
      "loss": 0.336,
      "step": 198300
    },
    {
      "epoch": 0.7022213412993926,
      "grad_norm": 0.04236992448568344,
      "learning_rate": 8.93335976101822e-05,
      "loss": 0.2362,
      "step": 198400
    },
    {
      "epoch": 0.7025752835077088,
      "grad_norm": 0.058468833565711975,
      "learning_rate": 8.922741494768733e-05,
      "loss": 0.2258,
      "step": 198500
    },
    {
      "epoch": 0.7029292257160251,
      "grad_norm": 0.0003536749863997102,
      "learning_rate": 8.912123228519247e-05,
      "loss": 0.2792,
      "step": 198600
    },
    {
      "epoch": 0.7032831679243413,
      "grad_norm": 0.0011344882659614086,
      "learning_rate": 8.90150496226976e-05,
      "loss": 0.2255,
      "step": 198700
    },
    {
      "epoch": 0.7036371101326575,
      "grad_norm": 0.003825961844995618,
      "learning_rate": 8.890886696020272e-05,
      "loss": 0.2305,
      "step": 198800
    },
    {
      "epoch": 0.7039910523409738,
      "grad_norm": 9.785925067262724e-05,
      "learning_rate": 8.880268429770786e-05,
      "loss": 0.3346,
      "step": 198900
    },
    {
      "epoch": 0.70434499454929,
      "grad_norm": 0.012345508672297001,
      "learning_rate": 8.869650163521299e-05,
      "loss": 0.2216,
      "step": 199000
    },
    {
      "epoch": 0.7046989367576062,
      "grad_norm": 0.00010956970072584227,
      "learning_rate": 8.859031897271813e-05,
      "loss": 0.1494,
      "step": 199100
    },
    {
      "epoch": 0.7050528789659224,
      "grad_norm": 0.0036420233082026243,
      "learning_rate": 8.848413631022326e-05,
      "loss": 0.2496,
      "step": 199200
    },
    {
      "epoch": 0.7054068211742387,
      "grad_norm": 0.0035377817694097757,
      "learning_rate": 8.837795364772838e-05,
      "loss": 0.2022,
      "step": 199300
    },
    {
      "epoch": 0.7057607633825549,
      "grad_norm": 0.4015532433986664,
      "learning_rate": 8.827177098523352e-05,
      "loss": 0.1689,
      "step": 199400
    },
    {
      "epoch": 0.7061147055908711,
      "grad_norm": 0.0010079366620630026,
      "learning_rate": 8.816558832273865e-05,
      "loss": 0.2806,
      "step": 199500
    },
    {
      "epoch": 0.7064686477991874,
      "grad_norm": 0.0005541431019082665,
      "learning_rate": 8.805940566024379e-05,
      "loss": 0.2888,
      "step": 199600
    },
    {
      "epoch": 0.7068225900075036,
      "grad_norm": 0.0007874089060351253,
      "learning_rate": 8.795322299774892e-05,
      "loss": 0.2674,
      "step": 199700
    },
    {
      "epoch": 0.7071765322158198,
      "grad_norm": 0.00011199902655789629,
      "learning_rate": 8.784704033525406e-05,
      "loss": 0.2249,
      "step": 199800
    },
    {
      "epoch": 0.7075304744241361,
      "grad_norm": 0.0007496355101466179,
      "learning_rate": 8.774085767275919e-05,
      "loss": 0.1381,
      "step": 199900
    },
    {
      "epoch": 0.7078844166324523,
      "grad_norm": 0.0030501200817525387,
      "learning_rate": 8.763467501026431e-05,
      "loss": 0.3489,
      "step": 200000
    },
    {
      "epoch": 0.7082383588407685,
      "grad_norm": 6.853540980955586e-05,
      "learning_rate": 8.752849234776945e-05,
      "loss": 0.1948,
      "step": 200100
    },
    {
      "epoch": 0.7085923010490847,
      "grad_norm": 0.003635452361777425,
      "learning_rate": 8.742230968527458e-05,
      "loss": 0.2062,
      "step": 200200
    },
    {
      "epoch": 0.708946243257401,
      "grad_norm": 44.39374923706055,
      "learning_rate": 8.731612702277972e-05,
      "loss": 0.427,
      "step": 200300
    },
    {
      "epoch": 0.7093001854657172,
      "grad_norm": 0.00038310469244606793,
      "learning_rate": 8.720994436028485e-05,
      "loss": 0.2679,
      "step": 200400
    },
    {
      "epoch": 0.7096541276740334,
      "grad_norm": 0.011085325852036476,
      "learning_rate": 8.710376169778997e-05,
      "loss": 0.2422,
      "step": 200500
    },
    {
      "epoch": 0.7100080698823497,
      "grad_norm": 0.0002500386326573789,
      "learning_rate": 8.699757903529512e-05,
      "loss": 0.3598,
      "step": 200600
    },
    {
      "epoch": 0.7103620120906659,
      "grad_norm": 0.009363035671412945,
      "learning_rate": 8.689139637280024e-05,
      "loss": 0.2551,
      "step": 200700
    },
    {
      "epoch": 0.710715954298982,
      "grad_norm": 0.0016771778464317322,
      "learning_rate": 8.678521371030538e-05,
      "loss": 0.1821,
      "step": 200800
    },
    {
      "epoch": 0.7110698965072982,
      "grad_norm": 0.0006162702920846641,
      "learning_rate": 8.667903104781051e-05,
      "loss": 0.2637,
      "step": 200900
    },
    {
      "epoch": 0.7114238387156145,
      "grad_norm": 0.0004086168191861361,
      "learning_rate": 8.657284838531565e-05,
      "loss": 0.3227,
      "step": 201000
    },
    {
      "epoch": 0.7117777809239307,
      "grad_norm": 0.020510252565145493,
      "learning_rate": 8.646666572282078e-05,
      "loss": 0.2922,
      "step": 201100
    },
    {
      "epoch": 0.7121317231322469,
      "grad_norm": 0.000636436278000474,
      "learning_rate": 8.636048306032589e-05,
      "loss": 0.1287,
      "step": 201200
    },
    {
      "epoch": 0.7124856653405632,
      "grad_norm": 0.015321267768740654,
      "learning_rate": 8.625430039783105e-05,
      "loss": 0.0639,
      "step": 201300
    },
    {
      "epoch": 0.7128396075488794,
      "grad_norm": 0.0021660933271050453,
      "learning_rate": 8.614811773533616e-05,
      "loss": 0.3357,
      "step": 201400
    },
    {
      "epoch": 0.7131935497571956,
      "grad_norm": 0.5845043659210205,
      "learning_rate": 8.604193507284131e-05,
      "loss": 0.1645,
      "step": 201500
    },
    {
      "epoch": 0.7135474919655119,
      "grad_norm": 0.003407104639336467,
      "learning_rate": 8.593575241034643e-05,
      "loss": 0.2537,
      "step": 201600
    },
    {
      "epoch": 0.7139014341738281,
      "grad_norm": 0.9495242238044739,
      "learning_rate": 8.582956974785155e-05,
      "loss": 0.1526,
      "step": 201700
    },
    {
      "epoch": 0.7142553763821443,
      "grad_norm": 0.0015173119027167559,
      "learning_rate": 8.57233870853567e-05,
      "loss": 0.144,
      "step": 201800
    },
    {
      "epoch": 0.7146093185904605,
      "grad_norm": 0.001408001990057528,
      "learning_rate": 8.561720442286182e-05,
      "loss": 0.2754,
      "step": 201900
    },
    {
      "epoch": 0.7149632607987768,
      "grad_norm": 0.1529581993818283,
      "learning_rate": 8.551102176036696e-05,
      "loss": 0.1819,
      "step": 202000
    },
    {
      "epoch": 0.715317203007093,
      "grad_norm": 0.2667132318019867,
      "learning_rate": 8.540483909787209e-05,
      "loss": 0.2072,
      "step": 202100
    },
    {
      "epoch": 0.7156711452154092,
      "grad_norm": 0.0024000522680580616,
      "learning_rate": 8.529865643537722e-05,
      "loss": 0.1514,
      "step": 202200
    },
    {
      "epoch": 0.7160250874237255,
      "grad_norm": 20.562379837036133,
      "learning_rate": 8.519247377288236e-05,
      "loss": 0.2821,
      "step": 202300
    },
    {
      "epoch": 0.7163790296320417,
      "grad_norm": 0.00042199587915092707,
      "learning_rate": 8.508629111038748e-05,
      "loss": 0.2894,
      "step": 202400
    },
    {
      "epoch": 0.7167329718403579,
      "grad_norm": 0.001291036605834961,
      "learning_rate": 8.498010844789262e-05,
      "loss": 0.1969,
      "step": 202500
    },
    {
      "epoch": 0.7170869140486741,
      "grad_norm": 0.0012664365349337459,
      "learning_rate": 8.487392578539775e-05,
      "loss": 0.2843,
      "step": 202600
    },
    {
      "epoch": 0.7174408562569904,
      "grad_norm": 0.002638416364789009,
      "learning_rate": 8.476774312290289e-05,
      "loss": 0.1786,
      "step": 202700
    },
    {
      "epoch": 0.7177947984653066,
      "grad_norm": 0.001100375084206462,
      "learning_rate": 8.466156046040802e-05,
      "loss": 0.1456,
      "step": 202800
    },
    {
      "epoch": 0.7181487406736228,
      "grad_norm": 0.0009789428440853953,
      "learning_rate": 8.455537779791314e-05,
      "loss": 0.2771,
      "step": 202900
    },
    {
      "epoch": 0.7185026828819391,
      "grad_norm": 0.000428346247645095,
      "learning_rate": 8.444919513541829e-05,
      "loss": 0.096,
      "step": 203000
    },
    {
      "epoch": 0.7188566250902553,
      "grad_norm": 0.009075594134628773,
      "learning_rate": 8.434301247292341e-05,
      "loss": 0.1654,
      "step": 203100
    },
    {
      "epoch": 0.7192105672985715,
      "grad_norm": 0.0018976149149239063,
      "learning_rate": 8.423682981042855e-05,
      "loss": 0.1961,
      "step": 203200
    },
    {
      "epoch": 0.7195645095068877,
      "grad_norm": 0.00022906129015609622,
      "learning_rate": 8.413064714793368e-05,
      "loss": 0.3218,
      "step": 203300
    },
    {
      "epoch": 0.719918451715204,
      "grad_norm": 94.13921356201172,
      "learning_rate": 8.402446448543881e-05,
      "loss": 0.2988,
      "step": 203400
    },
    {
      "epoch": 0.7202723939235202,
      "grad_norm": 0.0015759795205667615,
      "learning_rate": 8.391828182294395e-05,
      "loss": 0.1195,
      "step": 203500
    },
    {
      "epoch": 0.7206263361318364,
      "grad_norm": 0.5444590449333191,
      "learning_rate": 8.381209916044907e-05,
      "loss": 0.2161,
      "step": 203600
    },
    {
      "epoch": 0.7209802783401527,
      "grad_norm": 0.00186438939999789,
      "learning_rate": 8.370591649795421e-05,
      "loss": 0.3555,
      "step": 203700
    },
    {
      "epoch": 0.7213342205484689,
      "grad_norm": 0.302142858505249,
      "learning_rate": 8.359973383545934e-05,
      "loss": 0.3807,
      "step": 203800
    },
    {
      "epoch": 0.721688162756785,
      "grad_norm": 0.08408133685588837,
      "learning_rate": 8.349355117296448e-05,
      "loss": 0.3688,
      "step": 203900
    },
    {
      "epoch": 0.7220421049651014,
      "grad_norm": 0.0044722831808030605,
      "learning_rate": 8.338736851046961e-05,
      "loss": 0.3142,
      "step": 204000
    },
    {
      "epoch": 0.7223960471734175,
      "grad_norm": 0.10382309556007385,
      "learning_rate": 8.328118584797472e-05,
      "loss": 0.3162,
      "step": 204100
    },
    {
      "epoch": 0.7227499893817337,
      "grad_norm": 0.01846904121339321,
      "learning_rate": 8.317500318547988e-05,
      "loss": 0.2889,
      "step": 204200
    },
    {
      "epoch": 0.7231039315900499,
      "grad_norm": 9.68227323028259e-05,
      "learning_rate": 8.306882052298499e-05,
      "loss": 0.2293,
      "step": 204300
    },
    {
      "epoch": 0.7234578737983662,
      "grad_norm": 0.003430056618526578,
      "learning_rate": 8.296263786049014e-05,
      "loss": 0.155,
      "step": 204400
    },
    {
      "epoch": 0.7238118160066824,
      "grad_norm": 0.010960621759295464,
      "learning_rate": 8.285645519799526e-05,
      "loss": 0.419,
      "step": 204500
    },
    {
      "epoch": 0.7241657582149986,
      "grad_norm": 0.0004493790911510587,
      "learning_rate": 8.275027253550039e-05,
      "loss": 0.2657,
      "step": 204600
    },
    {
      "epoch": 0.7245197004233149,
      "grad_norm": 0.00011129827180411667,
      "learning_rate": 8.264408987300553e-05,
      "loss": 0.3186,
      "step": 204700
    },
    {
      "epoch": 0.7248736426316311,
      "grad_norm": 0.07489307224750519,
      "learning_rate": 8.253790721051065e-05,
      "loss": 0.2769,
      "step": 204800
    },
    {
      "epoch": 0.7252275848399473,
      "grad_norm": 0.01801426149904728,
      "learning_rate": 8.243172454801579e-05,
      "loss": 0.182,
      "step": 204900
    },
    {
      "epoch": 0.7255815270482635,
      "grad_norm": 0.03560563549399376,
      "learning_rate": 8.232554188552092e-05,
      "loss": 0.2142,
      "step": 205000
    },
    {
      "epoch": 0.7259354692565798,
      "grad_norm": 0.00016551597218494862,
      "learning_rate": 8.221935922302605e-05,
      "loss": 0.2269,
      "step": 205100
    },
    {
      "epoch": 0.726289411464896,
      "grad_norm": 0.0005871237954124808,
      "learning_rate": 8.211317656053119e-05,
      "loss": 0.1619,
      "step": 205200
    },
    {
      "epoch": 0.7266433536732122,
      "grad_norm": 0.004314428195357323,
      "learning_rate": 8.200699389803631e-05,
      "loss": 0.1822,
      "step": 205300
    },
    {
      "epoch": 0.7269972958815285,
      "grad_norm": 0.029433676972985268,
      "learning_rate": 8.190081123554146e-05,
      "loss": 0.2103,
      "step": 205400
    },
    {
      "epoch": 0.7273512380898447,
      "grad_norm": 1.0377253293991089,
      "learning_rate": 8.179462857304658e-05,
      "loss": 0.3312,
      "step": 205500
    },
    {
      "epoch": 0.7277051802981609,
      "grad_norm": 44.63011169433594,
      "learning_rate": 8.168844591055172e-05,
      "loss": 0.2847,
      "step": 205600
    },
    {
      "epoch": 0.7280591225064772,
      "grad_norm": 0.08027029037475586,
      "learning_rate": 8.158226324805685e-05,
      "loss": 0.2033,
      "step": 205700
    },
    {
      "epoch": 0.7284130647147934,
      "grad_norm": 0.00454784044995904,
      "learning_rate": 8.147608058556198e-05,
      "loss": 0.1534,
      "step": 205800
    },
    {
      "epoch": 0.7287670069231096,
      "grad_norm": 0.11458806693553925,
      "learning_rate": 8.136989792306712e-05,
      "loss": 0.1784,
      "step": 205900
    },
    {
      "epoch": 0.7291209491314258,
      "grad_norm": 17.320762634277344,
      "learning_rate": 8.126371526057224e-05,
      "loss": 0.2925,
      "step": 206000
    },
    {
      "epoch": 0.7294748913397421,
      "grad_norm": 0.0005885896389372647,
      "learning_rate": 8.115753259807738e-05,
      "loss": 0.2655,
      "step": 206100
    },
    {
      "epoch": 0.7298288335480583,
      "grad_norm": 0.00021692417794838548,
      "learning_rate": 8.105134993558251e-05,
      "loss": 0.1688,
      "step": 206200
    },
    {
      "epoch": 0.7301827757563745,
      "grad_norm": 19.95318603515625,
      "learning_rate": 8.094516727308764e-05,
      "loss": 0.1972,
      "step": 206300
    },
    {
      "epoch": 0.7305367179646908,
      "grad_norm": 0.00010345642658649012,
      "learning_rate": 8.083898461059278e-05,
      "loss": 0.3063,
      "step": 206400
    },
    {
      "epoch": 0.730890660173007,
      "grad_norm": 0.0003552952839527279,
      "learning_rate": 8.07328019480979e-05,
      "loss": 0.2991,
      "step": 206500
    },
    {
      "epoch": 0.7312446023813232,
      "grad_norm": 0.00011682923650369048,
      "learning_rate": 8.062661928560305e-05,
      "loss": 0.0859,
      "step": 206600
    },
    {
      "epoch": 0.7315985445896394,
      "grad_norm": 0.0009983661584556103,
      "learning_rate": 8.052043662310817e-05,
      "loss": 0.2371,
      "step": 206700
    },
    {
      "epoch": 0.7319524867979557,
      "grad_norm": 0.0002566467737779021,
      "learning_rate": 8.041425396061331e-05,
      "loss": 0.2507,
      "step": 206800
    },
    {
      "epoch": 0.7323064290062719,
      "grad_norm": 1.1363481283187866,
      "learning_rate": 8.030807129811844e-05,
      "loss": 0.1621,
      "step": 206900
    },
    {
      "epoch": 0.732660371214588,
      "grad_norm": 46.49571990966797,
      "learning_rate": 8.020188863562357e-05,
      "loss": 0.2363,
      "step": 207000
    },
    {
      "epoch": 0.7330143134229044,
      "grad_norm": 0.015102296136319637,
      "learning_rate": 8.009570597312871e-05,
      "loss": 0.2689,
      "step": 207100
    },
    {
      "epoch": 0.7333682556312205,
      "grad_norm": 29.148860931396484,
      "learning_rate": 7.998952331063384e-05,
      "loss": 0.4492,
      "step": 207200
    },
    {
      "epoch": 0.7337221978395367,
      "grad_norm": 0.14118210971355438,
      "learning_rate": 7.988334064813898e-05,
      "loss": 0.1788,
      "step": 207300
    },
    {
      "epoch": 0.7340761400478529,
      "grad_norm": 0.0008051999029703438,
      "learning_rate": 7.977715798564409e-05,
      "loss": 0.3536,
      "step": 207400
    },
    {
      "epoch": 0.7344300822561692,
      "grad_norm": 0.00230596330948174,
      "learning_rate": 7.967097532314922e-05,
      "loss": 0.3306,
      "step": 207500
    },
    {
      "epoch": 0.7347840244644854,
      "grad_norm": 0.0009529136586934328,
      "learning_rate": 7.956479266065436e-05,
      "loss": 0.344,
      "step": 207600
    },
    {
      "epoch": 0.7351379666728016,
      "grad_norm": 0.000791988626588136,
      "learning_rate": 7.945860999815948e-05,
      "loss": 0.2405,
      "step": 207700
    },
    {
      "epoch": 0.7354919088811179,
      "grad_norm": 0.013330555520951748,
      "learning_rate": 7.935242733566463e-05,
      "loss": 0.1254,
      "step": 207800
    },
    {
      "epoch": 0.7358458510894341,
      "grad_norm": 10.19345760345459,
      "learning_rate": 7.924624467316975e-05,
      "loss": 0.2484,
      "step": 207900
    },
    {
      "epoch": 0.7361997932977503,
      "grad_norm": 0.0002550467033870518,
      "learning_rate": 7.914006201067489e-05,
      "loss": 0.1493,
      "step": 208000
    },
    {
      "epoch": 0.7365537355060666,
      "grad_norm": 0.001338966190814972,
      "learning_rate": 7.903387934818002e-05,
      "loss": 0.2407,
      "step": 208100
    },
    {
      "epoch": 0.7369076777143828,
      "grad_norm": 0.026292594149708748,
      "learning_rate": 7.892769668568515e-05,
      "loss": 0.2211,
      "step": 208200
    },
    {
      "epoch": 0.737261619922699,
      "grad_norm": 0.0011428133584558964,
      "learning_rate": 7.882151402319029e-05,
      "loss": 0.3208,
      "step": 208300
    },
    {
      "epoch": 0.7376155621310152,
      "grad_norm": 0.010934821330010891,
      "learning_rate": 7.871533136069541e-05,
      "loss": 0.1569,
      "step": 208400
    },
    {
      "epoch": 0.7379695043393315,
      "grad_norm": 0.001163730164989829,
      "learning_rate": 7.860914869820055e-05,
      "loss": 0.2246,
      "step": 208500
    },
    {
      "epoch": 0.7383234465476477,
      "grad_norm": 49.87257766723633,
      "learning_rate": 7.850296603570568e-05,
      "loss": 0.2877,
      "step": 208600
    },
    {
      "epoch": 0.7386773887559639,
      "grad_norm": 0.012331600300967693,
      "learning_rate": 7.839678337321081e-05,
      "loss": 0.1521,
      "step": 208700
    },
    {
      "epoch": 0.7390313309642802,
      "grad_norm": 0.003912034444510937,
      "learning_rate": 7.829060071071595e-05,
      "loss": 0.1782,
      "step": 208800
    },
    {
      "epoch": 0.7393852731725964,
      "grad_norm": 0.0020990234334021807,
      "learning_rate": 7.818441804822108e-05,
      "loss": 0.1914,
      "step": 208900
    },
    {
      "epoch": 0.7397392153809126,
      "grad_norm": 0.0022508222609758377,
      "learning_rate": 7.807823538572622e-05,
      "loss": 0.2395,
      "step": 209000
    },
    {
      "epoch": 0.7400931575892288,
      "grad_norm": 0.0009663173113949597,
      "learning_rate": 7.797205272323134e-05,
      "loss": 0.1198,
      "step": 209100
    },
    {
      "epoch": 0.7404470997975451,
      "grad_norm": 0.0010909237898886204,
      "learning_rate": 7.786587006073647e-05,
      "loss": 0.0788,
      "step": 209200
    },
    {
      "epoch": 0.7408010420058613,
      "grad_norm": 0.0005987293552607298,
      "learning_rate": 7.775968739824161e-05,
      "loss": 0.1993,
      "step": 209300
    },
    {
      "epoch": 0.7411549842141775,
      "grad_norm": 0.0001992560864891857,
      "learning_rate": 7.765350473574674e-05,
      "loss": 0.1238,
      "step": 209400
    },
    {
      "epoch": 0.7415089264224938,
      "grad_norm": 0.0006686078850179911,
      "learning_rate": 7.754732207325188e-05,
      "loss": 0.1822,
      "step": 209500
    },
    {
      "epoch": 0.74186286863081,
      "grad_norm": 0.00024252836010418832,
      "learning_rate": 7.7441139410757e-05,
      "loss": 0.1429,
      "step": 209600
    },
    {
      "epoch": 0.7422168108391262,
      "grad_norm": 8.518699645996094,
      "learning_rate": 7.733495674826215e-05,
      "loss": 0.1594,
      "step": 209700
    },
    {
      "epoch": 0.7425707530474425,
      "grad_norm": 8.097256660461426,
      "learning_rate": 7.722877408576727e-05,
      "loss": 0.1495,
      "step": 209800
    },
    {
      "epoch": 0.7429246952557587,
      "grad_norm": 10.084461212158203,
      "learning_rate": 7.71225914232724e-05,
      "loss": 0.4291,
      "step": 209900
    },
    {
      "epoch": 0.7432786374640749,
      "grad_norm": 0.28961730003356934,
      "learning_rate": 7.701640876077754e-05,
      "loss": 0.1577,
      "step": 210000
    },
    {
      "epoch": 0.743632579672391,
      "grad_norm": 0.00028808281058445573,
      "learning_rate": 7.691022609828267e-05,
      "loss": 0.246,
      "step": 210100
    },
    {
      "epoch": 0.7439865218807074,
      "grad_norm": 0.001038895221427083,
      "learning_rate": 7.680404343578781e-05,
      "loss": 0.2914,
      "step": 210200
    },
    {
      "epoch": 0.7443404640890235,
      "grad_norm": 0.004373086616396904,
      "learning_rate": 7.669786077329294e-05,
      "loss": 0.0792,
      "step": 210300
    },
    {
      "epoch": 0.7446944062973397,
      "grad_norm": 0.0013885648222640157,
      "learning_rate": 7.659167811079805e-05,
      "loss": 0.1692,
      "step": 210400
    },
    {
      "epoch": 0.745048348505656,
      "grad_norm": 0.0006297464133240283,
      "learning_rate": 7.64854954483032e-05,
      "loss": 0.2284,
      "step": 210500
    },
    {
      "epoch": 0.7454022907139722,
      "grad_norm": 0.000914576172363013,
      "learning_rate": 7.637931278580832e-05,
      "loss": 0.2057,
      "step": 210600
    },
    {
      "epoch": 0.7457562329222884,
      "grad_norm": 1.294214129447937,
      "learning_rate": 7.627313012331346e-05,
      "loss": 0.2887,
      "step": 210700
    },
    {
      "epoch": 0.7461101751306046,
      "grad_norm": 0.0007468490512110293,
      "learning_rate": 7.616694746081858e-05,
      "loss": 0.2702,
      "step": 210800
    },
    {
      "epoch": 0.7464641173389209,
      "grad_norm": 0.0003490864473860711,
      "learning_rate": 7.606076479832372e-05,
      "loss": 0.1054,
      "step": 210900
    },
    {
      "epoch": 0.7468180595472371,
      "grad_norm": 0.0006395270465873182,
      "learning_rate": 7.595458213582885e-05,
      "loss": 0.205,
      "step": 211000
    },
    {
      "epoch": 0.7471720017555533,
      "grad_norm": 0.0024829069152474403,
      "learning_rate": 7.584839947333398e-05,
      "loss": 0.2079,
      "step": 211100
    },
    {
      "epoch": 0.7475259439638696,
      "grad_norm": 0.008201073855161667,
      "learning_rate": 7.574221681083912e-05,
      "loss": 0.2657,
      "step": 211200
    },
    {
      "epoch": 0.7478798861721858,
      "grad_norm": 20.438692092895508,
      "learning_rate": 7.563603414834425e-05,
      "loss": 0.2457,
      "step": 211300
    },
    {
      "epoch": 0.748233828380502,
      "grad_norm": 0.005095646716654301,
      "learning_rate": 7.552985148584939e-05,
      "loss": 0.3115,
      "step": 211400
    },
    {
      "epoch": 0.7485877705888183,
      "grad_norm": 0.17203481495380402,
      "learning_rate": 7.542366882335451e-05,
      "loss": 0.176,
      "step": 211500
    },
    {
      "epoch": 0.7489417127971345,
      "grad_norm": 134.54713439941406,
      "learning_rate": 7.531748616085964e-05,
      "loss": 0.2089,
      "step": 211600
    },
    {
      "epoch": 0.7492956550054507,
      "grad_norm": 0.013962335884571075,
      "learning_rate": 7.521130349836478e-05,
      "loss": 0.1845,
      "step": 211700
    },
    {
      "epoch": 0.7496495972137669,
      "grad_norm": 0.003817033488303423,
      "learning_rate": 7.510512083586991e-05,
      "loss": 0.2193,
      "step": 211800
    },
    {
      "epoch": 0.7500035394220832,
      "grad_norm": 0.004895776975899935,
      "learning_rate": 7.499893817337504e-05,
      "loss": 0.3332,
      "step": 211900
    },
    {
      "epoch": 0.7503574816303994,
      "grad_norm": 9.713735926197842e-05,
      "learning_rate": 7.489275551088018e-05,
      "loss": 0.3209,
      "step": 212000
    },
    {
      "epoch": 0.7507114238387156,
      "grad_norm": 30.295700073242188,
      "learning_rate": 7.47865728483853e-05,
      "loss": 0.3264,
      "step": 212100
    },
    {
      "epoch": 0.7510653660470319,
      "grad_norm": 3.2132203578948975,
      "learning_rate": 7.468039018589044e-05,
      "loss": 0.2602,
      "step": 212200
    },
    {
      "epoch": 0.7514193082553481,
      "grad_norm": 0.3926542103290558,
      "learning_rate": 7.457420752339557e-05,
      "loss": 0.1485,
      "step": 212300
    },
    {
      "epoch": 0.7517732504636643,
      "grad_norm": 0.08491591364145279,
      "learning_rate": 7.44680248609007e-05,
      "loss": 0.3002,
      "step": 212400
    },
    {
      "epoch": 0.7521271926719805,
      "grad_norm": 0.015232750214636326,
      "learning_rate": 7.436184219840584e-05,
      "loss": 0.2815,
      "step": 212500
    },
    {
      "epoch": 0.7524811348802968,
      "grad_norm": 8.94356632232666,
      "learning_rate": 7.425565953591097e-05,
      "loss": 0.2007,
      "step": 212600
    },
    {
      "epoch": 0.752835077088613,
      "grad_norm": 10.413899421691895,
      "learning_rate": 7.41494768734161e-05,
      "loss": 0.3565,
      "step": 212700
    },
    {
      "epoch": 0.7531890192969292,
      "grad_norm": 18.328968048095703,
      "learning_rate": 7.404329421092123e-05,
      "loss": 0.2754,
      "step": 212800
    },
    {
      "epoch": 0.7535429615052455,
      "grad_norm": 0.0007533042225986719,
      "learning_rate": 7.393711154842637e-05,
      "loss": 0.2228,
      "step": 212900
    },
    {
      "epoch": 0.7538969037135617,
      "grad_norm": 0.007054849993437529,
      "learning_rate": 7.38309288859315e-05,
      "loss": 0.1614,
      "step": 213000
    },
    {
      "epoch": 0.7542508459218779,
      "grad_norm": 0.0010926887625828385,
      "learning_rate": 7.372474622343663e-05,
      "loss": 0.244,
      "step": 213100
    },
    {
      "epoch": 0.754604788130194,
      "grad_norm": 0.15633468329906464,
      "learning_rate": 7.361856356094177e-05,
      "loss": 0.3388,
      "step": 213200
    },
    {
      "epoch": 0.7549587303385104,
      "grad_norm": 0.11586137861013412,
      "learning_rate": 7.35123808984469e-05,
      "loss": 0.2266,
      "step": 213300
    },
    {
      "epoch": 0.7553126725468265,
      "grad_norm": 0.015927549451589584,
      "learning_rate": 7.340619823595204e-05,
      "loss": 0.2281,
      "step": 213400
    },
    {
      "epoch": 0.7556666147551427,
      "grad_norm": 24.80317497253418,
      "learning_rate": 7.330001557345716e-05,
      "loss": 0.3659,
      "step": 213500
    },
    {
      "epoch": 0.756020556963459,
      "grad_norm": 0.000766713754273951,
      "learning_rate": 7.319383291096229e-05,
      "loss": 0.3189,
      "step": 213600
    },
    {
      "epoch": 0.7563744991717752,
      "grad_norm": 6.414897507056594e-05,
      "learning_rate": 7.308765024846742e-05,
      "loss": 0.1763,
      "step": 213700
    },
    {
      "epoch": 0.7567284413800914,
      "grad_norm": 7.327760977204889e-05,
      "learning_rate": 7.298146758597256e-05,
      "loss": 0.2341,
      "step": 213800
    },
    {
      "epoch": 0.7570823835884077,
      "grad_norm": 8.44965543365106e-05,
      "learning_rate": 7.287528492347768e-05,
      "loss": 0.2011,
      "step": 213900
    },
    {
      "epoch": 0.7574363257967239,
      "grad_norm": 0.00047604122664779425,
      "learning_rate": 7.276910226098282e-05,
      "loss": 0.2857,
      "step": 214000
    },
    {
      "epoch": 0.7577902680050401,
      "grad_norm": 0.00030380149837583303,
      "learning_rate": 7.266291959848795e-05,
      "loss": 0.3819,
      "step": 214100
    },
    {
      "epoch": 0.7581442102133563,
      "grad_norm": 0.0005855855997651815,
      "learning_rate": 7.255673693599308e-05,
      "loss": 0.2815,
      "step": 214200
    },
    {
      "epoch": 0.7584981524216726,
      "grad_norm": 0.00015447450277861208,
      "learning_rate": 7.245055427349822e-05,
      "loss": 0.2852,
      "step": 214300
    },
    {
      "epoch": 0.7588520946299888,
      "grad_norm": 0.001550263143144548,
      "learning_rate": 7.234437161100335e-05,
      "loss": 0.3051,
      "step": 214400
    },
    {
      "epoch": 0.759206036838305,
      "grad_norm": 0.0002994317910633981,
      "learning_rate": 7.223818894850849e-05,
      "loss": 0.1812,
      "step": 214500
    },
    {
      "epoch": 0.7595599790466213,
      "grad_norm": 7.990661833900958e-05,
      "learning_rate": 7.213200628601361e-05,
      "loss": 0.4439,
      "step": 214600
    },
    {
      "epoch": 0.7599139212549375,
      "grad_norm": 0.0083664795383811,
      "learning_rate": 7.202582362351875e-05,
      "loss": 0.138,
      "step": 214700
    },
    {
      "epoch": 0.7602678634632537,
      "grad_norm": 0.005268176551908255,
      "learning_rate": 7.191964096102388e-05,
      "loss": 0.1503,
      "step": 214800
    },
    {
      "epoch": 0.7606218056715699,
      "grad_norm": 0.0035034529864788055,
      "learning_rate": 7.181345829852901e-05,
      "loss": 0.197,
      "step": 214900
    },
    {
      "epoch": 0.7609757478798862,
      "grad_norm": 1.975138729903847e-05,
      "learning_rate": 7.170727563603415e-05,
      "loss": 0.2757,
      "step": 215000
    },
    {
      "epoch": 0.7613296900882024,
      "grad_norm": 0.0023726692888885736,
      "learning_rate": 7.160109297353928e-05,
      "loss": 0.159,
      "step": 215100
    },
    {
      "epoch": 0.7616836322965186,
      "grad_norm": 30.88467788696289,
      "learning_rate": 7.14949103110444e-05,
      "loss": 0.2968,
      "step": 215200
    },
    {
      "epoch": 0.7620375745048349,
      "grad_norm": 0.0011120085837319493,
      "learning_rate": 7.138872764854953e-05,
      "loss": 0.1217,
      "step": 215300
    },
    {
      "epoch": 0.7623915167131511,
      "grad_norm": 0.00032965425634756684,
      "learning_rate": 7.128254498605467e-05,
      "loss": 0.1374,
      "step": 215400
    },
    {
      "epoch": 0.7627454589214673,
      "grad_norm": 7.65495715313591e-05,
      "learning_rate": 7.11763623235598e-05,
      "loss": 0.1543,
      "step": 215500
    },
    {
      "epoch": 0.7630994011297836,
      "grad_norm": 80.081787109375,
      "learning_rate": 7.107017966106494e-05,
      "loss": 0.3766,
      "step": 215600
    },
    {
      "epoch": 0.7634533433380998,
      "grad_norm": 0.02153588831424713,
      "learning_rate": 7.096399699857006e-05,
      "loss": 0.2635,
      "step": 215700
    },
    {
      "epoch": 0.763807285546416,
      "grad_norm": 0.001300626085139811,
      "learning_rate": 7.08578143360752e-05,
      "loss": 0.3551,
      "step": 215800
    },
    {
      "epoch": 0.7641612277547322,
      "grad_norm": 0.00015975182759575546,
      "learning_rate": 7.075163167358033e-05,
      "loss": 0.3316,
      "step": 215900
    },
    {
      "epoch": 0.7645151699630485,
      "grad_norm": 0.0025533006992191076,
      "learning_rate": 7.064544901108546e-05,
      "loss": 0.2079,
      "step": 216000
    },
    {
      "epoch": 0.7648691121713647,
      "grad_norm": 0.04167766496539116,
      "learning_rate": 7.05392663485906e-05,
      "loss": 0.0808,
      "step": 216100
    },
    {
      "epoch": 0.7652230543796809,
      "grad_norm": 6.78200158290565e-05,
      "learning_rate": 7.043308368609573e-05,
      "loss": 0.2808,
      "step": 216200
    },
    {
      "epoch": 0.7655769965879972,
      "grad_norm": 66.04059600830078,
      "learning_rate": 7.032690102360087e-05,
      "loss": 0.2014,
      "step": 216300
    },
    {
      "epoch": 0.7659309387963134,
      "grad_norm": 18.09079933166504,
      "learning_rate": 7.0220718361106e-05,
      "loss": 0.2495,
      "step": 216400
    },
    {
      "epoch": 0.7662848810046295,
      "grad_norm": 0.002630299888551235,
      "learning_rate": 7.011453569861112e-05,
      "loss": 0.3251,
      "step": 216500
    },
    {
      "epoch": 0.7666388232129457,
      "grad_norm": 0.3117254674434662,
      "learning_rate": 7.000835303611625e-05,
      "loss": 0.2613,
      "step": 216600
    },
    {
      "epoch": 0.766992765421262,
      "grad_norm": 0.0004522247763816267,
      "learning_rate": 6.990217037362139e-05,
      "loss": 0.2269,
      "step": 216700
    },
    {
      "epoch": 0.7673467076295782,
      "grad_norm": 0.0001923314412124455,
      "learning_rate": 6.979598771112652e-05,
      "loss": 0.2068,
      "step": 216800
    },
    {
      "epoch": 0.7677006498378944,
      "grad_norm": 0.0007623431156389415,
      "learning_rate": 6.968980504863166e-05,
      "loss": 0.1174,
      "step": 216900
    },
    {
      "epoch": 0.7680545920462107,
      "grad_norm": 0.00033015719964168966,
      "learning_rate": 6.958362238613678e-05,
      "loss": 0.3745,
      "step": 217000
    },
    {
      "epoch": 0.7684085342545269,
      "grad_norm": 0.0030389989260584116,
      "learning_rate": 6.947743972364191e-05,
      "loss": 0.3237,
      "step": 217100
    },
    {
      "epoch": 0.7687624764628431,
      "grad_norm": 0.027823729440569878,
      "learning_rate": 6.937125706114705e-05,
      "loss": 0.2706,
      "step": 217200
    },
    {
      "epoch": 0.7691164186711593,
      "grad_norm": 4.665453910827637,
      "learning_rate": 6.926507439865218e-05,
      "loss": 0.1461,
      "step": 217300
    },
    {
      "epoch": 0.7694703608794756,
      "grad_norm": 0.05189569666981697,
      "learning_rate": 6.915889173615732e-05,
      "loss": 0.1515,
      "step": 217400
    },
    {
      "epoch": 0.7698243030877918,
      "grad_norm": 0.00035813881549984217,
      "learning_rate": 6.905270907366245e-05,
      "loss": 0.2588,
      "step": 217500
    },
    {
      "epoch": 0.770178245296108,
      "grad_norm": 0.0027316452469676733,
      "learning_rate": 6.894652641116759e-05,
      "loss": 0.1907,
      "step": 217600
    },
    {
      "epoch": 0.7705321875044243,
      "grad_norm": 0.0007314893882721663,
      "learning_rate": 6.884034374867271e-05,
      "loss": 0.21,
      "step": 217700
    },
    {
      "epoch": 0.7708861297127405,
      "grad_norm": 0.00324687035754323,
      "learning_rate": 6.873416108617784e-05,
      "loss": 0.0901,
      "step": 217800
    },
    {
      "epoch": 0.7712400719210567,
      "grad_norm": 0.005227334331721067,
      "learning_rate": 6.862797842368298e-05,
      "loss": 0.2011,
      "step": 217900
    },
    {
      "epoch": 0.771594014129373,
      "grad_norm": 0.0006545554497279227,
      "learning_rate": 6.852179576118811e-05,
      "loss": 0.2668,
      "step": 218000
    },
    {
      "epoch": 0.7719479563376892,
      "grad_norm": 0.0020680560264736414,
      "learning_rate": 6.841561309869325e-05,
      "loss": 0.1682,
      "step": 218100
    },
    {
      "epoch": 0.7723018985460054,
      "grad_norm": 0.17834386229515076,
      "learning_rate": 6.830943043619836e-05,
      "loss": 0.2619,
      "step": 218200
    },
    {
      "epoch": 0.7726558407543216,
      "grad_norm": 13.499287605285645,
      "learning_rate": 6.82032477737035e-05,
      "loss": 0.2462,
      "step": 218300
    },
    {
      "epoch": 0.7730097829626379,
      "grad_norm": 0.0004872209392488003,
      "learning_rate": 6.809706511120863e-05,
      "loss": 0.1606,
      "step": 218400
    },
    {
      "epoch": 0.7733637251709541,
      "grad_norm": 0.0001936547487275675,
      "learning_rate": 6.799088244871377e-05,
      "loss": 0.3476,
      "step": 218500
    },
    {
      "epoch": 0.7737176673792703,
      "grad_norm": 0.000816957326605916,
      "learning_rate": 6.78846997862189e-05,
      "loss": 0.224,
      "step": 218600
    },
    {
      "epoch": 0.7740716095875866,
      "grad_norm": 0.005195288918912411,
      "learning_rate": 6.777851712372404e-05,
      "loss": 0.2788,
      "step": 218700
    },
    {
      "epoch": 0.7744255517959028,
      "grad_norm": 0.031967613846063614,
      "learning_rate": 6.767233446122916e-05,
      "loss": 0.2319,
      "step": 218800
    },
    {
      "epoch": 0.774779494004219,
      "grad_norm": 0.013010108843445778,
      "learning_rate": 6.756615179873429e-05,
      "loss": 0.0987,
      "step": 218900
    },
    {
      "epoch": 0.7751334362125352,
      "grad_norm": 0.00014883751282468438,
      "learning_rate": 6.745996913623943e-05,
      "loss": 0.4154,
      "step": 219000
    },
    {
      "epoch": 0.7754873784208515,
      "grad_norm": 0.00036791706224903464,
      "learning_rate": 6.735378647374456e-05,
      "loss": 0.1304,
      "step": 219100
    },
    {
      "epoch": 0.7758413206291677,
      "grad_norm": 0.02301342599093914,
      "learning_rate": 6.72476038112497e-05,
      "loss": 0.1529,
      "step": 219200
    },
    {
      "epoch": 0.7761952628374839,
      "grad_norm": 0.0033013150095939636,
      "learning_rate": 6.714142114875483e-05,
      "loss": 0.1744,
      "step": 219300
    },
    {
      "epoch": 0.7765492050458002,
      "grad_norm": 0.00020994135411456227,
      "learning_rate": 6.703523848625995e-05,
      "loss": 0.3272,
      "step": 219400
    },
    {
      "epoch": 0.7769031472541164,
      "grad_norm": 0.01693297177553177,
      "learning_rate": 6.69290558237651e-05,
      "loss": 0.2525,
      "step": 219500
    },
    {
      "epoch": 0.7772570894624325,
      "grad_norm": 5.362508296966553,
      "learning_rate": 6.682287316127022e-05,
      "loss": 0.2217,
      "step": 219600
    },
    {
      "epoch": 0.7776110316707489,
      "grad_norm": 0.006436688359826803,
      "learning_rate": 6.671669049877535e-05,
      "loss": 0.1286,
      "step": 219700
    },
    {
      "epoch": 0.777964973879065,
      "grad_norm": 0.006226501893252134,
      "learning_rate": 6.661050783628049e-05,
      "loss": 0.3087,
      "step": 219800
    },
    {
      "epoch": 0.7783189160873812,
      "grad_norm": 0.038266249001026154,
      "learning_rate": 6.650432517378562e-05,
      "loss": 0.1942,
      "step": 219900
    },
    {
      "epoch": 0.7786728582956974,
      "grad_norm": 0.00027311264420859516,
      "learning_rate": 6.639814251129074e-05,
      "loss": 0.1209,
      "step": 220000
    },
    {
      "epoch": 0.7790268005040137,
      "grad_norm": 0.0020229353103786707,
      "learning_rate": 6.629195984879588e-05,
      "loss": 0.2119,
      "step": 220100
    },
    {
      "epoch": 0.7793807427123299,
      "grad_norm": 0.0007003412465564907,
      "learning_rate": 6.618577718630101e-05,
      "loss": 0.1392,
      "step": 220200
    },
    {
      "epoch": 0.7797346849206461,
      "grad_norm": 0.0049719554372131824,
      "learning_rate": 6.607959452380615e-05,
      "loss": 0.3048,
      "step": 220300
    },
    {
      "epoch": 0.7800886271289624,
      "grad_norm": 0.00021279818611219525,
      "learning_rate": 6.597341186131128e-05,
      "loss": 0.1217,
      "step": 220400
    },
    {
      "epoch": 0.7804425693372786,
      "grad_norm": 6.295552884694189e-05,
      "learning_rate": 6.586722919881642e-05,
      "loss": 0.2586,
      "step": 220500
    },
    {
      "epoch": 0.7807965115455948,
      "grad_norm": 7.738968270132318e-05,
      "learning_rate": 6.576104653632155e-05,
      "loss": 0.1445,
      "step": 220600
    },
    {
      "epoch": 0.781150453753911,
      "grad_norm": 0.000928958470467478,
      "learning_rate": 6.565486387382667e-05,
      "loss": 0.1852,
      "step": 220700
    },
    {
      "epoch": 0.7815043959622273,
      "grad_norm": 0.0013440530747175217,
      "learning_rate": 6.554868121133181e-05,
      "loss": 0.1809,
      "step": 220800
    },
    {
      "epoch": 0.7818583381705435,
      "grad_norm": 0.020977525040507317,
      "learning_rate": 6.544249854883694e-05,
      "loss": 0.2102,
      "step": 220900
    },
    {
      "epoch": 0.7822122803788597,
      "grad_norm": 0.00031553831649944186,
      "learning_rate": 6.533631588634208e-05,
      "loss": 0.2111,
      "step": 221000
    },
    {
      "epoch": 0.782566222587176,
      "grad_norm": 0.036174073815345764,
      "learning_rate": 6.523013322384721e-05,
      "loss": 0.1686,
      "step": 221100
    },
    {
      "epoch": 0.7829201647954922,
      "grad_norm": 0.0002445644640829414,
      "learning_rate": 6.512395056135233e-05,
      "loss": 0.1031,
      "step": 221200
    },
    {
      "epoch": 0.7832741070038084,
      "grad_norm": 0.0002141951845260337,
      "learning_rate": 6.501776789885746e-05,
      "loss": 0.2435,
      "step": 221300
    },
    {
      "epoch": 0.7836280492121246,
      "grad_norm": 2.3797916583134793e-05,
      "learning_rate": 6.49115852363626e-05,
      "loss": 0.1884,
      "step": 221400
    },
    {
      "epoch": 0.7839819914204409,
      "grad_norm": 53.50555419921875,
      "learning_rate": 6.480540257386773e-05,
      "loss": 0.2439,
      "step": 221500
    },
    {
      "epoch": 0.7843359336287571,
      "grad_norm": 0.00022261105186771601,
      "learning_rate": 6.469921991137287e-05,
      "loss": 0.1244,
      "step": 221600
    },
    {
      "epoch": 0.7846898758370733,
      "grad_norm": 0.004958153702318668,
      "learning_rate": 6.4593037248878e-05,
      "loss": 0.0409,
      "step": 221700
    },
    {
      "epoch": 0.7850438180453896,
      "grad_norm": 27.236188888549805,
      "learning_rate": 6.448685458638312e-05,
      "loss": 0.46,
      "step": 221800
    },
    {
      "epoch": 0.7853977602537058,
      "grad_norm": 82.1507568359375,
      "learning_rate": 6.438067192388826e-05,
      "loss": 0.1302,
      "step": 221900
    },
    {
      "epoch": 0.785751702462022,
      "grad_norm": 8.023446571314707e-05,
      "learning_rate": 6.427448926139339e-05,
      "loss": 0.1823,
      "step": 222000
    },
    {
      "epoch": 0.7861056446703383,
      "grad_norm": 79.7548828125,
      "learning_rate": 6.416830659889853e-05,
      "loss": 0.3833,
      "step": 222100
    },
    {
      "epoch": 0.7864595868786545,
      "grad_norm": 0.0005660903407260776,
      "learning_rate": 6.406212393640366e-05,
      "loss": 0.1933,
      "step": 222200
    },
    {
      "epoch": 0.7868135290869707,
      "grad_norm": 0.0007205046713352203,
      "learning_rate": 6.395594127390879e-05,
      "loss": 0.1794,
      "step": 222300
    },
    {
      "epoch": 0.7871674712952869,
      "grad_norm": 0.4125521779060364,
      "learning_rate": 6.384975861141393e-05,
      "loss": 0.1452,
      "step": 222400
    },
    {
      "epoch": 0.7875214135036032,
      "grad_norm": 23.97472381591797,
      "learning_rate": 6.374357594891905e-05,
      "loss": 0.0826,
      "step": 222500
    },
    {
      "epoch": 0.7878753557119194,
      "grad_norm": 0.0002588799688965082,
      "learning_rate": 6.36373932864242e-05,
      "loss": 0.2041,
      "step": 222600
    },
    {
      "epoch": 0.7882292979202355,
      "grad_norm": 0.00013323407620191574,
      "learning_rate": 6.353121062392932e-05,
      "loss": 0.1891,
      "step": 222700
    },
    {
      "epoch": 0.7885832401285519,
      "grad_norm": 0.0003326229052618146,
      "learning_rate": 6.342502796143446e-05,
      "loss": 0.1159,
      "step": 222800
    },
    {
      "epoch": 0.788937182336868,
      "grad_norm": 0.00028424046467989683,
      "learning_rate": 6.331884529893957e-05,
      "loss": 0.2461,
      "step": 222900
    },
    {
      "epoch": 0.7892911245451842,
      "grad_norm": 0.947682797908783,
      "learning_rate": 6.321266263644472e-05,
      "loss": 0.2405,
      "step": 223000
    },
    {
      "epoch": 0.7896450667535004,
      "grad_norm": 2.195126398873981e-05,
      "learning_rate": 6.310647997394984e-05,
      "loss": 0.1193,
      "step": 223100
    },
    {
      "epoch": 0.7899990089618167,
      "grad_norm": 0.0030080925207585096,
      "learning_rate": 6.300029731145498e-05,
      "loss": 0.1964,
      "step": 223200
    },
    {
      "epoch": 0.7903529511701329,
      "grad_norm": 2.767660140991211,
      "learning_rate": 6.289411464896011e-05,
      "loss": 0.2241,
      "step": 223300
    },
    {
      "epoch": 0.7907068933784491,
      "grad_norm": 0.00045378552749753,
      "learning_rate": 6.278793198646525e-05,
      "loss": 0.263,
      "step": 223400
    },
    {
      "epoch": 0.7910608355867654,
      "grad_norm": 0.0002741004282142967,
      "learning_rate": 6.268174932397038e-05,
      "loss": 0.2428,
      "step": 223500
    },
    {
      "epoch": 0.7914147777950816,
      "grad_norm": 0.2221025824546814,
      "learning_rate": 6.25755666614755e-05,
      "loss": 0.0656,
      "step": 223600
    },
    {
      "epoch": 0.7917687200033978,
      "grad_norm": 0.00040916368016041815,
      "learning_rate": 6.246938399898064e-05,
      "loss": 0.1369,
      "step": 223700
    },
    {
      "epoch": 0.7921226622117141,
      "grad_norm": 0.0010532377054914832,
      "learning_rate": 6.236320133648577e-05,
      "loss": 0.2262,
      "step": 223800
    },
    {
      "epoch": 0.7924766044200303,
      "grad_norm": 0.0022544206585735083,
      "learning_rate": 6.225701867399091e-05,
      "loss": 0.1587,
      "step": 223900
    },
    {
      "epoch": 0.7928305466283465,
      "grad_norm": 63.57646942138672,
      "learning_rate": 6.215083601149604e-05,
      "loss": 0.3434,
      "step": 224000
    },
    {
      "epoch": 0.7931844888366627,
      "grad_norm": 0.007442060392349958,
      "learning_rate": 6.204465334900117e-05,
      "loss": 0.1549,
      "step": 224100
    },
    {
      "epoch": 0.793538431044979,
      "grad_norm": 0.011991847306489944,
      "learning_rate": 6.193847068650631e-05,
      "loss": 0.1831,
      "step": 224200
    },
    {
      "epoch": 0.7938923732532952,
      "grad_norm": 0.02099025622010231,
      "learning_rate": 6.183228802401143e-05,
      "loss": 0.1528,
      "step": 224300
    },
    {
      "epoch": 0.7942463154616114,
      "grad_norm": 29.41358184814453,
      "learning_rate": 6.172610536151656e-05,
      "loss": 0.3015,
      "step": 224400
    },
    {
      "epoch": 0.7946002576699277,
      "grad_norm": 0.00014177070988807827,
      "learning_rate": 6.16199226990217e-05,
      "loss": 0.3923,
      "step": 224500
    },
    {
      "epoch": 0.7949541998782439,
      "grad_norm": 0.00038857385516166687,
      "learning_rate": 6.151374003652683e-05,
      "loss": 0.1983,
      "step": 224600
    },
    {
      "epoch": 0.7953081420865601,
      "grad_norm": 5.022308826446533,
      "learning_rate": 6.140755737403196e-05,
      "loss": 0.1207,
      "step": 224700
    },
    {
      "epoch": 0.7956620842948763,
      "grad_norm": 7.334752444876358e-05,
      "learning_rate": 6.13013747115371e-05,
      "loss": 0.1853,
      "step": 224800
    },
    {
      "epoch": 0.7960160265031926,
      "grad_norm": 0.0002993312373291701,
      "learning_rate": 6.119519204904222e-05,
      "loss": 0.1854,
      "step": 224900
    },
    {
      "epoch": 0.7963699687115088,
      "grad_norm": 0.03157659247517586,
      "learning_rate": 6.108900938654736e-05,
      "loss": 0.1065,
      "step": 225000
    },
    {
      "epoch": 0.796723910919825,
      "grad_norm": 0.0008052721386775374,
      "learning_rate": 6.09828267240525e-05,
      "loss": 0.2691,
      "step": 225100
    },
    {
      "epoch": 0.7970778531281413,
      "grad_norm": 0.0003531480033416301,
      "learning_rate": 6.087664406155762e-05,
      "loss": 0.2144,
      "step": 225200
    },
    {
      "epoch": 0.7974317953364575,
      "grad_norm": 0.0006344886496663094,
      "learning_rate": 6.077046139906275e-05,
      "loss": 0.3367,
      "step": 225300
    },
    {
      "epoch": 0.7977857375447737,
      "grad_norm": 0.0025645701680332422,
      "learning_rate": 6.0664278736567885e-05,
      "loss": 0.203,
      "step": 225400
    },
    {
      "epoch": 0.79813967975309,
      "grad_norm": 0.00011718280438799411,
      "learning_rate": 6.055809607407302e-05,
      "loss": 0.3459,
      "step": 225500
    },
    {
      "epoch": 0.7984936219614062,
      "grad_norm": 0.014428396709263325,
      "learning_rate": 6.045191341157815e-05,
      "loss": 0.1794,
      "step": 225600
    },
    {
      "epoch": 0.7988475641697224,
      "grad_norm": 0.0001901348732644692,
      "learning_rate": 6.0345730749083286e-05,
      "loss": 0.2045,
      "step": 225700
    },
    {
      "epoch": 0.7992015063780386,
      "grad_norm": 0.02878931723535061,
      "learning_rate": 6.0239548086588413e-05,
      "loss": 0.1513,
      "step": 225800
    },
    {
      "epoch": 0.7995554485863549,
      "grad_norm": 3.733620178536512e-05,
      "learning_rate": 6.013336542409355e-05,
      "loss": 0.1642,
      "step": 225900
    },
    {
      "epoch": 0.799909390794671,
      "grad_norm": 0.018434932455420494,
      "learning_rate": 6.002718276159868e-05,
      "loss": 0.2705,
      "step": 226000
    },
    {
      "epoch": 0.8002633330029872,
      "grad_norm": 0.00044940150110051036,
      "learning_rate": 5.9921000099103815e-05,
      "loss": 0.0926,
      "step": 226100
    },
    {
      "epoch": 0.8006172752113035,
      "grad_norm": 0.3690369129180908,
      "learning_rate": 5.981481743660895e-05,
      "loss": 0.2751,
      "step": 226200
    },
    {
      "epoch": 0.8009712174196197,
      "grad_norm": 0.0002667302615009248,
      "learning_rate": 5.970863477411408e-05,
      "loss": 0.127,
      "step": 226300
    },
    {
      "epoch": 0.8013251596279359,
      "grad_norm": 0.0001416703889844939,
      "learning_rate": 5.960245211161921e-05,
      "loss": 0.3736,
      "step": 226400
    },
    {
      "epoch": 0.8016791018362521,
      "grad_norm": 0.09614667296409607,
      "learning_rate": 5.9496269449124336e-05,
      "loss": 0.2512,
      "step": 226500
    },
    {
      "epoch": 0.8020330440445684,
      "grad_norm": 0.0003249623696319759,
      "learning_rate": 5.939008678662947e-05,
      "loss": 0.2641,
      "step": 226600
    },
    {
      "epoch": 0.8023869862528846,
      "grad_norm": 88.04877471923828,
      "learning_rate": 5.9283904124134604e-05,
      "loss": 0.2331,
      "step": 226700
    },
    {
      "epoch": 0.8027409284612008,
      "grad_norm": 0.00022552242444362491,
      "learning_rate": 5.917772146163974e-05,
      "loss": 0.125,
      "step": 226800
    },
    {
      "epoch": 0.8030948706695171,
      "grad_norm": 0.0003702160029206425,
      "learning_rate": 5.907153879914487e-05,
      "loss": 0.346,
      "step": 226900
    },
    {
      "epoch": 0.8034488128778333,
      "grad_norm": 0.0006002428708598018,
      "learning_rate": 5.896535613665e-05,
      "loss": 0.219,
      "step": 227000
    },
    {
      "epoch": 0.8038027550861495,
      "grad_norm": 0.0005136182880960405,
      "learning_rate": 5.885917347415513e-05,
      "loss": 0.1406,
      "step": 227100
    },
    {
      "epoch": 0.8041566972944657,
      "grad_norm": 0.00018998146697413176,
      "learning_rate": 5.8752990811660266e-05,
      "loss": 0.0846,
      "step": 227200
    },
    {
      "epoch": 0.804510639502782,
      "grad_norm": 40.7729606628418,
      "learning_rate": 5.86468081491654e-05,
      "loss": 0.1551,
      "step": 227300
    },
    {
      "epoch": 0.8048645817110982,
      "grad_norm": 0.000916662102099508,
      "learning_rate": 5.8540625486670534e-05,
      "loss": 0.2935,
      "step": 227400
    },
    {
      "epoch": 0.8052185239194144,
      "grad_norm": 0.01407030038535595,
      "learning_rate": 5.843444282417567e-05,
      "loss": 0.0989,
      "step": 227500
    },
    {
      "epoch": 0.8055724661277307,
      "grad_norm": 65.49085998535156,
      "learning_rate": 5.8328260161680794e-05,
      "loss": 0.1386,
      "step": 227600
    },
    {
      "epoch": 0.8059264083360469,
      "grad_norm": 156.47572326660156,
      "learning_rate": 5.822207749918593e-05,
      "loss": 0.2439,
      "step": 227700
    },
    {
      "epoch": 0.8062803505443631,
      "grad_norm": 0.006681201979517937,
      "learning_rate": 5.811589483669106e-05,
      "loss": 0.1131,
      "step": 227800
    },
    {
      "epoch": 0.8066342927526794,
      "grad_norm": 42.220176696777344,
      "learning_rate": 5.8009712174196196e-05,
      "loss": 0.2784,
      "step": 227900
    },
    {
      "epoch": 0.8069882349609956,
      "grad_norm": 14.656476020812988,
      "learning_rate": 5.790352951170133e-05,
      "loss": 0.3076,
      "step": 228000
    },
    {
      "epoch": 0.8073421771693118,
      "grad_norm": 0.0001486391993239522,
      "learning_rate": 5.779734684920646e-05,
      "loss": 0.1657,
      "step": 228100
    },
    {
      "epoch": 0.807696119377628,
      "grad_norm": 8.707146480446681e-05,
      "learning_rate": 5.769116418671158e-05,
      "loss": 0.2133,
      "step": 228200
    },
    {
      "epoch": 0.8080500615859443,
      "grad_norm": 59.23091506958008,
      "learning_rate": 5.758498152421672e-05,
      "loss": 0.2459,
      "step": 228300
    },
    {
      "epoch": 0.8084040037942605,
      "grad_norm": 0.0016604728298261762,
      "learning_rate": 5.747879886172185e-05,
      "loss": 0.1642,
      "step": 228400
    },
    {
      "epoch": 0.8087579460025767,
      "grad_norm": 0.002043206011876464,
      "learning_rate": 5.7372616199226985e-05,
      "loss": 0.1557,
      "step": 228500
    },
    {
      "epoch": 0.809111888210893,
      "grad_norm": 0.007184141781181097,
      "learning_rate": 5.726643353673212e-05,
      "loss": 0.2603,
      "step": 228600
    },
    {
      "epoch": 0.8094658304192092,
      "grad_norm": 0.0005564112216234207,
      "learning_rate": 5.7160250874237245e-05,
      "loss": 0.2266,
      "step": 228700
    },
    {
      "epoch": 0.8098197726275254,
      "grad_norm": 0.0044570122845470905,
      "learning_rate": 5.705406821174238e-05,
      "loss": 0.1311,
      "step": 228800
    },
    {
      "epoch": 0.8101737148358416,
      "grad_norm": 0.00044253363739699125,
      "learning_rate": 5.694788554924751e-05,
      "loss": 0.3538,
      "step": 228900
    },
    {
      "epoch": 0.8105276570441579,
      "grad_norm": 0.0008940705447457731,
      "learning_rate": 5.684170288675265e-05,
      "loss": 0.0316,
      "step": 229000
    },
    {
      "epoch": 0.810881599252474,
      "grad_norm": 0.03143526613712311,
      "learning_rate": 5.673552022425778e-05,
      "loss": 0.3706,
      "step": 229100
    },
    {
      "epoch": 0.8112355414607902,
      "grad_norm": 0.0005509623442776501,
      "learning_rate": 5.6629337561762914e-05,
      "loss": 0.3771,
      "step": 229200
    },
    {
      "epoch": 0.8115894836691065,
      "grad_norm": 0.0005939328693784773,
      "learning_rate": 5.652315489926804e-05,
      "loss": 0.084,
      "step": 229300
    },
    {
      "epoch": 0.8119434258774227,
      "grad_norm": 0.9165182113647461,
      "learning_rate": 5.6416972236773175e-05,
      "loss": 0.1069,
      "step": 229400
    },
    {
      "epoch": 0.8122973680857389,
      "grad_norm": 8.163072925526649e-05,
      "learning_rate": 5.631078957427831e-05,
      "loss": 0.4061,
      "step": 229500
    },
    {
      "epoch": 0.8126513102940552,
      "grad_norm": 0.001305558136664331,
      "learning_rate": 5.620460691178344e-05,
      "loss": 0.1212,
      "step": 229600
    },
    {
      "epoch": 0.8130052525023714,
      "grad_norm": 0.00038598032551817596,
      "learning_rate": 5.6098424249288576e-05,
      "loss": 0.3852,
      "step": 229700
    },
    {
      "epoch": 0.8133591947106876,
      "grad_norm": 0.005758471786975861,
      "learning_rate": 5.599224158679371e-05,
      "loss": 0.1096,
      "step": 229800
    },
    {
      "epoch": 0.8137131369190038,
      "grad_norm": 0.00019063886429648846,
      "learning_rate": 5.588605892429883e-05,
      "loss": 0.1811,
      "step": 229900
    },
    {
      "epoch": 0.8140670791273201,
      "grad_norm": 0.0008247447549365461,
      "learning_rate": 5.5779876261803964e-05,
      "loss": 0.0912,
      "step": 230000
    },
    {
      "epoch": 0.8144210213356363,
      "grad_norm": 0.0034344804007560015,
      "learning_rate": 5.56736935993091e-05,
      "loss": 0.3443,
      "step": 230100
    },
    {
      "epoch": 0.8147749635439525,
      "grad_norm": 0.021802103146910667,
      "learning_rate": 5.556751093681423e-05,
      "loss": 0.3039,
      "step": 230200
    },
    {
      "epoch": 0.8151289057522688,
      "grad_norm": 0.24977806210517883,
      "learning_rate": 5.5461328274319366e-05,
      "loss": 0.2042,
      "step": 230300
    },
    {
      "epoch": 0.815482847960585,
      "grad_norm": 25.940065383911133,
      "learning_rate": 5.53551456118245e-05,
      "loss": 0.1548,
      "step": 230400
    },
    {
      "epoch": 0.8158367901689012,
      "grad_norm": 48.35872268676758,
      "learning_rate": 5.5248962949329626e-05,
      "loss": 0.3221,
      "step": 230500
    },
    {
      "epoch": 0.8161907323772174,
      "grad_norm": 0.0048774853348731995,
      "learning_rate": 5.514278028683476e-05,
      "loss": 0.1771,
      "step": 230600
    },
    {
      "epoch": 0.8165446745855337,
      "grad_norm": 0.002268702955916524,
      "learning_rate": 5.5036597624339894e-05,
      "loss": 0.2866,
      "step": 230700
    },
    {
      "epoch": 0.8168986167938499,
      "grad_norm": 4.903904118691571e-05,
      "learning_rate": 5.493041496184503e-05,
      "loss": 0.1514,
      "step": 230800
    },
    {
      "epoch": 0.8172525590021661,
      "grad_norm": 4.216832160949707,
      "learning_rate": 5.482423229935016e-05,
      "loss": 0.1208,
      "step": 230900
    },
    {
      "epoch": 0.8176065012104824,
      "grad_norm": 0.0012670112773776054,
      "learning_rate": 5.4718049636855295e-05,
      "loss": 0.2739,
      "step": 231000
    },
    {
      "epoch": 0.8179604434187986,
      "grad_norm": 0.010211730375885963,
      "learning_rate": 5.4611866974360415e-05,
      "loss": 0.2046,
      "step": 231100
    },
    {
      "epoch": 0.8183143856271148,
      "grad_norm": 7.57750021875836e-05,
      "learning_rate": 5.450568431186555e-05,
      "loss": 0.3078,
      "step": 231200
    },
    {
      "epoch": 0.818668327835431,
      "grad_norm": 0.009403474628925323,
      "learning_rate": 5.439950164937068e-05,
      "loss": 0.2322,
      "step": 231300
    },
    {
      "epoch": 0.8190222700437473,
      "grad_norm": 0.0017909373855218291,
      "learning_rate": 5.429331898687582e-05,
      "loss": 0.2157,
      "step": 231400
    },
    {
      "epoch": 0.8193762122520635,
      "grad_norm": 0.005165969021618366,
      "learning_rate": 5.418713632438095e-05,
      "loss": 0.1623,
      "step": 231500
    },
    {
      "epoch": 0.8197301544603797,
      "grad_norm": 0.005092259030789137,
      "learning_rate": 5.408095366188608e-05,
      "loss": 0.1998,
      "step": 231600
    },
    {
      "epoch": 0.820084096668696,
      "grad_norm": 0.01889805495738983,
      "learning_rate": 5.397477099939121e-05,
      "loss": 0.2006,
      "step": 231700
    },
    {
      "epoch": 0.8204380388770122,
      "grad_norm": 0.0005401963135227561,
      "learning_rate": 5.3868588336896345e-05,
      "loss": 0.1356,
      "step": 231800
    },
    {
      "epoch": 0.8207919810853284,
      "grad_norm": 0.0004559940716717392,
      "learning_rate": 5.376240567440148e-05,
      "loss": 0.2932,
      "step": 231900
    },
    {
      "epoch": 0.8211459232936447,
      "grad_norm": 0.006949171889573336,
      "learning_rate": 5.365622301190661e-05,
      "loss": 0.2583,
      "step": 232000
    },
    {
      "epoch": 0.8214998655019609,
      "grad_norm": 3.395407838979736e-05,
      "learning_rate": 5.3550040349411746e-05,
      "loss": 0.1868,
      "step": 232100
    },
    {
      "epoch": 0.821853807710277,
      "grad_norm": 0.00012874347157776356,
      "learning_rate": 5.3443857686916873e-05,
      "loss": 0.1775,
      "step": 232200
    },
    {
      "epoch": 0.8222077499185932,
      "grad_norm": 16.226707458496094,
      "learning_rate": 5.333767502442201e-05,
      "loss": 0.4073,
      "step": 232300
    },
    {
      "epoch": 0.8225616921269095,
      "grad_norm": 0.0007993499166332185,
      "learning_rate": 5.323149236192714e-05,
      "loss": 0.1245,
      "step": 232400
    },
    {
      "epoch": 0.8229156343352257,
      "grad_norm": 90.41924285888672,
      "learning_rate": 5.3125309699432275e-05,
      "loss": 0.0835,
      "step": 232500
    },
    {
      "epoch": 0.8232695765435419,
      "grad_norm": 0.0020564452279359102,
      "learning_rate": 5.301912703693741e-05,
      "loss": 0.1413,
      "step": 232600
    },
    {
      "epoch": 0.8236235187518582,
      "grad_norm": 0.003549192799255252,
      "learning_rate": 5.291294437444254e-05,
      "loss": 0.2426,
      "step": 232700
    },
    {
      "epoch": 0.8239774609601744,
      "grad_norm": 0.0011183982715010643,
      "learning_rate": 5.280676171194766e-05,
      "loss": 0.1964,
      "step": 232800
    },
    {
      "epoch": 0.8243314031684906,
      "grad_norm": 3.618309497833252,
      "learning_rate": 5.2700579049452796e-05,
      "loss": 0.3171,
      "step": 232900
    },
    {
      "epoch": 0.8246853453768068,
      "grad_norm": 0.000328917580191046,
      "learning_rate": 5.259439638695793e-05,
      "loss": 0.421,
      "step": 233000
    },
    {
      "epoch": 0.8250392875851231,
      "grad_norm": 42.62274932861328,
      "learning_rate": 5.2488213724463064e-05,
      "loss": 0.1709,
      "step": 233100
    },
    {
      "epoch": 0.8253932297934393,
      "grad_norm": 0.12350690364837646,
      "learning_rate": 5.23820310619682e-05,
      "loss": 0.2474,
      "step": 233200
    },
    {
      "epoch": 0.8257471720017555,
      "grad_norm": 2.235801366623491e-05,
      "learning_rate": 5.227584839947333e-05,
      "loss": 0.1897,
      "step": 233300
    },
    {
      "epoch": 0.8261011142100718,
      "grad_norm": 63.873783111572266,
      "learning_rate": 5.216966573697846e-05,
      "loss": 0.2343,
      "step": 233400
    },
    {
      "epoch": 0.826455056418388,
      "grad_norm": 0.30793121457099915,
      "learning_rate": 5.206348307448359e-05,
      "loss": 0.1572,
      "step": 233500
    },
    {
      "epoch": 0.8268089986267042,
      "grad_norm": 0.001940447953529656,
      "learning_rate": 5.1957300411988726e-05,
      "loss": 0.3198,
      "step": 233600
    },
    {
      "epoch": 0.8271629408350205,
      "grad_norm": 0.0008033488993532956,
      "learning_rate": 5.185111774949386e-05,
      "loss": 0.1475,
      "step": 233700
    },
    {
      "epoch": 0.8275168830433367,
      "grad_norm": 3.842541264020838e-05,
      "learning_rate": 5.1744935086998993e-05,
      "loss": 0.175,
      "step": 233800
    },
    {
      "epoch": 0.8278708252516529,
      "grad_norm": 0.0009369088802486658,
      "learning_rate": 5.163875242450413e-05,
      "loss": 0.3022,
      "step": 233900
    },
    {
      "epoch": 0.8282247674599691,
      "grad_norm": 0.06810867041349411,
      "learning_rate": 5.1532569762009254e-05,
      "loss": 0.2405,
      "step": 234000
    },
    {
      "epoch": 0.8285787096682854,
      "grad_norm": 6.876270344946533e-05,
      "learning_rate": 5.142638709951439e-05,
      "loss": 0.1402,
      "step": 234100
    },
    {
      "epoch": 0.8289326518766016,
      "grad_norm": 0.0019880966283380985,
      "learning_rate": 5.132020443701952e-05,
      "loss": 0.1702,
      "step": 234200
    },
    {
      "epoch": 0.8292865940849178,
      "grad_norm": 0.0009751443867571652,
      "learning_rate": 5.1214021774524656e-05,
      "loss": 0.1257,
      "step": 234300
    },
    {
      "epoch": 0.8296405362932341,
      "grad_norm": 0.012817692942917347,
      "learning_rate": 5.110783911202978e-05,
      "loss": 0.1411,
      "step": 234400
    },
    {
      "epoch": 0.8299944785015503,
      "grad_norm": 0.5592098832130432,
      "learning_rate": 5.1001656449534916e-05,
      "loss": 0.1263,
      "step": 234500
    },
    {
      "epoch": 0.8303484207098665,
      "grad_norm": 0.02572549507021904,
      "learning_rate": 5.089547378704004e-05,
      "loss": 0.133,
      "step": 234600
    },
    {
      "epoch": 0.8307023629181827,
      "grad_norm": 0.0019029348623007536,
      "learning_rate": 5.078929112454518e-05,
      "loss": 0.1248,
      "step": 234700
    },
    {
      "epoch": 0.831056305126499,
      "grad_norm": 0.00019686436280608177,
      "learning_rate": 5.068310846205031e-05,
      "loss": 0.1766,
      "step": 234800
    },
    {
      "epoch": 0.8314102473348152,
      "grad_norm": 0.004317767918109894,
      "learning_rate": 5.0576925799555445e-05,
      "loss": 0.2748,
      "step": 234900
    },
    {
      "epoch": 0.8317641895431314,
      "grad_norm": 23.749540328979492,
      "learning_rate": 5.047074313706058e-05,
      "loss": 0.2,
      "step": 235000
    },
    {
      "epoch": 0.8321181317514477,
      "grad_norm": 0.00011874202755279839,
      "learning_rate": 5.0364560474565705e-05,
      "loss": 0.2201,
      "step": 235100
    },
    {
      "epoch": 0.8324720739597639,
      "grad_norm": 0.00030848680762574077,
      "learning_rate": 5.025837781207084e-05,
      "loss": 0.1971,
      "step": 235200
    },
    {
      "epoch": 0.83282601616808,
      "grad_norm": 0.01910070702433586,
      "learning_rate": 5.015219514957597e-05,
      "loss": 0.228,
      "step": 235300
    },
    {
      "epoch": 0.8331799583763964,
      "grad_norm": 11.364924430847168,
      "learning_rate": 5.004601248708111e-05,
      "loss": 0.1788,
      "step": 235400
    },
    {
      "epoch": 0.8335339005847126,
      "grad_norm": 0.000918794481549412,
      "learning_rate": 4.993982982458624e-05,
      "loss": 0.0737,
      "step": 235500
    },
    {
      "epoch": 0.8338878427930287,
      "grad_norm": 0.0018371397163718939,
      "learning_rate": 4.9833647162091374e-05,
      "loss": 0.2343,
      "step": 235600
    },
    {
      "epoch": 0.8342417850013449,
      "grad_norm": 0.00033123407047241926,
      "learning_rate": 4.9727464499596495e-05,
      "loss": 0.1718,
      "step": 235700
    },
    {
      "epoch": 0.8345957272096612,
      "grad_norm": 0.07462048530578613,
      "learning_rate": 4.962128183710163e-05,
      "loss": 0.2175,
      "step": 235800
    },
    {
      "epoch": 0.8349496694179774,
      "grad_norm": 5.61685737920925e-05,
      "learning_rate": 4.951509917460676e-05,
      "loss": 0.2905,
      "step": 235900
    },
    {
      "epoch": 0.8353036116262936,
      "grad_norm": 0.0003010020882356912,
      "learning_rate": 4.9408916512111896e-05,
      "loss": 0.2496,
      "step": 236000
    },
    {
      "epoch": 0.8356575538346099,
      "grad_norm": 0.003364074742421508,
      "learning_rate": 4.930273384961703e-05,
      "loss": 0.1831,
      "step": 236100
    },
    {
      "epoch": 0.8360114960429261,
      "grad_norm": 1.6307791156577878e-05,
      "learning_rate": 4.9196551187122163e-05,
      "loss": 0.0706,
      "step": 236200
    },
    {
      "epoch": 0.8363654382512423,
      "grad_norm": 0.013320362195372581,
      "learning_rate": 4.909036852462729e-05,
      "loss": 0.3663,
      "step": 236300
    },
    {
      "epoch": 0.8367193804595585,
      "grad_norm": 0.2665976285934448,
      "learning_rate": 4.8984185862132424e-05,
      "loss": 0.1337,
      "step": 236400
    },
    {
      "epoch": 0.8370733226678748,
      "grad_norm": 0.0019253608770668507,
      "learning_rate": 4.887800319963756e-05,
      "loss": 0.199,
      "step": 236500
    },
    {
      "epoch": 0.837427264876191,
      "grad_norm": 0.4609971046447754,
      "learning_rate": 4.877182053714269e-05,
      "loss": 0.2885,
      "step": 236600
    },
    {
      "epoch": 0.8377812070845072,
      "grad_norm": 24.149600982666016,
      "learning_rate": 4.8665637874647826e-05,
      "loss": 0.3238,
      "step": 236700
    },
    {
      "epoch": 0.8381351492928235,
      "grad_norm": 0.3897290825843811,
      "learning_rate": 4.855945521215296e-05,
      "loss": 0.107,
      "step": 236800
    },
    {
      "epoch": 0.8384890915011397,
      "grad_norm": 0.000640737940557301,
      "learning_rate": 4.8453272549658086e-05,
      "loss": 0.334,
      "step": 236900
    },
    {
      "epoch": 0.8388430337094559,
      "grad_norm": 0.00020865752594545484,
      "learning_rate": 4.834708988716322e-05,
      "loss": 0.2625,
      "step": 237000
    },
    {
      "epoch": 0.8391969759177721,
      "grad_norm": 13.330068588256836,
      "learning_rate": 4.8240907224668354e-05,
      "loss": 0.1926,
      "step": 237100
    },
    {
      "epoch": 0.8395509181260884,
      "grad_norm": 0.0005901831318624318,
      "learning_rate": 4.813472456217349e-05,
      "loss": 0.1806,
      "step": 237200
    },
    {
      "epoch": 0.8399048603344046,
      "grad_norm": 2.9140452170395292e-05,
      "learning_rate": 4.802854189967862e-05,
      "loss": 0.2637,
      "step": 237300
    },
    {
      "epoch": 0.8402588025427208,
      "grad_norm": 0.00016462878556922078,
      "learning_rate": 4.7922359237183755e-05,
      "loss": 0.1893,
      "step": 237400
    },
    {
      "epoch": 0.8406127447510371,
      "grad_norm": 0.0004444597288966179,
      "learning_rate": 4.7816176574688875e-05,
      "loss": 0.1331,
      "step": 237500
    },
    {
      "epoch": 0.8409666869593533,
      "grad_norm": 1.5835321391932666e-05,
      "learning_rate": 4.770999391219401e-05,
      "loss": 0.1996,
      "step": 237600
    },
    {
      "epoch": 0.8413206291676695,
      "grad_norm": 0.051124878227710724,
      "learning_rate": 4.760381124969914e-05,
      "loss": 0.326,
      "step": 237700
    },
    {
      "epoch": 0.8416745713759858,
      "grad_norm": 0.0005369175341911614,
      "learning_rate": 4.749762858720428e-05,
      "loss": 0.0559,
      "step": 237800
    },
    {
      "epoch": 0.842028513584302,
      "grad_norm": 0.0005344988312572241,
      "learning_rate": 4.739144592470941e-05,
      "loss": 0.0998,
      "step": 237900
    },
    {
      "epoch": 0.8423824557926182,
      "grad_norm": 0.05145874246954918,
      "learning_rate": 4.7285263262214544e-05,
      "loss": 0.2905,
      "step": 238000
    },
    {
      "epoch": 0.8427363980009344,
      "grad_norm": 0.008256163448095322,
      "learning_rate": 4.717908059971967e-05,
      "loss": 0.1449,
      "step": 238100
    },
    {
      "epoch": 0.8430903402092507,
      "grad_norm": 0.002829251578077674,
      "learning_rate": 4.7072897937224805e-05,
      "loss": 0.197,
      "step": 238200
    },
    {
      "epoch": 0.8434442824175669,
      "grad_norm": 3.5064385883742943e-05,
      "learning_rate": 4.696671527472994e-05,
      "loss": 0.1985,
      "step": 238300
    },
    {
      "epoch": 0.843798224625883,
      "grad_norm": 8.78281207405962e-05,
      "learning_rate": 4.686053261223507e-05,
      "loss": 0.4085,
      "step": 238400
    },
    {
      "epoch": 0.8441521668341994,
      "grad_norm": 0.000656657328363508,
      "learning_rate": 4.6754349949740206e-05,
      "loss": 0.1448,
      "step": 238500
    },
    {
      "epoch": 0.8445061090425156,
      "grad_norm": 0.0015282590175047517,
      "learning_rate": 4.6648167287245333e-05,
      "loss": 0.1389,
      "step": 238600
    },
    {
      "epoch": 0.8448600512508317,
      "grad_norm": 0.003458601189777255,
      "learning_rate": 4.654198462475047e-05,
      "loss": 0.0104,
      "step": 238700
    },
    {
      "epoch": 0.8452139934591479,
      "grad_norm": 0.11949994415044785,
      "learning_rate": 4.64358019622556e-05,
      "loss": 0.2732,
      "step": 238800
    },
    {
      "epoch": 0.8455679356674642,
      "grad_norm": 0.0038018468767404556,
      "learning_rate": 4.632961929976073e-05,
      "loss": 0.2284,
      "step": 238900
    },
    {
      "epoch": 0.8459218778757804,
      "grad_norm": 0.0006668113055638969,
      "learning_rate": 4.622343663726586e-05,
      "loss": 0.1855,
      "step": 239000
    },
    {
      "epoch": 0.8462758200840966,
      "grad_norm": 4.685136809712276e-05,
      "learning_rate": 4.6117253974770996e-05,
      "loss": 0.1628,
      "step": 239100
    },
    {
      "epoch": 0.8466297622924129,
      "grad_norm": 0.005729565862566233,
      "learning_rate": 4.601107131227612e-05,
      "loss": 0.2177,
      "step": 239200
    },
    {
      "epoch": 0.8469837045007291,
      "grad_norm": 0.009194397367537022,
      "learning_rate": 4.5904888649781256e-05,
      "loss": 0.2265,
      "step": 239300
    },
    {
      "epoch": 0.8473376467090453,
      "grad_norm": 0.004834556952118874,
      "learning_rate": 4.579870598728639e-05,
      "loss": 0.213,
      "step": 239400
    },
    {
      "epoch": 0.8476915889173616,
      "grad_norm": 0.00017707272490952164,
      "learning_rate": 4.5692523324791524e-05,
      "loss": 0.1813,
      "step": 239500
    },
    {
      "epoch": 0.8480455311256778,
      "grad_norm": 0.00012216073810122907,
      "learning_rate": 4.558634066229666e-05,
      "loss": 0.2393,
      "step": 239600
    },
    {
      "epoch": 0.848399473333994,
      "grad_norm": 0.01164676807820797,
      "learning_rate": 4.548015799980179e-05,
      "loss": 0.1592,
      "step": 239700
    },
    {
      "epoch": 0.8487534155423102,
      "grad_norm": 26.33708381652832,
      "learning_rate": 4.537397533730692e-05,
      "loss": 0.116,
      "step": 239800
    },
    {
      "epoch": 0.8491073577506265,
      "grad_norm": 0.0004050809075124562,
      "learning_rate": 4.526779267481205e-05,
      "loss": 0.198,
      "step": 239900
    },
    {
      "epoch": 0.8494612999589427,
      "grad_norm": 0.00011352550791343674,
      "learning_rate": 4.5161610012317186e-05,
      "loss": 0.0944,
      "step": 240000
    },
    {
      "epoch": 0.8498152421672589,
      "grad_norm": 0.009106258861720562,
      "learning_rate": 4.505542734982232e-05,
      "loss": 0.1787,
      "step": 240100
    },
    {
      "epoch": 0.8501691843755752,
      "grad_norm": 0.017496077343821526,
      "learning_rate": 4.4949244687327453e-05,
      "loss": 0.2858,
      "step": 240200
    },
    {
      "epoch": 0.8505231265838914,
      "grad_norm": 0.006516207940876484,
      "learning_rate": 4.484306202483259e-05,
      "loss": 0.1542,
      "step": 240300
    },
    {
      "epoch": 0.8508770687922076,
      "grad_norm": 0.004589854273945093,
      "learning_rate": 4.473687936233771e-05,
      "loss": 0.2421,
      "step": 240400
    },
    {
      "epoch": 0.8512310110005238,
      "grad_norm": 0.49879294633865356,
      "learning_rate": 4.463069669984284e-05,
      "loss": 0.2554,
      "step": 240500
    },
    {
      "epoch": 0.8515849532088401,
      "grad_norm": 0.010513977147638798,
      "learning_rate": 4.4524514037347975e-05,
      "loss": 0.2128,
      "step": 240600
    },
    {
      "epoch": 0.8519388954171563,
      "grad_norm": 0.0011395183391869068,
      "learning_rate": 4.441833137485311e-05,
      "loss": 0.107,
      "step": 240700
    },
    {
      "epoch": 0.8522928376254725,
      "grad_norm": 0.00036104139871895313,
      "learning_rate": 4.431214871235824e-05,
      "loss": 0.362,
      "step": 240800
    },
    {
      "epoch": 0.8526467798337888,
      "grad_norm": 0.0002573499805293977,
      "learning_rate": 4.4205966049863376e-05,
      "loss": 0.2363,
      "step": 240900
    },
    {
      "epoch": 0.853000722042105,
      "grad_norm": 24.35438346862793,
      "learning_rate": 4.40997833873685e-05,
      "loss": 0.0955,
      "step": 241000
    },
    {
      "epoch": 0.8533546642504212,
      "grad_norm": 0.0004944401443935931,
      "learning_rate": 4.399360072487364e-05,
      "loss": 0.0992,
      "step": 241100
    },
    {
      "epoch": 0.8537086064587374,
      "grad_norm": 0.0004185832221992314,
      "learning_rate": 4.388741806237877e-05,
      "loss": 0.2116,
      "step": 241200
    },
    {
      "epoch": 0.8540625486670537,
      "grad_norm": 0.006851145997643471,
      "learning_rate": 4.3781235399883905e-05,
      "loss": 0.2894,
      "step": 241300
    },
    {
      "epoch": 0.8544164908753699,
      "grad_norm": 1.4093066453933716,
      "learning_rate": 4.367505273738904e-05,
      "loss": 0.1422,
      "step": 241400
    },
    {
      "epoch": 0.854770433083686,
      "grad_norm": 0.010437756776809692,
      "learning_rate": 4.356887007489417e-05,
      "loss": 0.2208,
      "step": 241500
    },
    {
      "epoch": 0.8551243752920024,
      "grad_norm": 0.00044060806976631284,
      "learning_rate": 4.34626874123993e-05,
      "loss": 0.4259,
      "step": 241600
    },
    {
      "epoch": 0.8554783175003186,
      "grad_norm": 0.04139263182878494,
      "learning_rate": 4.335650474990443e-05,
      "loss": 0.2846,
      "step": 241700
    },
    {
      "epoch": 0.8558322597086347,
      "grad_norm": 7.5809149742126465,
      "learning_rate": 4.325032208740957e-05,
      "loss": 0.0892,
      "step": 241800
    },
    {
      "epoch": 0.856186201916951,
      "grad_norm": 0.00016080641944427043,
      "learning_rate": 4.31441394249147e-05,
      "loss": 0.2448,
      "step": 241900
    },
    {
      "epoch": 0.8565401441252672,
      "grad_norm": 0.0001062527226167731,
      "learning_rate": 4.3037956762419834e-05,
      "loss": 0.1362,
      "step": 242000
    },
    {
      "epoch": 0.8568940863335834,
      "grad_norm": 0.0074071199633181095,
      "learning_rate": 4.2931774099924955e-05,
      "loss": 0.0925,
      "step": 242100
    },
    {
      "epoch": 0.8572480285418996,
      "grad_norm": 0.00037086603697389364,
      "learning_rate": 4.282559143743009e-05,
      "loss": 0.2571,
      "step": 242200
    },
    {
      "epoch": 0.8576019707502159,
      "grad_norm": 4.0380375139648095e-05,
      "learning_rate": 4.271940877493522e-05,
      "loss": 0.092,
      "step": 242300
    },
    {
      "epoch": 0.8579559129585321,
      "grad_norm": 0.00038604409201070666,
      "learning_rate": 4.2613226112440356e-05,
      "loss": 0.1907,
      "step": 242400
    },
    {
      "epoch": 0.8583098551668483,
      "grad_norm": 0.003192279953509569,
      "learning_rate": 4.250704344994549e-05,
      "loss": 0.1728,
      "step": 242500
    },
    {
      "epoch": 0.8586637973751646,
      "grad_norm": 7.551940507255495e-05,
      "learning_rate": 4.2400860787450623e-05,
      "loss": 0.113,
      "step": 242600
    },
    {
      "epoch": 0.8590177395834808,
      "grad_norm": 0.01225069910287857,
      "learning_rate": 4.229467812495575e-05,
      "loss": 0.1399,
      "step": 242700
    },
    {
      "epoch": 0.859371681791797,
      "grad_norm": 0.0019376090494915843,
      "learning_rate": 4.2188495462460884e-05,
      "loss": 0.2351,
      "step": 242800
    },
    {
      "epoch": 0.8597256240001132,
      "grad_norm": 0.00019588557188399136,
      "learning_rate": 4.208231279996602e-05,
      "loss": 0.1692,
      "step": 242900
    },
    {
      "epoch": 0.8600795662084295,
      "grad_norm": 3.3299213100690395e-05,
      "learning_rate": 4.197613013747115e-05,
      "loss": 0.1476,
      "step": 243000
    },
    {
      "epoch": 0.8604335084167457,
      "grad_norm": 0.007259091828018427,
      "learning_rate": 4.1869947474976286e-05,
      "loss": 0.1127,
      "step": 243100
    },
    {
      "epoch": 0.8607874506250619,
      "grad_norm": 6.1026905314065516e-05,
      "learning_rate": 4.176376481248142e-05,
      "loss": 0.158,
      "step": 243200
    },
    {
      "epoch": 0.8611413928333782,
      "grad_norm": 0.0017006603302434087,
      "learning_rate": 4.1657582149986546e-05,
      "loss": 0.1837,
      "step": 243300
    },
    {
      "epoch": 0.8614953350416944,
      "grad_norm": 8.089209586614743e-05,
      "learning_rate": 4.155139948749167e-05,
      "loss": 0.1657,
      "step": 243400
    },
    {
      "epoch": 0.8618492772500106,
      "grad_norm": 0.0008256295113824308,
      "learning_rate": 4.144521682499681e-05,
      "loss": 0.206,
      "step": 243500
    },
    {
      "epoch": 0.8622032194583269,
      "grad_norm": 0.0017408968415111303,
      "learning_rate": 4.133903416250194e-05,
      "loss": 0.1169,
      "step": 243600
    },
    {
      "epoch": 0.8625571616666431,
      "grad_norm": 0.00011526756134117022,
      "learning_rate": 4.1232851500007075e-05,
      "loss": 0.1242,
      "step": 243700
    },
    {
      "epoch": 0.8629111038749593,
      "grad_norm": 4.652023792266846,
      "learning_rate": 4.112666883751221e-05,
      "loss": 0.1318,
      "step": 243800
    },
    {
      "epoch": 0.8632650460832755,
      "grad_norm": 7.698089757468551e-05,
      "learning_rate": 4.1020486175017335e-05,
      "loss": 0.1745,
      "step": 243900
    },
    {
      "epoch": 0.8636189882915918,
      "grad_norm": 0.0030834802892059088,
      "learning_rate": 4.091430351252247e-05,
      "loss": 0.2001,
      "step": 244000
    },
    {
      "epoch": 0.863972930499908,
      "grad_norm": 0.00016622951079625636,
      "learning_rate": 4.08081208500276e-05,
      "loss": 0.1236,
      "step": 244100
    },
    {
      "epoch": 0.8643268727082242,
      "grad_norm": 0.030788687989115715,
      "learning_rate": 4.070193818753274e-05,
      "loss": 0.1581,
      "step": 244200
    },
    {
      "epoch": 0.8646808149165405,
      "grad_norm": 10.169416427612305,
      "learning_rate": 4.059575552503787e-05,
      "loss": 0.2936,
      "step": 244300
    },
    {
      "epoch": 0.8650347571248567,
      "grad_norm": 0.0002263030328322202,
      "learning_rate": 4.0489572862543004e-05,
      "loss": 0.1187,
      "step": 244400
    },
    {
      "epoch": 0.8653886993331729,
      "grad_norm": 0.0016159965889528394,
      "learning_rate": 4.038339020004813e-05,
      "loss": 0.1493,
      "step": 244500
    },
    {
      "epoch": 0.865742641541489,
      "grad_norm": 7.140787784010172e-05,
      "learning_rate": 4.0277207537553265e-05,
      "loss": 0.2101,
      "step": 244600
    },
    {
      "epoch": 0.8660965837498054,
      "grad_norm": 117.19624328613281,
      "learning_rate": 4.01710248750584e-05,
      "loss": 0.2088,
      "step": 244700
    },
    {
      "epoch": 0.8664505259581216,
      "grad_norm": 0.0067652929574251175,
      "learning_rate": 4.006484221256353e-05,
      "loss": 0.2269,
      "step": 244800
    },
    {
      "epoch": 0.8668044681664377,
      "grad_norm": 0.00024456766550429165,
      "learning_rate": 3.9958659550068666e-05,
      "loss": 0.2405,
      "step": 244900
    },
    {
      "epoch": 0.867158410374754,
      "grad_norm": 0.001141811953857541,
      "learning_rate": 3.985247688757379e-05,
      "loss": 0.102,
      "step": 245000
    },
    {
      "epoch": 0.8675123525830702,
      "grad_norm": 0.0008645047200843692,
      "learning_rate": 3.974629422507892e-05,
      "loss": 0.0637,
      "step": 245100
    },
    {
      "epoch": 0.8678662947913864,
      "grad_norm": 0.000261999957729131,
      "learning_rate": 3.9640111562584054e-05,
      "loss": 0.1338,
      "step": 245200
    },
    {
      "epoch": 0.8682202369997027,
      "grad_norm": 0.01153638120740652,
      "learning_rate": 3.953392890008919e-05,
      "loss": 0.2273,
      "step": 245300
    },
    {
      "epoch": 0.8685741792080189,
      "grad_norm": 0.0008909644093364477,
      "learning_rate": 3.942774623759432e-05,
      "loss": 0.2831,
      "step": 245400
    },
    {
      "epoch": 0.8689281214163351,
      "grad_norm": 0.0007984198164194822,
      "learning_rate": 3.9321563575099455e-05,
      "loss": 0.2962,
      "step": 245500
    },
    {
      "epoch": 0.8692820636246513,
      "grad_norm": 75.71271514892578,
      "learning_rate": 3.921538091260458e-05,
      "loss": 0.1135,
      "step": 245600
    },
    {
      "epoch": 0.8696360058329676,
      "grad_norm": 13.270423889160156,
      "learning_rate": 3.9109198250109716e-05,
      "loss": 0.1584,
      "step": 245700
    },
    {
      "epoch": 0.8699899480412838,
      "grad_norm": 8.892254118109122e-05,
      "learning_rate": 3.900301558761485e-05,
      "loss": 0.1587,
      "step": 245800
    },
    {
      "epoch": 0.8703438902496,
      "grad_norm": 0.0008389653521589935,
      "learning_rate": 3.8896832925119984e-05,
      "loss": 0.1353,
      "step": 245900
    },
    {
      "epoch": 0.8706978324579163,
      "grad_norm": 29.56011199951172,
      "learning_rate": 3.879065026262512e-05,
      "loss": 0.1581,
      "step": 246000
    },
    {
      "epoch": 0.8710517746662325,
      "grad_norm": 0.00015997442824300379,
      "learning_rate": 3.868446760013025e-05,
      "loss": 0.1738,
      "step": 246100
    },
    {
      "epoch": 0.8714057168745487,
      "grad_norm": 32.3282356262207,
      "learning_rate": 3.857828493763538e-05,
      "loss": 0.2857,
      "step": 246200
    },
    {
      "epoch": 0.8717596590828649,
      "grad_norm": 0.0008310438133776188,
      "learning_rate": 3.847210227514051e-05,
      "loss": 0.2538,
      "step": 246300
    },
    {
      "epoch": 0.8721136012911812,
      "grad_norm": 0.0008190397638827562,
      "learning_rate": 3.8365919612645646e-05,
      "loss": 0.2409,
      "step": 246400
    },
    {
      "epoch": 0.8724675434994974,
      "grad_norm": 0.00036393862683326006,
      "learning_rate": 3.825973695015078e-05,
      "loss": 0.0874,
      "step": 246500
    },
    {
      "epoch": 0.8728214857078136,
      "grad_norm": 0.0007661677664145827,
      "learning_rate": 3.8153554287655913e-05,
      "loss": 0.2254,
      "step": 246600
    },
    {
      "epoch": 0.8731754279161299,
      "grad_norm": 3.728255033493042,
      "learning_rate": 3.804737162516105e-05,
      "loss": 0.277,
      "step": 246700
    },
    {
      "epoch": 0.8735293701244461,
      "grad_norm": 0.1960606724023819,
      "learning_rate": 3.794118896266617e-05,
      "loss": 0.1697,
      "step": 246800
    },
    {
      "epoch": 0.8738833123327623,
      "grad_norm": 0.00027070831856690347,
      "learning_rate": 3.78350063001713e-05,
      "loss": 0.1415,
      "step": 246900
    },
    {
      "epoch": 0.8742372545410785,
      "grad_norm": 0.0007699851994402707,
      "learning_rate": 3.7728823637676435e-05,
      "loss": 0.1007,
      "step": 247000
    },
    {
      "epoch": 0.8745911967493948,
      "grad_norm": 0.000333855306962505,
      "learning_rate": 3.762264097518157e-05,
      "loss": 0.2258,
      "step": 247100
    },
    {
      "epoch": 0.874945138957711,
      "grad_norm": 5.318057537078857,
      "learning_rate": 3.75164583126867e-05,
      "loss": 0.1238,
      "step": 247200
    },
    {
      "epoch": 0.8752990811660272,
      "grad_norm": 0.0002402072277618572,
      "learning_rate": 3.7410275650191836e-05,
      "loss": 0.1514,
      "step": 247300
    },
    {
      "epoch": 0.8756530233743435,
      "grad_norm": 5.645817756652832,
      "learning_rate": 3.730409298769696e-05,
      "loss": 0.1339,
      "step": 247400
    },
    {
      "epoch": 0.8760069655826597,
      "grad_norm": 0.027034331113100052,
      "learning_rate": 3.71979103252021e-05,
      "loss": 0.1519,
      "step": 247500
    },
    {
      "epoch": 0.8763609077909759,
      "grad_norm": 0.1017998605966568,
      "learning_rate": 3.709172766270723e-05,
      "loss": 0.1416,
      "step": 247600
    },
    {
      "epoch": 0.8767148499992922,
      "grad_norm": 18.102476119995117,
      "learning_rate": 3.6985545000212365e-05,
      "loss": 0.1816,
      "step": 247700
    },
    {
      "epoch": 0.8770687922076084,
      "grad_norm": 7.597717922180891e-05,
      "learning_rate": 3.687936233771749e-05,
      "loss": 0.2242,
      "step": 247800
    },
    {
      "epoch": 0.8774227344159246,
      "grad_norm": 1.0492154359817505,
      "learning_rate": 3.6773179675222625e-05,
      "loss": 0.1782,
      "step": 247900
    },
    {
      "epoch": 0.8777766766242407,
      "grad_norm": 0.0007494253804907203,
      "learning_rate": 3.666699701272776e-05,
      "loss": 0.2186,
      "step": 248000
    },
    {
      "epoch": 0.878130618832557,
      "grad_norm": 0.001636543427594006,
      "learning_rate": 3.6560814350232886e-05,
      "loss": 0.2106,
      "step": 248100
    },
    {
      "epoch": 0.8784845610408732,
      "grad_norm": 0.00025918034953065217,
      "learning_rate": 3.645463168773802e-05,
      "loss": 0.3314,
      "step": 248200
    },
    {
      "epoch": 0.8788385032491894,
      "grad_norm": 0.00032756439759396017,
      "learning_rate": 3.6348449025243154e-05,
      "loss": 0.2493,
      "step": 248300
    },
    {
      "epoch": 0.8791924454575057,
      "grad_norm": 0.20383891463279724,
      "learning_rate": 3.624226636274829e-05,
      "loss": 0.2847,
      "step": 248400
    },
    {
      "epoch": 0.8795463876658219,
      "grad_norm": 15.456968307495117,
      "learning_rate": 3.613608370025342e-05,
      "loss": 0.2463,
      "step": 248500
    },
    {
      "epoch": 0.8799003298741381,
      "grad_norm": 0.00039069619379006326,
      "learning_rate": 3.6029901037758555e-05,
      "loss": 0.1416,
      "step": 248600
    },
    {
      "epoch": 0.8802542720824543,
      "grad_norm": 0.0018422984285280108,
      "learning_rate": 3.592371837526368e-05,
      "loss": 0.0936,
      "step": 248700
    },
    {
      "epoch": 0.8806082142907706,
      "grad_norm": 0.00021407516032923013,
      "learning_rate": 3.5817535712768816e-05,
      "loss": 0.2052,
      "step": 248800
    },
    {
      "epoch": 0.8809621564990868,
      "grad_norm": 0.03225424885749817,
      "learning_rate": 3.571135305027395e-05,
      "loss": 0.192,
      "step": 248900
    },
    {
      "epoch": 0.881316098707403,
      "grad_norm": 0.0005342514486983418,
      "learning_rate": 3.560517038777908e-05,
      "loss": 0.0513,
      "step": 249000
    },
    {
      "epoch": 0.8816700409157193,
      "grad_norm": 105.7606201171875,
      "learning_rate": 3.549898772528421e-05,
      "loss": 0.2167,
      "step": 249100
    },
    {
      "epoch": 0.8820239831240355,
      "grad_norm": 0.0007074816385284066,
      "learning_rate": 3.5392805062789344e-05,
      "loss": 0.0947,
      "step": 249200
    },
    {
      "epoch": 0.8823779253323517,
      "grad_norm": 0.010761291719973087,
      "learning_rate": 3.528662240029448e-05,
      "loss": 0.2367,
      "step": 249300
    },
    {
      "epoch": 0.882731867540668,
      "grad_norm": 0.0015712982276454568,
      "learning_rate": 3.518043973779961e-05,
      "loss": 0.1431,
      "step": 249400
    },
    {
      "epoch": 0.8830858097489842,
      "grad_norm": 0.00047997571527957916,
      "learning_rate": 3.5074257075304746e-05,
      "loss": 0.1627,
      "step": 249500
    },
    {
      "epoch": 0.8834397519573004,
      "grad_norm": 0.006346798036247492,
      "learning_rate": 3.496807441280987e-05,
      "loss": 0.1179,
      "step": 249600
    },
    {
      "epoch": 0.8837936941656166,
      "grad_norm": 0.0005730061675421894,
      "learning_rate": 3.4861891750315006e-05,
      "loss": 0.1815,
      "step": 249700
    },
    {
      "epoch": 0.8841476363739329,
      "grad_norm": 0.006449081003665924,
      "learning_rate": 3.475570908782014e-05,
      "loss": 0.0583,
      "step": 249800
    },
    {
      "epoch": 0.8845015785822491,
      "grad_norm": 9.089762170333415e-05,
      "learning_rate": 3.464952642532527e-05,
      "loss": 0.1018,
      "step": 249900
    },
    {
      "epoch": 0.8848555207905653,
      "grad_norm": 0.00028077748720534146,
      "learning_rate": 3.45433437628304e-05,
      "loss": 0.1824,
      "step": 250000
    },
    {
      "epoch": 0.8852094629988816,
      "grad_norm": 0.00015846212045289576,
      "learning_rate": 3.4437161100335535e-05,
      "loss": 0.3733,
      "step": 250100
    },
    {
      "epoch": 0.8855634052071978,
      "grad_norm": 0.0002137667906936258,
      "learning_rate": 3.433097843784067e-05,
      "loss": 0.1485,
      "step": 250200
    },
    {
      "epoch": 0.885917347415514,
      "grad_norm": 0.057126984000205994,
      "learning_rate": 3.42247957753458e-05,
      "loss": 0.1874,
      "step": 250300
    },
    {
      "epoch": 0.8862712896238302,
      "grad_norm": 0.0008635343401692808,
      "learning_rate": 3.411861311285093e-05,
      "loss": 0.2326,
      "step": 250400
    },
    {
      "epoch": 0.8866252318321465,
      "grad_norm": 0.0011570921633392572,
      "learning_rate": 3.401243045035606e-05,
      "loss": 0.2815,
      "step": 250500
    },
    {
      "epoch": 0.8869791740404627,
      "grad_norm": 3.832470247289166e-05,
      "learning_rate": 3.39062477878612e-05,
      "loss": 0.1974,
      "step": 250600
    },
    {
      "epoch": 0.8873331162487789,
      "grad_norm": 1.7424646615982056,
      "learning_rate": 3.3800065125366324e-05,
      "loss": 0.1872,
      "step": 250700
    },
    {
      "epoch": 0.8876870584570952,
      "grad_norm": 0.0018323897384107113,
      "learning_rate": 3.369388246287146e-05,
      "loss": 0.2242,
      "step": 250800
    },
    {
      "epoch": 0.8880410006654114,
      "grad_norm": 0.012729531154036522,
      "learning_rate": 3.358769980037659e-05,
      "loss": 0.1319,
      "step": 250900
    },
    {
      "epoch": 0.8883949428737276,
      "grad_norm": 7.813546835677698e-05,
      "learning_rate": 3.3481517137881725e-05,
      "loss": 0.2237,
      "step": 251000
    },
    {
      "epoch": 0.8887488850820437,
      "grad_norm": 0.00010539934737607837,
      "learning_rate": 3.337533447538686e-05,
      "loss": 0.1344,
      "step": 251100
    },
    {
      "epoch": 0.88910282729036,
      "grad_norm": 0.007157749030739069,
      "learning_rate": 3.326915181289199e-05,
      "loss": 0.2055,
      "step": 251200
    },
    {
      "epoch": 0.8894567694986762,
      "grad_norm": 0.0024809869937598705,
      "learning_rate": 3.316296915039712e-05,
      "loss": 0.1409,
      "step": 251300
    },
    {
      "epoch": 0.8898107117069924,
      "grad_norm": 2.8373353481292725,
      "learning_rate": 3.305678648790225e-05,
      "loss": 0.1793,
      "step": 251400
    },
    {
      "epoch": 0.8901646539153087,
      "grad_norm": 7.371642277576029e-05,
      "learning_rate": 3.295060382540739e-05,
      "loss": 0.2144,
      "step": 251500
    },
    {
      "epoch": 0.8905185961236249,
      "grad_norm": 0.003165728412568569,
      "learning_rate": 3.2844421162912514e-05,
      "loss": 0.1697,
      "step": 251600
    },
    {
      "epoch": 0.8908725383319411,
      "grad_norm": 0.8298339247703552,
      "learning_rate": 3.273823850041765e-05,
      "loss": 0.2768,
      "step": 251700
    },
    {
      "epoch": 0.8912264805402574,
      "grad_norm": 39.44496536254883,
      "learning_rate": 3.263205583792278e-05,
      "loss": 0.1265,
      "step": 251800
    },
    {
      "epoch": 0.8915804227485736,
      "grad_norm": 0.03934092074632645,
      "learning_rate": 3.252587317542791e-05,
      "loss": 0.2199,
      "step": 251900
    },
    {
      "epoch": 0.8919343649568898,
      "grad_norm": 0.01690775156021118,
      "learning_rate": 3.241969051293304e-05,
      "loss": 0.1466,
      "step": 252000
    },
    {
      "epoch": 0.892288307165206,
      "grad_norm": 0.002492920495569706,
      "learning_rate": 3.2313507850438176e-05,
      "loss": 0.1593,
      "step": 252100
    },
    {
      "epoch": 0.8926422493735223,
      "grad_norm": 0.0012818382820114493,
      "learning_rate": 3.220732518794331e-05,
      "loss": 0.3649,
      "step": 252200
    },
    {
      "epoch": 0.8929961915818385,
      "grad_norm": 0.005240046419203281,
      "learning_rate": 3.2101142525448444e-05,
      "loss": 0.2372,
      "step": 252300
    },
    {
      "epoch": 0.8933501337901547,
      "grad_norm": 56.742095947265625,
      "learning_rate": 3.199495986295358e-05,
      "loss": 0.3508,
      "step": 252400
    },
    {
      "epoch": 0.893704075998471,
      "grad_norm": 0.03265965357422829,
      "learning_rate": 3.1888777200458705e-05,
      "loss": 0.1734,
      "step": 252500
    },
    {
      "epoch": 0.8940580182067872,
      "grad_norm": 0.00047711399383842945,
      "learning_rate": 3.178259453796384e-05,
      "loss": 0.1961,
      "step": 252600
    },
    {
      "epoch": 0.8944119604151034,
      "grad_norm": 0.0007613282068632543,
      "learning_rate": 3.167641187546897e-05,
      "loss": 0.1508,
      "step": 252700
    },
    {
      "epoch": 0.8947659026234196,
      "grad_norm": 7.373657717835158e-05,
      "learning_rate": 3.15702292129741e-05,
      "loss": 0.1817,
      "step": 252800
    },
    {
      "epoch": 0.8951198448317359,
      "grad_norm": 9.008340835571289,
      "learning_rate": 3.146404655047923e-05,
      "loss": 0.2236,
      "step": 252900
    },
    {
      "epoch": 0.8954737870400521,
      "grad_norm": 0.016066009178757668,
      "learning_rate": 3.135786388798437e-05,
      "loss": 0.3329,
      "step": 253000
    },
    {
      "epoch": 0.8958277292483683,
      "grad_norm": 26.719810485839844,
      "learning_rate": 3.12516812254895e-05,
      "loss": 0.2083,
      "step": 253100
    },
    {
      "epoch": 0.8961816714566846,
      "grad_norm": 0.0007128325523808599,
      "learning_rate": 3.1145498562994634e-05,
      "loss": 0.2289,
      "step": 253200
    },
    {
      "epoch": 0.8965356136650008,
      "grad_norm": 0.059321437031030655,
      "learning_rate": 3.103931590049977e-05,
      "loss": 0.1273,
      "step": 253300
    },
    {
      "epoch": 0.896889555873317,
      "grad_norm": 0.0014145206660032272,
      "learning_rate": 3.0933133238004895e-05,
      "loss": 0.1549,
      "step": 253400
    },
    {
      "epoch": 0.8972434980816333,
      "grad_norm": 0.6294470429420471,
      "learning_rate": 3.082695057551003e-05,
      "loss": 0.1377,
      "step": 253500
    },
    {
      "epoch": 0.8975974402899495,
      "grad_norm": 1.2117379903793335,
      "learning_rate": 3.0720767913015156e-05,
      "loss": 0.1883,
      "step": 253600
    },
    {
      "epoch": 0.8979513824982657,
      "grad_norm": 0.008494403213262558,
      "learning_rate": 3.061458525052029e-05,
      "loss": 0.229,
      "step": 253700
    },
    {
      "epoch": 0.8983053247065819,
      "grad_norm": 0.00019866797083523124,
      "learning_rate": 3.0508402588025427e-05,
      "loss": 0.1846,
      "step": 253800
    },
    {
      "epoch": 0.8986592669148982,
      "grad_norm": 0.0013530937721952796,
      "learning_rate": 3.0402219925530554e-05,
      "loss": 0.2529,
      "step": 253900
    },
    {
      "epoch": 0.8990132091232144,
      "grad_norm": 0.03580333665013313,
      "learning_rate": 3.0296037263035687e-05,
      "loss": 0.268,
      "step": 254000
    },
    {
      "epoch": 0.8993671513315306,
      "grad_norm": 0.0021285712718963623,
      "learning_rate": 3.018985460054082e-05,
      "loss": 0.1457,
      "step": 254100
    },
    {
      "epoch": 0.8997210935398469,
      "grad_norm": 0.0004877003957517445,
      "learning_rate": 3.008367193804595e-05,
      "loss": 0.2064,
      "step": 254200
    },
    {
      "epoch": 0.900075035748163,
      "grad_norm": 0.0014150735223665833,
      "learning_rate": 2.9977489275551085e-05,
      "loss": 0.1859,
      "step": 254300
    },
    {
      "epoch": 0.9004289779564792,
      "grad_norm": 0.00791869219392538,
      "learning_rate": 2.987130661305622e-05,
      "loss": 0.2656,
      "step": 254400
    },
    {
      "epoch": 0.9007829201647954,
      "grad_norm": 0.00017916003707796335,
      "learning_rate": 2.976512395056135e-05,
      "loss": 0.0473,
      "step": 254500
    },
    {
      "epoch": 0.9011368623731117,
      "grad_norm": 0.000861393054947257,
      "learning_rate": 2.9658941288066483e-05,
      "loss": 0.3149,
      "step": 254600
    },
    {
      "epoch": 0.9014908045814279,
      "grad_norm": 0.00015109516971278936,
      "learning_rate": 2.9552758625571614e-05,
      "loss": 0.1281,
      "step": 254700
    },
    {
      "epoch": 0.9018447467897441,
      "grad_norm": 0.0002956916578114033,
      "learning_rate": 2.9446575963076744e-05,
      "loss": 0.0178,
      "step": 254800
    },
    {
      "epoch": 0.9021986889980604,
      "grad_norm": 0.03835310414433479,
      "learning_rate": 2.9340393300581878e-05,
      "loss": 0.1944,
      "step": 254900
    },
    {
      "epoch": 0.9025526312063766,
      "grad_norm": 0.0012278511421754956,
      "learning_rate": 2.9234210638087012e-05,
      "loss": 0.2103,
      "step": 255000
    },
    {
      "epoch": 0.9029065734146928,
      "grad_norm": 0.001002483069896698,
      "learning_rate": 2.9128027975592142e-05,
      "loss": 0.1095,
      "step": 255100
    },
    {
      "epoch": 0.9032605156230091,
      "grad_norm": 4.6959350584074855e-05,
      "learning_rate": 2.9021845313097276e-05,
      "loss": 0.1451,
      "step": 255200
    },
    {
      "epoch": 0.9036144578313253,
      "grad_norm": 86.00602722167969,
      "learning_rate": 2.891566265060241e-05,
      "loss": 0.1159,
      "step": 255300
    },
    {
      "epoch": 0.9039684000396415,
      "grad_norm": 0.0003317167575005442,
      "learning_rate": 2.8809479988107537e-05,
      "loss": 0.1669,
      "step": 255400
    },
    {
      "epoch": 0.9043223422479577,
      "grad_norm": 0.00033007736783474684,
      "learning_rate": 2.870329732561267e-05,
      "loss": 0.1276,
      "step": 255500
    },
    {
      "epoch": 0.904676284456274,
      "grad_norm": 0.0006195571040734649,
      "learning_rate": 2.8597114663117804e-05,
      "loss": 0.109,
      "step": 255600
    },
    {
      "epoch": 0.9050302266645902,
      "grad_norm": 8.982065992313437e-06,
      "learning_rate": 2.8490932000622935e-05,
      "loss": 0.223,
      "step": 255700
    },
    {
      "epoch": 0.9053841688729064,
      "grad_norm": 0.0035166586749255657,
      "learning_rate": 2.838474933812807e-05,
      "loss": 0.0946,
      "step": 255800
    },
    {
      "epoch": 0.9057381110812227,
      "grad_norm": 0.0018922570161521435,
      "learning_rate": 2.8278566675633202e-05,
      "loss": 0.0862,
      "step": 255900
    },
    {
      "epoch": 0.9060920532895389,
      "grad_norm": 0.00040078681195154786,
      "learning_rate": 2.8172384013138333e-05,
      "loss": 0.1326,
      "step": 256000
    },
    {
      "epoch": 0.9064459954978551,
      "grad_norm": 0.003238723147660494,
      "learning_rate": 2.8066201350643466e-05,
      "loss": 0.2149,
      "step": 256100
    },
    {
      "epoch": 0.9067999377061713,
      "grad_norm": 0.002540852641686797,
      "learning_rate": 2.79600186881486e-05,
      "loss": 0.1246,
      "step": 256200
    },
    {
      "epoch": 0.9071538799144876,
      "grad_norm": 0.08670489490032196,
      "learning_rate": 2.7853836025653727e-05,
      "loss": 0.1711,
      "step": 256300
    },
    {
      "epoch": 0.9075078221228038,
      "grad_norm": 0.0003662695817183703,
      "learning_rate": 2.774765336315886e-05,
      "loss": 0.1601,
      "step": 256400
    },
    {
      "epoch": 0.90786176433112,
      "grad_norm": 0.0006319489912129939,
      "learning_rate": 2.7641470700663995e-05,
      "loss": 0.2863,
      "step": 256500
    },
    {
      "epoch": 0.9082157065394363,
      "grad_norm": 0.0006282046088017523,
      "learning_rate": 2.7535288038169125e-05,
      "loss": 0.1444,
      "step": 256600
    },
    {
      "epoch": 0.9085696487477525,
      "grad_norm": 13.457613945007324,
      "learning_rate": 2.742910537567426e-05,
      "loss": 0.256,
      "step": 256700
    },
    {
      "epoch": 0.9089235909560687,
      "grad_norm": 0.00025603853282518685,
      "learning_rate": 2.732292271317939e-05,
      "loss": 0.0703,
      "step": 256800
    },
    {
      "epoch": 0.9092775331643849,
      "grad_norm": 0.00012360051914583892,
      "learning_rate": 2.721674005068452e-05,
      "loss": 0.2124,
      "step": 256900
    },
    {
      "epoch": 0.9096314753727012,
      "grad_norm": 0.0001057281406247057,
      "learning_rate": 2.7110557388189653e-05,
      "loss": 0.1213,
      "step": 257000
    },
    {
      "epoch": 0.9099854175810174,
      "grad_norm": 0.0020671908278018236,
      "learning_rate": 2.7004374725694784e-05,
      "loss": 0.1717,
      "step": 257100
    },
    {
      "epoch": 0.9103393597893336,
      "grad_norm": 0.0038503045216202736,
      "learning_rate": 2.6898192063199917e-05,
      "loss": 0.0937,
      "step": 257200
    },
    {
      "epoch": 0.9106933019976499,
      "grad_norm": 0.00040130302659235895,
      "learning_rate": 2.679200940070505e-05,
      "loss": 0.1257,
      "step": 257300
    },
    {
      "epoch": 0.9110472442059661,
      "grad_norm": 0.0003986780939158052,
      "learning_rate": 2.668582673821018e-05,
      "loss": 0.1749,
      "step": 257400
    },
    {
      "epoch": 0.9114011864142822,
      "grad_norm": 0.0006903210887685418,
      "learning_rate": 2.6579644075715315e-05,
      "loss": 0.1868,
      "step": 257500
    },
    {
      "epoch": 0.9117551286225986,
      "grad_norm": 0.0009692988242022693,
      "learning_rate": 2.647346141322045e-05,
      "loss": 0.185,
      "step": 257600
    },
    {
      "epoch": 0.9121090708309147,
      "grad_norm": 33.62287139892578,
      "learning_rate": 2.6367278750725576e-05,
      "loss": 0.1663,
      "step": 257700
    },
    {
      "epoch": 0.9124630130392309,
      "grad_norm": 0.3412382900714874,
      "learning_rate": 2.626109608823071e-05,
      "loss": 0.2329,
      "step": 257800
    },
    {
      "epoch": 0.9128169552475471,
      "grad_norm": 0.06838667392730713,
      "learning_rate": 2.6154913425735844e-05,
      "loss": 0.1599,
      "step": 257900
    },
    {
      "epoch": 0.9131708974558634,
      "grad_norm": 3.455594539642334,
      "learning_rate": 2.6048730763240974e-05,
      "loss": 0.1392,
      "step": 258000
    },
    {
      "epoch": 0.9135248396641796,
      "grad_norm": 70.30055236816406,
      "learning_rate": 2.5942548100746108e-05,
      "loss": 0.2754,
      "step": 258100
    },
    {
      "epoch": 0.9138787818724958,
      "grad_norm": 0.00028619004297070205,
      "learning_rate": 2.583636543825124e-05,
      "loss": 0.1285,
      "step": 258200
    },
    {
      "epoch": 0.9142327240808121,
      "grad_norm": 0.00036096666008234024,
      "learning_rate": 2.5730182775756372e-05,
      "loss": 0.0755,
      "step": 258300
    },
    {
      "epoch": 0.9145866662891283,
      "grad_norm": 0.0010519650531932712,
      "learning_rate": 2.5624000113261506e-05,
      "loss": 0.0978,
      "step": 258400
    },
    {
      "epoch": 0.9149406084974445,
      "grad_norm": 55.38677978515625,
      "learning_rate": 2.551781745076664e-05,
      "loss": 0.2607,
      "step": 258500
    },
    {
      "epoch": 0.9152945507057607,
      "grad_norm": 0.002958615543320775,
      "learning_rate": 2.5411634788271767e-05,
      "loss": 0.2718,
      "step": 258600
    },
    {
      "epoch": 0.915648492914077,
      "grad_norm": 0.12745213508605957,
      "learning_rate": 2.53054521257769e-05,
      "loss": 0.2355,
      "step": 258700
    },
    {
      "epoch": 0.9160024351223932,
      "grad_norm": 4.23139899794478e-05,
      "learning_rate": 2.5199269463282034e-05,
      "loss": 0.2424,
      "step": 258800
    },
    {
      "epoch": 0.9163563773307094,
      "grad_norm": 0.0006844311719760299,
      "learning_rate": 2.5093086800787165e-05,
      "loss": 0.1125,
      "step": 258900
    },
    {
      "epoch": 0.9167103195390257,
      "grad_norm": 44.96340560913086,
      "learning_rate": 2.49869041382923e-05,
      "loss": 0.1998,
      "step": 259000
    },
    {
      "epoch": 0.9170642617473419,
      "grad_norm": 0.0028761769644916058,
      "learning_rate": 2.4880721475797432e-05,
      "loss": 0.1724,
      "step": 259100
    },
    {
      "epoch": 0.9174182039556581,
      "grad_norm": 0.00015237703337334096,
      "learning_rate": 2.477453881330256e-05,
      "loss": 0.1932,
      "step": 259200
    },
    {
      "epoch": 0.9177721461639744,
      "grad_norm": 0.0003939160378649831,
      "learning_rate": 2.4668356150807693e-05,
      "loss": 0.2671,
      "step": 259300
    },
    {
      "epoch": 0.9181260883722906,
      "grad_norm": 7.435285078827292e-05,
      "learning_rate": 2.4562173488312827e-05,
      "loss": 0.1245,
      "step": 259400
    },
    {
      "epoch": 0.9184800305806068,
      "grad_norm": 6.414749077521265e-05,
      "learning_rate": 2.4455990825817957e-05,
      "loss": 0.1334,
      "step": 259500
    },
    {
      "epoch": 0.918833972788923,
      "grad_norm": 0.003230193629860878,
      "learning_rate": 2.434980816332309e-05,
      "loss": 0.0914,
      "step": 259600
    },
    {
      "epoch": 0.9191879149972393,
      "grad_norm": 0.0007932600565254688,
      "learning_rate": 2.4243625500828225e-05,
      "loss": 0.2166,
      "step": 259700
    },
    {
      "epoch": 0.9195418572055555,
      "grad_norm": 0.0001643383438931778,
      "learning_rate": 2.4137442838333355e-05,
      "loss": 0.147,
      "step": 259800
    },
    {
      "epoch": 0.9198957994138717,
      "grad_norm": 0.00011672523396555334,
      "learning_rate": 2.403126017583849e-05,
      "loss": 0.1525,
      "step": 259900
    },
    {
      "epoch": 0.920249741622188,
      "grad_norm": 19.471521377563477,
      "learning_rate": 2.3925077513343623e-05,
      "loss": 0.2568,
      "step": 260000
    },
    {
      "epoch": 0.9206036838305042,
      "grad_norm": 0.886036217212677,
      "learning_rate": 2.381889485084875e-05,
      "loss": 0.2459,
      "step": 260100
    },
    {
      "epoch": 0.9209576260388204,
      "grad_norm": 0.01979835517704487,
      "learning_rate": 2.3712712188353883e-05,
      "loss": 0.2634,
      "step": 260200
    },
    {
      "epoch": 0.9213115682471366,
      "grad_norm": 0.0002983006997965276,
      "learning_rate": 2.3606529525859014e-05,
      "loss": 0.1738,
      "step": 260300
    },
    {
      "epoch": 0.9216655104554529,
      "grad_norm": 0.0001183415442937985,
      "learning_rate": 2.3500346863364147e-05,
      "loss": 0.1125,
      "step": 260400
    },
    {
      "epoch": 0.9220194526637691,
      "grad_norm": 0.002799306996166706,
      "learning_rate": 2.339416420086928e-05,
      "loss": 0.1924,
      "step": 260500
    },
    {
      "epoch": 0.9223733948720853,
      "grad_norm": 0.0018057429697364569,
      "learning_rate": 2.328798153837441e-05,
      "loss": 0.1337,
      "step": 260600
    },
    {
      "epoch": 0.9227273370804016,
      "grad_norm": 0.0017169049242511392,
      "learning_rate": 2.3181798875879545e-05,
      "loss": 0.1955,
      "step": 260700
    },
    {
      "epoch": 0.9230812792887177,
      "grad_norm": 0.00023580605920869857,
      "learning_rate": 2.307561621338468e-05,
      "loss": 0.1856,
      "step": 260800
    },
    {
      "epoch": 0.9234352214970339,
      "grad_norm": 0.003935889806598425,
      "learning_rate": 2.2969433550889806e-05,
      "loss": 0.2631,
      "step": 260900
    },
    {
      "epoch": 0.9237891637053501,
      "grad_norm": 11.724699974060059,
      "learning_rate": 2.286325088839494e-05,
      "loss": 0.1099,
      "step": 261000
    },
    {
      "epoch": 0.9241431059136664,
      "grad_norm": 0.0028027198277413845,
      "learning_rate": 2.2757068225900074e-05,
      "loss": 0.0998,
      "step": 261100
    },
    {
      "epoch": 0.9244970481219826,
      "grad_norm": 9.459051216254011e-05,
      "learning_rate": 2.2650885563405204e-05,
      "loss": 0.0938,
      "step": 261200
    },
    {
      "epoch": 0.9248509903302988,
      "grad_norm": 0.00044962289393879473,
      "learning_rate": 2.2544702900910338e-05,
      "loss": 0.1916,
      "step": 261300
    },
    {
      "epoch": 0.9252049325386151,
      "grad_norm": 0.00010417388693895191,
      "learning_rate": 2.243852023841547e-05,
      "loss": 0.1704,
      "step": 261400
    },
    {
      "epoch": 0.9255588747469313,
      "grad_norm": 4.4969830923946574e-05,
      "learning_rate": 2.23323375759206e-05,
      "loss": 0.1581,
      "step": 261500
    },
    {
      "epoch": 0.9259128169552475,
      "grad_norm": 0.00017609531641937792,
      "learning_rate": 2.2226154913425732e-05,
      "loss": 0.1395,
      "step": 261600
    },
    {
      "epoch": 0.9262667591635638,
      "grad_norm": 0.015897851437330246,
      "learning_rate": 2.2119972250930866e-05,
      "loss": 0.1161,
      "step": 261700
    },
    {
      "epoch": 0.92662070137188,
      "grad_norm": 0.00015508581418544054,
      "learning_rate": 2.2013789588435997e-05,
      "loss": 0.185,
      "step": 261800
    },
    {
      "epoch": 0.9269746435801962,
      "grad_norm": 18.351078033447266,
      "learning_rate": 2.190760692594113e-05,
      "loss": 0.1284,
      "step": 261900
    },
    {
      "epoch": 0.9273285857885124,
      "grad_norm": 0.0009165668161585927,
      "learning_rate": 2.1801424263446264e-05,
      "loss": 0.396,
      "step": 262000
    },
    {
      "epoch": 0.9276825279968287,
      "grad_norm": 0.0001410347904311493,
      "learning_rate": 2.1695241600951395e-05,
      "loss": 0.0688,
      "step": 262100
    },
    {
      "epoch": 0.9280364702051449,
      "grad_norm": 14.372299194335938,
      "learning_rate": 2.158905893845653e-05,
      "loss": 0.1893,
      "step": 262200
    },
    {
      "epoch": 0.9283904124134611,
      "grad_norm": 0.000145764512126334,
      "learning_rate": 2.1482876275961662e-05,
      "loss": 0.2282,
      "step": 262300
    },
    {
      "epoch": 0.9287443546217774,
      "grad_norm": 0.0006453784299083054,
      "learning_rate": 2.137669361346679e-05,
      "loss": 0.2305,
      "step": 262400
    },
    {
      "epoch": 0.9290982968300936,
      "grad_norm": 0.004444676451385021,
      "learning_rate": 2.1270510950971923e-05,
      "loss": 0.1228,
      "step": 262500
    },
    {
      "epoch": 0.9294522390384098,
      "grad_norm": 0.00015220209024846554,
      "learning_rate": 2.1164328288477057e-05,
      "loss": 0.2253,
      "step": 262600
    },
    {
      "epoch": 0.929806181246726,
      "grad_norm": 0.00048125413013622165,
      "learning_rate": 2.1058145625982187e-05,
      "loss": 0.0324,
      "step": 262700
    },
    {
      "epoch": 0.9301601234550423,
      "grad_norm": 0.0007505423855036497,
      "learning_rate": 2.095196296348732e-05,
      "loss": 0.2205,
      "step": 262800
    },
    {
      "epoch": 0.9305140656633585,
      "grad_norm": 27.466388702392578,
      "learning_rate": 2.0845780300992455e-05,
      "loss": 0.255,
      "step": 262900
    },
    {
      "epoch": 0.9308680078716747,
      "grad_norm": 0.01688654161989689,
      "learning_rate": 2.0739597638497585e-05,
      "loss": 0.2206,
      "step": 263000
    },
    {
      "epoch": 0.931221950079991,
      "grad_norm": 0.0009812272619456053,
      "learning_rate": 2.0633414976002715e-05,
      "loss": 0.1827,
      "step": 263100
    },
    {
      "epoch": 0.9315758922883072,
      "grad_norm": 0.0007338612340390682,
      "learning_rate": 2.052723231350785e-05,
      "loss": 0.1597,
      "step": 263200
    },
    {
      "epoch": 0.9319298344966234,
      "grad_norm": 0.0005820626392960548,
      "learning_rate": 2.042104965101298e-05,
      "loss": 0.0894,
      "step": 263300
    },
    {
      "epoch": 0.9322837767049397,
      "grad_norm": 0.0007941153016872704,
      "learning_rate": 2.0314866988518113e-05,
      "loss": 0.2089,
      "step": 263400
    },
    {
      "epoch": 0.9326377189132559,
      "grad_norm": 0.0005569561617448926,
      "learning_rate": 2.0208684326023244e-05,
      "loss": 0.2005,
      "step": 263500
    },
    {
      "epoch": 0.9329916611215721,
      "grad_norm": 0.0008790057618170977,
      "learning_rate": 2.0102501663528377e-05,
      "loss": 0.1262,
      "step": 263600
    },
    {
      "epoch": 0.9333456033298883,
      "grad_norm": 7.393208215944469e-05,
      "learning_rate": 1.999631900103351e-05,
      "loss": 0.2611,
      "step": 263700
    },
    {
      "epoch": 0.9336995455382046,
      "grad_norm": 0.000511729100253433,
      "learning_rate": 1.9890136338538638e-05,
      "loss": 0.0288,
      "step": 263800
    },
    {
      "epoch": 0.9340534877465207,
      "grad_norm": 0.0014214818365871906,
      "learning_rate": 1.9783953676043772e-05,
      "loss": 0.1596,
      "step": 263900
    },
    {
      "epoch": 0.9344074299548369,
      "grad_norm": 69.09791564941406,
      "learning_rate": 1.9677771013548906e-05,
      "loss": 0.1853,
      "step": 264000
    },
    {
      "epoch": 0.9347613721631532,
      "grad_norm": 0.003658802481368184,
      "learning_rate": 1.9571588351054036e-05,
      "loss": 0.1577,
      "step": 264100
    },
    {
      "epoch": 0.9351153143714694,
      "grad_norm": 0.00042425363790243864,
      "learning_rate": 1.946540568855917e-05,
      "loss": 0.3264,
      "step": 264200
    },
    {
      "epoch": 0.9354692565797856,
      "grad_norm": 0.00031444549676962197,
      "learning_rate": 1.9359223026064304e-05,
      "loss": 0.2853,
      "step": 264300
    },
    {
      "epoch": 0.9358231987881018,
      "grad_norm": 0.0009862440638244152,
      "learning_rate": 1.9253040363569434e-05,
      "loss": 0.0693,
      "step": 264400
    },
    {
      "epoch": 0.9361771409964181,
      "grad_norm": 0.0003395499079488218,
      "learning_rate": 1.9146857701074568e-05,
      "loss": 0.2287,
      "step": 264500
    },
    {
      "epoch": 0.9365310832047343,
      "grad_norm": 0.00015236256876960397,
      "learning_rate": 1.90406750385797e-05,
      "loss": 0.1208,
      "step": 264600
    },
    {
      "epoch": 0.9368850254130505,
      "grad_norm": 8.241097930294927e-06,
      "learning_rate": 1.893449237608483e-05,
      "loss": 0.2162,
      "step": 264700
    },
    {
      "epoch": 0.9372389676213668,
      "grad_norm": 1.6733438968658447,
      "learning_rate": 1.8828309713589962e-05,
      "loss": 0.171,
      "step": 264800
    },
    {
      "epoch": 0.937592909829683,
      "grad_norm": 0.00020918517839163542,
      "learning_rate": 1.8722127051095096e-05,
      "loss": 0.1365,
      "step": 264900
    },
    {
      "epoch": 0.9379468520379992,
      "grad_norm": 0.0006924921181052923,
      "learning_rate": 1.861594438860023e-05,
      "loss": 0.2062,
      "step": 265000
    },
    {
      "epoch": 0.9383007942463155,
      "grad_norm": 0.00033962196903303266,
      "learning_rate": 1.850976172610536e-05,
      "loss": 0.1238,
      "step": 265100
    },
    {
      "epoch": 0.9386547364546317,
      "grad_norm": 0.0014381894143298268,
      "learning_rate": 1.840357906361049e-05,
      "loss": 0.2874,
      "step": 265200
    },
    {
      "epoch": 0.9390086786629479,
      "grad_norm": 3.1744355510454625e-05,
      "learning_rate": 1.8297396401115625e-05,
      "loss": 0.1672,
      "step": 265300
    },
    {
      "epoch": 0.9393626208712641,
      "grad_norm": 0.0006276053609326482,
      "learning_rate": 1.8191213738620755e-05,
      "loss": 0.0968,
      "step": 265400
    },
    {
      "epoch": 0.9397165630795804,
      "grad_norm": 0.001236187876202166,
      "learning_rate": 1.808503107612589e-05,
      "loss": 0.1851,
      "step": 265500
    },
    {
      "epoch": 0.9400705052878966,
      "grad_norm": 12.340850830078125,
      "learning_rate": 1.7978848413631022e-05,
      "loss": 0.3177,
      "step": 265600
    },
    {
      "epoch": 0.9404244474962128,
      "grad_norm": 14.922197341918945,
      "learning_rate": 1.7872665751136153e-05,
      "loss": 0.2583,
      "step": 265700
    },
    {
      "epoch": 0.9407783897045291,
      "grad_norm": 0.00023179846175480634,
      "learning_rate": 1.7766483088641283e-05,
      "loss": 0.3504,
      "step": 265800
    },
    {
      "epoch": 0.9411323319128453,
      "grad_norm": 0.03794972971081734,
      "learning_rate": 1.7660300426146417e-05,
      "loss": 0.1202,
      "step": 265900
    },
    {
      "epoch": 0.9414862741211615,
      "grad_norm": 87.01726531982422,
      "learning_rate": 1.755411776365155e-05,
      "loss": 0.0967,
      "step": 266000
    },
    {
      "epoch": 0.9418402163294777,
      "grad_norm": 0.0009013309609144926,
      "learning_rate": 1.744793510115668e-05,
      "loss": 0.0832,
      "step": 266100
    },
    {
      "epoch": 0.942194158537794,
      "grad_norm": 0.09180328994989395,
      "learning_rate": 1.734175243866181e-05,
      "loss": 0.1953,
      "step": 266200
    },
    {
      "epoch": 0.9425481007461102,
      "grad_norm": 9.823629079619423e-05,
      "learning_rate": 1.7235569776166945e-05,
      "loss": 0.1375,
      "step": 266300
    },
    {
      "epoch": 0.9429020429544264,
      "grad_norm": 0.08780331164598465,
      "learning_rate": 1.712938711367208e-05,
      "loss": 0.1944,
      "step": 266400
    },
    {
      "epoch": 0.9432559851627427,
      "grad_norm": 0.0004035114252474159,
      "learning_rate": 1.702320445117721e-05,
      "loss": 0.2223,
      "step": 266500
    },
    {
      "epoch": 0.9436099273710589,
      "grad_norm": 0.04115782678127289,
      "learning_rate": 1.6917021788682343e-05,
      "loss": 0.1098,
      "step": 266600
    },
    {
      "epoch": 0.9439638695793751,
      "grad_norm": 101.78945922851562,
      "learning_rate": 1.6810839126187474e-05,
      "loss": 0.1154,
      "step": 266700
    },
    {
      "epoch": 0.9443178117876913,
      "grad_norm": 1.139155387878418,
      "learning_rate": 1.6704656463692607e-05,
      "loss": 0.2189,
      "step": 266800
    },
    {
      "epoch": 0.9446717539960076,
      "grad_norm": 6.897503772052005e-05,
      "learning_rate": 1.659847380119774e-05,
      "loss": 0.148,
      "step": 266900
    },
    {
      "epoch": 0.9450256962043238,
      "grad_norm": 0.005914686713367701,
      "learning_rate": 1.649229113870287e-05,
      "loss": 0.1443,
      "step": 267000
    },
    {
      "epoch": 0.9453796384126399,
      "grad_norm": 0.00026255976990796626,
      "learning_rate": 1.6386108476208002e-05,
      "loss": 0.1215,
      "step": 267100
    },
    {
      "epoch": 0.9457335806209562,
      "grad_norm": 0.00017610446957405657,
      "learning_rate": 1.6279925813713136e-05,
      "loss": 0.0876,
      "step": 267200
    },
    {
      "epoch": 0.9460875228292724,
      "grad_norm": 0.00889279879629612,
      "learning_rate": 1.617374315121827e-05,
      "loss": 0.4153,
      "step": 267300
    },
    {
      "epoch": 0.9464414650375886,
      "grad_norm": 46.89923858642578,
      "learning_rate": 1.60675604887234e-05,
      "loss": 0.2809,
      "step": 267400
    },
    {
      "epoch": 0.9467954072459049,
      "grad_norm": 0.015937473624944687,
      "learning_rate": 1.5961377826228534e-05,
      "loss": 0.2283,
      "step": 267500
    },
    {
      "epoch": 0.9471493494542211,
      "grad_norm": 0.0001183102504000999,
      "learning_rate": 1.5855195163733664e-05,
      "loss": 0.1035,
      "step": 267600
    },
    {
      "epoch": 0.9475032916625373,
      "grad_norm": 0.0005517028039321303,
      "learning_rate": 1.5749012501238795e-05,
      "loss": 0.1578,
      "step": 267700
    },
    {
      "epoch": 0.9478572338708535,
      "grad_norm": 0.002384781138971448,
      "learning_rate": 1.5642829838743928e-05,
      "loss": 0.0804,
      "step": 267800
    },
    {
      "epoch": 0.9482111760791698,
      "grad_norm": 0.0007707159384153783,
      "learning_rate": 1.5536647176249062e-05,
      "loss": 0.1455,
      "step": 267900
    },
    {
      "epoch": 0.948565118287486,
      "grad_norm": 0.007339791860431433,
      "learning_rate": 1.5430464513754192e-05,
      "loss": 0.1444,
      "step": 268000
    },
    {
      "epoch": 0.9489190604958022,
      "grad_norm": 0.00021225558884907514,
      "learning_rate": 1.5324281851259323e-05,
      "loss": 0.2823,
      "step": 268100
    },
    {
      "epoch": 0.9492730027041185,
      "grad_norm": 0.0005999827408231795,
      "learning_rate": 1.5218099188764458e-05,
      "loss": 0.189,
      "step": 268200
    },
    {
      "epoch": 0.9496269449124347,
      "grad_norm": 0.00016551389126107097,
      "learning_rate": 1.5111916526269589e-05,
      "loss": 0.1288,
      "step": 268300
    },
    {
      "epoch": 0.9499808871207509,
      "grad_norm": 15.177220344543457,
      "learning_rate": 1.500573386377472e-05,
      "loss": 0.1101,
      "step": 268400
    },
    {
      "epoch": 0.9503348293290671,
      "grad_norm": 0.049363210797309875,
      "learning_rate": 1.4899551201279855e-05,
      "loss": 0.2431,
      "step": 268500
    },
    {
      "epoch": 0.9506887715373834,
      "grad_norm": 0.00035026538535021245,
      "learning_rate": 1.4793368538784987e-05,
      "loss": 0.3303,
      "step": 268600
    },
    {
      "epoch": 0.9510427137456996,
      "grad_norm": 50.89426040649414,
      "learning_rate": 1.4687185876290117e-05,
      "loss": 0.0772,
      "step": 268700
    },
    {
      "epoch": 0.9513966559540158,
      "grad_norm": 0.0007363642798736691,
      "learning_rate": 1.458100321379525e-05,
      "loss": 0.1123,
      "step": 268800
    },
    {
      "epoch": 0.9517505981623321,
      "grad_norm": 2.532041072845459,
      "learning_rate": 1.4474820551300383e-05,
      "loss": 0.0955,
      "step": 268900
    },
    {
      "epoch": 0.9521045403706483,
      "grad_norm": 3.6635778087656945e-05,
      "learning_rate": 1.4368637888805515e-05,
      "loss": 0.0669,
      "step": 269000
    },
    {
      "epoch": 0.9524584825789645,
      "grad_norm": 0.0023843867238610983,
      "learning_rate": 1.4262455226310647e-05,
      "loss": 0.1418,
      "step": 269100
    },
    {
      "epoch": 0.9528124247872808,
      "grad_norm": 103.84705352783203,
      "learning_rate": 1.4156272563815779e-05,
      "loss": 0.1893,
      "step": 269200
    },
    {
      "epoch": 0.953166366995597,
      "grad_norm": 0.0002250532852485776,
      "learning_rate": 1.4050089901320911e-05,
      "loss": 0.2101,
      "step": 269300
    },
    {
      "epoch": 0.9535203092039132,
      "grad_norm": 0.3811241686344147,
      "learning_rate": 1.3943907238826042e-05,
      "loss": 0.2181,
      "step": 269400
    },
    {
      "epoch": 0.9538742514122294,
      "grad_norm": 0.001103707472793758,
      "learning_rate": 1.3837724576331175e-05,
      "loss": 0.1626,
      "step": 269500
    },
    {
      "epoch": 0.9542281936205457,
      "grad_norm": 0.0036283242516219616,
      "learning_rate": 1.3731541913836307e-05,
      "loss": 0.3932,
      "step": 269600
    },
    {
      "epoch": 0.9545821358288619,
      "grad_norm": 0.0002764433447737247,
      "learning_rate": 1.362535925134144e-05,
      "loss": 0.1411,
      "step": 269700
    },
    {
      "epoch": 0.9549360780371781,
      "grad_norm": 0.00342923984862864,
      "learning_rate": 1.3519176588846573e-05,
      "loss": 0.1616,
      "step": 269800
    },
    {
      "epoch": 0.9552900202454944,
      "grad_norm": 0.0796172171831131,
      "learning_rate": 1.3412993926351704e-05,
      "loss": 0.2166,
      "step": 269900
    },
    {
      "epoch": 0.9556439624538106,
      "grad_norm": 0.05528832972049713,
      "learning_rate": 1.3306811263856836e-05,
      "loss": 0.0646,
      "step": 270000
    },
    {
      "epoch": 0.9559979046621268,
      "grad_norm": 0.0024268347769975662,
      "learning_rate": 1.320062860136197e-05,
      "loss": 0.2111,
      "step": 270100
    },
    {
      "epoch": 0.956351846870443,
      "grad_norm": 0.009456026367843151,
      "learning_rate": 1.30944459388671e-05,
      "loss": 0.1827,
      "step": 270200
    },
    {
      "epoch": 0.9567057890787593,
      "grad_norm": 0.00041735227569006383,
      "learning_rate": 1.2988263276372232e-05,
      "loss": 0.2598,
      "step": 270300
    },
    {
      "epoch": 0.9570597312870754,
      "grad_norm": 0.00039539107820019126,
      "learning_rate": 1.2882080613877366e-05,
      "loss": 0.13,
      "step": 270400
    },
    {
      "epoch": 0.9574136734953916,
      "grad_norm": 42.281578063964844,
      "learning_rate": 1.2775897951382498e-05,
      "loss": 0.1409,
      "step": 270500
    },
    {
      "epoch": 0.9577676157037079,
      "grad_norm": 0.010548832826316357,
      "learning_rate": 1.2669715288887628e-05,
      "loss": 0.1549,
      "step": 270600
    },
    {
      "epoch": 0.9581215579120241,
      "grad_norm": 0.056310370564460754,
      "learning_rate": 1.2563532626392762e-05,
      "loss": 0.1693,
      "step": 270700
    },
    {
      "epoch": 0.9584755001203403,
      "grad_norm": 0.00030109655926935375,
      "learning_rate": 1.2457349963897894e-05,
      "loss": 0.1242,
      "step": 270800
    },
    {
      "epoch": 0.9588294423286565,
      "grad_norm": 0.0026140334084630013,
      "learning_rate": 1.2351167301403026e-05,
      "loss": 0.1389,
      "step": 270900
    },
    {
      "epoch": 0.9591833845369728,
      "grad_norm": 0.0001509940339019522,
      "learning_rate": 1.2244984638908157e-05,
      "loss": 0.1186,
      "step": 271000
    },
    {
      "epoch": 0.959537326745289,
      "grad_norm": 0.0003677522763609886,
      "learning_rate": 1.213880197641329e-05,
      "loss": 0.1027,
      "step": 271100
    },
    {
      "epoch": 0.9598912689536052,
      "grad_norm": 0.05833083763718605,
      "learning_rate": 1.2032619313918422e-05,
      "loss": 0.2481,
      "step": 271200
    },
    {
      "epoch": 0.9602452111619215,
      "grad_norm": 0.017410092055797577,
      "learning_rate": 1.1926436651423553e-05,
      "loss": 0.2508,
      "step": 271300
    },
    {
      "epoch": 0.9605991533702377,
      "grad_norm": 0.0004041208594571799,
      "learning_rate": 1.1820253988928687e-05,
      "loss": 0.3349,
      "step": 271400
    },
    {
      "epoch": 0.9609530955785539,
      "grad_norm": 0.0008987512555904686,
      "learning_rate": 1.1714071326433819e-05,
      "loss": 0.1941,
      "step": 271500
    },
    {
      "epoch": 0.9613070377868702,
      "grad_norm": 0.0010119148064404726,
      "learning_rate": 1.160788866393895e-05,
      "loss": 0.1406,
      "step": 271600
    },
    {
      "epoch": 0.9616609799951864,
      "grad_norm": 130.8680877685547,
      "learning_rate": 1.1501706001444085e-05,
      "loss": 0.26,
      "step": 271700
    },
    {
      "epoch": 0.9620149222035026,
      "grad_norm": 0.0004410139808896929,
      "learning_rate": 1.1395523338949215e-05,
      "loss": 0.1386,
      "step": 271800
    },
    {
      "epoch": 0.9623688644118188,
      "grad_norm": 0.0006790334009565413,
      "learning_rate": 1.1289340676454347e-05,
      "loss": 0.1761,
      "step": 271900
    },
    {
      "epoch": 0.9627228066201351,
      "grad_norm": 0.0004946469562128186,
      "learning_rate": 1.118315801395948e-05,
      "loss": 0.1407,
      "step": 272000
    },
    {
      "epoch": 0.9630767488284513,
      "grad_norm": 0.0006899546133354306,
      "learning_rate": 1.1076975351464611e-05,
      "loss": 0.0984,
      "step": 272100
    },
    {
      "epoch": 0.9634306910367675,
      "grad_norm": 0.002136023249477148,
      "learning_rate": 1.0970792688969743e-05,
      "loss": 0.2802,
      "step": 272200
    },
    {
      "epoch": 0.9637846332450838,
      "grad_norm": 0.004305991344153881,
      "learning_rate": 1.0864610026474877e-05,
      "loss": 0.1729,
      "step": 272300
    },
    {
      "epoch": 0.9641385754534,
      "grad_norm": 0.003086945740506053,
      "learning_rate": 1.0758427363980009e-05,
      "loss": 0.1474,
      "step": 272400
    },
    {
      "epoch": 0.9644925176617162,
      "grad_norm": 0.0007983467075973749,
      "learning_rate": 1.065224470148514e-05,
      "loss": 0.1841,
      "step": 272500
    },
    {
      "epoch": 0.9648464598700324,
      "grad_norm": 5.172499731997959e-05,
      "learning_rate": 1.0546062038990273e-05,
      "loss": 0.1831,
      "step": 272600
    },
    {
      "epoch": 0.9652004020783487,
      "grad_norm": 0.2519117593765259,
      "learning_rate": 1.0439879376495405e-05,
      "loss": 0.0537,
      "step": 272700
    },
    {
      "epoch": 0.9655543442866649,
      "grad_norm": 0.001318846712820232,
      "learning_rate": 1.0333696714000537e-05,
      "loss": 0.2049,
      "step": 272800
    },
    {
      "epoch": 0.9659082864949811,
      "grad_norm": 3.8767776489257812,
      "learning_rate": 1.0227514051505668e-05,
      "loss": 0.1656,
      "step": 272900
    },
    {
      "epoch": 0.9662622287032974,
      "grad_norm": 0.11739206314086914,
      "learning_rate": 1.0121331389010802e-05,
      "loss": 0.2073,
      "step": 273000
    },
    {
      "epoch": 0.9666161709116136,
      "grad_norm": 0.000269488082267344,
      "learning_rate": 1.0015148726515934e-05,
      "loss": 0.1242,
      "step": 273100
    },
    {
      "epoch": 0.9669701131199298,
      "grad_norm": 8.961237472249195e-05,
      "learning_rate": 9.908966064021066e-06,
      "loss": 0.1357,
      "step": 273200
    },
    {
      "epoch": 0.9673240553282461,
      "grad_norm": 0.0010435594012960792,
      "learning_rate": 9.802783401526198e-06,
      "loss": 0.1871,
      "step": 273300
    },
    {
      "epoch": 0.9676779975365623,
      "grad_norm": 0.0002302076027262956,
      "learning_rate": 9.69660073903133e-06,
      "loss": 0.1852,
      "step": 273400
    },
    {
      "epoch": 0.9680319397448784,
      "grad_norm": 0.001638775342144072,
      "learning_rate": 9.590418076536462e-06,
      "loss": 0.2122,
      "step": 273500
    },
    {
      "epoch": 0.9683858819531946,
      "grad_norm": 0.00042167134233750403,
      "learning_rate": 9.484235414041596e-06,
      "loss": 0.1269,
      "step": 273600
    },
    {
      "epoch": 0.9687398241615109,
      "grad_norm": 0.00021087129425723106,
      "learning_rate": 9.378052751546726e-06,
      "loss": 0.1927,
      "step": 273700
    },
    {
      "epoch": 0.9690937663698271,
      "grad_norm": 0.00014225352788344026,
      "learning_rate": 9.27187008905186e-06,
      "loss": 0.0804,
      "step": 273800
    },
    {
      "epoch": 0.9694477085781433,
      "grad_norm": 0.00021430889319162816,
      "learning_rate": 9.16568742655699e-06,
      "loss": 0.1124,
      "step": 273900
    },
    {
      "epoch": 0.9698016507864596,
      "grad_norm": 3.941618706448935e-05,
      "learning_rate": 9.059504764062124e-06,
      "loss": 0.0981,
      "step": 274000
    },
    {
      "epoch": 0.9701555929947758,
      "grad_norm": 0.000597693957388401,
      "learning_rate": 8.953322101567254e-06,
      "loss": 0.182,
      "step": 274100
    },
    {
      "epoch": 0.970509535203092,
      "grad_norm": 0.0001902875810628757,
      "learning_rate": 8.847139439072388e-06,
      "loss": 0.1318,
      "step": 274200
    },
    {
      "epoch": 0.9708634774114082,
      "grad_norm": 0.03393823280930519,
      "learning_rate": 8.74095677657752e-06,
      "loss": 0.124,
      "step": 274300
    },
    {
      "epoch": 0.9712174196197245,
      "grad_norm": 0.5553169846534729,
      "learning_rate": 8.63477411408265e-06,
      "loss": 0.1167,
      "step": 274400
    },
    {
      "epoch": 0.9715713618280407,
      "grad_norm": 0.00043044707854278386,
      "learning_rate": 8.528591451587785e-06,
      "loss": 0.0975,
      "step": 274500
    },
    {
      "epoch": 0.9719253040363569,
      "grad_norm": 0.00020740168110933155,
      "learning_rate": 8.422408789092917e-06,
      "loss": 0.1121,
      "step": 274600
    },
    {
      "epoch": 0.9722792462446732,
      "grad_norm": 0.043869275599718094,
      "learning_rate": 8.316226126598049e-06,
      "loss": 0.1228,
      "step": 274700
    },
    {
      "epoch": 0.9726331884529894,
      "grad_norm": 0.00011406344856368378,
      "learning_rate": 8.21004346410318e-06,
      "loss": 0.1001,
      "step": 274800
    },
    {
      "epoch": 0.9729871306613056,
      "grad_norm": 9.690425940789282e-05,
      "learning_rate": 8.103860801608313e-06,
      "loss": 0.1225,
      "step": 274900
    },
    {
      "epoch": 0.9733410728696218,
      "grad_norm": 0.0027976352721452713,
      "learning_rate": 7.997678139113445e-06,
      "loss": 0.1407,
      "step": 275000
    },
    {
      "epoch": 0.9736950150779381,
      "grad_norm": 0.0001294683024752885,
      "learning_rate": 7.891495476618577e-06,
      "loss": 0.1656,
      "step": 275100
    },
    {
      "epoch": 0.9740489572862543,
      "grad_norm": 0.0001860263873822987,
      "learning_rate": 7.785312814123709e-06,
      "loss": 0.1832,
      "step": 275200
    },
    {
      "epoch": 0.9744028994945705,
      "grad_norm": 0.010467484593391418,
      "learning_rate": 7.679130151628841e-06,
      "loss": 0.1476,
      "step": 275300
    },
    {
      "epoch": 0.9747568417028868,
      "grad_norm": 0.1433788388967514,
      "learning_rate": 7.572947489133974e-06,
      "loss": 0.2668,
      "step": 275400
    },
    {
      "epoch": 0.975110783911203,
      "grad_norm": 0.0004122415848542005,
      "learning_rate": 7.466764826639105e-06,
      "loss": 0.145,
      "step": 275500
    },
    {
      "epoch": 0.9754647261195192,
      "grad_norm": 48.171226501464844,
      "learning_rate": 7.360582164144238e-06,
      "loss": 0.1193,
      "step": 275600
    },
    {
      "epoch": 0.9758186683278355,
      "grad_norm": 0.0007965315598994493,
      "learning_rate": 7.2543995016493695e-06,
      "loss": 0.1615,
      "step": 275700
    },
    {
      "epoch": 0.9761726105361517,
      "grad_norm": 0.02187337540090084,
      "learning_rate": 7.148216839154502e-06,
      "loss": 0.0513,
      "step": 275800
    },
    {
      "epoch": 0.9765265527444679,
      "grad_norm": 0.0007942800293676555,
      "learning_rate": 7.0420341766596345e-06,
      "loss": 0.1115,
      "step": 275900
    },
    {
      "epoch": 0.9768804949527841,
      "grad_norm": 20.202594757080078,
      "learning_rate": 6.935851514164767e-06,
      "loss": 0.2088,
      "step": 276000
    },
    {
      "epoch": 0.9772344371611004,
      "grad_norm": 0.0007283824379555881,
      "learning_rate": 6.829668851669899e-06,
      "loss": 0.3252,
      "step": 276100
    },
    {
      "epoch": 0.9775883793694166,
      "grad_norm": 0.0005812434246763587,
      "learning_rate": 6.723486189175032e-06,
      "loss": 0.2429,
      "step": 276200
    },
    {
      "epoch": 0.9779423215777328,
      "grad_norm": 0.0002335946774110198,
      "learning_rate": 6.617303526680163e-06,
      "loss": 0.1592,
      "step": 276300
    },
    {
      "epoch": 0.9782962637860491,
      "grad_norm": 0.0007794402190484107,
      "learning_rate": 6.511120864185296e-06,
      "loss": 0.1338,
      "step": 276400
    },
    {
      "epoch": 0.9786502059943653,
      "grad_norm": 0.0012627200921997428,
      "learning_rate": 6.404938201690427e-06,
      "loss": 0.0709,
      "step": 276500
    },
    {
      "epoch": 0.9790041482026814,
      "grad_norm": 0.00022653966152574867,
      "learning_rate": 6.29875553919556e-06,
      "loss": 0.1089,
      "step": 276600
    },
    {
      "epoch": 0.9793580904109976,
      "grad_norm": 0.009644531644880772,
      "learning_rate": 6.192572876700692e-06,
      "loss": 0.1549,
      "step": 276700
    },
    {
      "epoch": 0.9797120326193139,
      "grad_norm": 0.0020114982035011053,
      "learning_rate": 6.086390214205823e-06,
      "loss": 0.1654,
      "step": 276800
    },
    {
      "epoch": 0.9800659748276301,
      "grad_norm": 0.0032665631733834743,
      "learning_rate": 5.980207551710956e-06,
      "loss": 0.1007,
      "step": 276900
    },
    {
      "epoch": 0.9804199170359463,
      "grad_norm": 0.0031544289086014032,
      "learning_rate": 5.874024889216089e-06,
      "loss": 0.4372,
      "step": 277000
    },
    {
      "epoch": 0.9807738592442626,
      "grad_norm": 0.0006105363718234003,
      "learning_rate": 5.76784222672122e-06,
      "loss": 0.223,
      "step": 277100
    },
    {
      "epoch": 0.9811278014525788,
      "grad_norm": 9.287473221775144e-05,
      "learning_rate": 5.661659564226352e-06,
      "loss": 0.1477,
      "step": 277200
    },
    {
      "epoch": 0.981481743660895,
      "grad_norm": 0.001498115831054747,
      "learning_rate": 5.5554769017314845e-06,
      "loss": 0.1904,
      "step": 277300
    },
    {
      "epoch": 0.9818356858692113,
      "grad_norm": 0.006265947129577398,
      "learning_rate": 5.4492942392366166e-06,
      "loss": 0.0479,
      "step": 277400
    },
    {
      "epoch": 0.9821896280775275,
      "grad_norm": 0.00492273923009634,
      "learning_rate": 5.3431115767417495e-06,
      "loss": 0.0423,
      "step": 277500
    },
    {
      "epoch": 0.9825435702858437,
      "grad_norm": 0.0006041639717295766,
      "learning_rate": 5.236928914246881e-06,
      "loss": 0.2543,
      "step": 277600
    },
    {
      "epoch": 0.9828975124941599,
      "grad_norm": 0.00012133684504078701,
      "learning_rate": 5.130746251752014e-06,
      "loss": 0.1494,
      "step": 277700
    },
    {
      "epoch": 0.9832514547024762,
      "grad_norm": 0.0028928357642143965,
      "learning_rate": 5.024563589257146e-06,
      "loss": 0.1911,
      "step": 277800
    },
    {
      "epoch": 0.9836053969107924,
      "grad_norm": 7.12490946170874e-05,
      "learning_rate": 4.918380926762278e-06,
      "loss": 0.1103,
      "step": 277900
    },
    {
      "epoch": 0.9839593391191086,
      "grad_norm": 0.0007525895489379764,
      "learning_rate": 4.81219826426741e-06,
      "loss": 0.1954,
      "step": 278000
    },
    {
      "epoch": 0.9843132813274249,
      "grad_norm": 0.0026059336960315704,
      "learning_rate": 4.706015601772542e-06,
      "loss": 0.2335,
      "step": 278100
    },
    {
      "epoch": 0.9846672235357411,
      "grad_norm": 0.0023785154335200787,
      "learning_rate": 4.599832939277674e-06,
      "loss": 0.2743,
      "step": 278200
    },
    {
      "epoch": 0.9850211657440573,
      "grad_norm": 18.92154312133789,
      "learning_rate": 4.493650276782806e-06,
      "loss": 0.143,
      "step": 278300
    },
    {
      "epoch": 0.9853751079523735,
      "grad_norm": 8.138486737152562e-05,
      "learning_rate": 4.387467614287939e-06,
      "loss": 0.1673,
      "step": 278400
    },
    {
      "epoch": 0.9857290501606898,
      "grad_norm": 0.00023583440633956343,
      "learning_rate": 4.281284951793071e-06,
      "loss": 0.0944,
      "step": 278500
    },
    {
      "epoch": 0.986082992369006,
      "grad_norm": 0.00016966801194939762,
      "learning_rate": 4.175102289298203e-06,
      "loss": 0.135,
      "step": 278600
    },
    {
      "epoch": 0.9864369345773222,
      "grad_norm": 0.00011325886589474976,
      "learning_rate": 4.068919626803335e-06,
      "loss": 0.2302,
      "step": 278700
    },
    {
      "epoch": 0.9867908767856385,
      "grad_norm": 0.00013657583622261882,
      "learning_rate": 3.962736964308467e-06,
      "loss": 0.2152,
      "step": 278800
    },
    {
      "epoch": 0.9871448189939547,
      "grad_norm": 5.51978409930598e-05,
      "learning_rate": 3.8565543018135995e-06,
      "loss": 0.1756,
      "step": 278900
    },
    {
      "epoch": 0.9874987612022709,
      "grad_norm": 0.012832039035856724,
      "learning_rate": 3.7503716393187316e-06,
      "loss": 0.1043,
      "step": 279000
    },
    {
      "epoch": 0.9878527034105872,
      "grad_norm": 0.0005298040923662484,
      "learning_rate": 3.644188976823864e-06,
      "loss": 0.0756,
      "step": 279100
    },
    {
      "epoch": 0.9882066456189034,
      "grad_norm": 0.00016543353558517992,
      "learning_rate": 3.538006314328996e-06,
      "loss": 0.2779,
      "step": 279200
    },
    {
      "epoch": 0.9885605878272196,
      "grad_norm": 0.0006143521750345826,
      "learning_rate": 3.4318236518341282e-06,
      "loss": 0.086,
      "step": 279300
    },
    {
      "epoch": 0.9889145300355358,
      "grad_norm": 0.0025934765581041574,
      "learning_rate": 3.3256409893392603e-06,
      "loss": 0.2099,
      "step": 279400
    },
    {
      "epoch": 0.9892684722438521,
      "grad_norm": 0.0008305397350341082,
      "learning_rate": 3.219458326844393e-06,
      "loss": 0.0696,
      "step": 279500
    },
    {
      "epoch": 0.9896224144521683,
      "grad_norm": 9.143512215814553e-06,
      "learning_rate": 3.113275664349525e-06,
      "loss": 0.0725,
      "step": 279600
    },
    {
      "epoch": 0.9899763566604844,
      "grad_norm": 0.00041180916014127433,
      "learning_rate": 3.007093001854657e-06,
      "loss": 0.1246,
      "step": 279700
    },
    {
      "epoch": 0.9903302988688008,
      "grad_norm": 0.027186637744307518,
      "learning_rate": 2.900910339359789e-06,
      "loss": 0.1764,
      "step": 279800
    },
    {
      "epoch": 0.990684241077117,
      "grad_norm": 0.00044356222497299314,
      "learning_rate": 2.7947276768649216e-06,
      "loss": 0.1263,
      "step": 279900
    },
    {
      "epoch": 0.9910381832854331,
      "grad_norm": 0.029679473489522934,
      "learning_rate": 2.6885450143700536e-06,
      "loss": 0.1533,
      "step": 280000
    },
    {
      "epoch": 0.9913921254937493,
      "grad_norm": 0.0010898177279159427,
      "learning_rate": 2.5823623518751857e-06,
      "loss": 0.0369,
      "step": 280100
    },
    {
      "epoch": 0.9917460677020656,
      "grad_norm": 0.001257362775504589,
      "learning_rate": 2.4761796893803174e-06,
      "loss": 0.2172,
      "step": 280200
    },
    {
      "epoch": 0.9921000099103818,
      "grad_norm": 0.00013760494766756892,
      "learning_rate": 2.3699970268854503e-06,
      "loss": 0.1398,
      "step": 280300
    },
    {
      "epoch": 0.992453952118698,
      "grad_norm": 0.00010855782602448016,
      "learning_rate": 2.2638143643905824e-06,
      "loss": 0.1883,
      "step": 280400
    },
    {
      "epoch": 0.9928078943270143,
      "grad_norm": 0.0013721926370635629,
      "learning_rate": 2.157631701895714e-06,
      "loss": 0.1568,
      "step": 280500
    },
    {
      "epoch": 0.9931618365353305,
      "grad_norm": 0.002156447619199753,
      "learning_rate": 2.0514490394008466e-06,
      "loss": 0.2298,
      "step": 280600
    },
    {
      "epoch": 0.9935157787436467,
      "grad_norm": 21.160850524902344,
      "learning_rate": 1.9452663769059786e-06,
      "loss": 0.1361,
      "step": 280700
    },
    {
      "epoch": 0.9938697209519629,
      "grad_norm": 0.00025980547070503235,
      "learning_rate": 1.839083714411111e-06,
      "loss": 0.2319,
      "step": 280800
    },
    {
      "epoch": 0.9942236631602792,
      "grad_norm": 0.011253594420850277,
      "learning_rate": 1.732901051916243e-06,
      "loss": 0.0606,
      "step": 280900
    },
    {
      "epoch": 0.9945776053685954,
      "grad_norm": 2.4178903913707472e-05,
      "learning_rate": 1.6267183894213753e-06,
      "loss": 0.3033,
      "step": 281000
    },
    {
      "epoch": 0.9949315475769116,
      "grad_norm": 0.00012106096255593002,
      "learning_rate": 1.5205357269265072e-06,
      "loss": 0.3078,
      "step": 281100
    },
    {
      "epoch": 0.9952854897852279,
      "grad_norm": 0.0017073254566639662,
      "learning_rate": 1.4143530644316395e-06,
      "loss": 0.0525,
      "step": 281200
    },
    {
      "epoch": 0.9956394319935441,
      "grad_norm": 0.0002694884897209704,
      "learning_rate": 1.3081704019367716e-06,
      "loss": 0.1804,
      "step": 281300
    },
    {
      "epoch": 0.9959933742018603,
      "grad_norm": 0.0009603893849998713,
      "learning_rate": 1.2019877394419038e-06,
      "loss": 0.1839,
      "step": 281400
    },
    {
      "epoch": 0.9963473164101766,
      "grad_norm": 0.24551305174827576,
      "learning_rate": 1.095805076947036e-06,
      "loss": 0.1399,
      "step": 281500
    },
    {
      "epoch": 0.9967012586184928,
      "grad_norm": 17.489871978759766,
      "learning_rate": 9.89622414452168e-07,
      "loss": 0.2002,
      "step": 281600
    },
    {
      "epoch": 0.997055200826809,
      "grad_norm": 8.458965749014169e-05,
      "learning_rate": 8.834397519573004e-07,
      "loss": 0.2491,
      "step": 281700
    },
    {
      "epoch": 0.9974091430351252,
      "grad_norm": 0.0001313202956225723,
      "learning_rate": 7.772570894624326e-07,
      "loss": 0.1109,
      "step": 281800
    },
    {
      "epoch": 0.9977630852434415,
      "grad_norm": 30.596317291259766,
      "learning_rate": 6.710744269675648e-07,
      "loss": 0.2175,
      "step": 281900
    },
    {
      "epoch": 0.9981170274517577,
      "grad_norm": 0.011688075959682465,
      "learning_rate": 5.648917644726969e-07,
      "loss": 0.164,
      "step": 282000
    },
    {
      "epoch": 0.9984709696600739,
      "grad_norm": 0.00014505835133604705,
      "learning_rate": 4.58709101977829e-07,
      "loss": 0.1292,
      "step": 282100
    },
    {
      "epoch": 0.9988249118683902,
      "grad_norm": 0.00025040892069227993,
      "learning_rate": 3.525264394829612e-07,
      "loss": 0.1661,
      "step": 282200
    },
    {
      "epoch": 0.9991788540767064,
      "grad_norm": 0.0024986336939036846,
      "learning_rate": 2.463437769880934e-07,
      "loss": 0.2013,
      "step": 282300
    },
    {
      "epoch": 0.9995327962850226,
      "grad_norm": 0.0018859494011849165,
      "learning_rate": 1.4016111449322554e-07,
      "loss": 0.1035,
      "step": 282400
    },
    {
      "epoch": 0.9998867384933388,
      "grad_norm": 0.0661417618393898,
      "learning_rate": 3.39784519983577e-08,
      "loss": 0.2059,
      "step": 282500
    }
  ],
  "logging_steps": 100,
  "max_steps": 282532,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.141412135168316e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
