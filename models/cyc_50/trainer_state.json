{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 322973,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0003096234050524347,
      "grad_norm": 3.031993865966797,
      "learning_rate": 0.00029990711297848423,
      "loss": 3.635,
      "step": 100
    },
    {
      "epoch": 0.0006192468101048694,
      "grad_norm": 4.716477870941162,
      "learning_rate": 0.00029981422595696855,
      "loss": 3.6076,
      "step": 200
    },
    {
      "epoch": 0.0009288702151573041,
      "grad_norm": 4.213082313537598,
      "learning_rate": 0.00029972133893545276,
      "loss": 3.5872,
      "step": 300
    },
    {
      "epoch": 0.0012384936202097388,
      "grad_norm": 6.120473384857178,
      "learning_rate": 0.000299628451913937,
      "loss": 3.4629,
      "step": 400
    },
    {
      "epoch": 0.0015481170252621735,
      "grad_norm": 5.5103759765625,
      "learning_rate": 0.00029953556489242133,
      "loss": 3.4018,
      "step": 500
    },
    {
      "epoch": 0.0018577404303146083,
      "grad_norm": 9.729350090026855,
      "learning_rate": 0.0002994426778709056,
      "loss": 3.3068,
      "step": 600
    },
    {
      "epoch": 0.002167363835367043,
      "grad_norm": 6.416210651397705,
      "learning_rate": 0.00029934979084938985,
      "loss": 3.2556,
      "step": 700
    },
    {
      "epoch": 0.0024769872404194777,
      "grad_norm": 9.849979400634766,
      "learning_rate": 0.00029925690382787417,
      "loss": 2.9998,
      "step": 800
    },
    {
      "epoch": 0.0027866106454719126,
      "grad_norm": 7.844111442565918,
      "learning_rate": 0.00029916401680635837,
      "loss": 3.12,
      "step": 900
    },
    {
      "epoch": 0.003096234050524347,
      "grad_norm": 5.777373313903809,
      "learning_rate": 0.00029907112978484263,
      "loss": 3.0822,
      "step": 1000
    },
    {
      "epoch": 0.003405857455576782,
      "grad_norm": 9.265521049499512,
      "learning_rate": 0.00029897824276332695,
      "loss": 2.9453,
      "step": 1100
    },
    {
      "epoch": 0.0037154808606292165,
      "grad_norm": 8.14907169342041,
      "learning_rate": 0.0002988853557418112,
      "loss": 2.8464,
      "step": 1200
    },
    {
      "epoch": 0.004025104265681651,
      "grad_norm": 8.673964500427246,
      "learning_rate": 0.00029879246872029547,
      "loss": 2.8758,
      "step": 1300
    },
    {
      "epoch": 0.004334727670734086,
      "grad_norm": 6.7669782638549805,
      "learning_rate": 0.00029869958169877973,
      "loss": 2.8349,
      "step": 1400
    },
    {
      "epoch": 0.004644351075786521,
      "grad_norm": 8.284723281860352,
      "learning_rate": 0.000298606694677264,
      "loss": 2.6049,
      "step": 1500
    },
    {
      "epoch": 0.004953974480838955,
      "grad_norm": 9.002421379089355,
      "learning_rate": 0.00029851380765574825,
      "loss": 2.8708,
      "step": 1600
    },
    {
      "epoch": 0.005263597885891391,
      "grad_norm": 9.68659782409668,
      "learning_rate": 0.00029842092063423256,
      "loss": 2.592,
      "step": 1700
    },
    {
      "epoch": 0.005573221290943825,
      "grad_norm": 9.110605239868164,
      "learning_rate": 0.0002983280336127168,
      "loss": 2.5242,
      "step": 1800
    },
    {
      "epoch": 0.00588284469599626,
      "grad_norm": 11.067947387695312,
      "learning_rate": 0.0002982351465912011,
      "loss": 2.4653,
      "step": 1900
    },
    {
      "epoch": 0.006192468101048694,
      "grad_norm": 10.093635559082031,
      "learning_rate": 0.00029814225956968535,
      "loss": 2.3369,
      "step": 2000
    },
    {
      "epoch": 0.0065020915061011295,
      "grad_norm": 3.7738821506500244,
      "learning_rate": 0.0002980493725481696,
      "loss": 2.3445,
      "step": 2100
    },
    {
      "epoch": 0.006811714911153564,
      "grad_norm": 18.54742431640625,
      "learning_rate": 0.0002979564855266539,
      "loss": 2.4307,
      "step": 2200
    },
    {
      "epoch": 0.0071213383162059985,
      "grad_norm": 26.213638305664062,
      "learning_rate": 0.0002978635985051382,
      "loss": 2.3955,
      "step": 2300
    },
    {
      "epoch": 0.007430961721258433,
      "grad_norm": 11.593778610229492,
      "learning_rate": 0.00029777071148362244,
      "loss": 2.3964,
      "step": 2400
    },
    {
      "epoch": 0.007740585126310868,
      "grad_norm": 6.267376899719238,
      "learning_rate": 0.0002976778244621067,
      "loss": 2.3223,
      "step": 2500
    },
    {
      "epoch": 0.008050208531363302,
      "grad_norm": 5.417151927947998,
      "learning_rate": 0.00029758493744059096,
      "loss": 2.1419,
      "step": 2600
    },
    {
      "epoch": 0.008359831936415738,
      "grad_norm": 10.125938415527344,
      "learning_rate": 0.0002974920504190752,
      "loss": 2.155,
      "step": 2700
    },
    {
      "epoch": 0.008669455341468173,
      "grad_norm": 14.664475440979004,
      "learning_rate": 0.00029739916339755954,
      "loss": 2.1593,
      "step": 2800
    },
    {
      "epoch": 0.008979078746520607,
      "grad_norm": 13.817350387573242,
      "learning_rate": 0.0002973062763760438,
      "loss": 2.0394,
      "step": 2900
    },
    {
      "epoch": 0.009288702151573042,
      "grad_norm": 10.913914680480957,
      "learning_rate": 0.00029721338935452806,
      "loss": 2.2062,
      "step": 3000
    },
    {
      "epoch": 0.009598325556625476,
      "grad_norm": 6.170228958129883,
      "learning_rate": 0.0002971205023330123,
      "loss": 2.1299,
      "step": 3100
    },
    {
      "epoch": 0.00990794896167791,
      "grad_norm": 8.692188262939453,
      "learning_rate": 0.0002970276153114966,
      "loss": 2.1166,
      "step": 3200
    },
    {
      "epoch": 0.010217572366730345,
      "grad_norm": 11.115066528320312,
      "learning_rate": 0.00029693472828998084,
      "loss": 2.1542,
      "step": 3300
    },
    {
      "epoch": 0.010527195771782781,
      "grad_norm": 11.355752944946289,
      "learning_rate": 0.00029684184126846515,
      "loss": 2.0496,
      "step": 3400
    },
    {
      "epoch": 0.010836819176835216,
      "grad_norm": 7.598057746887207,
      "learning_rate": 0.0002967489542469494,
      "loss": 1.9845,
      "step": 3500
    },
    {
      "epoch": 0.01114644258188765,
      "grad_norm": 21.14727783203125,
      "learning_rate": 0.0002966560672254337,
      "loss": 1.9464,
      "step": 3600
    },
    {
      "epoch": 0.011456065986940085,
      "grad_norm": 32.110389709472656,
      "learning_rate": 0.00029656318020391794,
      "loss": 1.7389,
      "step": 3700
    },
    {
      "epoch": 0.01176568939199252,
      "grad_norm": 9.098015785217285,
      "learning_rate": 0.0002964702931824022,
      "loss": 1.9968,
      "step": 3800
    },
    {
      "epoch": 0.012075312797044954,
      "grad_norm": 8.138916969299316,
      "learning_rate": 0.00029637740616088646,
      "loss": 1.847,
      "step": 3900
    },
    {
      "epoch": 0.012384936202097388,
      "grad_norm": 38.90875244140625,
      "learning_rate": 0.00029628451913937077,
      "loss": 1.7902,
      "step": 4000
    },
    {
      "epoch": 0.012694559607149823,
      "grad_norm": 13.358641624450684,
      "learning_rate": 0.00029619163211785503,
      "loss": 1.8628,
      "step": 4100
    },
    {
      "epoch": 0.013004183012202259,
      "grad_norm": 18.09977149963379,
      "learning_rate": 0.0002960987450963393,
      "loss": 1.6232,
      "step": 4200
    },
    {
      "epoch": 0.013313806417254694,
      "grad_norm": 4.400700569152832,
      "learning_rate": 0.00029600585807482355,
      "loss": 1.6709,
      "step": 4300
    },
    {
      "epoch": 0.013623429822307128,
      "grad_norm": 8.234465599060059,
      "learning_rate": 0.0002959129710533078,
      "loss": 1.9362,
      "step": 4400
    },
    {
      "epoch": 0.013933053227359563,
      "grad_norm": 10.345022201538086,
      "learning_rate": 0.0002958200840317921,
      "loss": 1.844,
      "step": 4500
    },
    {
      "epoch": 0.014242676632411997,
      "grad_norm": 13.649721145629883,
      "learning_rate": 0.0002957271970102764,
      "loss": 1.6398,
      "step": 4600
    },
    {
      "epoch": 0.014552300037464432,
      "grad_norm": 25.74169158935547,
      "learning_rate": 0.00029563430998876065,
      "loss": 1.7024,
      "step": 4700
    },
    {
      "epoch": 0.014861923442516866,
      "grad_norm": 9.458311080932617,
      "learning_rate": 0.0002955414229672449,
      "loss": 1.9172,
      "step": 4800
    },
    {
      "epoch": 0.015171546847569302,
      "grad_norm": 16.380399703979492,
      "learning_rate": 0.00029544853594572917,
      "loss": 1.627,
      "step": 4900
    },
    {
      "epoch": 0.015481170252621737,
      "grad_norm": 5.279160976409912,
      "learning_rate": 0.00029535564892421343,
      "loss": 1.7829,
      "step": 5000
    },
    {
      "epoch": 0.01579079365767417,
      "grad_norm": 9.115608215332031,
      "learning_rate": 0.0002952627619026977,
      "loss": 1.7342,
      "step": 5100
    },
    {
      "epoch": 0.016100417062726604,
      "grad_norm": 8.32801628112793,
      "learning_rate": 0.000295169874881182,
      "loss": 1.6189,
      "step": 5200
    },
    {
      "epoch": 0.016410040467779042,
      "grad_norm": 25.2193660736084,
      "learning_rate": 0.00029507698785966627,
      "loss": 1.845,
      "step": 5300
    },
    {
      "epoch": 0.016719663872831476,
      "grad_norm": 1.062078595161438,
      "learning_rate": 0.0002949841008381505,
      "loss": 1.4663,
      "step": 5400
    },
    {
      "epoch": 0.01702928727788391,
      "grad_norm": 10.885221481323242,
      "learning_rate": 0.0002948912138166348,
      "loss": 1.7495,
      "step": 5500
    },
    {
      "epoch": 0.017338910682936345,
      "grad_norm": 6.9473137855529785,
      "learning_rate": 0.00029479832679511905,
      "loss": 1.5182,
      "step": 5600
    },
    {
      "epoch": 0.01764853408798878,
      "grad_norm": 8.699026107788086,
      "learning_rate": 0.0002947054397736033,
      "loss": 1.6344,
      "step": 5700
    },
    {
      "epoch": 0.017958157493041214,
      "grad_norm": 7.438393592834473,
      "learning_rate": 0.0002946125527520876,
      "loss": 1.4737,
      "step": 5800
    },
    {
      "epoch": 0.01826778089809365,
      "grad_norm": 14.411155700683594,
      "learning_rate": 0.0002945196657305719,
      "loss": 1.4865,
      "step": 5900
    },
    {
      "epoch": 0.018577404303146083,
      "grad_norm": 6.062617301940918,
      "learning_rate": 0.00029442677870905614,
      "loss": 1.6746,
      "step": 6000
    },
    {
      "epoch": 0.018887027708198518,
      "grad_norm": 7.205744743347168,
      "learning_rate": 0.0002943338916875404,
      "loss": 1.2474,
      "step": 6100
    },
    {
      "epoch": 0.019196651113250952,
      "grad_norm": 21.534791946411133,
      "learning_rate": 0.00029424100466602466,
      "loss": 1.4733,
      "step": 6200
    },
    {
      "epoch": 0.019506274518303387,
      "grad_norm": 12.23834228515625,
      "learning_rate": 0.000294148117644509,
      "loss": 1.6553,
      "step": 6300
    },
    {
      "epoch": 0.01981589792335582,
      "grad_norm": 21.268409729003906,
      "learning_rate": 0.00029405523062299324,
      "loss": 1.4303,
      "step": 6400
    },
    {
      "epoch": 0.020125521328408256,
      "grad_norm": 7.918949604034424,
      "learning_rate": 0.0002939623436014775,
      "loss": 1.4884,
      "step": 6500
    },
    {
      "epoch": 0.02043514473346069,
      "grad_norm": 18.5622615814209,
      "learning_rate": 0.00029386945657996176,
      "loss": 1.2771,
      "step": 6600
    },
    {
      "epoch": 0.020744768138513125,
      "grad_norm": 17.747106552124023,
      "learning_rate": 0.000293776569558446,
      "loss": 1.3846,
      "step": 6700
    },
    {
      "epoch": 0.021054391543565563,
      "grad_norm": 15.909086227416992,
      "learning_rate": 0.0002936836825369303,
      "loss": 1.5563,
      "step": 6800
    },
    {
      "epoch": 0.021364014948617997,
      "grad_norm": 11.627721786499023,
      "learning_rate": 0.0002935907955154146,
      "loss": 1.5093,
      "step": 6900
    },
    {
      "epoch": 0.021673638353670432,
      "grad_norm": 3.553884506225586,
      "learning_rate": 0.00029349790849389886,
      "loss": 1.5467,
      "step": 7000
    },
    {
      "epoch": 0.021983261758722866,
      "grad_norm": 0.1432681530714035,
      "learning_rate": 0.0002934050214723831,
      "loss": 1.29,
      "step": 7100
    },
    {
      "epoch": 0.0222928851637753,
      "grad_norm": 77.62921142578125,
      "learning_rate": 0.0002933121344508674,
      "loss": 1.5084,
      "step": 7200
    },
    {
      "epoch": 0.022602508568827735,
      "grad_norm": 22.29564666748047,
      "learning_rate": 0.00029321924742935164,
      "loss": 1.4095,
      "step": 7300
    },
    {
      "epoch": 0.02291213197388017,
      "grad_norm": 10.227212905883789,
      "learning_rate": 0.0002931263604078359,
      "loss": 1.1273,
      "step": 7400
    },
    {
      "epoch": 0.023221755378932604,
      "grad_norm": 6.165567398071289,
      "learning_rate": 0.0002930334733863202,
      "loss": 1.4671,
      "step": 7500
    },
    {
      "epoch": 0.02353137878398504,
      "grad_norm": 22.65556526184082,
      "learning_rate": 0.0002929405863648045,
      "loss": 1.44,
      "step": 7600
    },
    {
      "epoch": 0.023841002189037473,
      "grad_norm": 31.608692169189453,
      "learning_rate": 0.00029284769934328873,
      "loss": 1.0541,
      "step": 7700
    },
    {
      "epoch": 0.024150625594089908,
      "grad_norm": 13.65699291229248,
      "learning_rate": 0.000292754812321773,
      "loss": 1.1791,
      "step": 7800
    },
    {
      "epoch": 0.024460248999142342,
      "grad_norm": 11.565945625305176,
      "learning_rate": 0.00029266192530025725,
      "loss": 1.3826,
      "step": 7900
    },
    {
      "epoch": 0.024769872404194777,
      "grad_norm": 11.212873458862305,
      "learning_rate": 0.0002925690382787415,
      "loss": 1.7458,
      "step": 8000
    },
    {
      "epoch": 0.02507949580924721,
      "grad_norm": 3.3359227180480957,
      "learning_rate": 0.00029247615125722583,
      "loss": 1.1376,
      "step": 8100
    },
    {
      "epoch": 0.025389119214299646,
      "grad_norm": 10.455036163330078,
      "learning_rate": 0.0002923832642357101,
      "loss": 1.4384,
      "step": 8200
    },
    {
      "epoch": 0.025698742619352084,
      "grad_norm": 24.27922821044922,
      "learning_rate": 0.00029229037721419435,
      "loss": 1.4143,
      "step": 8300
    },
    {
      "epoch": 0.026008366024404518,
      "grad_norm": 6.189897060394287,
      "learning_rate": 0.0002921974901926786,
      "loss": 1.3601,
      "step": 8400
    },
    {
      "epoch": 0.026317989429456953,
      "grad_norm": 26.371959686279297,
      "learning_rate": 0.00029210460317116287,
      "loss": 1.2189,
      "step": 8500
    },
    {
      "epoch": 0.026627612834509387,
      "grad_norm": 20.43024444580078,
      "learning_rate": 0.00029201171614964713,
      "loss": 1.7295,
      "step": 8600
    },
    {
      "epoch": 0.02693723623956182,
      "grad_norm": 7.937863826751709,
      "learning_rate": 0.00029191882912813145,
      "loss": 1.3388,
      "step": 8700
    },
    {
      "epoch": 0.027246859644614256,
      "grad_norm": 15.68947696685791,
      "learning_rate": 0.0002918259421066157,
      "loss": 1.1207,
      "step": 8800
    },
    {
      "epoch": 0.02755648304966669,
      "grad_norm": 21.870290756225586,
      "learning_rate": 0.00029173305508509997,
      "loss": 1.355,
      "step": 8900
    },
    {
      "epoch": 0.027866106454719125,
      "grad_norm": 32.35545349121094,
      "learning_rate": 0.00029164016806358423,
      "loss": 1.3759,
      "step": 9000
    },
    {
      "epoch": 0.02817572985977156,
      "grad_norm": 36.397071838378906,
      "learning_rate": 0.0002915472810420685,
      "loss": 1.5698,
      "step": 9100
    },
    {
      "epoch": 0.028485353264823994,
      "grad_norm": 27.01913070678711,
      "learning_rate": 0.00029145439402055275,
      "loss": 1.2105,
      "step": 9200
    },
    {
      "epoch": 0.02879497666987643,
      "grad_norm": 5.431384086608887,
      "learning_rate": 0.00029136150699903706,
      "loss": 1.3174,
      "step": 9300
    },
    {
      "epoch": 0.029104600074928863,
      "grad_norm": 36.830135345458984,
      "learning_rate": 0.0002912686199775213,
      "loss": 1.5869,
      "step": 9400
    },
    {
      "epoch": 0.029414223479981298,
      "grad_norm": 29.715177536010742,
      "learning_rate": 0.0002911757329560056,
      "loss": 1.4937,
      "step": 9500
    },
    {
      "epoch": 0.029723846885033732,
      "grad_norm": 10.935800552368164,
      "learning_rate": 0.00029108284593448985,
      "loss": 1.2586,
      "step": 9600
    },
    {
      "epoch": 0.030033470290086167,
      "grad_norm": 25.27760124206543,
      "learning_rate": 0.0002909899589129741,
      "loss": 1.2174,
      "step": 9700
    },
    {
      "epoch": 0.030343093695138604,
      "grad_norm": 17.053672790527344,
      "learning_rate": 0.00029089707189145837,
      "loss": 1.3096,
      "step": 9800
    },
    {
      "epoch": 0.03065271710019104,
      "grad_norm": 6.565666675567627,
      "learning_rate": 0.0002908041848699427,
      "loss": 1.3403,
      "step": 9900
    },
    {
      "epoch": 0.030962340505243473,
      "grad_norm": 9.603548049926758,
      "learning_rate": 0.00029071129784842694,
      "loss": 1.5236,
      "step": 10000
    },
    {
      "epoch": 0.03127196391029591,
      "grad_norm": 18.52578353881836,
      "learning_rate": 0.0002906184108269112,
      "loss": 1.1458,
      "step": 10100
    },
    {
      "epoch": 0.03158158731534834,
      "grad_norm": 12.889039039611816,
      "learning_rate": 0.00029052552380539546,
      "loss": 1.1965,
      "step": 10200
    },
    {
      "epoch": 0.03189121072040078,
      "grad_norm": 16.09236717224121,
      "learning_rate": 0.0002904326367838797,
      "loss": 1.4137,
      "step": 10300
    },
    {
      "epoch": 0.03220083412545321,
      "grad_norm": 16.62919807434082,
      "learning_rate": 0.00029033974976236404,
      "loss": 1.1426,
      "step": 10400
    },
    {
      "epoch": 0.032510457530505646,
      "grad_norm": 0.7545339465141296,
      "learning_rate": 0.0002902468627408483,
      "loss": 1.0472,
      "step": 10500
    },
    {
      "epoch": 0.032820080935558084,
      "grad_norm": 3.5354840755462646,
      "learning_rate": 0.00029015397571933256,
      "loss": 1.2556,
      "step": 10600
    },
    {
      "epoch": 0.033129704340610515,
      "grad_norm": 2.093487501144409,
      "learning_rate": 0.0002900610886978168,
      "loss": 1.3933,
      "step": 10700
    },
    {
      "epoch": 0.03343932774566295,
      "grad_norm": 1.660264492034912,
      "learning_rate": 0.0002899682016763011,
      "loss": 1.078,
      "step": 10800
    },
    {
      "epoch": 0.033748951150715384,
      "grad_norm": 4.08363676071167,
      "learning_rate": 0.00028987531465478534,
      "loss": 1.2119,
      "step": 10900
    },
    {
      "epoch": 0.03405857455576782,
      "grad_norm": 26.156312942504883,
      "learning_rate": 0.00028978242763326965,
      "loss": 1.0161,
      "step": 11000
    },
    {
      "epoch": 0.03436819796082025,
      "grad_norm": 1.8561075925827026,
      "learning_rate": 0.0002896895406117539,
      "loss": 1.3232,
      "step": 11100
    },
    {
      "epoch": 0.03467782136587269,
      "grad_norm": 16.241750717163086,
      "learning_rate": 0.0002895966535902382,
      "loss": 1.4532,
      "step": 11200
    },
    {
      "epoch": 0.03498744477092512,
      "grad_norm": 0.15454350411891937,
      "learning_rate": 0.00028950376656872244,
      "loss": 0.9503,
      "step": 11300
    },
    {
      "epoch": 0.03529706817597756,
      "grad_norm": 10.802166938781738,
      "learning_rate": 0.0002894108795472067,
      "loss": 1.1209,
      "step": 11400
    },
    {
      "epoch": 0.03560669158102999,
      "grad_norm": 0.01598178781569004,
      "learning_rate": 0.00028931799252569096,
      "loss": 1.2297,
      "step": 11500
    },
    {
      "epoch": 0.03591631498608243,
      "grad_norm": 18.625598907470703,
      "learning_rate": 0.00028922510550417527,
      "loss": 1.1526,
      "step": 11600
    },
    {
      "epoch": 0.03622593839113486,
      "grad_norm": 0.13659757375717163,
      "learning_rate": 0.00028913221848265953,
      "loss": 1.1411,
      "step": 11700
    },
    {
      "epoch": 0.0365355617961873,
      "grad_norm": 0.18900926411151886,
      "learning_rate": 0.0002890393314611438,
      "loss": 1.2742,
      "step": 11800
    },
    {
      "epoch": 0.03684518520123973,
      "grad_norm": 12.595318794250488,
      "learning_rate": 0.00028894644443962805,
      "loss": 1.0668,
      "step": 11900
    },
    {
      "epoch": 0.03715480860629217,
      "grad_norm": 33.886314392089844,
      "learning_rate": 0.0002888535574181123,
      "loss": 0.9445,
      "step": 12000
    },
    {
      "epoch": 0.037464432011344605,
      "grad_norm": 81.22023010253906,
      "learning_rate": 0.0002887606703965966,
      "loss": 1.0142,
      "step": 12100
    },
    {
      "epoch": 0.037774055416397036,
      "grad_norm": 2.1400208473205566,
      "learning_rate": 0.0002886677833750809,
      "loss": 0.8431,
      "step": 12200
    },
    {
      "epoch": 0.038083678821449474,
      "grad_norm": 14.239761352539062,
      "learning_rate": 0.00028857489635356515,
      "loss": 1.1637,
      "step": 12300
    },
    {
      "epoch": 0.038393302226501905,
      "grad_norm": 74.50314331054688,
      "learning_rate": 0.0002884820093320494,
      "loss": 1.0612,
      "step": 12400
    },
    {
      "epoch": 0.03870292563155434,
      "grad_norm": 7.334095001220703,
      "learning_rate": 0.00028838912231053367,
      "loss": 1.3067,
      "step": 12500
    },
    {
      "epoch": 0.039012549036606774,
      "grad_norm": 0.12806764245033264,
      "learning_rate": 0.00028829623528901793,
      "loss": 1.4732,
      "step": 12600
    },
    {
      "epoch": 0.03932217244165921,
      "grad_norm": 23.269861221313477,
      "learning_rate": 0.0002882033482675022,
      "loss": 1.1944,
      "step": 12700
    },
    {
      "epoch": 0.03963179584671164,
      "grad_norm": 18.305622100830078,
      "learning_rate": 0.0002881104612459865,
      "loss": 1.2849,
      "step": 12800
    },
    {
      "epoch": 0.03994141925176408,
      "grad_norm": 10.59835433959961,
      "learning_rate": 0.00028801757422447077,
      "loss": 1.2072,
      "step": 12900
    },
    {
      "epoch": 0.04025104265681651,
      "grad_norm": 20.911325454711914,
      "learning_rate": 0.000287924687202955,
      "loss": 0.9318,
      "step": 13000
    },
    {
      "epoch": 0.04056066606186895,
      "grad_norm": 52.35264587402344,
      "learning_rate": 0.0002878318001814393,
      "loss": 0.9931,
      "step": 13100
    },
    {
      "epoch": 0.04087028946692138,
      "grad_norm": 0.06764820963144302,
      "learning_rate": 0.00028773891315992355,
      "loss": 1.1411,
      "step": 13200
    },
    {
      "epoch": 0.04117991287197382,
      "grad_norm": 28.368650436401367,
      "learning_rate": 0.0002876460261384078,
      "loss": 1.3072,
      "step": 13300
    },
    {
      "epoch": 0.04148953627702625,
      "grad_norm": 16.444692611694336,
      "learning_rate": 0.0002875531391168921,
      "loss": 0.9329,
      "step": 13400
    },
    {
      "epoch": 0.04179915968207869,
      "grad_norm": 11.173669815063477,
      "learning_rate": 0.0002874602520953764,
      "loss": 1.0438,
      "step": 13500
    },
    {
      "epoch": 0.042108783087131126,
      "grad_norm": 0.02661890722811222,
      "learning_rate": 0.00028736736507386064,
      "loss": 1.2752,
      "step": 13600
    },
    {
      "epoch": 0.04241840649218356,
      "grad_norm": 0.010504042729735374,
      "learning_rate": 0.0002872744780523449,
      "loss": 1.117,
      "step": 13700
    },
    {
      "epoch": 0.042728029897235995,
      "grad_norm": 21.230749130249023,
      "learning_rate": 0.00028718159103082916,
      "loss": 1.1542,
      "step": 13800
    },
    {
      "epoch": 0.043037653302288426,
      "grad_norm": 0.20583879947662354,
      "learning_rate": 0.0002870887040093134,
      "loss": 1.3644,
      "step": 13900
    },
    {
      "epoch": 0.043347276707340864,
      "grad_norm": 0.9460955858230591,
      "learning_rate": 0.00028699581698779774,
      "loss": 1.0864,
      "step": 14000
    },
    {
      "epoch": 0.043656900112393295,
      "grad_norm": 1.9405372142791748,
      "learning_rate": 0.000286902929966282,
      "loss": 1.4607,
      "step": 14100
    },
    {
      "epoch": 0.04396652351744573,
      "grad_norm": 21.625900268554688,
      "learning_rate": 0.00028681004294476626,
      "loss": 1.2556,
      "step": 14200
    },
    {
      "epoch": 0.044276146922498164,
      "grad_norm": 64.96153259277344,
      "learning_rate": 0.0002867171559232505,
      "loss": 0.8859,
      "step": 14300
    },
    {
      "epoch": 0.0445857703275506,
      "grad_norm": 0.2898312211036682,
      "learning_rate": 0.0002866242689017348,
      "loss": 1.3162,
      "step": 14400
    },
    {
      "epoch": 0.04489539373260303,
      "grad_norm": 3.2769715785980225,
      "learning_rate": 0.0002865313818802191,
      "loss": 1.0985,
      "step": 14500
    },
    {
      "epoch": 0.04520501713765547,
      "grad_norm": 20.807491302490234,
      "learning_rate": 0.00028643849485870336,
      "loss": 0.9939,
      "step": 14600
    },
    {
      "epoch": 0.0455146405427079,
      "grad_norm": 0.4567941129207611,
      "learning_rate": 0.0002863456078371876,
      "loss": 1.2911,
      "step": 14700
    },
    {
      "epoch": 0.04582426394776034,
      "grad_norm": 12.808119773864746,
      "learning_rate": 0.0002862527208156719,
      "loss": 1.0952,
      "step": 14800
    },
    {
      "epoch": 0.04613388735281277,
      "grad_norm": 4.455042362213135,
      "learning_rate": 0.00028615983379415614,
      "loss": 1.1439,
      "step": 14900
    },
    {
      "epoch": 0.04644351075786521,
      "grad_norm": 35.797908782958984,
      "learning_rate": 0.0002860669467726404,
      "loss": 1.1499,
      "step": 15000
    },
    {
      "epoch": 0.046753134162917646,
      "grad_norm": 24.939403533935547,
      "learning_rate": 0.0002859740597511247,
      "loss": 0.8493,
      "step": 15100
    },
    {
      "epoch": 0.04706275756797008,
      "grad_norm": 15.273497581481934,
      "learning_rate": 0.00028588117272960897,
      "loss": 0.8565,
      "step": 15200
    },
    {
      "epoch": 0.047372380973022515,
      "grad_norm": 0.18519605696201324,
      "learning_rate": 0.00028578828570809323,
      "loss": 0.934,
      "step": 15300
    },
    {
      "epoch": 0.047682004378074946,
      "grad_norm": 3.4972281455993652,
      "learning_rate": 0.0002856953986865775,
      "loss": 1.3124,
      "step": 15400
    },
    {
      "epoch": 0.047991627783127384,
      "grad_norm": 17.03363800048828,
      "learning_rate": 0.00028560251166506175,
      "loss": 1.019,
      "step": 15500
    },
    {
      "epoch": 0.048301251188179815,
      "grad_norm": 1.024841547012329,
      "learning_rate": 0.000285509624643546,
      "loss": 1.1948,
      "step": 15600
    },
    {
      "epoch": 0.04861087459323225,
      "grad_norm": 44.695716857910156,
      "learning_rate": 0.00028541673762203033,
      "loss": 1.2072,
      "step": 15700
    },
    {
      "epoch": 0.048920497998284684,
      "grad_norm": 54.530357360839844,
      "learning_rate": 0.0002853238506005146,
      "loss": 0.9272,
      "step": 15800
    },
    {
      "epoch": 0.04923012140333712,
      "grad_norm": 2.2749671936035156,
      "learning_rate": 0.00028523096357899885,
      "loss": 1.2337,
      "step": 15900
    },
    {
      "epoch": 0.04953974480838955,
      "grad_norm": 8.058456420898438,
      "learning_rate": 0.0002851380765574831,
      "loss": 1.2954,
      "step": 16000
    },
    {
      "epoch": 0.04984936821344199,
      "grad_norm": 40.060359954833984,
      "learning_rate": 0.00028504518953596737,
      "loss": 0.9725,
      "step": 16100
    },
    {
      "epoch": 0.05015899161849442,
      "grad_norm": 15.132670402526855,
      "learning_rate": 0.00028495230251445163,
      "loss": 0.9003,
      "step": 16200
    },
    {
      "epoch": 0.05046861502354686,
      "grad_norm": 1.054335594177246,
      "learning_rate": 0.00028485941549293595,
      "loss": 0.9923,
      "step": 16300
    },
    {
      "epoch": 0.05077823842859929,
      "grad_norm": 13.33642292022705,
      "learning_rate": 0.0002847665284714202,
      "loss": 1.1546,
      "step": 16400
    },
    {
      "epoch": 0.05108786183365173,
      "grad_norm": 11.703081130981445,
      "learning_rate": 0.00028467364144990447,
      "loss": 1.3081,
      "step": 16500
    },
    {
      "epoch": 0.05139748523870417,
      "grad_norm": 20.907455444335938,
      "learning_rate": 0.00028458075442838873,
      "loss": 1.1478,
      "step": 16600
    },
    {
      "epoch": 0.0517071086437566,
      "grad_norm": 0.0741967260837555,
      "learning_rate": 0.000284487867406873,
      "loss": 0.953,
      "step": 16700
    },
    {
      "epoch": 0.052016732048809036,
      "grad_norm": 1.8568902015686035,
      "learning_rate": 0.00028439498038535725,
      "loss": 0.9788,
      "step": 16800
    },
    {
      "epoch": 0.05232635545386147,
      "grad_norm": 9.364009857177734,
      "learning_rate": 0.00028430209336384156,
      "loss": 0.8426,
      "step": 16900
    },
    {
      "epoch": 0.052635978858913905,
      "grad_norm": 0.7924937009811401,
      "learning_rate": 0.0002842092063423258,
      "loss": 0.8127,
      "step": 17000
    },
    {
      "epoch": 0.052945602263966336,
      "grad_norm": 4.439726829528809,
      "learning_rate": 0.0002841163193208101,
      "loss": 1.2996,
      "step": 17100
    },
    {
      "epoch": 0.053255225669018774,
      "grad_norm": 0.012481248006224632,
      "learning_rate": 0.00028402343229929434,
      "loss": 1.0836,
      "step": 17200
    },
    {
      "epoch": 0.053564849074071205,
      "grad_norm": 2.786318063735962,
      "learning_rate": 0.0002839305452777786,
      "loss": 1.0208,
      "step": 17300
    },
    {
      "epoch": 0.05387447247912364,
      "grad_norm": 0.05532975122332573,
      "learning_rate": 0.00028383765825626287,
      "loss": 1.0557,
      "step": 17400
    },
    {
      "epoch": 0.054184095884176074,
      "grad_norm": 0.11902101337909698,
      "learning_rate": 0.0002837447712347472,
      "loss": 1.2686,
      "step": 17500
    },
    {
      "epoch": 0.05449371928922851,
      "grad_norm": 30.74341583251953,
      "learning_rate": 0.00028365188421323144,
      "loss": 1.0214,
      "step": 17600
    },
    {
      "epoch": 0.05480334269428094,
      "grad_norm": 0.003104171482846141,
      "learning_rate": 0.0002835589971917157,
      "loss": 1.0739,
      "step": 17700
    },
    {
      "epoch": 0.05511296609933338,
      "grad_norm": 0.41998714208602905,
      "learning_rate": 0.00028346611017019996,
      "loss": 1.2421,
      "step": 17800
    },
    {
      "epoch": 0.05542258950438581,
      "grad_norm": 0.17526738345623016,
      "learning_rate": 0.0002833732231486842,
      "loss": 1.1873,
      "step": 17900
    },
    {
      "epoch": 0.05573221290943825,
      "grad_norm": 0.27473771572113037,
      "learning_rate": 0.0002832803361271685,
      "loss": 0.7841,
      "step": 18000
    },
    {
      "epoch": 0.05604183631449069,
      "grad_norm": 33.71232604980469,
      "learning_rate": 0.0002831874491056528,
      "loss": 0.9135,
      "step": 18100
    },
    {
      "epoch": 0.05635145971954312,
      "grad_norm": 30.715166091918945,
      "learning_rate": 0.00028309456208413706,
      "loss": 0.9348,
      "step": 18200
    },
    {
      "epoch": 0.05666108312459556,
      "grad_norm": 85.09661865234375,
      "learning_rate": 0.0002830016750626213,
      "loss": 1.1311,
      "step": 18300
    },
    {
      "epoch": 0.05697070652964799,
      "grad_norm": 0.05230041220784187,
      "learning_rate": 0.0002829087880411056,
      "loss": 0.8729,
      "step": 18400
    },
    {
      "epoch": 0.057280329934700426,
      "grad_norm": 0.18250393867492676,
      "learning_rate": 0.00028281590101958984,
      "loss": 0.784,
      "step": 18500
    },
    {
      "epoch": 0.05758995333975286,
      "grad_norm": 0.013043482787907124,
      "learning_rate": 0.00028272301399807415,
      "loss": 0.9134,
      "step": 18600
    },
    {
      "epoch": 0.057899576744805295,
      "grad_norm": 18.540943145751953,
      "learning_rate": 0.0002826301269765584,
      "loss": 1.1255,
      "step": 18700
    },
    {
      "epoch": 0.058209200149857726,
      "grad_norm": 38.002113342285156,
      "learning_rate": 0.0002825372399550427,
      "loss": 0.8313,
      "step": 18800
    },
    {
      "epoch": 0.058518823554910164,
      "grad_norm": 36.77028274536133,
      "learning_rate": 0.00028244435293352694,
      "loss": 1.0493,
      "step": 18900
    },
    {
      "epoch": 0.058828446959962595,
      "grad_norm": 0.0561971552670002,
      "learning_rate": 0.0002823514659120112,
      "loss": 0.9187,
      "step": 19000
    },
    {
      "epoch": 0.05913807036501503,
      "grad_norm": 6.332746505737305,
      "learning_rate": 0.00028225857889049546,
      "loss": 1.1296,
      "step": 19100
    },
    {
      "epoch": 0.059447693770067464,
      "grad_norm": 2.3864386081695557,
      "learning_rate": 0.00028216569186897977,
      "loss": 0.9942,
      "step": 19200
    },
    {
      "epoch": 0.0597573171751199,
      "grad_norm": 30.586645126342773,
      "learning_rate": 0.00028207280484746403,
      "loss": 0.9927,
      "step": 19300
    },
    {
      "epoch": 0.06006694058017233,
      "grad_norm": 0.09430446475744247,
      "learning_rate": 0.0002819799178259483,
      "loss": 0.8916,
      "step": 19400
    },
    {
      "epoch": 0.06037656398522477,
      "grad_norm": 0.015254072844982147,
      "learning_rate": 0.00028188703080443255,
      "loss": 1.254,
      "step": 19500
    },
    {
      "epoch": 0.06068618739027721,
      "grad_norm": 2.2112040519714355,
      "learning_rate": 0.0002817941437829168,
      "loss": 1.1433,
      "step": 19600
    },
    {
      "epoch": 0.06099581079532964,
      "grad_norm": 6.152335166931152,
      "learning_rate": 0.0002817012567614011,
      "loss": 0.8862,
      "step": 19700
    },
    {
      "epoch": 0.06130543420038208,
      "grad_norm": 1.9889527559280396,
      "learning_rate": 0.0002816083697398854,
      "loss": 1.2227,
      "step": 19800
    },
    {
      "epoch": 0.06161505760543451,
      "grad_norm": 26.40965461730957,
      "learning_rate": 0.00028151548271836965,
      "loss": 1.1846,
      "step": 19900
    },
    {
      "epoch": 0.06192468101048695,
      "grad_norm": 0.002061094855889678,
      "learning_rate": 0.00028142259569685385,
      "loss": 0.9404,
      "step": 20000
    },
    {
      "epoch": 0.06223430441553938,
      "grad_norm": 1.611570119857788,
      "learning_rate": 0.00028132970867533817,
      "loss": 0.9521,
      "step": 20100
    },
    {
      "epoch": 0.06254392782059182,
      "grad_norm": 0.08383885025978088,
      "learning_rate": 0.00028123682165382243,
      "loss": 1.279,
      "step": 20200
    },
    {
      "epoch": 0.06285355122564425,
      "grad_norm": 0.24331016838550568,
      "learning_rate": 0.0002811439346323067,
      "loss": 0.7794,
      "step": 20300
    },
    {
      "epoch": 0.06316317463069668,
      "grad_norm": 0.13959814608097076,
      "learning_rate": 0.000281051047610791,
      "loss": 1.1435,
      "step": 20400
    },
    {
      "epoch": 0.06347279803574912,
      "grad_norm": 0.034289807081222534,
      "learning_rate": 0.00028095816058927527,
      "loss": 1.0539,
      "step": 20500
    },
    {
      "epoch": 0.06378242144080155,
      "grad_norm": 40.686622619628906,
      "learning_rate": 0.00028086527356775947,
      "loss": 0.9073,
      "step": 20600
    },
    {
      "epoch": 0.06409204484585398,
      "grad_norm": 0.9845354557037354,
      "learning_rate": 0.0002807723865462438,
      "loss": 1.0907,
      "step": 20700
    },
    {
      "epoch": 0.06440166825090642,
      "grad_norm": 0.008271091617643833,
      "learning_rate": 0.00028067949952472805,
      "loss": 0.8334,
      "step": 20800
    },
    {
      "epoch": 0.06471129165595886,
      "grad_norm": 4.183626651763916,
      "learning_rate": 0.0002805866125032123,
      "loss": 0.8656,
      "step": 20900
    },
    {
      "epoch": 0.06502091506101129,
      "grad_norm": 26.312740325927734,
      "learning_rate": 0.0002804937254816966,
      "loss": 0.9271,
      "step": 21000
    },
    {
      "epoch": 0.06533053846606372,
      "grad_norm": 0.09941530227661133,
      "learning_rate": 0.00028040083846018083,
      "loss": 0.9366,
      "step": 21100
    },
    {
      "epoch": 0.06564016187111617,
      "grad_norm": 2.5181446075439453,
      "learning_rate": 0.0002803079514386651,
      "loss": 0.9165,
      "step": 21200
    },
    {
      "epoch": 0.0659497852761686,
      "grad_norm": 34.54363250732422,
      "learning_rate": 0.0002802150644171494,
      "loss": 0.7815,
      "step": 21300
    },
    {
      "epoch": 0.06625940868122103,
      "grad_norm": 19.25860023498535,
      "learning_rate": 0.00028012217739563366,
      "loss": 1.0463,
      "step": 21400
    },
    {
      "epoch": 0.06656903208627346,
      "grad_norm": 10.120397567749023,
      "learning_rate": 0.0002800292903741179,
      "loss": 0.7386,
      "step": 21500
    },
    {
      "epoch": 0.0668786554913259,
      "grad_norm": 0.014783019199967384,
      "learning_rate": 0.00027993640335260224,
      "loss": 0.8622,
      "step": 21600
    },
    {
      "epoch": 0.06718827889637834,
      "grad_norm": 85.0919418334961,
      "learning_rate": 0.00027984351633108644,
      "loss": 0.7539,
      "step": 21700
    },
    {
      "epoch": 0.06749790230143077,
      "grad_norm": 0.04007489234209061,
      "learning_rate": 0.0002797506293095707,
      "loss": 1.004,
      "step": 21800
    },
    {
      "epoch": 0.0678075257064832,
      "grad_norm": 3.088987112045288,
      "learning_rate": 0.000279657742288055,
      "loss": 1.0213,
      "step": 21900
    },
    {
      "epoch": 0.06811714911153564,
      "grad_norm": 21.83422088623047,
      "learning_rate": 0.0002795648552665393,
      "loss": 1.0042,
      "step": 22000
    },
    {
      "epoch": 0.06842677251658807,
      "grad_norm": 19.510089874267578,
      "learning_rate": 0.0002794719682450236,
      "loss": 0.9394,
      "step": 22100
    },
    {
      "epoch": 0.0687363959216405,
      "grad_norm": 21.003765106201172,
      "learning_rate": 0.0002793790812235078,
      "loss": 1.1447,
      "step": 22200
    },
    {
      "epoch": 0.06904601932669294,
      "grad_norm": 0.014587180688977242,
      "learning_rate": 0.00027928619420199206,
      "loss": 0.9232,
      "step": 22300
    },
    {
      "epoch": 0.06935564273174538,
      "grad_norm": 17.528545379638672,
      "learning_rate": 0.0002791933071804764,
      "loss": 0.9371,
      "step": 22400
    },
    {
      "epoch": 0.06966526613679781,
      "grad_norm": 0.5769743919372559,
      "learning_rate": 0.00027910042015896064,
      "loss": 0.9297,
      "step": 22500
    },
    {
      "epoch": 0.06997488954185024,
      "grad_norm": 23.885459899902344,
      "learning_rate": 0.0002790075331374449,
      "loss": 0.9071,
      "step": 22600
    },
    {
      "epoch": 0.07028451294690269,
      "grad_norm": 18.917495727539062,
      "learning_rate": 0.0002789146461159292,
      "loss": 0.9484,
      "step": 22700
    },
    {
      "epoch": 0.07059413635195512,
      "grad_norm": 67.44685363769531,
      "learning_rate": 0.0002788217590944134,
      "loss": 0.9362,
      "step": 22800
    },
    {
      "epoch": 0.07090375975700755,
      "grad_norm": 34.35641860961914,
      "learning_rate": 0.0002787288720728977,
      "loss": 0.8838,
      "step": 22900
    },
    {
      "epoch": 0.07121338316205998,
      "grad_norm": 61.53843307495117,
      "learning_rate": 0.000278635985051382,
      "loss": 1.0629,
      "step": 23000
    },
    {
      "epoch": 0.07152300656711243,
      "grad_norm": 0.01715257577598095,
      "learning_rate": 0.00027854309802986625,
      "loss": 0.9296,
      "step": 23100
    },
    {
      "epoch": 0.07183262997216486,
      "grad_norm": 15.277290344238281,
      "learning_rate": 0.0002784502110083505,
      "loss": 0.873,
      "step": 23200
    },
    {
      "epoch": 0.07214225337721729,
      "grad_norm": 0.3690440356731415,
      "learning_rate": 0.0002783573239868348,
      "loss": 0.9346,
      "step": 23300
    },
    {
      "epoch": 0.07245187678226972,
      "grad_norm": 0.5040123462677002,
      "learning_rate": 0.00027826443696531904,
      "loss": 0.8805,
      "step": 23400
    },
    {
      "epoch": 0.07276150018732216,
      "grad_norm": 0.08286500722169876,
      "learning_rate": 0.0002781715499438033,
      "loss": 0.8687,
      "step": 23500
    },
    {
      "epoch": 0.0730711235923746,
      "grad_norm": 86.90526580810547,
      "learning_rate": 0.0002780786629222876,
      "loss": 0.9254,
      "step": 23600
    },
    {
      "epoch": 0.07338074699742703,
      "grad_norm": 64.17063903808594,
      "learning_rate": 0.00027798577590077187,
      "loss": 0.7356,
      "step": 23700
    },
    {
      "epoch": 0.07369037040247946,
      "grad_norm": 0.07425842434167862,
      "learning_rate": 0.00027789288887925613,
      "loss": 0.8579,
      "step": 23800
    },
    {
      "epoch": 0.0739999938075319,
      "grad_norm": 73.43672180175781,
      "learning_rate": 0.0002778000018577404,
      "loss": 1.1836,
      "step": 23900
    },
    {
      "epoch": 0.07430961721258433,
      "grad_norm": 0.4289803206920624,
      "learning_rate": 0.00027770711483622465,
      "loss": 0.9695,
      "step": 24000
    },
    {
      "epoch": 0.07461924061763676,
      "grad_norm": 0.24469402432441711,
      "learning_rate": 0.0002776142278147089,
      "loss": 0.6475,
      "step": 24100
    },
    {
      "epoch": 0.07492886402268921,
      "grad_norm": 3.4686837196350098,
      "learning_rate": 0.00027752134079319323,
      "loss": 0.936,
      "step": 24200
    },
    {
      "epoch": 0.07523848742774164,
      "grad_norm": 36.67182540893555,
      "learning_rate": 0.0002774284537716775,
      "loss": 0.8855,
      "step": 24300
    },
    {
      "epoch": 0.07554811083279407,
      "grad_norm": 14.810420036315918,
      "learning_rate": 0.00027733556675016175,
      "loss": 0.9868,
      "step": 24400
    },
    {
      "epoch": 0.0758577342378465,
      "grad_norm": 50.86140060424805,
      "learning_rate": 0.000277242679728646,
      "loss": 0.9525,
      "step": 24500
    },
    {
      "epoch": 0.07616735764289895,
      "grad_norm": 21.878173828125,
      "learning_rate": 0.00027714979270713027,
      "loss": 0.8341,
      "step": 24600
    },
    {
      "epoch": 0.07647698104795138,
      "grad_norm": 0.002750108251348138,
      "learning_rate": 0.00027705690568561453,
      "loss": 1.2884,
      "step": 24700
    },
    {
      "epoch": 0.07678660445300381,
      "grad_norm": 0.020831406116485596,
      "learning_rate": 0.00027696401866409884,
      "loss": 0.7684,
      "step": 24800
    },
    {
      "epoch": 0.07709622785805624,
      "grad_norm": 6.839546203613281,
      "learning_rate": 0.0002768711316425831,
      "loss": 1.1387,
      "step": 24900
    },
    {
      "epoch": 0.07740585126310869,
      "grad_norm": 0.12369192391633987,
      "learning_rate": 0.00027677824462106737,
      "loss": 0.9323,
      "step": 25000
    },
    {
      "epoch": 0.07771547466816112,
      "grad_norm": 0.08812940120697021,
      "learning_rate": 0.0002766853575995516,
      "loss": 0.6168,
      "step": 25100
    },
    {
      "epoch": 0.07802509807321355,
      "grad_norm": 0.012582339346408844,
      "learning_rate": 0.0002765924705780359,
      "loss": 0.884,
      "step": 25200
    },
    {
      "epoch": 0.07833472147826598,
      "grad_norm": 35.91668701171875,
      "learning_rate": 0.00027649958355652015,
      "loss": 0.8731,
      "step": 25300
    },
    {
      "epoch": 0.07864434488331842,
      "grad_norm": 31.406492233276367,
      "learning_rate": 0.00027640669653500446,
      "loss": 0.8858,
      "step": 25400
    },
    {
      "epoch": 0.07895396828837085,
      "grad_norm": 0.020198147743940353,
      "learning_rate": 0.0002763138095134887,
      "loss": 0.8548,
      "step": 25500
    },
    {
      "epoch": 0.07926359169342329,
      "grad_norm": 0.1618589460849762,
      "learning_rate": 0.000276220922491973,
      "loss": 1.0128,
      "step": 25600
    },
    {
      "epoch": 0.07957321509847573,
      "grad_norm": 52.49799728393555,
      "learning_rate": 0.00027612803547045724,
      "loss": 0.8653,
      "step": 25700
    },
    {
      "epoch": 0.07988283850352816,
      "grad_norm": 21.122283935546875,
      "learning_rate": 0.0002760351484489415,
      "loss": 0.8811,
      "step": 25800
    },
    {
      "epoch": 0.08019246190858059,
      "grad_norm": 24.589862823486328,
      "learning_rate": 0.00027594226142742576,
      "loss": 0.8784,
      "step": 25900
    },
    {
      "epoch": 0.08050208531363302,
      "grad_norm": 0.026908548548817635,
      "learning_rate": 0.0002758493744059101,
      "loss": 0.8382,
      "step": 26000
    },
    {
      "epoch": 0.08081170871868547,
      "grad_norm": 0.00149178272113204,
      "learning_rate": 0.00027575648738439434,
      "loss": 0.9916,
      "step": 26100
    },
    {
      "epoch": 0.0811213321237379,
      "grad_norm": 28.020578384399414,
      "learning_rate": 0.0002756636003628786,
      "loss": 0.8725,
      "step": 26200
    },
    {
      "epoch": 0.08143095552879033,
      "grad_norm": 5.073699951171875,
      "learning_rate": 0.00027557071334136286,
      "loss": 0.8631,
      "step": 26300
    },
    {
      "epoch": 0.08174057893384276,
      "grad_norm": 41.48857879638672,
      "learning_rate": 0.0002754778263198471,
      "loss": 0.9092,
      "step": 26400
    },
    {
      "epoch": 0.0820502023388952,
      "grad_norm": 0.003357658861204982,
      "learning_rate": 0.00027538493929833143,
      "loss": 0.9729,
      "step": 26500
    },
    {
      "epoch": 0.08235982574394764,
      "grad_norm": 1.1472173929214478,
      "learning_rate": 0.0002752920522768157,
      "loss": 0.5436,
      "step": 26600
    },
    {
      "epoch": 0.08266944914900007,
      "grad_norm": 19.79651641845703,
      "learning_rate": 0.00027519916525529996,
      "loss": 0.8147,
      "step": 26700
    },
    {
      "epoch": 0.0829790725540525,
      "grad_norm": 35.09684371948242,
      "learning_rate": 0.0002751062782337842,
      "loss": 0.6938,
      "step": 26800
    },
    {
      "epoch": 0.08328869595910494,
      "grad_norm": 17.51171112060547,
      "learning_rate": 0.0002750133912122685,
      "loss": 0.9102,
      "step": 26900
    },
    {
      "epoch": 0.08359831936415738,
      "grad_norm": 0.3186597526073456,
      "learning_rate": 0.00027492050419075274,
      "loss": 1.1735,
      "step": 27000
    },
    {
      "epoch": 0.0839079427692098,
      "grad_norm": 25.554460525512695,
      "learning_rate": 0.00027482761716923705,
      "loss": 0.6181,
      "step": 27100
    },
    {
      "epoch": 0.08421756617426225,
      "grad_norm": 51.13100814819336,
      "learning_rate": 0.0002747347301477213,
      "loss": 0.932,
      "step": 27200
    },
    {
      "epoch": 0.08452718957931468,
      "grad_norm": 0.007074681110680103,
      "learning_rate": 0.00027464184312620557,
      "loss": 0.9953,
      "step": 27300
    },
    {
      "epoch": 0.08483681298436711,
      "grad_norm": 0.09831506758928299,
      "learning_rate": 0.00027454895610468983,
      "loss": 0.9228,
      "step": 27400
    },
    {
      "epoch": 0.08514643638941954,
      "grad_norm": 0.023898491635918617,
      "learning_rate": 0.0002744560690831741,
      "loss": 0.8528,
      "step": 27500
    },
    {
      "epoch": 0.08545605979447199,
      "grad_norm": 0.010704165324568748,
      "learning_rate": 0.00027436318206165835,
      "loss": 0.6753,
      "step": 27600
    },
    {
      "epoch": 0.08576568319952442,
      "grad_norm": 0.06598113477230072,
      "learning_rate": 0.00027427029504014267,
      "loss": 0.848,
      "step": 27700
    },
    {
      "epoch": 0.08607530660457685,
      "grad_norm": 3.9240517616271973,
      "learning_rate": 0.00027417740801862693,
      "loss": 0.6149,
      "step": 27800
    },
    {
      "epoch": 0.08638493000962928,
      "grad_norm": 0.40818655490875244,
      "learning_rate": 0.0002740845209971112,
      "loss": 0.755,
      "step": 27900
    },
    {
      "epoch": 0.08669455341468173,
      "grad_norm": 0.1422501653432846,
      "learning_rate": 0.00027399163397559545,
      "loss": 0.6708,
      "step": 28000
    },
    {
      "epoch": 0.08700417681973416,
      "grad_norm": 35.04644012451172,
      "learning_rate": 0.0002738987469540797,
      "loss": 0.8361,
      "step": 28100
    },
    {
      "epoch": 0.08731380022478659,
      "grad_norm": 0.02717595547437668,
      "learning_rate": 0.00027380585993256397,
      "loss": 0.8104,
      "step": 28200
    },
    {
      "epoch": 0.08762342362983902,
      "grad_norm": 13.32253646850586,
      "learning_rate": 0.0002737129729110483,
      "loss": 0.772,
      "step": 28300
    },
    {
      "epoch": 0.08793304703489147,
      "grad_norm": 46.11845779418945,
      "learning_rate": 0.00027362008588953255,
      "loss": 0.9811,
      "step": 28400
    },
    {
      "epoch": 0.0882426704399439,
      "grad_norm": 26.479644775390625,
      "learning_rate": 0.0002735271988680168,
      "loss": 0.9285,
      "step": 28500
    },
    {
      "epoch": 0.08855229384499633,
      "grad_norm": 0.03441040962934494,
      "learning_rate": 0.00027343431184650107,
      "loss": 0.9272,
      "step": 28600
    },
    {
      "epoch": 0.08886191725004877,
      "grad_norm": 0.007540640886873007,
      "learning_rate": 0.00027334142482498533,
      "loss": 0.6977,
      "step": 28700
    },
    {
      "epoch": 0.0891715406551012,
      "grad_norm": 82.20762634277344,
      "learning_rate": 0.0002732485378034696,
      "loss": 0.8088,
      "step": 28800
    },
    {
      "epoch": 0.08948116406015363,
      "grad_norm": 0.07311879843473434,
      "learning_rate": 0.0002731556507819539,
      "loss": 1.0039,
      "step": 28900
    },
    {
      "epoch": 0.08979078746520607,
      "grad_norm": 0.30596816539764404,
      "learning_rate": 0.00027306276376043816,
      "loss": 0.5029,
      "step": 29000
    },
    {
      "epoch": 0.09010041087025851,
      "grad_norm": 0.002161165000870824,
      "learning_rate": 0.0002729698767389224,
      "loss": 0.8489,
      "step": 29100
    },
    {
      "epoch": 0.09041003427531094,
      "grad_norm": 0.45784538984298706,
      "learning_rate": 0.0002728769897174067,
      "loss": 0.5144,
      "step": 29200
    },
    {
      "epoch": 0.09071965768036337,
      "grad_norm": 0.2407395839691162,
      "learning_rate": 0.00027278410269589094,
      "loss": 0.6583,
      "step": 29300
    },
    {
      "epoch": 0.0910292810854158,
      "grad_norm": 0.9026462435722351,
      "learning_rate": 0.0002726912156743752,
      "loss": 1.032,
      "step": 29400
    },
    {
      "epoch": 0.09133890449046825,
      "grad_norm": 0.0023162267170846462,
      "learning_rate": 0.0002725983286528595,
      "loss": 0.773,
      "step": 29500
    },
    {
      "epoch": 0.09164852789552068,
      "grad_norm": 0.44343459606170654,
      "learning_rate": 0.0002725054416313438,
      "loss": 0.673,
      "step": 29600
    },
    {
      "epoch": 0.09195815130057311,
      "grad_norm": 0.028949499130249023,
      "learning_rate": 0.00027241255460982804,
      "loss": 0.7845,
      "step": 29700
    },
    {
      "epoch": 0.09226777470562554,
      "grad_norm": 9.808902740478516,
      "learning_rate": 0.0002723196675883123,
      "loss": 0.9018,
      "step": 29800
    },
    {
      "epoch": 0.09257739811067799,
      "grad_norm": 33.61723327636719,
      "learning_rate": 0.00027222678056679656,
      "loss": 0.8678,
      "step": 29900
    },
    {
      "epoch": 0.09288702151573042,
      "grad_norm": 0.01671316847205162,
      "learning_rate": 0.0002721338935452808,
      "loss": 0.7132,
      "step": 30000
    },
    {
      "epoch": 0.09319664492078285,
      "grad_norm": 0.004632440395653248,
      "learning_rate": 0.00027204100652376514,
      "loss": 0.9887,
      "step": 30100
    },
    {
      "epoch": 0.09350626832583529,
      "grad_norm": 47.38445281982422,
      "learning_rate": 0.0002719481195022494,
      "loss": 0.6747,
      "step": 30200
    },
    {
      "epoch": 0.09381589173088772,
      "grad_norm": 0.01072979997843504,
      "learning_rate": 0.00027185523248073366,
      "loss": 0.7195,
      "step": 30300
    },
    {
      "epoch": 0.09412551513594015,
      "grad_norm": 12.312127113342285,
      "learning_rate": 0.0002717623454592179,
      "loss": 0.8879,
      "step": 30400
    },
    {
      "epoch": 0.09443513854099259,
      "grad_norm": 1.3843066692352295,
      "learning_rate": 0.0002716694584377022,
      "loss": 0.6742,
      "step": 30500
    },
    {
      "epoch": 0.09474476194604503,
      "grad_norm": 0.006969550158828497,
      "learning_rate": 0.0002715765714161865,
      "loss": 0.9365,
      "step": 30600
    },
    {
      "epoch": 0.09505438535109746,
      "grad_norm": 0.03038572333753109,
      "learning_rate": 0.00027148368439467075,
      "loss": 0.8556,
      "step": 30700
    },
    {
      "epoch": 0.09536400875614989,
      "grad_norm": 32.77730178833008,
      "learning_rate": 0.000271390797373155,
      "loss": 1.1038,
      "step": 30800
    },
    {
      "epoch": 0.09567363216120232,
      "grad_norm": 29.859392166137695,
      "learning_rate": 0.0002712979103516393,
      "loss": 0.7649,
      "step": 30900
    },
    {
      "epoch": 0.09598325556625477,
      "grad_norm": 5.9851555824279785,
      "learning_rate": 0.00027120502333012353,
      "loss": 0.7265,
      "step": 31000
    },
    {
      "epoch": 0.0962928789713072,
      "grad_norm": 3.811554431915283,
      "learning_rate": 0.0002711121363086078,
      "loss": 0.6898,
      "step": 31100
    },
    {
      "epoch": 0.09660250237635963,
      "grad_norm": 0.02756989747285843,
      "learning_rate": 0.0002710192492870921,
      "loss": 0.8264,
      "step": 31200
    },
    {
      "epoch": 0.09691212578141206,
      "grad_norm": 22.378585815429688,
      "learning_rate": 0.00027092636226557637,
      "loss": 0.7012,
      "step": 31300
    },
    {
      "epoch": 0.0972217491864645,
      "grad_norm": 22.741540908813477,
      "learning_rate": 0.00027083347524406063,
      "loss": 0.9089,
      "step": 31400
    },
    {
      "epoch": 0.09753137259151694,
      "grad_norm": 41.92893981933594,
      "learning_rate": 0.0002707405882225449,
      "loss": 0.7199,
      "step": 31500
    },
    {
      "epoch": 0.09784099599656937,
      "grad_norm": 61.85560989379883,
      "learning_rate": 0.00027064770120102915,
      "loss": 0.7384,
      "step": 31600
    },
    {
      "epoch": 0.09815061940162181,
      "grad_norm": 43.16142272949219,
      "learning_rate": 0.0002705548141795134,
      "loss": 1.0304,
      "step": 31700
    },
    {
      "epoch": 0.09846024280667424,
      "grad_norm": 0.01084255613386631,
      "learning_rate": 0.0002704619271579977,
      "loss": 0.9379,
      "step": 31800
    },
    {
      "epoch": 0.09876986621172668,
      "grad_norm": 37.46257400512695,
      "learning_rate": 0.000270369040136482,
      "loss": 0.9255,
      "step": 31900
    },
    {
      "epoch": 0.0990794896167791,
      "grad_norm": 0.0036020076368004084,
      "learning_rate": 0.00027027615311496625,
      "loss": 0.6395,
      "step": 32000
    },
    {
      "epoch": 0.09938911302183155,
      "grad_norm": 4.155707359313965,
      "learning_rate": 0.0002701832660934505,
      "loss": 1.0324,
      "step": 32100
    },
    {
      "epoch": 0.09969873642688398,
      "grad_norm": 0.7561227679252625,
      "learning_rate": 0.00027009037907193477,
      "loss": 0.7711,
      "step": 32200
    },
    {
      "epoch": 0.10000835983193641,
      "grad_norm": 41.02752685546875,
      "learning_rate": 0.00026999749205041903,
      "loss": 0.664,
      "step": 32300
    },
    {
      "epoch": 0.10031798323698884,
      "grad_norm": 49.00044250488281,
      "learning_rate": 0.00026990460502890334,
      "loss": 0.9454,
      "step": 32400
    },
    {
      "epoch": 0.10062760664204129,
      "grad_norm": 4.240883827209473,
      "learning_rate": 0.0002698117180073876,
      "loss": 0.5752,
      "step": 32500
    },
    {
      "epoch": 0.10093723004709372,
      "grad_norm": 0.006366915535181761,
      "learning_rate": 0.00026971883098587186,
      "loss": 0.6705,
      "step": 32600
    },
    {
      "epoch": 0.10124685345214615,
      "grad_norm": 0.7017671465873718,
      "learning_rate": 0.0002696259439643561,
      "loss": 0.8853,
      "step": 32700
    },
    {
      "epoch": 0.10155647685719858,
      "grad_norm": 0.030438639223575592,
      "learning_rate": 0.0002695330569428404,
      "loss": 0.8844,
      "step": 32800
    },
    {
      "epoch": 0.10186610026225103,
      "grad_norm": 0.010662795975804329,
      "learning_rate": 0.00026944016992132465,
      "loss": 0.9186,
      "step": 32900
    },
    {
      "epoch": 0.10217572366730346,
      "grad_norm": 0.023160310462117195,
      "learning_rate": 0.00026934728289980896,
      "loss": 0.6336,
      "step": 33000
    },
    {
      "epoch": 0.10248534707235589,
      "grad_norm": 55.0420036315918,
      "learning_rate": 0.0002692543958782932,
      "loss": 0.8088,
      "step": 33100
    },
    {
      "epoch": 0.10279497047740833,
      "grad_norm": 0.6888642907142639,
      "learning_rate": 0.0002691615088567775,
      "loss": 0.7943,
      "step": 33200
    },
    {
      "epoch": 0.10310459388246077,
      "grad_norm": 0.06106690689921379,
      "learning_rate": 0.00026906862183526174,
      "loss": 0.9058,
      "step": 33300
    },
    {
      "epoch": 0.1034142172875132,
      "grad_norm": 16.717876434326172,
      "learning_rate": 0.000268975734813746,
      "loss": 0.6488,
      "step": 33400
    },
    {
      "epoch": 0.10372384069256563,
      "grad_norm": 0.00813397765159607,
      "learning_rate": 0.00026888284779223026,
      "loss": 0.7498,
      "step": 33500
    },
    {
      "epoch": 0.10403346409761807,
      "grad_norm": 18.601593017578125,
      "learning_rate": 0.0002687899607707146,
      "loss": 0.9918,
      "step": 33600
    },
    {
      "epoch": 0.1043430875026705,
      "grad_norm": 21.697551727294922,
      "learning_rate": 0.00026869707374919884,
      "loss": 0.7798,
      "step": 33700
    },
    {
      "epoch": 0.10465271090772293,
      "grad_norm": 0.024599572643637657,
      "learning_rate": 0.0002686041867276831,
      "loss": 0.6153,
      "step": 33800
    },
    {
      "epoch": 0.10496233431277537,
      "grad_norm": 0.0025642889086157084,
      "learning_rate": 0.00026851129970616736,
      "loss": 0.7042,
      "step": 33900
    },
    {
      "epoch": 0.10527195771782781,
      "grad_norm": 18.850448608398438,
      "learning_rate": 0.0002684184126846516,
      "loss": 0.6102,
      "step": 34000
    },
    {
      "epoch": 0.10558158112288024,
      "grad_norm": 0.009519311599433422,
      "learning_rate": 0.00026832552566313593,
      "loss": 0.7716,
      "step": 34100
    },
    {
      "epoch": 0.10589120452793267,
      "grad_norm": 0.013216961175203323,
      "learning_rate": 0.0002682326386416202,
      "loss": 0.8101,
      "step": 34200
    },
    {
      "epoch": 0.1062008279329851,
      "grad_norm": 53.05105972290039,
      "learning_rate": 0.00026813975162010446,
      "loss": 0.8947,
      "step": 34300
    },
    {
      "epoch": 0.10651045133803755,
      "grad_norm": 0.08597684651613235,
      "learning_rate": 0.0002680468645985887,
      "loss": 0.8561,
      "step": 34400
    },
    {
      "epoch": 0.10682007474308998,
      "grad_norm": 21.671775817871094,
      "learning_rate": 0.000267953977577073,
      "loss": 0.8654,
      "step": 34500
    },
    {
      "epoch": 0.10712969814814241,
      "grad_norm": 0.19599205255508423,
      "learning_rate": 0.00026786109055555724,
      "loss": 0.6554,
      "step": 34600
    },
    {
      "epoch": 0.10743932155319486,
      "grad_norm": 9.020543098449707,
      "learning_rate": 0.00026776820353404155,
      "loss": 1.0579,
      "step": 34700
    },
    {
      "epoch": 0.10774894495824729,
      "grad_norm": 39.53853988647461,
      "learning_rate": 0.0002676753165125258,
      "loss": 0.6383,
      "step": 34800
    },
    {
      "epoch": 0.10805856836329972,
      "grad_norm": 141.0603485107422,
      "learning_rate": 0.00026758242949101007,
      "loss": 0.9195,
      "step": 34900
    },
    {
      "epoch": 0.10836819176835215,
      "grad_norm": 48.50802993774414,
      "learning_rate": 0.00026748954246949433,
      "loss": 0.8041,
      "step": 35000
    },
    {
      "epoch": 0.1086778151734046,
      "grad_norm": 0.007990519516170025,
      "learning_rate": 0.0002673966554479786,
      "loss": 0.769,
      "step": 35100
    },
    {
      "epoch": 0.10898743857845702,
      "grad_norm": 0.014056243933737278,
      "learning_rate": 0.00026730376842646285,
      "loss": 0.7943,
      "step": 35200
    },
    {
      "epoch": 0.10929706198350946,
      "grad_norm": 0.8017542362213135,
      "learning_rate": 0.00026721088140494717,
      "loss": 0.9184,
      "step": 35300
    },
    {
      "epoch": 0.10960668538856189,
      "grad_norm": 58.36787796020508,
      "learning_rate": 0.00026711799438343143,
      "loss": 0.7327,
      "step": 35400
    },
    {
      "epoch": 0.10991630879361433,
      "grad_norm": 0.05013848468661308,
      "learning_rate": 0.0002670251073619157,
      "loss": 0.835,
      "step": 35500
    },
    {
      "epoch": 0.11022593219866676,
      "grad_norm": 2.521365165710449,
      "learning_rate": 0.00026693222034039995,
      "loss": 0.5208,
      "step": 35600
    },
    {
      "epoch": 0.1105355556037192,
      "grad_norm": 6.524844646453857,
      "learning_rate": 0.0002668393333188842,
      "loss": 0.7566,
      "step": 35700
    },
    {
      "epoch": 0.11084517900877162,
      "grad_norm": 72.16377258300781,
      "learning_rate": 0.00026674644629736847,
      "loss": 0.7984,
      "step": 35800
    },
    {
      "epoch": 0.11115480241382407,
      "grad_norm": 0.05147308111190796,
      "learning_rate": 0.0002666535592758528,
      "loss": 0.959,
      "step": 35900
    },
    {
      "epoch": 0.1114644258188765,
      "grad_norm": 0.01615774631500244,
      "learning_rate": 0.00026656067225433705,
      "loss": 0.8143,
      "step": 36000
    },
    {
      "epoch": 0.11177404922392893,
      "grad_norm": 7.745899200439453,
      "learning_rate": 0.0002664677852328213,
      "loss": 0.5321,
      "step": 36100
    },
    {
      "epoch": 0.11208367262898138,
      "grad_norm": 6.883225440979004,
      "learning_rate": 0.00026637489821130557,
      "loss": 0.8957,
      "step": 36200
    },
    {
      "epoch": 0.11239329603403381,
      "grad_norm": 90.06720733642578,
      "learning_rate": 0.0002662820111897898,
      "loss": 0.7852,
      "step": 36300
    },
    {
      "epoch": 0.11270291943908624,
      "grad_norm": 44.38864517211914,
      "learning_rate": 0.0002661891241682741,
      "loss": 0.6339,
      "step": 36400
    },
    {
      "epoch": 0.11301254284413867,
      "grad_norm": 0.005212491378188133,
      "learning_rate": 0.0002660962371467584,
      "loss": 0.5751,
      "step": 36500
    },
    {
      "epoch": 0.11332216624919111,
      "grad_norm": 0.12344871461391449,
      "learning_rate": 0.00026600335012524266,
      "loss": 0.5879,
      "step": 36600
    },
    {
      "epoch": 0.11363178965424355,
      "grad_norm": 0.01995750702917576,
      "learning_rate": 0.0002659104631037269,
      "loss": 0.8791,
      "step": 36700
    },
    {
      "epoch": 0.11394141305929598,
      "grad_norm": 0.026129474863409996,
      "learning_rate": 0.0002658175760822112,
      "loss": 0.5666,
      "step": 36800
    },
    {
      "epoch": 0.11425103646434841,
      "grad_norm": 0.02842579409480095,
      "learning_rate": 0.00026572468906069544,
      "loss": 0.8901,
      "step": 36900
    },
    {
      "epoch": 0.11456065986940085,
      "grad_norm": 26.882280349731445,
      "learning_rate": 0.0002656318020391797,
      "loss": 0.6961,
      "step": 37000
    },
    {
      "epoch": 0.11487028327445328,
      "grad_norm": 0.012386457063257694,
      "learning_rate": 0.000265538915017664,
      "loss": 0.6017,
      "step": 37100
    },
    {
      "epoch": 0.11517990667950571,
      "grad_norm": 44.895912170410156,
      "learning_rate": 0.0002654460279961483,
      "loss": 0.7467,
      "step": 37200
    },
    {
      "epoch": 0.11548953008455815,
      "grad_norm": 0.6085771322250366,
      "learning_rate": 0.00026535314097463254,
      "loss": 0.7791,
      "step": 37300
    },
    {
      "epoch": 0.11579915348961059,
      "grad_norm": 0.01924660988152027,
      "learning_rate": 0.0002652602539531168,
      "loss": 0.5215,
      "step": 37400
    },
    {
      "epoch": 0.11610877689466302,
      "grad_norm": 39.45454025268555,
      "learning_rate": 0.00026516736693160106,
      "loss": 0.7193,
      "step": 37500
    },
    {
      "epoch": 0.11641840029971545,
      "grad_norm": 30.616924285888672,
      "learning_rate": 0.0002650744799100853,
      "loss": 0.6447,
      "step": 37600
    },
    {
      "epoch": 0.1167280237047679,
      "grad_norm": 36.79774856567383,
      "learning_rate": 0.00026498159288856964,
      "loss": 0.9645,
      "step": 37700
    },
    {
      "epoch": 0.11703764710982033,
      "grad_norm": 0.03338002786040306,
      "learning_rate": 0.0002648887058670539,
      "loss": 0.8548,
      "step": 37800
    },
    {
      "epoch": 0.11734727051487276,
      "grad_norm": 72.206298828125,
      "learning_rate": 0.00026479581884553816,
      "loss": 0.8186,
      "step": 37900
    },
    {
      "epoch": 0.11765689391992519,
      "grad_norm": 0.023962393403053284,
      "learning_rate": 0.0002647029318240224,
      "loss": 0.7017,
      "step": 38000
    },
    {
      "epoch": 0.11796651732497763,
      "grad_norm": 0.008847109973430634,
      "learning_rate": 0.0002646100448025067,
      "loss": 0.9274,
      "step": 38100
    },
    {
      "epoch": 0.11827614073003007,
      "grad_norm": 42.581390380859375,
      "learning_rate": 0.000264517157780991,
      "loss": 0.7865,
      "step": 38200
    },
    {
      "epoch": 0.1185857641350825,
      "grad_norm": 0.01297799777239561,
      "learning_rate": 0.00026442427075947525,
      "loss": 0.5513,
      "step": 38300
    },
    {
      "epoch": 0.11889538754013493,
      "grad_norm": 0.5192378759384155,
      "learning_rate": 0.0002643313837379595,
      "loss": 0.8759,
      "step": 38400
    },
    {
      "epoch": 0.11920501094518737,
      "grad_norm": 0.9203397035598755,
      "learning_rate": 0.0002642384967164438,
      "loss": 0.9657,
      "step": 38500
    },
    {
      "epoch": 0.1195146343502398,
      "grad_norm": 0.16824744641780853,
      "learning_rate": 0.00026414560969492803,
      "loss": 0.5992,
      "step": 38600
    },
    {
      "epoch": 0.11982425775529223,
      "grad_norm": 0.16080768406391144,
      "learning_rate": 0.0002640527226734123,
      "loss": 0.6191,
      "step": 38700
    },
    {
      "epoch": 0.12013388116034467,
      "grad_norm": 0.04202137514948845,
      "learning_rate": 0.0002639598356518966,
      "loss": 0.5511,
      "step": 38800
    },
    {
      "epoch": 0.12044350456539711,
      "grad_norm": 0.001107275951653719,
      "learning_rate": 0.00026386694863038087,
      "loss": 0.5877,
      "step": 38900
    },
    {
      "epoch": 0.12075312797044954,
      "grad_norm": 0.005273865535855293,
      "learning_rate": 0.00026377406160886513,
      "loss": 0.7364,
      "step": 39000
    },
    {
      "epoch": 0.12106275137550197,
      "grad_norm": 10.956762313842773,
      "learning_rate": 0.0002636811745873494,
      "loss": 0.7791,
      "step": 39100
    },
    {
      "epoch": 0.12137237478055442,
      "grad_norm": 74.60991668701172,
      "learning_rate": 0.00026358828756583365,
      "loss": 1.0619,
      "step": 39200
    },
    {
      "epoch": 0.12168199818560685,
      "grad_norm": 0.0012663003290072083,
      "learning_rate": 0.0002634954005443179,
      "loss": 0.5687,
      "step": 39300
    },
    {
      "epoch": 0.12199162159065928,
      "grad_norm": 0.007593556307256222,
      "learning_rate": 0.0002634025135228022,
      "loss": 0.8407,
      "step": 39400
    },
    {
      "epoch": 0.12230124499571171,
      "grad_norm": 55.714481353759766,
      "learning_rate": 0.0002633096265012865,
      "loss": 0.7314,
      "step": 39500
    },
    {
      "epoch": 0.12261086840076416,
      "grad_norm": 0.005494951736181974,
      "learning_rate": 0.00026321673947977075,
      "loss": 0.6736,
      "step": 39600
    },
    {
      "epoch": 0.12292049180581659,
      "grad_norm": 59.7289924621582,
      "learning_rate": 0.000263123852458255,
      "loss": 0.7362,
      "step": 39700
    },
    {
      "epoch": 0.12323011521086902,
      "grad_norm": 0.011423977091908455,
      "learning_rate": 0.00026303096543673927,
      "loss": 0.6889,
      "step": 39800
    },
    {
      "epoch": 0.12353973861592145,
      "grad_norm": 42.13529586791992,
      "learning_rate": 0.00026293807841522353,
      "loss": 0.7237,
      "step": 39900
    },
    {
      "epoch": 0.1238493620209739,
      "grad_norm": 0.003615215653553605,
      "learning_rate": 0.00026284519139370784,
      "loss": 0.8665,
      "step": 40000
    },
    {
      "epoch": 0.12415898542602632,
      "grad_norm": 0.011009003967046738,
      "learning_rate": 0.0002627523043721921,
      "loss": 0.611,
      "step": 40100
    },
    {
      "epoch": 0.12446860883107876,
      "grad_norm": 0.04383140057325363,
      "learning_rate": 0.00026265941735067636,
      "loss": 0.6052,
      "step": 40200
    },
    {
      "epoch": 0.12477823223613119,
      "grad_norm": 0.002382691018283367,
      "learning_rate": 0.0002625665303291606,
      "loss": 0.7099,
      "step": 40300
    },
    {
      "epoch": 0.12508785564118363,
      "grad_norm": 5.330833435058594,
      "learning_rate": 0.0002624736433076449,
      "loss": 0.5808,
      "step": 40400
    },
    {
      "epoch": 0.12539747904623608,
      "grad_norm": 23.071012496948242,
      "learning_rate": 0.00026238075628612915,
      "loss": 0.6691,
      "step": 40500
    },
    {
      "epoch": 0.1257071024512885,
      "grad_norm": 33.27326583862305,
      "learning_rate": 0.00026228786926461346,
      "loss": 0.9919,
      "step": 40600
    },
    {
      "epoch": 0.12601672585634094,
      "grad_norm": 16.67949104309082,
      "learning_rate": 0.0002621949822430977,
      "loss": 0.8279,
      "step": 40700
    },
    {
      "epoch": 0.12632634926139336,
      "grad_norm": 9.81039810180664,
      "learning_rate": 0.0002621020952215819,
      "loss": 0.6728,
      "step": 40800
    },
    {
      "epoch": 0.1266359726664458,
      "grad_norm": 0.03498045355081558,
      "learning_rate": 0.00026200920820006624,
      "loss": 0.7677,
      "step": 40900
    },
    {
      "epoch": 0.12694559607149825,
      "grad_norm": 17.036945343017578,
      "learning_rate": 0.0002619163211785505,
      "loss": 0.9406,
      "step": 41000
    },
    {
      "epoch": 0.12725521947655066,
      "grad_norm": 0.0021937682759016752,
      "learning_rate": 0.00026182343415703476,
      "loss": 0.5671,
      "step": 41100
    },
    {
      "epoch": 0.1275648428816031,
      "grad_norm": 0.02758747898042202,
      "learning_rate": 0.0002617305471355191,
      "loss": 0.67,
      "step": 41200
    },
    {
      "epoch": 0.12787446628665555,
      "grad_norm": 32.47550582885742,
      "learning_rate": 0.00026163766011400334,
      "loss": 0.7707,
      "step": 41300
    },
    {
      "epoch": 0.12818408969170797,
      "grad_norm": 0.004298690240830183,
      "learning_rate": 0.00026154477309248754,
      "loss": 0.5727,
      "step": 41400
    },
    {
      "epoch": 0.12849371309676041,
      "grad_norm": 0.16570736467838287,
      "learning_rate": 0.00026145188607097186,
      "loss": 0.8481,
      "step": 41500
    },
    {
      "epoch": 0.12880333650181283,
      "grad_norm": 57.569480895996094,
      "learning_rate": 0.0002613589990494561,
      "loss": 0.9154,
      "step": 41600
    },
    {
      "epoch": 0.12911295990686528,
      "grad_norm": 0.10226275771856308,
      "learning_rate": 0.0002612661120279404,
      "loss": 0.6281,
      "step": 41700
    },
    {
      "epoch": 0.12942258331191772,
      "grad_norm": 60.307254791259766,
      "learning_rate": 0.0002611732250064247,
      "loss": 0.7537,
      "step": 41800
    },
    {
      "epoch": 0.12973220671697014,
      "grad_norm": 1.4515999555587769,
      "learning_rate": 0.0002610803379849089,
      "loss": 0.6667,
      "step": 41900
    },
    {
      "epoch": 0.13004183012202258,
      "grad_norm": 0.19509992003440857,
      "learning_rate": 0.00026098745096339316,
      "loss": 0.6866,
      "step": 42000
    },
    {
      "epoch": 0.13035145352707503,
      "grad_norm": 1.0169428586959839,
      "learning_rate": 0.0002608945639418775,
      "loss": 0.7099,
      "step": 42100
    },
    {
      "epoch": 0.13066107693212745,
      "grad_norm": 0.24141624569892883,
      "learning_rate": 0.00026080167692036174,
      "loss": 0.6017,
      "step": 42200
    },
    {
      "epoch": 0.1309707003371799,
      "grad_norm": 0.33154913783073425,
      "learning_rate": 0.00026070878989884605,
      "loss": 0.795,
      "step": 42300
    },
    {
      "epoch": 0.13128032374223234,
      "grad_norm": 0.014954020269215107,
      "learning_rate": 0.0002606159028773303,
      "loss": 0.5748,
      "step": 42400
    },
    {
      "epoch": 0.13158994714728475,
      "grad_norm": 55.420875549316406,
      "learning_rate": 0.0002605230158558145,
      "loss": 0.4584,
      "step": 42500
    },
    {
      "epoch": 0.1318995705523372,
      "grad_norm": 0.4380301535129547,
      "learning_rate": 0.00026043012883429883,
      "loss": 0.8363,
      "step": 42600
    },
    {
      "epoch": 0.13220919395738961,
      "grad_norm": 79.96604919433594,
      "learning_rate": 0.0002603372418127831,
      "loss": 0.7992,
      "step": 42700
    },
    {
      "epoch": 0.13251881736244206,
      "grad_norm": 0.01116462703794241,
      "learning_rate": 0.00026024435479126735,
      "loss": 0.7621,
      "step": 42800
    },
    {
      "epoch": 0.1328284407674945,
      "grad_norm": 49.76287078857422,
      "learning_rate": 0.00026015146776975167,
      "loss": 0.7005,
      "step": 42900
    },
    {
      "epoch": 0.13313806417254692,
      "grad_norm": 0.5988059639930725,
      "learning_rate": 0.0002600585807482359,
      "loss": 0.4899,
      "step": 43000
    },
    {
      "epoch": 0.13344768757759937,
      "grad_norm": 0.03623887896537781,
      "learning_rate": 0.00025996569372672013,
      "loss": 1.066,
      "step": 43100
    },
    {
      "epoch": 0.1337573109826518,
      "grad_norm": 0.0027222083881497383,
      "learning_rate": 0.00025987280670520445,
      "loss": 0.6754,
      "step": 43200
    },
    {
      "epoch": 0.13406693438770423,
      "grad_norm": 0.0013349400833249092,
      "learning_rate": 0.0002597799196836887,
      "loss": 0.5134,
      "step": 43300
    },
    {
      "epoch": 0.13437655779275667,
      "grad_norm": 0.0009320633253082633,
      "learning_rate": 0.00025968703266217297,
      "loss": 0.7035,
      "step": 43400
    },
    {
      "epoch": 0.13468618119780912,
      "grad_norm": 0.04657112434506416,
      "learning_rate": 0.0002595941456406573,
      "loss": 0.6905,
      "step": 43500
    },
    {
      "epoch": 0.13499580460286154,
      "grad_norm": 0.0009379393304698169,
      "learning_rate": 0.0002595012586191415,
      "loss": 0.8912,
      "step": 43600
    },
    {
      "epoch": 0.13530542800791398,
      "grad_norm": 0.0010004742071032524,
      "learning_rate": 0.00025940837159762575,
      "loss": 0.9628,
      "step": 43700
    },
    {
      "epoch": 0.1356150514129664,
      "grad_norm": 0.07078979164361954,
      "learning_rate": 0.00025931548457611007,
      "loss": 0.6141,
      "step": 43800
    },
    {
      "epoch": 0.13592467481801884,
      "grad_norm": 0.0019389318767935038,
      "learning_rate": 0.0002592225975545943,
      "loss": 0.5057,
      "step": 43900
    },
    {
      "epoch": 0.1362342982230713,
      "grad_norm": 0.05645474046468735,
      "learning_rate": 0.0002591297105330786,
      "loss": 0.4902,
      "step": 44000
    },
    {
      "epoch": 0.1365439216281237,
      "grad_norm": 48.64927673339844,
      "learning_rate": 0.00025903682351156285,
      "loss": 0.8724,
      "step": 44100
    },
    {
      "epoch": 0.13685354503317615,
      "grad_norm": 31.535179138183594,
      "learning_rate": 0.0002589439364900471,
      "loss": 0.6926,
      "step": 44200
    },
    {
      "epoch": 0.1371631684382286,
      "grad_norm": 0.21363650262355804,
      "learning_rate": 0.00025885104946853137,
      "loss": 0.5735,
      "step": 44300
    },
    {
      "epoch": 0.137472791843281,
      "grad_norm": 0.07886453717947006,
      "learning_rate": 0.0002587581624470157,
      "loss": 0.4762,
      "step": 44400
    },
    {
      "epoch": 0.13778241524833346,
      "grad_norm": 0.08500970900058746,
      "learning_rate": 0.00025866527542549994,
      "loss": 0.608,
      "step": 44500
    },
    {
      "epoch": 0.13809203865338587,
      "grad_norm": 23.534866333007812,
      "learning_rate": 0.0002585723884039842,
      "loss": 0.9792,
      "step": 44600
    },
    {
      "epoch": 0.13840166205843832,
      "grad_norm": 9.577014923095703,
      "learning_rate": 0.00025847950138246846,
      "loss": 0.7329,
      "step": 44700
    },
    {
      "epoch": 0.13871128546349076,
      "grad_norm": 0.027442708611488342,
      "learning_rate": 0.0002583866143609527,
      "loss": 0.7706,
      "step": 44800
    },
    {
      "epoch": 0.13902090886854318,
      "grad_norm": 0.20980069041252136,
      "learning_rate": 0.000258293727339437,
      "loss": 0.4458,
      "step": 44900
    },
    {
      "epoch": 0.13933053227359563,
      "grad_norm": 25.36465072631836,
      "learning_rate": 0.0002582008403179213,
      "loss": 0.5861,
      "step": 45000
    },
    {
      "epoch": 0.13964015567864807,
      "grad_norm": 0.3072180151939392,
      "learning_rate": 0.00025810795329640556,
      "loss": 0.7235,
      "step": 45100
    },
    {
      "epoch": 0.1399497790837005,
      "grad_norm": 0.004884183406829834,
      "learning_rate": 0.0002580150662748898,
      "loss": 0.7582,
      "step": 45200
    },
    {
      "epoch": 0.14025940248875293,
      "grad_norm": 1.743546485900879,
      "learning_rate": 0.0002579221792533741,
      "loss": 0.6495,
      "step": 45300
    },
    {
      "epoch": 0.14056902589380538,
      "grad_norm": 0.5940309166908264,
      "learning_rate": 0.00025782929223185834,
      "loss": 0.9349,
      "step": 45400
    },
    {
      "epoch": 0.1408786492988578,
      "grad_norm": 12.413250923156738,
      "learning_rate": 0.0002577364052103426,
      "loss": 0.4929,
      "step": 45500
    },
    {
      "epoch": 0.14118827270391024,
      "grad_norm": 0.0017807197291404009,
      "learning_rate": 0.0002576435181888269,
      "loss": 0.4805,
      "step": 45600
    },
    {
      "epoch": 0.14149789610896266,
      "grad_norm": 0.005663708318024874,
      "learning_rate": 0.0002575506311673112,
      "loss": 0.822,
      "step": 45700
    },
    {
      "epoch": 0.1418075195140151,
      "grad_norm": 0.3371427655220032,
      "learning_rate": 0.00025745774414579544,
      "loss": 0.4473,
      "step": 45800
    },
    {
      "epoch": 0.14211714291906755,
      "grad_norm": 0.005288054700940847,
      "learning_rate": 0.0002573648571242797,
      "loss": 0.6318,
      "step": 45900
    },
    {
      "epoch": 0.14242676632411996,
      "grad_norm": 0.019929083064198494,
      "learning_rate": 0.00025727197010276396,
      "loss": 0.6648,
      "step": 46000
    },
    {
      "epoch": 0.1427363897291724,
      "grad_norm": 0.0026315185241401196,
      "learning_rate": 0.0002571790830812483,
      "loss": 0.7152,
      "step": 46100
    },
    {
      "epoch": 0.14304601313422485,
      "grad_norm": 0.02042306400835514,
      "learning_rate": 0.00025708619605973253,
      "loss": 0.6413,
      "step": 46200
    },
    {
      "epoch": 0.14335563653927727,
      "grad_norm": 0.8337588310241699,
      "learning_rate": 0.0002569933090382168,
      "loss": 0.5636,
      "step": 46300
    },
    {
      "epoch": 0.14366525994432972,
      "grad_norm": 32.734745025634766,
      "learning_rate": 0.00025690042201670105,
      "loss": 0.5715,
      "step": 46400
    },
    {
      "epoch": 0.14397488334938216,
      "grad_norm": 21.28999900817871,
      "learning_rate": 0.0002568075349951853,
      "loss": 0.7019,
      "step": 46500
    },
    {
      "epoch": 0.14428450675443458,
      "grad_norm": 0.00095600844360888,
      "learning_rate": 0.0002567146479736696,
      "loss": 0.7378,
      "step": 46600
    },
    {
      "epoch": 0.14459413015948702,
      "grad_norm": 0.055298808962106705,
      "learning_rate": 0.0002566217609521539,
      "loss": 0.6022,
      "step": 46700
    },
    {
      "epoch": 0.14490375356453944,
      "grad_norm": 0.007012478541582823,
      "learning_rate": 0.00025652887393063815,
      "loss": 0.4983,
      "step": 46800
    },
    {
      "epoch": 0.14521337696959188,
      "grad_norm": 0.002553490921854973,
      "learning_rate": 0.0002564359869091224,
      "loss": 0.7907,
      "step": 46900
    },
    {
      "epoch": 0.14552300037464433,
      "grad_norm": 1.7169746160507202,
      "learning_rate": 0.00025634309988760667,
      "loss": 0.7385,
      "step": 47000
    },
    {
      "epoch": 0.14583262377969675,
      "grad_norm": 48.96695327758789,
      "learning_rate": 0.00025625021286609093,
      "loss": 0.6939,
      "step": 47100
    },
    {
      "epoch": 0.1461422471847492,
      "grad_norm": 1.5587165355682373,
      "learning_rate": 0.0002561573258445752,
      "loss": 0.795,
      "step": 47200
    },
    {
      "epoch": 0.14645187058980164,
      "grad_norm": 0.05138680711388588,
      "learning_rate": 0.0002560644388230595,
      "loss": 0.5127,
      "step": 47300
    },
    {
      "epoch": 0.14676149399485405,
      "grad_norm": 0.42160725593566895,
      "learning_rate": 0.00025597155180154377,
      "loss": 0.83,
      "step": 47400
    },
    {
      "epoch": 0.1470711173999065,
      "grad_norm": 0.02961820177733898,
      "learning_rate": 0.00025587866478002803,
      "loss": 0.6965,
      "step": 47500
    },
    {
      "epoch": 0.14738074080495892,
      "grad_norm": 0.01799382083117962,
      "learning_rate": 0.0002557857777585123,
      "loss": 0.7154,
      "step": 47600
    },
    {
      "epoch": 0.14769036421001136,
      "grad_norm": 0.04993394762277603,
      "learning_rate": 0.00025569289073699655,
      "loss": 0.6103,
      "step": 47700
    },
    {
      "epoch": 0.1479999876150638,
      "grad_norm": 0.00987824983894825,
      "learning_rate": 0.0002556000037154808,
      "loss": 0.9337,
      "step": 47800
    },
    {
      "epoch": 0.14830961102011622,
      "grad_norm": 31.39847755432129,
      "learning_rate": 0.0002555071166939651,
      "loss": 0.7792,
      "step": 47900
    },
    {
      "epoch": 0.14861923442516867,
      "grad_norm": 0.006891809403896332,
      "learning_rate": 0.0002554142296724494,
      "loss": 0.504,
      "step": 48000
    },
    {
      "epoch": 0.1489288578302211,
      "grad_norm": 0.008952952921390533,
      "learning_rate": 0.00025532134265093365,
      "loss": 0.6874,
      "step": 48100
    },
    {
      "epoch": 0.14923848123527353,
      "grad_norm": 1.1955238580703735,
      "learning_rate": 0.0002552284556294179,
      "loss": 0.7468,
      "step": 48200
    },
    {
      "epoch": 0.14954810464032597,
      "grad_norm": 0.003921611234545708,
      "learning_rate": 0.00025513556860790217,
      "loss": 0.6114,
      "step": 48300
    },
    {
      "epoch": 0.14985772804537842,
      "grad_norm": 0.006491847801953554,
      "learning_rate": 0.0002550426815863864,
      "loss": 0.6906,
      "step": 48400
    },
    {
      "epoch": 0.15016735145043084,
      "grad_norm": 4.94959831237793,
      "learning_rate": 0.00025494979456487074,
      "loss": 0.7921,
      "step": 48500
    },
    {
      "epoch": 0.15047697485548328,
      "grad_norm": 0.12892533838748932,
      "learning_rate": 0.000254856907543355,
      "loss": 0.5765,
      "step": 48600
    },
    {
      "epoch": 0.1507865982605357,
      "grad_norm": 98.5770492553711,
      "learning_rate": 0.00025476402052183926,
      "loss": 0.5974,
      "step": 48700
    },
    {
      "epoch": 0.15109622166558814,
      "grad_norm": 0.0046958052553236485,
      "learning_rate": 0.0002546711335003235,
      "loss": 0.5471,
      "step": 48800
    },
    {
      "epoch": 0.1514058450706406,
      "grad_norm": 0.0019292886136099696,
      "learning_rate": 0.0002545782464788078,
      "loss": 0.6821,
      "step": 48900
    },
    {
      "epoch": 0.151715468475693,
      "grad_norm": 0.13786976039409637,
      "learning_rate": 0.00025448535945729204,
      "loss": 0.6966,
      "step": 49000
    },
    {
      "epoch": 0.15202509188074545,
      "grad_norm": 0.031541306525468826,
      "learning_rate": 0.00025439247243577636,
      "loss": 0.8151,
      "step": 49100
    },
    {
      "epoch": 0.1523347152857979,
      "grad_norm": 4.567634582519531,
      "learning_rate": 0.0002542995854142606,
      "loss": 0.3304,
      "step": 49200
    },
    {
      "epoch": 0.1526443386908503,
      "grad_norm": 0.0043016825802624226,
      "learning_rate": 0.0002542066983927449,
      "loss": 0.4949,
      "step": 49300
    },
    {
      "epoch": 0.15295396209590276,
      "grad_norm": 1.5416237115859985,
      "learning_rate": 0.00025411381137122914,
      "loss": 0.6247,
      "step": 49400
    },
    {
      "epoch": 0.1532635855009552,
      "grad_norm": 0.040377210825681686,
      "learning_rate": 0.0002540209243497134,
      "loss": 0.5143,
      "step": 49500
    },
    {
      "epoch": 0.15357320890600762,
      "grad_norm": 48.2475700378418,
      "learning_rate": 0.00025392803732819766,
      "loss": 0.7198,
      "step": 49600
    },
    {
      "epoch": 0.15388283231106006,
      "grad_norm": 59.1780891418457,
      "learning_rate": 0.000253835150306682,
      "loss": 0.6059,
      "step": 49700
    },
    {
      "epoch": 0.15419245571611248,
      "grad_norm": 6.562684535980225,
      "learning_rate": 0.00025374226328516624,
      "loss": 0.7674,
      "step": 49800
    },
    {
      "epoch": 0.15450207912116493,
      "grad_norm": 0.00539783388376236,
      "learning_rate": 0.0002536493762636505,
      "loss": 0.4328,
      "step": 49900
    },
    {
      "epoch": 0.15481170252621737,
      "grad_norm": 49.244041442871094,
      "learning_rate": 0.00025355648924213476,
      "loss": 0.5203,
      "step": 50000
    },
    {
      "epoch": 0.1551213259312698,
      "grad_norm": 1.1693334579467773,
      "learning_rate": 0.000253463602220619,
      "loss": 0.7561,
      "step": 50100
    },
    {
      "epoch": 0.15543094933632223,
      "grad_norm": 0.06408380717039108,
      "learning_rate": 0.00025337071519910333,
      "loss": 0.6947,
      "step": 50200
    },
    {
      "epoch": 0.15574057274137468,
      "grad_norm": 0.0036412610206753016,
      "learning_rate": 0.0002532778281775876,
      "loss": 0.6314,
      "step": 50300
    },
    {
      "epoch": 0.1560501961464271,
      "grad_norm": 0.005812791641801596,
      "learning_rate": 0.00025318494115607185,
      "loss": 0.3315,
      "step": 50400
    },
    {
      "epoch": 0.15635981955147954,
      "grad_norm": 0.00023896672064438462,
      "learning_rate": 0.0002530920541345561,
      "loss": 0.4625,
      "step": 50500
    },
    {
      "epoch": 0.15666944295653196,
      "grad_norm": 190.8560333251953,
      "learning_rate": 0.0002529991671130404,
      "loss": 0.5277,
      "step": 50600
    },
    {
      "epoch": 0.1569790663615844,
      "grad_norm": 42.12382888793945,
      "learning_rate": 0.00025290628009152463,
      "loss": 0.8146,
      "step": 50700
    },
    {
      "epoch": 0.15728868976663685,
      "grad_norm": 11.479610443115234,
      "learning_rate": 0.00025281339307000895,
      "loss": 0.6855,
      "step": 50800
    },
    {
      "epoch": 0.15759831317168926,
      "grad_norm": 99.74720001220703,
      "learning_rate": 0.0002527205060484932,
      "loss": 0.6472,
      "step": 50900
    },
    {
      "epoch": 0.1579079365767417,
      "grad_norm": 0.0028333456721156836,
      "learning_rate": 0.00025262761902697747,
      "loss": 0.7715,
      "step": 51000
    },
    {
      "epoch": 0.15821755998179415,
      "grad_norm": 0.006851211190223694,
      "learning_rate": 0.00025253473200546173,
      "loss": 0.6387,
      "step": 51100
    },
    {
      "epoch": 0.15852718338684657,
      "grad_norm": 20.027362823486328,
      "learning_rate": 0.000252441844983946,
      "loss": 0.797,
      "step": 51200
    },
    {
      "epoch": 0.15883680679189902,
      "grad_norm": 61.80406188964844,
      "learning_rate": 0.00025234895796243025,
      "loss": 0.6537,
      "step": 51300
    },
    {
      "epoch": 0.15914643019695146,
      "grad_norm": 0.00498818326741457,
      "learning_rate": 0.00025225607094091457,
      "loss": 0.4867,
      "step": 51400
    },
    {
      "epoch": 0.15945605360200388,
      "grad_norm": 51.38733673095703,
      "learning_rate": 0.0002521631839193988,
      "loss": 0.8837,
      "step": 51500
    },
    {
      "epoch": 0.15976567700705632,
      "grad_norm": 45.886043548583984,
      "learning_rate": 0.0002520702968978831,
      "loss": 0.6037,
      "step": 51600
    },
    {
      "epoch": 0.16007530041210874,
      "grad_norm": 23.977540969848633,
      "learning_rate": 0.00025197740987636735,
      "loss": 0.6474,
      "step": 51700
    },
    {
      "epoch": 0.16038492381716118,
      "grad_norm": 0.0033828639425337315,
      "learning_rate": 0.0002518845228548516,
      "loss": 0.5991,
      "step": 51800
    },
    {
      "epoch": 0.16069454722221363,
      "grad_norm": 7.752860069274902,
      "learning_rate": 0.00025179163583333587,
      "loss": 0.9522,
      "step": 51900
    },
    {
      "epoch": 0.16100417062726605,
      "grad_norm": 0.001318642869591713,
      "learning_rate": 0.0002516987488118202,
      "loss": 0.5885,
      "step": 52000
    },
    {
      "epoch": 0.1613137940323185,
      "grad_norm": 0.0018776118522509933,
      "learning_rate": 0.00025160586179030444,
      "loss": 0.5135,
      "step": 52100
    },
    {
      "epoch": 0.16162341743737094,
      "grad_norm": 0.01965338923037052,
      "learning_rate": 0.0002515129747687887,
      "loss": 0.6299,
      "step": 52200
    },
    {
      "epoch": 0.16193304084242335,
      "grad_norm": 0.0037642817478626966,
      "learning_rate": 0.00025142008774727296,
      "loss": 0.638,
      "step": 52300
    },
    {
      "epoch": 0.1622426642474758,
      "grad_norm": 25.553030014038086,
      "learning_rate": 0.0002513272007257572,
      "loss": 0.8479,
      "step": 52400
    },
    {
      "epoch": 0.16255228765252824,
      "grad_norm": 69.49325561523438,
      "learning_rate": 0.0002512343137042415,
      "loss": 0.6379,
      "step": 52500
    },
    {
      "epoch": 0.16286191105758066,
      "grad_norm": 37.85995101928711,
      "learning_rate": 0.0002511414266827258,
      "loss": 0.5319,
      "step": 52600
    },
    {
      "epoch": 0.1631715344626331,
      "grad_norm": 4.2583909034729,
      "learning_rate": 0.00025104853966121006,
      "loss": 0.7017,
      "step": 52700
    },
    {
      "epoch": 0.16348115786768552,
      "grad_norm": 37.36898422241211,
      "learning_rate": 0.0002509556526396943,
      "loss": 0.7993,
      "step": 52800
    },
    {
      "epoch": 0.16379078127273797,
      "grad_norm": 0.00045558626879937947,
      "learning_rate": 0.0002508627656181786,
      "loss": 0.733,
      "step": 52900
    },
    {
      "epoch": 0.1641004046777904,
      "grad_norm": 0.1460864543914795,
      "learning_rate": 0.00025076987859666284,
      "loss": 0.5392,
      "step": 53000
    },
    {
      "epoch": 0.16441002808284283,
      "grad_norm": 0.004044629633426666,
      "learning_rate": 0.0002506769915751471,
      "loss": 0.6939,
      "step": 53100
    },
    {
      "epoch": 0.16471965148789527,
      "grad_norm": 0.0007986098062247038,
      "learning_rate": 0.0002505841045536314,
      "loss": 0.6051,
      "step": 53200
    },
    {
      "epoch": 0.16502927489294772,
      "grad_norm": 0.003214087337255478,
      "learning_rate": 0.0002504912175321157,
      "loss": 0.6132,
      "step": 53300
    },
    {
      "epoch": 0.16533889829800014,
      "grad_norm": 0.4010009765625,
      "learning_rate": 0.00025039833051059994,
      "loss": 0.6288,
      "step": 53400
    },
    {
      "epoch": 0.16564852170305258,
      "grad_norm": 0.6797475218772888,
      "learning_rate": 0.0002503054434890842,
      "loss": 0.5059,
      "step": 53500
    },
    {
      "epoch": 0.165958145108105,
      "grad_norm": 0.030557604506611824,
      "learning_rate": 0.00025021255646756846,
      "loss": 0.5632,
      "step": 53600
    },
    {
      "epoch": 0.16626776851315744,
      "grad_norm": 0.0010551491286605597,
      "learning_rate": 0.0002501196694460527,
      "loss": 1.0961,
      "step": 53700
    },
    {
      "epoch": 0.1665773919182099,
      "grad_norm": 0.07171565294265747,
      "learning_rate": 0.00025002678242453703,
      "loss": 0.6778,
      "step": 53800
    },
    {
      "epoch": 0.1668870153232623,
      "grad_norm": 0.003647928824648261,
      "learning_rate": 0.0002499338954030213,
      "loss": 0.5878,
      "step": 53900
    },
    {
      "epoch": 0.16719663872831475,
      "grad_norm": 0.004356276243925095,
      "learning_rate": 0.00024984100838150555,
      "loss": 0.7408,
      "step": 54000
    },
    {
      "epoch": 0.1675062621333672,
      "grad_norm": 10.659003257751465,
      "learning_rate": 0.0002497481213599898,
      "loss": 0.6234,
      "step": 54100
    },
    {
      "epoch": 0.1678158855384196,
      "grad_norm": 0.013592960312962532,
      "learning_rate": 0.0002496552343384741,
      "loss": 0.4985,
      "step": 54200
    },
    {
      "epoch": 0.16812550894347206,
      "grad_norm": 47.104740142822266,
      "learning_rate": 0.0002495623473169584,
      "loss": 0.6643,
      "step": 54300
    },
    {
      "epoch": 0.1684351323485245,
      "grad_norm": 0.004291473887860775,
      "learning_rate": 0.00024946946029544265,
      "loss": 0.6627,
      "step": 54400
    },
    {
      "epoch": 0.16874475575357692,
      "grad_norm": 124.81409454345703,
      "learning_rate": 0.0002493765732739269,
      "loss": 0.7813,
      "step": 54500
    },
    {
      "epoch": 0.16905437915862936,
      "grad_norm": 36.54322052001953,
      "learning_rate": 0.00024928368625241117,
      "loss": 0.5632,
      "step": 54600
    },
    {
      "epoch": 0.16936400256368178,
      "grad_norm": 0.016241446137428284,
      "learning_rate": 0.00024919079923089543,
      "loss": 0.4223,
      "step": 54700
    },
    {
      "epoch": 0.16967362596873423,
      "grad_norm": 0.016813380643725395,
      "learning_rate": 0.0002490979122093797,
      "loss": 0.7145,
      "step": 54800
    },
    {
      "epoch": 0.16998324937378667,
      "grad_norm": 0.0019501019269227982,
      "learning_rate": 0.000249005025187864,
      "loss": 0.6589,
      "step": 54900
    },
    {
      "epoch": 0.1702928727788391,
      "grad_norm": 84.81424713134766,
      "learning_rate": 0.00024891213816634827,
      "loss": 0.5942,
      "step": 55000
    },
    {
      "epoch": 0.17060249618389153,
      "grad_norm": 3.2106151580810547,
      "learning_rate": 0.00024881925114483253,
      "loss": 0.5731,
      "step": 55100
    },
    {
      "epoch": 0.17091211958894398,
      "grad_norm": 0.023186659440398216,
      "learning_rate": 0.0002487263641233168,
      "loss": 0.7418,
      "step": 55200
    },
    {
      "epoch": 0.1712217429939964,
      "grad_norm": 14.168170928955078,
      "learning_rate": 0.00024863347710180105,
      "loss": 0.5384,
      "step": 55300
    },
    {
      "epoch": 0.17153136639904884,
      "grad_norm": 0.071347177028656,
      "learning_rate": 0.0002485405900802853,
      "loss": 0.5066,
      "step": 55400
    },
    {
      "epoch": 0.17184098980410129,
      "grad_norm": 0.022765984758734703,
      "learning_rate": 0.0002484477030587696,
      "loss": 0.676,
      "step": 55500
    },
    {
      "epoch": 0.1721506132091537,
      "grad_norm": 0.04567655920982361,
      "learning_rate": 0.0002483548160372539,
      "loss": 0.7227,
      "step": 55600
    },
    {
      "epoch": 0.17246023661420615,
      "grad_norm": 0.0005058790557086468,
      "learning_rate": 0.00024826192901573814,
      "loss": 0.7302,
      "step": 55700
    },
    {
      "epoch": 0.17276986001925856,
      "grad_norm": 7.624361038208008,
      "learning_rate": 0.0002481690419942224,
      "loss": 0.524,
      "step": 55800
    },
    {
      "epoch": 0.173079483424311,
      "grad_norm": 0.49944522976875305,
      "learning_rate": 0.00024807615497270667,
      "loss": 0.5041,
      "step": 55900
    },
    {
      "epoch": 0.17338910682936345,
      "grad_norm": 64.94967651367188,
      "learning_rate": 0.0002479832679511909,
      "loss": 0.6921,
      "step": 56000
    },
    {
      "epoch": 0.17369873023441587,
      "grad_norm": 0.005162404850125313,
      "learning_rate": 0.00024789038092967524,
      "loss": 0.7527,
      "step": 56100
    },
    {
      "epoch": 0.17400835363946832,
      "grad_norm": 15.09542465209961,
      "learning_rate": 0.0002477974939081595,
      "loss": 0.5891,
      "step": 56200
    },
    {
      "epoch": 0.17431797704452076,
      "grad_norm": 0.002865892369300127,
      "learning_rate": 0.00024770460688664376,
      "loss": 0.9005,
      "step": 56300
    },
    {
      "epoch": 0.17462760044957318,
      "grad_norm": 49.58146286010742,
      "learning_rate": 0.000247611719865128,
      "loss": 0.8808,
      "step": 56400
    },
    {
      "epoch": 0.17493722385462562,
      "grad_norm": 0.6969456672668457,
      "learning_rate": 0.0002475188328436123,
      "loss": 0.4921,
      "step": 56500
    },
    {
      "epoch": 0.17524684725967804,
      "grad_norm": 0.008382122963666916,
      "learning_rate": 0.00024742594582209654,
      "loss": 0.6484,
      "step": 56600
    },
    {
      "epoch": 0.17555647066473049,
      "grad_norm": 11.342987060546875,
      "learning_rate": 0.00024733305880058086,
      "loss": 0.5015,
      "step": 56700
    },
    {
      "epoch": 0.17586609406978293,
      "grad_norm": 0.5300306081771851,
      "learning_rate": 0.0002472401717790651,
      "loss": 0.7168,
      "step": 56800
    },
    {
      "epoch": 0.17617571747483535,
      "grad_norm": 0.0015171216800808907,
      "learning_rate": 0.0002471472847575494,
      "loss": 0.7899,
      "step": 56900
    },
    {
      "epoch": 0.1764853408798878,
      "grad_norm": 28.370786666870117,
      "learning_rate": 0.00024705439773603364,
      "loss": 0.5601,
      "step": 57000
    },
    {
      "epoch": 0.17679496428494024,
      "grad_norm": 0.025517011061310768,
      "learning_rate": 0.0002469615107145179,
      "loss": 0.6654,
      "step": 57100
    },
    {
      "epoch": 0.17710458768999265,
      "grad_norm": 17.274934768676758,
      "learning_rate": 0.00024686862369300216,
      "loss": 0.4536,
      "step": 57200
    },
    {
      "epoch": 0.1774142110950451,
      "grad_norm": 1.9702484607696533,
      "learning_rate": 0.0002467757366714865,
      "loss": 0.6976,
      "step": 57300
    },
    {
      "epoch": 0.17772383450009754,
      "grad_norm": 0.044984687119722366,
      "learning_rate": 0.00024668284964997074,
      "loss": 0.7743,
      "step": 57400
    },
    {
      "epoch": 0.17803345790514996,
      "grad_norm": 35.67873764038086,
      "learning_rate": 0.000246589962628455,
      "loss": 0.4751,
      "step": 57500
    },
    {
      "epoch": 0.1783430813102024,
      "grad_norm": 0.008997897617518902,
      "learning_rate": 0.00024649707560693926,
      "loss": 0.8689,
      "step": 57600
    },
    {
      "epoch": 0.17865270471525482,
      "grad_norm": 6.609826564788818,
      "learning_rate": 0.0002464041885854235,
      "loss": 0.6011,
      "step": 57700
    },
    {
      "epoch": 0.17896232812030727,
      "grad_norm": 95.59418487548828,
      "learning_rate": 0.0002463113015639078,
      "loss": 0.3881,
      "step": 57800
    },
    {
      "epoch": 0.1792719515253597,
      "grad_norm": 0.011320855468511581,
      "learning_rate": 0.0002462184145423921,
      "loss": 0.581,
      "step": 57900
    },
    {
      "epoch": 0.17958157493041213,
      "grad_norm": 0.008959406986832619,
      "learning_rate": 0.00024612552752087635,
      "loss": 0.4258,
      "step": 58000
    },
    {
      "epoch": 0.17989119833546457,
      "grad_norm": 0.005083763971924782,
      "learning_rate": 0.0002460326404993606,
      "loss": 0.632,
      "step": 58100
    },
    {
      "epoch": 0.18020082174051702,
      "grad_norm": 0.006644201464951038,
      "learning_rate": 0.0002459397534778449,
      "loss": 0.5289,
      "step": 58200
    },
    {
      "epoch": 0.18051044514556944,
      "grad_norm": 110.83892822265625,
      "learning_rate": 0.00024584686645632913,
      "loss": 0.7895,
      "step": 58300
    },
    {
      "epoch": 0.18082006855062188,
      "grad_norm": 67.44338989257812,
      "learning_rate": 0.00024575397943481345,
      "loss": 0.5947,
      "step": 58400
    },
    {
      "epoch": 0.18112969195567433,
      "grad_norm": 0.019378244876861572,
      "learning_rate": 0.0002456610924132977,
      "loss": 0.5487,
      "step": 58500
    },
    {
      "epoch": 0.18143931536072674,
      "grad_norm": 0.009091688320040703,
      "learning_rate": 0.00024556820539178197,
      "loss": 0.6441,
      "step": 58600
    },
    {
      "epoch": 0.1817489387657792,
      "grad_norm": 0.0007430106634274125,
      "learning_rate": 0.00024547531837026623,
      "loss": 0.5285,
      "step": 58700
    },
    {
      "epoch": 0.1820585621708316,
      "grad_norm": 0.022188231348991394,
      "learning_rate": 0.0002453824313487505,
      "loss": 0.5029,
      "step": 58800
    },
    {
      "epoch": 0.18236818557588405,
      "grad_norm": 0.01833437755703926,
      "learning_rate": 0.00024528954432723475,
      "loss": 0.5052,
      "step": 58900
    },
    {
      "epoch": 0.1826778089809365,
      "grad_norm": 0.31032076478004456,
      "learning_rate": 0.00024519665730571907,
      "loss": 0.7299,
      "step": 59000
    },
    {
      "epoch": 0.1829874323859889,
      "grad_norm": 152.21005249023438,
      "learning_rate": 0.0002451037702842033,
      "loss": 0.5868,
      "step": 59100
    },
    {
      "epoch": 0.18329705579104136,
      "grad_norm": 11.36033821105957,
      "learning_rate": 0.0002450108832626876,
      "loss": 0.5845,
      "step": 59200
    },
    {
      "epoch": 0.1836066791960938,
      "grad_norm": 89.00127410888672,
      "learning_rate": 0.00024491799624117185,
      "loss": 0.4713,
      "step": 59300
    },
    {
      "epoch": 0.18391630260114622,
      "grad_norm": 6.283215045928955,
      "learning_rate": 0.0002448251092196561,
      "loss": 0.4493,
      "step": 59400
    },
    {
      "epoch": 0.18422592600619866,
      "grad_norm": 0.02962147444486618,
      "learning_rate": 0.00024473222219814037,
      "loss": 0.7066,
      "step": 59500
    },
    {
      "epoch": 0.18453554941125108,
      "grad_norm": 0.19137179851531982,
      "learning_rate": 0.0002446393351766247,
      "loss": 0.6994,
      "step": 59600
    },
    {
      "epoch": 0.18484517281630353,
      "grad_norm": 0.028536971658468246,
      "learning_rate": 0.00024454644815510894,
      "loss": 0.5599,
      "step": 59700
    },
    {
      "epoch": 0.18515479622135597,
      "grad_norm": 2.4551327228546143,
      "learning_rate": 0.0002444535611335932,
      "loss": 0.4128,
      "step": 59800
    },
    {
      "epoch": 0.1854644196264084,
      "grad_norm": 0.008378342725336552,
      "learning_rate": 0.00024436067411207746,
      "loss": 0.6825,
      "step": 59900
    },
    {
      "epoch": 0.18577404303146083,
      "grad_norm": 0.0033841433469206095,
      "learning_rate": 0.0002442677870905617,
      "loss": 0.5944,
      "step": 60000
    },
    {
      "epoch": 0.18608366643651328,
      "grad_norm": 0.2674538791179657,
      "learning_rate": 0.000244174900069046,
      "loss": 0.8558,
      "step": 60100
    },
    {
      "epoch": 0.1863932898415657,
      "grad_norm": 5.390932083129883,
      "learning_rate": 0.00024408201304753027,
      "loss": 0.3914,
      "step": 60200
    },
    {
      "epoch": 0.18670291324661814,
      "grad_norm": 20.126747131347656,
      "learning_rate": 0.00024398912602601453,
      "loss": 0.413,
      "step": 60300
    },
    {
      "epoch": 0.18701253665167059,
      "grad_norm": 0.00022395016276277602,
      "learning_rate": 0.0002438962390044988,
      "loss": 0.7309,
      "step": 60400
    },
    {
      "epoch": 0.187322160056723,
      "grad_norm": 0.008557099848985672,
      "learning_rate": 0.00024380335198298308,
      "loss": 0.7022,
      "step": 60500
    },
    {
      "epoch": 0.18763178346177545,
      "grad_norm": 13.817113876342773,
      "learning_rate": 0.00024371046496146734,
      "loss": 0.6057,
      "step": 60600
    },
    {
      "epoch": 0.18794140686682786,
      "grad_norm": 0.009522208012640476,
      "learning_rate": 0.0002436175779399516,
      "loss": 0.6072,
      "step": 60700
    },
    {
      "epoch": 0.1882510302718803,
      "grad_norm": 0.03598841652274132,
      "learning_rate": 0.0002435246909184359,
      "loss": 0.6485,
      "step": 60800
    },
    {
      "epoch": 0.18856065367693275,
      "grad_norm": 0.7695821523666382,
      "learning_rate": 0.00024343180389692015,
      "loss": 0.7205,
      "step": 60900
    },
    {
      "epoch": 0.18887027708198517,
      "grad_norm": 72.29779815673828,
      "learning_rate": 0.0002433389168754044,
      "loss": 0.615,
      "step": 61000
    },
    {
      "epoch": 0.18917990048703762,
      "grad_norm": 0.020505141466856003,
      "learning_rate": 0.0002432460298538887,
      "loss": 0.5873,
      "step": 61100
    },
    {
      "epoch": 0.18948952389209006,
      "grad_norm": 0.012995046563446522,
      "learning_rate": 0.00024315314283237296,
      "loss": 0.5324,
      "step": 61200
    },
    {
      "epoch": 0.18979914729714248,
      "grad_norm": 0.0023120895493775606,
      "learning_rate": 0.00024306025581085722,
      "loss": 0.434,
      "step": 61300
    },
    {
      "epoch": 0.19010877070219492,
      "grad_norm": 0.019230445846915245,
      "learning_rate": 0.0002429673687893415,
      "loss": 0.6137,
      "step": 61400
    },
    {
      "epoch": 0.19041839410724737,
      "grad_norm": 41.15668487548828,
      "learning_rate": 0.00024287448176782577,
      "loss": 0.7976,
      "step": 61500
    },
    {
      "epoch": 0.19072801751229979,
      "grad_norm": 0.006715971976518631,
      "learning_rate": 0.00024278159474631003,
      "loss": 0.5295,
      "step": 61600
    },
    {
      "epoch": 0.19103764091735223,
      "grad_norm": 0.024128200486302376,
      "learning_rate": 0.00024268870772479431,
      "loss": 0.8023,
      "step": 61700
    },
    {
      "epoch": 0.19134726432240465,
      "grad_norm": 0.7049023509025574,
      "learning_rate": 0.00024259582070327857,
      "loss": 0.4882,
      "step": 61800
    },
    {
      "epoch": 0.1916568877274571,
      "grad_norm": 0.004849864169955254,
      "learning_rate": 0.00024250293368176284,
      "loss": 0.6052,
      "step": 61900
    },
    {
      "epoch": 0.19196651113250954,
      "grad_norm": 0.004029564559459686,
      "learning_rate": 0.00024241004666024712,
      "loss": 0.523,
      "step": 62000
    },
    {
      "epoch": 0.19227613453756195,
      "grad_norm": 48.70440673828125,
      "learning_rate": 0.00024231715963873138,
      "loss": 0.8318,
      "step": 62100
    },
    {
      "epoch": 0.1925857579426144,
      "grad_norm": 0.0008427365683019161,
      "learning_rate": 0.00024222427261721567,
      "loss": 0.7362,
      "step": 62200
    },
    {
      "epoch": 0.19289538134766684,
      "grad_norm": 0.015391722321510315,
      "learning_rate": 0.00024213138559569993,
      "loss": 0.6275,
      "step": 62300
    },
    {
      "epoch": 0.19320500475271926,
      "grad_norm": 14.544275283813477,
      "learning_rate": 0.0002420384985741842,
      "loss": 0.5469,
      "step": 62400
    },
    {
      "epoch": 0.1935146281577717,
      "grad_norm": 0.005012601148337126,
      "learning_rate": 0.00024194561155266848,
      "loss": 0.6479,
      "step": 62500
    },
    {
      "epoch": 0.19382425156282412,
      "grad_norm": 0.0013066211249679327,
      "learning_rate": 0.00024185272453115274,
      "loss": 0.5441,
      "step": 62600
    },
    {
      "epoch": 0.19413387496787657,
      "grad_norm": 0.002232383470982313,
      "learning_rate": 0.000241759837509637,
      "loss": 0.4384,
      "step": 62700
    },
    {
      "epoch": 0.194443498372929,
      "grad_norm": 18.55306625366211,
      "learning_rate": 0.0002416669504881213,
      "loss": 0.614,
      "step": 62800
    },
    {
      "epoch": 0.19475312177798143,
      "grad_norm": 66.79862213134766,
      "learning_rate": 0.00024157406346660555,
      "loss": 0.6848,
      "step": 62900
    },
    {
      "epoch": 0.19506274518303388,
      "grad_norm": 13.831049919128418,
      "learning_rate": 0.0002414811764450898,
      "loss": 0.716,
      "step": 63000
    },
    {
      "epoch": 0.19537236858808632,
      "grad_norm": 35.78461456298828,
      "learning_rate": 0.0002413882894235741,
      "loss": 0.6488,
      "step": 63100
    },
    {
      "epoch": 0.19568199199313874,
      "grad_norm": 0.0059334831312298775,
      "learning_rate": 0.00024129540240205836,
      "loss": 0.8736,
      "step": 63200
    },
    {
      "epoch": 0.19599161539819118,
      "grad_norm": 0.04799351841211319,
      "learning_rate": 0.00024120251538054262,
      "loss": 0.668,
      "step": 63300
    },
    {
      "epoch": 0.19630123880324363,
      "grad_norm": 0.042223844677209854,
      "learning_rate": 0.0002411096283590269,
      "loss": 0.4471,
      "step": 63400
    },
    {
      "epoch": 0.19661086220829604,
      "grad_norm": 17.679040908813477,
      "learning_rate": 0.00024101674133751117,
      "loss": 0.6002,
      "step": 63500
    },
    {
      "epoch": 0.1969204856133485,
      "grad_norm": 0.38636425137519836,
      "learning_rate": 0.00024092385431599543,
      "loss": 0.3438,
      "step": 63600
    },
    {
      "epoch": 0.1972301090184009,
      "grad_norm": 0.008273419924080372,
      "learning_rate": 0.0002408309672944797,
      "loss": 0.5622,
      "step": 63700
    },
    {
      "epoch": 0.19753973242345335,
      "grad_norm": 0.0016738815465942025,
      "learning_rate": 0.00024073808027296397,
      "loss": 0.8251,
      "step": 63800
    },
    {
      "epoch": 0.1978493558285058,
      "grad_norm": 4.382265567779541,
      "learning_rate": 0.00024064519325144823,
      "loss": 0.6054,
      "step": 63900
    },
    {
      "epoch": 0.1981589792335582,
      "grad_norm": 0.0016603657277300954,
      "learning_rate": 0.00024055230622993252,
      "loss": 0.7439,
      "step": 64000
    },
    {
      "epoch": 0.19846860263861066,
      "grad_norm": 28.64216423034668,
      "learning_rate": 0.00024045941920841678,
      "loss": 0.7231,
      "step": 64100
    },
    {
      "epoch": 0.1987782260436631,
      "grad_norm": 0.003934348002076149,
      "learning_rate": 0.00024036653218690104,
      "loss": 0.5978,
      "step": 64200
    },
    {
      "epoch": 0.19908784944871552,
      "grad_norm": 0.16880466043949127,
      "learning_rate": 0.00024027364516538533,
      "loss": 0.6305,
      "step": 64300
    },
    {
      "epoch": 0.19939747285376797,
      "grad_norm": 0.00570866372436285,
      "learning_rate": 0.0002401807581438696,
      "loss": 0.8294,
      "step": 64400
    },
    {
      "epoch": 0.1997070962588204,
      "grad_norm": 0.03262656182050705,
      "learning_rate": 0.00024008787112235385,
      "loss": 0.3691,
      "step": 64500
    },
    {
      "epoch": 0.20001671966387283,
      "grad_norm": 0.2598729431629181,
      "learning_rate": 0.00023999498410083814,
      "loss": 0.6468,
      "step": 64600
    },
    {
      "epoch": 0.20032634306892527,
      "grad_norm": 17.391756057739258,
      "learning_rate": 0.0002399020970793224,
      "loss": 0.5848,
      "step": 64700
    },
    {
      "epoch": 0.2006359664739777,
      "grad_norm": 0.030765864998102188,
      "learning_rate": 0.00023980921005780666,
      "loss": 0.5167,
      "step": 64800
    },
    {
      "epoch": 0.20094558987903013,
      "grad_norm": 92.91615295410156,
      "learning_rate": 0.00023971632303629095,
      "loss": 0.6489,
      "step": 64900
    },
    {
      "epoch": 0.20125521328408258,
      "grad_norm": 56.324798583984375,
      "learning_rate": 0.0002396234360147752,
      "loss": 0.6561,
      "step": 65000
    },
    {
      "epoch": 0.201564836689135,
      "grad_norm": 0.36618462204933167,
      "learning_rate": 0.00023953054899325947,
      "loss": 0.4202,
      "step": 65100
    },
    {
      "epoch": 0.20187446009418744,
      "grad_norm": 0.007904552854597569,
      "learning_rate": 0.00023943766197174376,
      "loss": 0.366,
      "step": 65200
    },
    {
      "epoch": 0.20218408349923989,
      "grad_norm": 0.024932924658060074,
      "learning_rate": 0.00023934477495022802,
      "loss": 0.4268,
      "step": 65300
    },
    {
      "epoch": 0.2024937069042923,
      "grad_norm": 0.007988912053406239,
      "learning_rate": 0.00023925188792871228,
      "loss": 0.764,
      "step": 65400
    },
    {
      "epoch": 0.20280333030934475,
      "grad_norm": 0.0006030399235896766,
      "learning_rate": 0.00023915900090719656,
      "loss": 0.4091,
      "step": 65500
    },
    {
      "epoch": 0.20311295371439717,
      "grad_norm": 0.000509324308950454,
      "learning_rate": 0.00023906611388568082,
      "loss": 0.6694,
      "step": 65600
    },
    {
      "epoch": 0.2034225771194496,
      "grad_norm": 0.013994166627526283,
      "learning_rate": 0.00023897322686416509,
      "loss": 0.781,
      "step": 65700
    },
    {
      "epoch": 0.20373220052450206,
      "grad_norm": 47.43149185180664,
      "learning_rate": 0.00023888033984264937,
      "loss": 0.5587,
      "step": 65800
    },
    {
      "epoch": 0.20404182392955447,
      "grad_norm": 0.003491509472951293,
      "learning_rate": 0.00023878745282113363,
      "loss": 0.5606,
      "step": 65900
    },
    {
      "epoch": 0.20435144733460692,
      "grad_norm": 14.968995094299316,
      "learning_rate": 0.0002386945657996179,
      "loss": 0.6583,
      "step": 66000
    },
    {
      "epoch": 0.20466107073965936,
      "grad_norm": 0.005487421061843634,
      "learning_rate": 0.00023860167877810218,
      "loss": 0.4312,
      "step": 66100
    },
    {
      "epoch": 0.20497069414471178,
      "grad_norm": 1.2212929725646973,
      "learning_rate": 0.00023850879175658644,
      "loss": 0.5402,
      "step": 66200
    },
    {
      "epoch": 0.20528031754976422,
      "grad_norm": 47.219329833984375,
      "learning_rate": 0.00023841590473507073,
      "loss": 0.5548,
      "step": 66300
    },
    {
      "epoch": 0.20558994095481667,
      "grad_norm": 25.63685417175293,
      "learning_rate": 0.000238323017713555,
      "loss": 0.579,
      "step": 66400
    },
    {
      "epoch": 0.20589956435986909,
      "grad_norm": 0.0007797313155606389,
      "learning_rate": 0.00023823013069203925,
      "loss": 0.2879,
      "step": 66500
    },
    {
      "epoch": 0.20620918776492153,
      "grad_norm": 0.013741674832999706,
      "learning_rate": 0.00023813724367052354,
      "loss": 0.6678,
      "step": 66600
    },
    {
      "epoch": 0.20651881116997395,
      "grad_norm": 0.2278156876564026,
      "learning_rate": 0.0002380443566490078,
      "loss": 0.7049,
      "step": 66700
    },
    {
      "epoch": 0.2068284345750264,
      "grad_norm": 17.759929656982422,
      "learning_rate": 0.00023795146962749206,
      "loss": 0.805,
      "step": 66800
    },
    {
      "epoch": 0.20713805798007884,
      "grad_norm": 0.00931557547301054,
      "learning_rate": 0.00023785858260597635,
      "loss": 0.5152,
      "step": 66900
    },
    {
      "epoch": 0.20744768138513126,
      "grad_norm": 0.010824793949723244,
      "learning_rate": 0.0002377656955844606,
      "loss": 0.679,
      "step": 67000
    },
    {
      "epoch": 0.2077573047901837,
      "grad_norm": 0.05325774475932121,
      "learning_rate": 0.00023767280856294487,
      "loss": 0.5188,
      "step": 67100
    },
    {
      "epoch": 0.20806692819523614,
      "grad_norm": 0.00873524695634842,
      "learning_rate": 0.00023757992154142915,
      "loss": 0.5437,
      "step": 67200
    },
    {
      "epoch": 0.20837655160028856,
      "grad_norm": 0.009515147656202316,
      "learning_rate": 0.00023748703451991342,
      "loss": 0.3713,
      "step": 67300
    },
    {
      "epoch": 0.208686175005341,
      "grad_norm": 13.6708345413208,
      "learning_rate": 0.00023739414749839768,
      "loss": 0.8336,
      "step": 67400
    },
    {
      "epoch": 0.20899579841039345,
      "grad_norm": 0.5576529502868652,
      "learning_rate": 0.00023730126047688196,
      "loss": 0.615,
      "step": 67500
    },
    {
      "epoch": 0.20930542181544587,
      "grad_norm": 0.04345761612057686,
      "learning_rate": 0.00023720837345536622,
      "loss": 0.5055,
      "step": 67600
    },
    {
      "epoch": 0.20961504522049831,
      "grad_norm": 10.37395191192627,
      "learning_rate": 0.00023711548643385048,
      "loss": 0.529,
      "step": 67700
    },
    {
      "epoch": 0.20992466862555073,
      "grad_norm": 73.29965209960938,
      "learning_rate": 0.00023702259941233477,
      "loss": 0.8013,
      "step": 67800
    },
    {
      "epoch": 0.21023429203060318,
      "grad_norm": 0.002749478677287698,
      "learning_rate": 0.00023692971239081903,
      "loss": 0.5513,
      "step": 67900
    },
    {
      "epoch": 0.21054391543565562,
      "grad_norm": 99.36502838134766,
      "learning_rate": 0.0002368368253693033,
      "loss": 0.5324,
      "step": 68000
    },
    {
      "epoch": 0.21085353884070804,
      "grad_norm": 1.1605384349822998,
      "learning_rate": 0.00023674393834778758,
      "loss": 0.4215,
      "step": 68100
    },
    {
      "epoch": 0.21116316224576048,
      "grad_norm": 0.015902545303106308,
      "learning_rate": 0.00023665105132627184,
      "loss": 0.6906,
      "step": 68200
    },
    {
      "epoch": 0.21147278565081293,
      "grad_norm": 0.04983861371874809,
      "learning_rate": 0.0002365581643047561,
      "loss": 0.6059,
      "step": 68300
    },
    {
      "epoch": 0.21178240905586534,
      "grad_norm": 0.0026386771351099014,
      "learning_rate": 0.0002364652772832404,
      "loss": 0.4481,
      "step": 68400
    },
    {
      "epoch": 0.2120920324609178,
      "grad_norm": 0.0008024924900382757,
      "learning_rate": 0.00023637239026172465,
      "loss": 0.6609,
      "step": 68500
    },
    {
      "epoch": 0.2124016558659702,
      "grad_norm": 22.338645935058594,
      "learning_rate": 0.0002362795032402089,
      "loss": 0.2863,
      "step": 68600
    },
    {
      "epoch": 0.21271127927102265,
      "grad_norm": 8.881366729736328,
      "learning_rate": 0.0002361866162186932,
      "loss": 0.413,
      "step": 68700
    },
    {
      "epoch": 0.2130209026760751,
      "grad_norm": 0.009786514565348625,
      "learning_rate": 0.00023609372919717746,
      "loss": 0.4029,
      "step": 68800
    },
    {
      "epoch": 0.21333052608112751,
      "grad_norm": 0.00037840561708435416,
      "learning_rate": 0.00023600084217566172,
      "loss": 0.5243,
      "step": 68900
    },
    {
      "epoch": 0.21364014948617996,
      "grad_norm": 58.84942626953125,
      "learning_rate": 0.000235907955154146,
      "loss": 0.6151,
      "step": 69000
    },
    {
      "epoch": 0.2139497728912324,
      "grad_norm": 88.7107925415039,
      "learning_rate": 0.00023581506813263027,
      "loss": 0.3523,
      "step": 69100
    },
    {
      "epoch": 0.21425939629628482,
      "grad_norm": 0.016407951712608337,
      "learning_rate": 0.00023572218111111453,
      "loss": 0.5341,
      "step": 69200
    },
    {
      "epoch": 0.21456901970133727,
      "grad_norm": 74.12229919433594,
      "learning_rate": 0.00023562929408959881,
      "loss": 0.5038,
      "step": 69300
    },
    {
      "epoch": 0.2148786431063897,
      "grad_norm": 0.6944173574447632,
      "learning_rate": 0.00023553640706808307,
      "loss": 0.5219,
      "step": 69400
    },
    {
      "epoch": 0.21518826651144213,
      "grad_norm": 0.0020122104324400425,
      "learning_rate": 0.00023544352004656733,
      "loss": 0.7014,
      "step": 69500
    },
    {
      "epoch": 0.21549788991649457,
      "grad_norm": 4.357915878295898,
      "learning_rate": 0.00023535063302505162,
      "loss": 0.4763,
      "step": 69600
    },
    {
      "epoch": 0.215807513321547,
      "grad_norm": 18.09648323059082,
      "learning_rate": 0.00023525774600353588,
      "loss": 0.5575,
      "step": 69700
    },
    {
      "epoch": 0.21611713672659943,
      "grad_norm": 0.015993336215615273,
      "learning_rate": 0.00023516485898202014,
      "loss": 0.5766,
      "step": 69800
    },
    {
      "epoch": 0.21642676013165188,
      "grad_norm": 0.004510561469942331,
      "learning_rate": 0.00023507197196050443,
      "loss": 0.4192,
      "step": 69900
    },
    {
      "epoch": 0.2167363835367043,
      "grad_norm": 0.004767683334648609,
      "learning_rate": 0.0002349790849389887,
      "loss": 0.6098,
      "step": 70000
    },
    {
      "epoch": 0.21704600694175674,
      "grad_norm": 0.6796236038208008,
      "learning_rate": 0.00023488619791747298,
      "loss": 0.7668,
      "step": 70100
    },
    {
      "epoch": 0.2173556303468092,
      "grad_norm": 0.0023381307255476713,
      "learning_rate": 0.00023479331089595724,
      "loss": 0.4556,
      "step": 70200
    },
    {
      "epoch": 0.2176652537518616,
      "grad_norm": 0.004320663865655661,
      "learning_rate": 0.0002347004238744415,
      "loss": 0.5907,
      "step": 70300
    },
    {
      "epoch": 0.21797487715691405,
      "grad_norm": 2.7057013511657715,
      "learning_rate": 0.0002346075368529258,
      "loss": 0.6372,
      "step": 70400
    },
    {
      "epoch": 0.2182845005619665,
      "grad_norm": 0.02467818185687065,
      "learning_rate": 0.00023451464983141005,
      "loss": 0.6546,
      "step": 70500
    },
    {
      "epoch": 0.2185941239670189,
      "grad_norm": 0.00037608464481309056,
      "learning_rate": 0.0002344217628098943,
      "loss": 0.4783,
      "step": 70600
    },
    {
      "epoch": 0.21890374737207136,
      "grad_norm": 1.0477871894836426,
      "learning_rate": 0.0002343288757883786,
      "loss": 0.7812,
      "step": 70700
    },
    {
      "epoch": 0.21921337077712377,
      "grad_norm": 0.048081763088703156,
      "learning_rate": 0.00023423598876686286,
      "loss": 0.5109,
      "step": 70800
    },
    {
      "epoch": 0.21952299418217622,
      "grad_norm": 10.13528823852539,
      "learning_rate": 0.00023414310174534712,
      "loss": 0.573,
      "step": 70900
    },
    {
      "epoch": 0.21983261758722866,
      "grad_norm": 0.0015867177862673998,
      "learning_rate": 0.0002340502147238314,
      "loss": 0.4904,
      "step": 71000
    },
    {
      "epoch": 0.22014224099228108,
      "grad_norm": 3.1184792518615723,
      "learning_rate": 0.00023395732770231566,
      "loss": 0.5461,
      "step": 71100
    },
    {
      "epoch": 0.22045186439733352,
      "grad_norm": 21.702251434326172,
      "learning_rate": 0.00023386444068079993,
      "loss": 0.7184,
      "step": 71200
    },
    {
      "epoch": 0.22076148780238597,
      "grad_norm": 5.417230129241943,
      "learning_rate": 0.0002337715536592842,
      "loss": 0.5353,
      "step": 71300
    },
    {
      "epoch": 0.2210711112074384,
      "grad_norm": 30.79702377319336,
      "learning_rate": 0.00023367866663776847,
      "loss": 0.5361,
      "step": 71400
    },
    {
      "epoch": 0.22138073461249083,
      "grad_norm": 0.015963545069098473,
      "learning_rate": 0.00023358577961625273,
      "loss": 0.5028,
      "step": 71500
    },
    {
      "epoch": 0.22169035801754325,
      "grad_norm": 41.410037994384766,
      "learning_rate": 0.00023349289259473702,
      "loss": 0.5301,
      "step": 71600
    },
    {
      "epoch": 0.2219999814225957,
      "grad_norm": 0.006404429208487272,
      "learning_rate": 0.00023340000557322128,
      "loss": 0.6011,
      "step": 71700
    },
    {
      "epoch": 0.22230960482764814,
      "grad_norm": 0.010075930505990982,
      "learning_rate": 0.00023330711855170554,
      "loss": 0.4373,
      "step": 71800
    },
    {
      "epoch": 0.22261922823270056,
      "grad_norm": 0.005944117438048124,
      "learning_rate": 0.00023321423153018983,
      "loss": 0.5416,
      "step": 71900
    },
    {
      "epoch": 0.222928851637753,
      "grad_norm": 0.0035329933743923903,
      "learning_rate": 0.0002331213445086741,
      "loss": 0.4895,
      "step": 72000
    },
    {
      "epoch": 0.22323847504280545,
      "grad_norm": 31.024173736572266,
      "learning_rate": 0.00023302845748715835,
      "loss": 0.3664,
      "step": 72100
    },
    {
      "epoch": 0.22354809844785786,
      "grad_norm": 0.0008006589487195015,
      "learning_rate": 0.00023293557046564264,
      "loss": 0.4969,
      "step": 72200
    },
    {
      "epoch": 0.2238577218529103,
      "grad_norm": 1.1283349990844727,
      "learning_rate": 0.0002328426834441269,
      "loss": 0.5845,
      "step": 72300
    },
    {
      "epoch": 0.22416734525796275,
      "grad_norm": 0.0016302322037518024,
      "learning_rate": 0.00023274979642261113,
      "loss": 0.4026,
      "step": 72400
    },
    {
      "epoch": 0.22447696866301517,
      "grad_norm": 0.000946783518884331,
      "learning_rate": 0.00023265690940109545,
      "loss": 0.5678,
      "step": 72500
    },
    {
      "epoch": 0.22478659206806761,
      "grad_norm": 25.482318878173828,
      "learning_rate": 0.0002325640223795797,
      "loss": 0.4091,
      "step": 72600
    },
    {
      "epoch": 0.22509621547312003,
      "grad_norm": 37.73815155029297,
      "learning_rate": 0.00023247113535806394,
      "loss": 0.5066,
      "step": 72700
    },
    {
      "epoch": 0.22540583887817248,
      "grad_norm": 0.7560127973556519,
      "learning_rate": 0.00023237824833654826,
      "loss": 0.7438,
      "step": 72800
    },
    {
      "epoch": 0.22571546228322492,
      "grad_norm": 0.0009369386825710535,
      "learning_rate": 0.00023228536131503252,
      "loss": 0.6562,
      "step": 72900
    },
    {
      "epoch": 0.22602508568827734,
      "grad_norm": 12.17615795135498,
      "learning_rate": 0.00023219247429351675,
      "loss": 0.5376,
      "step": 73000
    },
    {
      "epoch": 0.22633470909332978,
      "grad_norm": 127.75039672851562,
      "learning_rate": 0.00023209958727200106,
      "loss": 0.7022,
      "step": 73100
    },
    {
      "epoch": 0.22664433249838223,
      "grad_norm": 0.02298968844115734,
      "learning_rate": 0.00023200670025048532,
      "loss": 0.4349,
      "step": 73200
    },
    {
      "epoch": 0.22695395590343465,
      "grad_norm": 27.943525314331055,
      "learning_rate": 0.00023191381322896956,
      "loss": 0.5395,
      "step": 73300
    },
    {
      "epoch": 0.2272635793084871,
      "grad_norm": 0.02430974319577217,
      "learning_rate": 0.00023182092620745387,
      "loss": 0.3975,
      "step": 73400
    },
    {
      "epoch": 0.22757320271353954,
      "grad_norm": 0.00046263638068921864,
      "learning_rate": 0.0002317280391859381,
      "loss": 0.5701,
      "step": 73500
    },
    {
      "epoch": 0.22788282611859195,
      "grad_norm": 0.7212129235267639,
      "learning_rate": 0.00023163515216442237,
      "loss": 0.5443,
      "step": 73600
    },
    {
      "epoch": 0.2281924495236444,
      "grad_norm": 88.61368560791016,
      "learning_rate": 0.00023154226514290668,
      "loss": 0.6173,
      "step": 73700
    },
    {
      "epoch": 0.22850207292869681,
      "grad_norm": 0.29261523485183716,
      "learning_rate": 0.00023144937812139091,
      "loss": 0.5049,
      "step": 73800
    },
    {
      "epoch": 0.22881169633374926,
      "grad_norm": 18.75320053100586,
      "learning_rate": 0.00023135649109987517,
      "loss": 0.5823,
      "step": 73900
    },
    {
      "epoch": 0.2291213197388017,
      "grad_norm": 0.0022316521499305964,
      "learning_rate": 0.0002312636040783595,
      "loss": 0.4131,
      "step": 74000
    },
    {
      "epoch": 0.22943094314385412,
      "grad_norm": 43.26858901977539,
      "learning_rate": 0.00023117071705684372,
      "loss": 0.4517,
      "step": 74100
    },
    {
      "epoch": 0.22974056654890657,
      "grad_norm": 0.0057294643484056,
      "learning_rate": 0.00023107783003532804,
      "loss": 0.5569,
      "step": 74200
    },
    {
      "epoch": 0.230050189953959,
      "grad_norm": 0.007036532275378704,
      "learning_rate": 0.0002309849430138123,
      "loss": 0.6196,
      "step": 74300
    },
    {
      "epoch": 0.23035981335901143,
      "grad_norm": 0.01072742696851492,
      "learning_rate": 0.00023089205599229653,
      "loss": 0.5518,
      "step": 74400
    },
    {
      "epoch": 0.23066943676406387,
      "grad_norm": 0.006509189959615469,
      "learning_rate": 0.00023079916897078085,
      "loss": 0.5001,
      "step": 74500
    },
    {
      "epoch": 0.2309790601691163,
      "grad_norm": 0.0017297659069299698,
      "learning_rate": 0.0002307062819492651,
      "loss": 0.6615,
      "step": 74600
    },
    {
      "epoch": 0.23128868357416874,
      "grad_norm": 4.418078422546387,
      "learning_rate": 0.00023061339492774934,
      "loss": 0.6599,
      "step": 74700
    },
    {
      "epoch": 0.23159830697922118,
      "grad_norm": 0.0005808036075904965,
      "learning_rate": 0.00023052050790623365,
      "loss": 0.5098,
      "step": 74800
    },
    {
      "epoch": 0.2319079303842736,
      "grad_norm": 11.849614143371582,
      "learning_rate": 0.0002304276208847179,
      "loss": 0.6226,
      "step": 74900
    },
    {
      "epoch": 0.23221755378932604,
      "grad_norm": 0.00047394202556461096,
      "learning_rate": 0.00023033473386320215,
      "loss": 0.4669,
      "step": 75000
    },
    {
      "epoch": 0.2325271771943785,
      "grad_norm": 0.0029245789628475904,
      "learning_rate": 0.00023024184684168646,
      "loss": 0.541,
      "step": 75100
    },
    {
      "epoch": 0.2328368005994309,
      "grad_norm": 0.002576059428974986,
      "learning_rate": 0.0002301489598201707,
      "loss": 0.5273,
      "step": 75200
    },
    {
      "epoch": 0.23314642400448335,
      "grad_norm": 9.433249473571777,
      "learning_rate": 0.00023005607279865496,
      "loss": 0.4971,
      "step": 75300
    },
    {
      "epoch": 0.2334560474095358,
      "grad_norm": 0.05065401270985603,
      "learning_rate": 0.00022996318577713927,
      "loss": 0.4287,
      "step": 75400
    },
    {
      "epoch": 0.2337656708145882,
      "grad_norm": 0.0016050295671448112,
      "learning_rate": 0.0002298702987556235,
      "loss": 0.3511,
      "step": 75500
    },
    {
      "epoch": 0.23407529421964066,
      "grad_norm": 6.555344581604004,
      "learning_rate": 0.00022977741173410777,
      "loss": 0.413,
      "step": 75600
    },
    {
      "epoch": 0.23438491762469307,
      "grad_norm": 47.371891021728516,
      "learning_rate": 0.00022968452471259208,
      "loss": 0.6972,
      "step": 75700
    },
    {
      "epoch": 0.23469454102974552,
      "grad_norm": 0.0024193832650780678,
      "learning_rate": 0.0002295916376910763,
      "loss": 0.5285,
      "step": 75800
    },
    {
      "epoch": 0.23500416443479796,
      "grad_norm": 0.003126665484160185,
      "learning_rate": 0.00022949875066956057,
      "loss": 0.6715,
      "step": 75900
    },
    {
      "epoch": 0.23531378783985038,
      "grad_norm": 98.24739837646484,
      "learning_rate": 0.00022940586364804486,
      "loss": 0.5735,
      "step": 76000
    },
    {
      "epoch": 0.23562341124490283,
      "grad_norm": 0.1495472490787506,
      "learning_rate": 0.00022931297662652912,
      "loss": 0.3947,
      "step": 76100
    },
    {
      "epoch": 0.23593303464995527,
      "grad_norm": 0.06962265819311142,
      "learning_rate": 0.00022922008960501338,
      "loss": 0.3787,
      "step": 76200
    },
    {
      "epoch": 0.2362426580550077,
      "grad_norm": 0.000815081933978945,
      "learning_rate": 0.00022912720258349767,
      "loss": 0.6762,
      "step": 76300
    },
    {
      "epoch": 0.23655228146006013,
      "grad_norm": 0.0034245385322719812,
      "learning_rate": 0.00022903431556198193,
      "loss": 0.6479,
      "step": 76400
    },
    {
      "epoch": 0.23686190486511258,
      "grad_norm": 0.001063971547409892,
      "learning_rate": 0.0002289414285404662,
      "loss": 0.3968,
      "step": 76500
    },
    {
      "epoch": 0.237171528270165,
      "grad_norm": 0.27865588665008545,
      "learning_rate": 0.00022884854151895048,
      "loss": 0.4841,
      "step": 76600
    },
    {
      "epoch": 0.23748115167521744,
      "grad_norm": 90.89814758300781,
      "learning_rate": 0.00022875565449743474,
      "loss": 0.7286,
      "step": 76700
    },
    {
      "epoch": 0.23779077508026986,
      "grad_norm": 0.011061053723096848,
      "learning_rate": 0.000228662767475919,
      "loss": 0.5344,
      "step": 76800
    },
    {
      "epoch": 0.2381003984853223,
      "grad_norm": 0.001624882104806602,
      "learning_rate": 0.0002285698804544033,
      "loss": 0.5712,
      "step": 76900
    },
    {
      "epoch": 0.23841002189037475,
      "grad_norm": 8.129126217681915e-05,
      "learning_rate": 0.00022847699343288755,
      "loss": 0.492,
      "step": 77000
    },
    {
      "epoch": 0.23871964529542716,
      "grad_norm": 0.0009096069261431694,
      "learning_rate": 0.0002283841064113718,
      "loss": 0.4135,
      "step": 77100
    },
    {
      "epoch": 0.2390292687004796,
      "grad_norm": 0.0020348108373582363,
      "learning_rate": 0.0002282912193898561,
      "loss": 0.6547,
      "step": 77200
    },
    {
      "epoch": 0.23933889210553205,
      "grad_norm": 0.011582189239561558,
      "learning_rate": 0.00022819833236834036,
      "loss": 0.6059,
      "step": 77300
    },
    {
      "epoch": 0.23964851551058447,
      "grad_norm": 0.016842715442180634,
      "learning_rate": 0.00022810544534682462,
      "loss": 0.4911,
      "step": 77400
    },
    {
      "epoch": 0.23995813891563691,
      "grad_norm": 0.010121980682015419,
      "learning_rate": 0.0002280125583253089,
      "loss": 0.7158,
      "step": 77500
    },
    {
      "epoch": 0.24026776232068933,
      "grad_norm": 0.0022707409225404263,
      "learning_rate": 0.00022791967130379316,
      "loss": 0.5295,
      "step": 77600
    },
    {
      "epoch": 0.24057738572574178,
      "grad_norm": 48.10946273803711,
      "learning_rate": 0.00022782678428227742,
      "loss": 0.6319,
      "step": 77700
    },
    {
      "epoch": 0.24088700913079422,
      "grad_norm": 8.967635154724121,
      "learning_rate": 0.0002277338972607617,
      "loss": 0.4492,
      "step": 77800
    },
    {
      "epoch": 0.24119663253584664,
      "grad_norm": 0.018385007977485657,
      "learning_rate": 0.00022764101023924597,
      "loss": 0.4008,
      "step": 77900
    },
    {
      "epoch": 0.24150625594089908,
      "grad_norm": 54.31916427612305,
      "learning_rate": 0.00022754812321773023,
      "loss": 0.2907,
      "step": 78000
    },
    {
      "epoch": 0.24181587934595153,
      "grad_norm": 0.00042295065941289067,
      "learning_rate": 0.00022745523619621452,
      "loss": 0.5554,
      "step": 78100
    },
    {
      "epoch": 0.24212550275100395,
      "grad_norm": 30.52289390563965,
      "learning_rate": 0.00022736234917469878,
      "loss": 0.453,
      "step": 78200
    },
    {
      "epoch": 0.2424351261560564,
      "grad_norm": 3.692272901535034,
      "learning_rate": 0.00022726946215318307,
      "loss": 0.6358,
      "step": 78300
    },
    {
      "epoch": 0.24274474956110884,
      "grad_norm": 0.7037914991378784,
      "learning_rate": 0.00022717657513166733,
      "loss": 0.6575,
      "step": 78400
    },
    {
      "epoch": 0.24305437296616125,
      "grad_norm": 0.878542423248291,
      "learning_rate": 0.0002270836881101516,
      "loss": 0.4913,
      "step": 78500
    },
    {
      "epoch": 0.2433639963712137,
      "grad_norm": 0.0010887505486607552,
      "learning_rate": 0.00022699080108863588,
      "loss": 0.4283,
      "step": 78600
    },
    {
      "epoch": 0.24367361977626611,
      "grad_norm": 100.52965545654297,
      "learning_rate": 0.00022689791406712014,
      "loss": 0.6048,
      "step": 78700
    },
    {
      "epoch": 0.24398324318131856,
      "grad_norm": 145.34361267089844,
      "learning_rate": 0.0002268050270456044,
      "loss": 0.5571,
      "step": 78800
    },
    {
      "epoch": 0.244292866586371,
      "grad_norm": 0.09589429944753647,
      "learning_rate": 0.00022671214002408869,
      "loss": 0.5185,
      "step": 78900
    },
    {
      "epoch": 0.24460248999142342,
      "grad_norm": 0.014896193519234657,
      "learning_rate": 0.00022661925300257295,
      "loss": 0.4713,
      "step": 79000
    },
    {
      "epoch": 0.24491211339647587,
      "grad_norm": 0.14373283088207245,
      "learning_rate": 0.0002265263659810572,
      "loss": 0.7045,
      "step": 79100
    },
    {
      "epoch": 0.2452217368015283,
      "grad_norm": 0.027086623013019562,
      "learning_rate": 0.0002264334789595415,
      "loss": 0.6615,
      "step": 79200
    },
    {
      "epoch": 0.24553136020658073,
      "grad_norm": 31.199779510498047,
      "learning_rate": 0.00022634059193802575,
      "loss": 0.4316,
      "step": 79300
    },
    {
      "epoch": 0.24584098361163317,
      "grad_norm": 0.0020330767147243023,
      "learning_rate": 0.00022624770491651001,
      "loss": 0.4565,
      "step": 79400
    },
    {
      "epoch": 0.24615060701668562,
      "grad_norm": 29.25932502746582,
      "learning_rate": 0.0002261548178949943,
      "loss": 0.5515,
      "step": 79500
    },
    {
      "epoch": 0.24646023042173804,
      "grad_norm": 0.04094425588846207,
      "learning_rate": 0.00022606193087347856,
      "loss": 0.445,
      "step": 79600
    },
    {
      "epoch": 0.24676985382679048,
      "grad_norm": 0.00504795927554369,
      "learning_rate": 0.00022596904385196282,
      "loss": 0.6072,
      "step": 79700
    },
    {
      "epoch": 0.2470794772318429,
      "grad_norm": 0.06051066145300865,
      "learning_rate": 0.0002258761568304471,
      "loss": 0.6572,
      "step": 79800
    },
    {
      "epoch": 0.24738910063689534,
      "grad_norm": 0.002811938989907503,
      "learning_rate": 0.00022578326980893137,
      "loss": 0.6614,
      "step": 79900
    },
    {
      "epoch": 0.2476987240419478,
      "grad_norm": 1.9470207691192627,
      "learning_rate": 0.00022569038278741563,
      "loss": 0.3358,
      "step": 80000
    },
    {
      "epoch": 0.2480083474470002,
      "grad_norm": 131.6936798095703,
      "learning_rate": 0.00022559749576589992,
      "loss": 0.7518,
      "step": 80100
    },
    {
      "epoch": 0.24831797085205265,
      "grad_norm": 0.26578471064567566,
      "learning_rate": 0.00022550460874438418,
      "loss": 0.6706,
      "step": 80200
    },
    {
      "epoch": 0.2486275942571051,
      "grad_norm": 0.22363440692424774,
      "learning_rate": 0.00022541172172286844,
      "loss": 0.6035,
      "step": 80300
    },
    {
      "epoch": 0.2489372176621575,
      "grad_norm": 8.882468223571777,
      "learning_rate": 0.00022531883470135273,
      "loss": 0.5041,
      "step": 80400
    },
    {
      "epoch": 0.24924684106720996,
      "grad_norm": 0.010154950432479382,
      "learning_rate": 0.000225225947679837,
      "loss": 0.825,
      "step": 80500
    },
    {
      "epoch": 0.24955646447226237,
      "grad_norm": 0.00124321726616472,
      "learning_rate": 0.00022513306065832125,
      "loss": 0.7135,
      "step": 80600
    },
    {
      "epoch": 0.24986608787731482,
      "grad_norm": 3.068460464477539,
      "learning_rate": 0.00022504017363680554,
      "loss": 0.549,
      "step": 80700
    },
    {
      "epoch": 0.25017571128236726,
      "grad_norm": 0.00031831799424253404,
      "learning_rate": 0.0002249472866152898,
      "loss": 0.6387,
      "step": 80800
    },
    {
      "epoch": 0.2504853346874197,
      "grad_norm": 0.004293919540941715,
      "learning_rate": 0.00022485439959377406,
      "loss": 0.3574,
      "step": 80900
    },
    {
      "epoch": 0.25079495809247215,
      "grad_norm": 61.28981018066406,
      "learning_rate": 0.00022476151257225834,
      "loss": 0.5916,
      "step": 81000
    },
    {
      "epoch": 0.25110458149752457,
      "grad_norm": 0.0001993296027649194,
      "learning_rate": 0.0002246686255507426,
      "loss": 0.7139,
      "step": 81100
    },
    {
      "epoch": 0.251414204902577,
      "grad_norm": 45.85444641113281,
      "learning_rate": 0.00022457573852922687,
      "loss": 0.5386,
      "step": 81200
    },
    {
      "epoch": 0.2517238283076294,
      "grad_norm": 0.03255491703748703,
      "learning_rate": 0.00022448285150771115,
      "loss": 0.5052,
      "step": 81300
    },
    {
      "epoch": 0.2520334517126819,
      "grad_norm": 0.0007147208089008927,
      "learning_rate": 0.00022438996448619541,
      "loss": 0.5333,
      "step": 81400
    },
    {
      "epoch": 0.2523430751177343,
      "grad_norm": 0.04733600094914436,
      "learning_rate": 0.00022429707746467967,
      "loss": 0.7016,
      "step": 81500
    },
    {
      "epoch": 0.2526526985227867,
      "grad_norm": 23.151201248168945,
      "learning_rate": 0.00022420419044316396,
      "loss": 0.6886,
      "step": 81600
    },
    {
      "epoch": 0.2529623219278392,
      "grad_norm": 16.546024322509766,
      "learning_rate": 0.00022411130342164822,
      "loss": 0.5514,
      "step": 81700
    },
    {
      "epoch": 0.2532719453328916,
      "grad_norm": 7.63834810256958,
      "learning_rate": 0.00022401841640013248,
      "loss": 0.4786,
      "step": 81800
    },
    {
      "epoch": 0.253581568737944,
      "grad_norm": 0.005095295142382383,
      "learning_rate": 0.00022392552937861677,
      "loss": 0.3452,
      "step": 81900
    },
    {
      "epoch": 0.2538911921429965,
      "grad_norm": 0.003387961769476533,
      "learning_rate": 0.00022383264235710103,
      "loss": 0.5334,
      "step": 82000
    },
    {
      "epoch": 0.2542008155480489,
      "grad_norm": 0.17660318315029144,
      "learning_rate": 0.00022373975533558532,
      "loss": 0.5882,
      "step": 82100
    },
    {
      "epoch": 0.2545104389531013,
      "grad_norm": 0.008722749538719654,
      "learning_rate": 0.00022364686831406958,
      "loss": 0.6714,
      "step": 82200
    },
    {
      "epoch": 0.2548200623581538,
      "grad_norm": 10.039700508117676,
      "learning_rate": 0.00022355398129255384,
      "loss": 0.3204,
      "step": 82300
    },
    {
      "epoch": 0.2551296857632062,
      "grad_norm": 0.006766756530851126,
      "learning_rate": 0.00022346109427103813,
      "loss": 0.6215,
      "step": 82400
    },
    {
      "epoch": 0.25543930916825863,
      "grad_norm": 0.015073223039507866,
      "learning_rate": 0.0002233682072495224,
      "loss": 0.4779,
      "step": 82500
    },
    {
      "epoch": 0.2557489325733111,
      "grad_norm": 0.004690262023359537,
      "learning_rate": 0.00022327532022800665,
      "loss": 0.4028,
      "step": 82600
    },
    {
      "epoch": 0.2560585559783635,
      "grad_norm": 0.07309702783823013,
      "learning_rate": 0.00022318243320649094,
      "loss": 0.4239,
      "step": 82700
    },
    {
      "epoch": 0.25636817938341594,
      "grad_norm": 0.0010519652860239148,
      "learning_rate": 0.0002230895461849752,
      "loss": 0.4218,
      "step": 82800
    },
    {
      "epoch": 0.2566778027884684,
      "grad_norm": 0.0038660734426230192,
      "learning_rate": 0.00022299665916345946,
      "loss": 0.4751,
      "step": 82900
    },
    {
      "epoch": 0.25698742619352083,
      "grad_norm": 0.011513227596879005,
      "learning_rate": 0.00022290377214194374,
      "loss": 0.6369,
      "step": 83000
    },
    {
      "epoch": 0.25729704959857325,
      "grad_norm": 18.04090690612793,
      "learning_rate": 0.000222810885120428,
      "loss": 0.5066,
      "step": 83100
    },
    {
      "epoch": 0.25760667300362566,
      "grad_norm": 0.07776722311973572,
      "learning_rate": 0.00022271799809891226,
      "loss": 0.4628,
      "step": 83200
    },
    {
      "epoch": 0.25791629640867814,
      "grad_norm": 0.0018642247887328267,
      "learning_rate": 0.00022262511107739655,
      "loss": 0.5457,
      "step": 83300
    },
    {
      "epoch": 0.25822591981373055,
      "grad_norm": 0.0013941924553364515,
      "learning_rate": 0.0002225322240558808,
      "loss": 0.3636,
      "step": 83400
    },
    {
      "epoch": 0.25853554321878297,
      "grad_norm": 0.009787892922759056,
      "learning_rate": 0.00022243933703436507,
      "loss": 0.7236,
      "step": 83500
    },
    {
      "epoch": 0.25884516662383544,
      "grad_norm": 41.232948303222656,
      "learning_rate": 0.00022234645001284936,
      "loss": 0.7848,
      "step": 83600
    },
    {
      "epoch": 0.25915479002888786,
      "grad_norm": 1.7428101301193237,
      "learning_rate": 0.00022225356299133362,
      "loss": 0.4505,
      "step": 83700
    },
    {
      "epoch": 0.2594644134339403,
      "grad_norm": 0.00433369493111968,
      "learning_rate": 0.00022216067596981788,
      "loss": 0.4832,
      "step": 83800
    },
    {
      "epoch": 0.25977403683899275,
      "grad_norm": 16.411144256591797,
      "learning_rate": 0.00022206778894830217,
      "loss": 0.4607,
      "step": 83900
    },
    {
      "epoch": 0.26008366024404517,
      "grad_norm": 0.1661042720079422,
      "learning_rate": 0.00022197490192678643,
      "loss": 0.4867,
      "step": 84000
    },
    {
      "epoch": 0.2603932836490976,
      "grad_norm": 0.008258250541985035,
      "learning_rate": 0.0002218820149052707,
      "loss": 0.4298,
      "step": 84100
    },
    {
      "epoch": 0.26070290705415006,
      "grad_norm": 10.41195297241211,
      "learning_rate": 0.00022178912788375498,
      "loss": 0.3223,
      "step": 84200
    },
    {
      "epoch": 0.2610125304592025,
      "grad_norm": 0.03097248263657093,
      "learning_rate": 0.00022169624086223924,
      "loss": 0.4221,
      "step": 84300
    },
    {
      "epoch": 0.2613221538642549,
      "grad_norm": 5.7596588134765625,
      "learning_rate": 0.0002216033538407235,
      "loss": 0.4896,
      "step": 84400
    },
    {
      "epoch": 0.26163177726930736,
      "grad_norm": 0.001551713445223868,
      "learning_rate": 0.00022151046681920779,
      "loss": 0.6061,
      "step": 84500
    },
    {
      "epoch": 0.2619414006743598,
      "grad_norm": 0.11242415010929108,
      "learning_rate": 0.00022141757979769205,
      "loss": 0.4105,
      "step": 84600
    },
    {
      "epoch": 0.2622510240794122,
      "grad_norm": 5.624869346618652,
      "learning_rate": 0.0002213246927761763,
      "loss": 0.3368,
      "step": 84700
    },
    {
      "epoch": 0.26256064748446467,
      "grad_norm": 0.4158084988594055,
      "learning_rate": 0.0002212318057546606,
      "loss": 0.4755,
      "step": 84800
    },
    {
      "epoch": 0.2628702708895171,
      "grad_norm": 0.00034557245089672506,
      "learning_rate": 0.00022113891873314486,
      "loss": 0.7892,
      "step": 84900
    },
    {
      "epoch": 0.2631798942945695,
      "grad_norm": 0.1692795306444168,
      "learning_rate": 0.00022104603171162912,
      "loss": 0.3456,
      "step": 85000
    },
    {
      "epoch": 0.2634895176996219,
      "grad_norm": 0.019678741693496704,
      "learning_rate": 0.0002209531446901134,
      "loss": 0.4488,
      "step": 85100
    },
    {
      "epoch": 0.2637991411046744,
      "grad_norm": 0.00033434361102990806,
      "learning_rate": 0.00022086025766859766,
      "loss": 0.4279,
      "step": 85200
    },
    {
      "epoch": 0.2641087645097268,
      "grad_norm": 0.06620776653289795,
      "learning_rate": 0.00022076737064708192,
      "loss": 0.5291,
      "step": 85300
    },
    {
      "epoch": 0.26441838791477923,
      "grad_norm": 52.196067810058594,
      "learning_rate": 0.0002206744836255662,
      "loss": 0.4812,
      "step": 85400
    },
    {
      "epoch": 0.2647280113198317,
      "grad_norm": 0.1397126317024231,
      "learning_rate": 0.00022058159660405047,
      "loss": 0.675,
      "step": 85500
    },
    {
      "epoch": 0.2650376347248841,
      "grad_norm": 0.0028576403856277466,
      "learning_rate": 0.00022048870958253473,
      "loss": 0.5106,
      "step": 85600
    },
    {
      "epoch": 0.26534725812993654,
      "grad_norm": 32.286617279052734,
      "learning_rate": 0.00022039582256101902,
      "loss": 0.5122,
      "step": 85700
    },
    {
      "epoch": 0.265656881534989,
      "grad_norm": 37.403202056884766,
      "learning_rate": 0.00022030293553950328,
      "loss": 0.5547,
      "step": 85800
    },
    {
      "epoch": 0.2659665049400414,
      "grad_norm": 0.0012151201954111457,
      "learning_rate": 0.00022021004851798754,
      "loss": 0.4255,
      "step": 85900
    },
    {
      "epoch": 0.26627612834509384,
      "grad_norm": 0.013780800625681877,
      "learning_rate": 0.00022011716149647183,
      "loss": 0.5516,
      "step": 86000
    },
    {
      "epoch": 0.2665857517501463,
      "grad_norm": 18.940237045288086,
      "learning_rate": 0.0002200242744749561,
      "loss": 0.5618,
      "step": 86100
    },
    {
      "epoch": 0.26689537515519873,
      "grad_norm": 0.02629716321825981,
      "learning_rate": 0.00021993138745344038,
      "loss": 0.5679,
      "step": 86200
    },
    {
      "epoch": 0.26720499856025115,
      "grad_norm": 0.04213415086269379,
      "learning_rate": 0.00021983850043192464,
      "loss": 0.4627,
      "step": 86300
    },
    {
      "epoch": 0.2675146219653036,
      "grad_norm": 0.9544457793235779,
      "learning_rate": 0.0002197456134104089,
      "loss": 0.665,
      "step": 86400
    },
    {
      "epoch": 0.26782424537035604,
      "grad_norm": 1.4252362251281738,
      "learning_rate": 0.00021965272638889318,
      "loss": 0.412,
      "step": 86500
    },
    {
      "epoch": 0.26813386877540846,
      "grad_norm": 0.5343594551086426,
      "learning_rate": 0.00021955983936737745,
      "loss": 0.4955,
      "step": 86600
    },
    {
      "epoch": 0.26844349218046093,
      "grad_norm": 0.004551384598016739,
      "learning_rate": 0.0002194669523458617,
      "loss": 0.3682,
      "step": 86700
    },
    {
      "epoch": 0.26875311558551335,
      "grad_norm": 76.21578216552734,
      "learning_rate": 0.000219374065324346,
      "loss": 0.3022,
      "step": 86800
    },
    {
      "epoch": 0.26906273899056576,
      "grad_norm": 0.014824097976088524,
      "learning_rate": 0.00021928117830283025,
      "loss": 0.4031,
      "step": 86900
    },
    {
      "epoch": 0.26937236239561824,
      "grad_norm": 0.473341703414917,
      "learning_rate": 0.00021918829128131451,
      "loss": 0.4646,
      "step": 87000
    },
    {
      "epoch": 0.26968198580067065,
      "grad_norm": 23.798723220825195,
      "learning_rate": 0.0002190954042597988,
      "loss": 0.6024,
      "step": 87100
    },
    {
      "epoch": 0.26999160920572307,
      "grad_norm": 7.079448699951172,
      "learning_rate": 0.00021900251723828306,
      "loss": 0.3561,
      "step": 87200
    },
    {
      "epoch": 0.2703012326107755,
      "grad_norm": 221.9791717529297,
      "learning_rate": 0.00021890963021676732,
      "loss": 0.333,
      "step": 87300
    },
    {
      "epoch": 0.27061085601582796,
      "grad_norm": 0.10823981463909149,
      "learning_rate": 0.0002188167431952516,
      "loss": 0.7039,
      "step": 87400
    },
    {
      "epoch": 0.2709204794208804,
      "grad_norm": 41.38913345336914,
      "learning_rate": 0.00021872385617373587,
      "loss": 0.521,
      "step": 87500
    },
    {
      "epoch": 0.2712301028259328,
      "grad_norm": 0.42481693625450134,
      "learning_rate": 0.00021863096915222013,
      "loss": 0.5492,
      "step": 87600
    },
    {
      "epoch": 0.27153972623098527,
      "grad_norm": 0.0008775523747317493,
      "learning_rate": 0.00021853808213070442,
      "loss": 0.503,
      "step": 87700
    },
    {
      "epoch": 0.2718493496360377,
      "grad_norm": 0.020746078342199326,
      "learning_rate": 0.00021844519510918868,
      "loss": 0.6606,
      "step": 87800
    },
    {
      "epoch": 0.2721589730410901,
      "grad_norm": 0.004435314796864986,
      "learning_rate": 0.00021835230808767294,
      "loss": 0.4249,
      "step": 87900
    },
    {
      "epoch": 0.2724685964461426,
      "grad_norm": 0.06274088472127914,
      "learning_rate": 0.00021825942106615723,
      "loss": 0.4349,
      "step": 88000
    },
    {
      "epoch": 0.272778219851195,
      "grad_norm": 0.0032616062089800835,
      "learning_rate": 0.0002181665340446415,
      "loss": 0.4205,
      "step": 88100
    },
    {
      "epoch": 0.2730878432562474,
      "grad_norm": 0.04982827603816986,
      "learning_rate": 0.00021807364702312575,
      "loss": 0.4801,
      "step": 88200
    },
    {
      "epoch": 0.2733974666612999,
      "grad_norm": 21.519624710083008,
      "learning_rate": 0.00021798076000161004,
      "loss": 0.4272,
      "step": 88300
    },
    {
      "epoch": 0.2737070900663523,
      "grad_norm": 0.002297905972227454,
      "learning_rate": 0.0002178878729800943,
      "loss": 0.3643,
      "step": 88400
    },
    {
      "epoch": 0.2740167134714047,
      "grad_norm": 0.057739898562431335,
      "learning_rate": 0.00021779498595857856,
      "loss": 0.4696,
      "step": 88500
    },
    {
      "epoch": 0.2743263368764572,
      "grad_norm": 15.305948257446289,
      "learning_rate": 0.00021770209893706284,
      "loss": 0.461,
      "step": 88600
    },
    {
      "epoch": 0.2746359602815096,
      "grad_norm": 0.004239014815539122,
      "learning_rate": 0.0002176092119155471,
      "loss": 0.473,
      "step": 88700
    },
    {
      "epoch": 0.274945583686562,
      "grad_norm": 69.71881866455078,
      "learning_rate": 0.00021751632489403137,
      "loss": 0.789,
      "step": 88800
    },
    {
      "epoch": 0.2752552070916145,
      "grad_norm": 0.01174711063504219,
      "learning_rate": 0.00021742343787251565,
      "loss": 0.4224,
      "step": 88900
    },
    {
      "epoch": 0.2755648304966669,
      "grad_norm": 0.21474865078926086,
      "learning_rate": 0.0002173305508509999,
      "loss": 0.5248,
      "step": 89000
    },
    {
      "epoch": 0.27587445390171933,
      "grad_norm": 0.08551359921693802,
      "learning_rate": 0.00021723766382948417,
      "loss": 0.4545,
      "step": 89100
    },
    {
      "epoch": 0.27618407730677175,
      "grad_norm": 0.0070631243288517,
      "learning_rate": 0.00021714477680796846,
      "loss": 0.3307,
      "step": 89200
    },
    {
      "epoch": 0.2764937007118242,
      "grad_norm": 41.85178756713867,
      "learning_rate": 0.00021705188978645272,
      "loss": 0.3257,
      "step": 89300
    },
    {
      "epoch": 0.27680332411687664,
      "grad_norm": 1.1895490884780884,
      "learning_rate": 0.00021695900276493698,
      "loss": 0.7422,
      "step": 89400
    },
    {
      "epoch": 0.27711294752192905,
      "grad_norm": 20.216554641723633,
      "learning_rate": 0.00021686611574342127,
      "loss": 0.5621,
      "step": 89500
    },
    {
      "epoch": 0.2774225709269815,
      "grad_norm": 0.010117084719240665,
      "learning_rate": 0.00021677322872190553,
      "loss": 0.5872,
      "step": 89600
    },
    {
      "epoch": 0.27773219433203394,
      "grad_norm": 0.07986190170049667,
      "learning_rate": 0.0002166803417003898,
      "loss": 0.5084,
      "step": 89700
    },
    {
      "epoch": 0.27804181773708636,
      "grad_norm": 0.004243795294314623,
      "learning_rate": 0.00021658745467887408,
      "loss": 0.4189,
      "step": 89800
    },
    {
      "epoch": 0.27835144114213883,
      "grad_norm": 0.007088901940733194,
      "learning_rate": 0.00021649456765735834,
      "loss": 0.4347,
      "step": 89900
    },
    {
      "epoch": 0.27866106454719125,
      "grad_norm": 75.5694351196289,
      "learning_rate": 0.0002164016806358426,
      "loss": 0.3412,
      "step": 90000
    },
    {
      "epoch": 0.27897068795224367,
      "grad_norm": 0.024174895137548447,
      "learning_rate": 0.0002163087936143269,
      "loss": 0.2517,
      "step": 90100
    },
    {
      "epoch": 0.27928031135729614,
      "grad_norm": 0.00518655963242054,
      "learning_rate": 0.00021621590659281115,
      "loss": 0.576,
      "step": 90200
    },
    {
      "epoch": 0.27958993476234856,
      "grad_norm": 0.18714065849781036,
      "learning_rate": 0.00021612301957129543,
      "loss": 0.3716,
      "step": 90300
    },
    {
      "epoch": 0.279899558167401,
      "grad_norm": 0.0009713770123198628,
      "learning_rate": 0.0002160301325497797,
      "loss": 0.2194,
      "step": 90400
    },
    {
      "epoch": 0.28020918157245345,
      "grad_norm": 0.006108608562499285,
      "learning_rate": 0.00021593724552826396,
      "loss": 0.4885,
      "step": 90500
    },
    {
      "epoch": 0.28051880497750586,
      "grad_norm": 8.22195434011519e-05,
      "learning_rate": 0.00021584435850674824,
      "loss": 0.4335,
      "step": 90600
    },
    {
      "epoch": 0.2808284283825583,
      "grad_norm": 26.395732879638672,
      "learning_rate": 0.0002157514714852325,
      "loss": 0.5421,
      "step": 90700
    },
    {
      "epoch": 0.28113805178761075,
      "grad_norm": 0.13967202603816986,
      "learning_rate": 0.00021565858446371676,
      "loss": 0.4544,
      "step": 90800
    },
    {
      "epoch": 0.28144767519266317,
      "grad_norm": 0.017346708104014397,
      "learning_rate": 0.00021556569744220105,
      "loss": 0.4992,
      "step": 90900
    },
    {
      "epoch": 0.2817572985977156,
      "grad_norm": 1.5052299499511719,
      "learning_rate": 0.0002154728104206853,
      "loss": 0.581,
      "step": 91000
    },
    {
      "epoch": 0.282066922002768,
      "grad_norm": 0.05826938897371292,
      "learning_rate": 0.00021537992339916957,
      "loss": 0.4769,
      "step": 91100
    },
    {
      "epoch": 0.2823765454078205,
      "grad_norm": 0.028356757014989853,
      "learning_rate": 0.00021528703637765386,
      "loss": 0.4892,
      "step": 91200
    },
    {
      "epoch": 0.2826861688128729,
      "grad_norm": 0.14194351434707642,
      "learning_rate": 0.00021519414935613812,
      "loss": 0.5261,
      "step": 91300
    },
    {
      "epoch": 0.2829957922179253,
      "grad_norm": 2.029508113861084,
      "learning_rate": 0.00021510126233462238,
      "loss": 0.3893,
      "step": 91400
    },
    {
      "epoch": 0.2833054156229778,
      "grad_norm": 0.0024436465464532375,
      "learning_rate": 0.00021500837531310667,
      "loss": 0.5806,
      "step": 91500
    },
    {
      "epoch": 0.2836150390280302,
      "grad_norm": 0.00498979864642024,
      "learning_rate": 0.00021491548829159093,
      "loss": 0.8339,
      "step": 91600
    },
    {
      "epoch": 0.2839246624330826,
      "grad_norm": 0.030570615082979202,
      "learning_rate": 0.0002148226012700752,
      "loss": 0.5673,
      "step": 91700
    },
    {
      "epoch": 0.2842342858381351,
      "grad_norm": 0.008701169863343239,
      "learning_rate": 0.00021472971424855948,
      "loss": 0.454,
      "step": 91800
    },
    {
      "epoch": 0.2845439092431875,
      "grad_norm": 0.000907585141249001,
      "learning_rate": 0.00021463682722704374,
      "loss": 0.2852,
      "step": 91900
    },
    {
      "epoch": 0.2848535326482399,
      "grad_norm": 0.0011779642663896084,
      "learning_rate": 0.000214543940205528,
      "loss": 0.4991,
      "step": 92000
    },
    {
      "epoch": 0.2851631560532924,
      "grad_norm": 22.26720428466797,
      "learning_rate": 0.00021445105318401229,
      "loss": 0.4126,
      "step": 92100
    },
    {
      "epoch": 0.2854727794583448,
      "grad_norm": 190.0554656982422,
      "learning_rate": 0.00021435816616249655,
      "loss": 0.5051,
      "step": 92200
    },
    {
      "epoch": 0.28578240286339723,
      "grad_norm": 0.016284355893731117,
      "learning_rate": 0.0002142652791409808,
      "loss": 0.5249,
      "step": 92300
    },
    {
      "epoch": 0.2860920262684497,
      "grad_norm": 0.011982454918324947,
      "learning_rate": 0.0002141723921194651,
      "loss": 0.4149,
      "step": 92400
    },
    {
      "epoch": 0.2864016496735021,
      "grad_norm": 0.004122041631489992,
      "learning_rate": 0.00021407950509794935,
      "loss": 0.7618,
      "step": 92500
    },
    {
      "epoch": 0.28671127307855454,
      "grad_norm": 0.01191458385437727,
      "learning_rate": 0.00021398661807643362,
      "loss": 0.4977,
      "step": 92600
    },
    {
      "epoch": 0.287020896483607,
      "grad_norm": 0.005496349185705185,
      "learning_rate": 0.0002138937310549179,
      "loss": 0.4857,
      "step": 92700
    },
    {
      "epoch": 0.28733051988865943,
      "grad_norm": 0.0001929006539285183,
      "learning_rate": 0.00021380084403340216,
      "loss": 0.4452,
      "step": 92800
    },
    {
      "epoch": 0.28764014329371185,
      "grad_norm": 0.002187343779951334,
      "learning_rate": 0.00021370795701188642,
      "loss": 0.5352,
      "step": 92900
    },
    {
      "epoch": 0.2879497666987643,
      "grad_norm": 0.0026061064563691616,
      "learning_rate": 0.0002136150699903707,
      "loss": 0.2962,
      "step": 93000
    },
    {
      "epoch": 0.28825939010381674,
      "grad_norm": 15.003331184387207,
      "learning_rate": 0.00021352218296885497,
      "loss": 0.5156,
      "step": 93100
    },
    {
      "epoch": 0.28856901350886915,
      "grad_norm": 0.027127131819725037,
      "learning_rate": 0.0002134292959473392,
      "loss": 0.5513,
      "step": 93200
    },
    {
      "epoch": 0.28887863691392157,
      "grad_norm": 0.00037133152363821864,
      "learning_rate": 0.00021333640892582352,
      "loss": 0.3632,
      "step": 93300
    },
    {
      "epoch": 0.28918826031897404,
      "grad_norm": 0.0013985788682475686,
      "learning_rate": 0.00021324352190430778,
      "loss": 0.3169,
      "step": 93400
    },
    {
      "epoch": 0.28949788372402646,
      "grad_norm": 22.47788429260254,
      "learning_rate": 0.000213150634882792,
      "loss": 0.4694,
      "step": 93500
    },
    {
      "epoch": 0.2898075071290789,
      "grad_norm": 0.0004106420965399593,
      "learning_rate": 0.00021305774786127633,
      "loss": 0.4352,
      "step": 93600
    },
    {
      "epoch": 0.29011713053413135,
      "grad_norm": 0.0005283914506435394,
      "learning_rate": 0.0002129648608397606,
      "loss": 0.3608,
      "step": 93700
    },
    {
      "epoch": 0.29042675393918377,
      "grad_norm": 0.002618784550577402,
      "learning_rate": 0.00021287197381824482,
      "loss": 0.4541,
      "step": 93800
    },
    {
      "epoch": 0.2907363773442362,
      "grad_norm": 0.04923565685749054,
      "learning_rate": 0.00021277908679672914,
      "loss": 0.4917,
      "step": 93900
    },
    {
      "epoch": 0.29104600074928866,
      "grad_norm": 0.2221749871969223,
      "learning_rate": 0.0002126861997752134,
      "loss": 0.6632,
      "step": 94000
    },
    {
      "epoch": 0.2913556241543411,
      "grad_norm": 0.03116544336080551,
      "learning_rate": 0.00021259331275369768,
      "loss": 0.4317,
      "step": 94100
    },
    {
      "epoch": 0.2916652475593935,
      "grad_norm": 8.518558502197266,
      "learning_rate": 0.00021250042573218195,
      "loss": 0.4546,
      "step": 94200
    },
    {
      "epoch": 0.29197487096444596,
      "grad_norm": 44.446903228759766,
      "learning_rate": 0.00021240753871066618,
      "loss": 0.3459,
      "step": 94300
    },
    {
      "epoch": 0.2922844943694984,
      "grad_norm": 0.007451734039932489,
      "learning_rate": 0.0002123146516891505,
      "loss": 0.4828,
      "step": 94400
    },
    {
      "epoch": 0.2925941177745508,
      "grad_norm": 0.00588581059128046,
      "learning_rate": 0.00021222176466763475,
      "loss": 0.6112,
      "step": 94500
    },
    {
      "epoch": 0.29290374117960327,
      "grad_norm": 34.031803131103516,
      "learning_rate": 0.000212128877646119,
      "loss": 0.5133,
      "step": 94600
    },
    {
      "epoch": 0.2932133645846557,
      "grad_norm": 0.00290471687912941,
      "learning_rate": 0.0002120359906246033,
      "loss": 0.4235,
      "step": 94700
    },
    {
      "epoch": 0.2935229879897081,
      "grad_norm": 0.011144102551043034,
      "learning_rate": 0.00021194310360308756,
      "loss": 0.4051,
      "step": 94800
    },
    {
      "epoch": 0.2938326113947606,
      "grad_norm": 0.005459155887365341,
      "learning_rate": 0.0002118502165815718,
      "loss": 0.4765,
      "step": 94900
    },
    {
      "epoch": 0.294142234799813,
      "grad_norm": 0.0015491892118006945,
      "learning_rate": 0.0002117573295600561,
      "loss": 0.4261,
      "step": 95000
    },
    {
      "epoch": 0.2944518582048654,
      "grad_norm": 23.17316436767578,
      "learning_rate": 0.00021166444253854037,
      "loss": 0.5098,
      "step": 95100
    },
    {
      "epoch": 0.29476148160991783,
      "grad_norm": 5.418534755706787,
      "learning_rate": 0.0002115715555170246,
      "loss": 0.5984,
      "step": 95200
    },
    {
      "epoch": 0.2950711050149703,
      "grad_norm": 0.009269713424146175,
      "learning_rate": 0.00021147866849550892,
      "loss": 0.538,
      "step": 95300
    },
    {
      "epoch": 0.2953807284200227,
      "grad_norm": 0.0012212333967909217,
      "learning_rate": 0.00021138578147399318,
      "loss": 0.5764,
      "step": 95400
    },
    {
      "epoch": 0.29569035182507514,
      "grad_norm": 2.4389472007751465,
      "learning_rate": 0.0002112928944524774,
      "loss": 0.3828,
      "step": 95500
    },
    {
      "epoch": 0.2959999752301276,
      "grad_norm": 70.52838134765625,
      "learning_rate": 0.00021120000743096173,
      "loss": 0.5745,
      "step": 95600
    },
    {
      "epoch": 0.29630959863518,
      "grad_norm": 0.005518719088286161,
      "learning_rate": 0.00021110712040944596,
      "loss": 0.4839,
      "step": 95700
    },
    {
      "epoch": 0.29661922204023244,
      "grad_norm": 0.18781918287277222,
      "learning_rate": 0.00021101423338793022,
      "loss": 0.5359,
      "step": 95800
    },
    {
      "epoch": 0.2969288454452849,
      "grad_norm": 0.008872953243553638,
      "learning_rate": 0.00021092134636641454,
      "loss": 0.5554,
      "step": 95900
    },
    {
      "epoch": 0.29723846885033733,
      "grad_norm": 34.62736511230469,
      "learning_rate": 0.00021082845934489877,
      "loss": 0.4895,
      "step": 96000
    },
    {
      "epoch": 0.29754809225538975,
      "grad_norm": 0.006224161013960838,
      "learning_rate": 0.00021073557232338303,
      "loss": 0.5661,
      "step": 96100
    },
    {
      "epoch": 0.2978577156604422,
      "grad_norm": 0.00010045356611954048,
      "learning_rate": 0.00021064268530186734,
      "loss": 0.5676,
      "step": 96200
    },
    {
      "epoch": 0.29816733906549464,
      "grad_norm": 0.02274457924067974,
      "learning_rate": 0.00021054979828035158,
      "loss": 0.5045,
      "step": 96300
    },
    {
      "epoch": 0.29847696247054706,
      "grad_norm": 1.115380048751831,
      "learning_rate": 0.00021045691125883584,
      "loss": 0.498,
      "step": 96400
    },
    {
      "epoch": 0.29878658587559953,
      "grad_norm": 2.024393320083618,
      "learning_rate": 0.00021036402423732015,
      "loss": 0.3601,
      "step": 96500
    },
    {
      "epoch": 0.29909620928065195,
      "grad_norm": 0.47457414865493774,
      "learning_rate": 0.00021027113721580439,
      "loss": 0.5495,
      "step": 96600
    },
    {
      "epoch": 0.29940583268570437,
      "grad_norm": 0.0017984245205298066,
      "learning_rate": 0.00021017825019428865,
      "loss": 0.4204,
      "step": 96700
    },
    {
      "epoch": 0.29971545609075684,
      "grad_norm": 0.03488472104072571,
      "learning_rate": 0.00021008536317277293,
      "loss": 0.6321,
      "step": 96800
    },
    {
      "epoch": 0.30002507949580925,
      "grad_norm": 0.0012242585653439164,
      "learning_rate": 0.0002099924761512572,
      "loss": 0.4532,
      "step": 96900
    },
    {
      "epoch": 0.30033470290086167,
      "grad_norm": 0.03673652932047844,
      "learning_rate": 0.00020989958912974145,
      "loss": 0.5241,
      "step": 97000
    },
    {
      "epoch": 0.3006443263059141,
      "grad_norm": 0.013539187610149384,
      "learning_rate": 0.00020980670210822574,
      "loss": 0.4033,
      "step": 97100
    },
    {
      "epoch": 0.30095394971096656,
      "grad_norm": 3.1683661937713623,
      "learning_rate": 0.00020971381508671,
      "loss": 0.4058,
      "step": 97200
    },
    {
      "epoch": 0.301263573116019,
      "grad_norm": 24.48532485961914,
      "learning_rate": 0.00020962092806519426,
      "loss": 0.4753,
      "step": 97300
    },
    {
      "epoch": 0.3015731965210714,
      "grad_norm": 43.0951042175293,
      "learning_rate": 0.00020952804104367855,
      "loss": 0.5075,
      "step": 97400
    },
    {
      "epoch": 0.30188281992612387,
      "grad_norm": 0.6641541123390198,
      "learning_rate": 0.0002094351540221628,
      "loss": 0.4619,
      "step": 97500
    },
    {
      "epoch": 0.3021924433311763,
      "grad_norm": 0.04576339200139046,
      "learning_rate": 0.00020934226700064707,
      "loss": 0.4384,
      "step": 97600
    },
    {
      "epoch": 0.3025020667362287,
      "grad_norm": 2.3083877563476562,
      "learning_rate": 0.00020924937997913136,
      "loss": 0.5498,
      "step": 97700
    },
    {
      "epoch": 0.3028116901412812,
      "grad_norm": 0.005399353336542845,
      "learning_rate": 0.00020915649295761562,
      "loss": 0.4833,
      "step": 97800
    },
    {
      "epoch": 0.3031213135463336,
      "grad_norm": 0.014320926740765572,
      "learning_rate": 0.00020906360593609988,
      "loss": 0.4938,
      "step": 97900
    },
    {
      "epoch": 0.303430936951386,
      "grad_norm": 26.892675399780273,
      "learning_rate": 0.00020897071891458417,
      "loss": 0.5914,
      "step": 98000
    },
    {
      "epoch": 0.3037405603564385,
      "grad_norm": 0.0020058301743119955,
      "learning_rate": 0.00020887783189306843,
      "loss": 0.548,
      "step": 98100
    },
    {
      "epoch": 0.3040501837614909,
      "grad_norm": 0.01215978804975748,
      "learning_rate": 0.00020878494487155272,
      "loss": 0.453,
      "step": 98200
    },
    {
      "epoch": 0.3043598071665433,
      "grad_norm": 0.007238592952489853,
      "learning_rate": 0.00020869205785003698,
      "loss": 0.419,
      "step": 98300
    },
    {
      "epoch": 0.3046694305715958,
      "grad_norm": 0.0012308669975027442,
      "learning_rate": 0.00020859917082852124,
      "loss": 0.4966,
      "step": 98400
    },
    {
      "epoch": 0.3049790539766482,
      "grad_norm": 0.005784651264548302,
      "learning_rate": 0.00020850628380700552,
      "loss": 0.5966,
      "step": 98500
    },
    {
      "epoch": 0.3052886773817006,
      "grad_norm": 0.019566582515835762,
      "learning_rate": 0.00020841339678548978,
      "loss": 0.5792,
      "step": 98600
    },
    {
      "epoch": 0.3055983007867531,
      "grad_norm": 0.27694836258888245,
      "learning_rate": 0.00020832050976397405,
      "loss": 0.4845,
      "step": 98700
    },
    {
      "epoch": 0.3059079241918055,
      "grad_norm": 32.75822448730469,
      "learning_rate": 0.00020822762274245833,
      "loss": 0.4857,
      "step": 98800
    },
    {
      "epoch": 0.30621754759685793,
      "grad_norm": 4.956171989440918,
      "learning_rate": 0.0002081347357209426,
      "loss": 0.3183,
      "step": 98900
    },
    {
      "epoch": 0.3065271710019104,
      "grad_norm": 0.011650238186120987,
      "learning_rate": 0.00020804184869942685,
      "loss": 0.6916,
      "step": 99000
    },
    {
      "epoch": 0.3068367944069628,
      "grad_norm": 10.863016128540039,
      "learning_rate": 0.00020794896167791114,
      "loss": 0.4484,
      "step": 99100
    },
    {
      "epoch": 0.30714641781201524,
      "grad_norm": 0.008227890357375145,
      "learning_rate": 0.0002078560746563954,
      "loss": 0.3126,
      "step": 99200
    },
    {
      "epoch": 0.30745604121706765,
      "grad_norm": 8.04246997833252,
      "learning_rate": 0.00020776318763487966,
      "loss": 0.6136,
      "step": 99300
    },
    {
      "epoch": 0.3077656646221201,
      "grad_norm": 65.77444458007812,
      "learning_rate": 0.00020767030061336395,
      "loss": 0.4355,
      "step": 99400
    },
    {
      "epoch": 0.30807528802717254,
      "grad_norm": 0.0028262753039598465,
      "learning_rate": 0.0002075774135918482,
      "loss": 0.3387,
      "step": 99500
    },
    {
      "epoch": 0.30838491143222496,
      "grad_norm": 0.015011887066066265,
      "learning_rate": 0.00020748452657033247,
      "loss": 0.4779,
      "step": 99600
    },
    {
      "epoch": 0.30869453483727743,
      "grad_norm": 0.003249454777687788,
      "learning_rate": 0.00020739163954881676,
      "loss": 0.4914,
      "step": 99700
    },
    {
      "epoch": 0.30900415824232985,
      "grad_norm": 0.005650346167385578,
      "learning_rate": 0.00020729875252730102,
      "loss": 0.4092,
      "step": 99800
    },
    {
      "epoch": 0.30931378164738227,
      "grad_norm": 0.008675837889313698,
      "learning_rate": 0.00020720586550578528,
      "loss": 0.3566,
      "step": 99900
    },
    {
      "epoch": 0.30962340505243474,
      "grad_norm": 0.0024565060157328844,
      "learning_rate": 0.00020711297848426957,
      "loss": 0.3504,
      "step": 100000
    },
    {
      "epoch": 0.30993302845748716,
      "grad_norm": 0.00814197026193142,
      "learning_rate": 0.00020702009146275383,
      "loss": 0.3222,
      "step": 100100
    },
    {
      "epoch": 0.3102426518625396,
      "grad_norm": 0.0007615590584464371,
      "learning_rate": 0.0002069272044412381,
      "loss": 0.4438,
      "step": 100200
    },
    {
      "epoch": 0.31055227526759205,
      "grad_norm": 0.006175415124744177,
      "learning_rate": 0.00020683431741972238,
      "loss": 0.4617,
      "step": 100300
    },
    {
      "epoch": 0.31086189867264447,
      "grad_norm": 0.22338148951530457,
      "learning_rate": 0.00020674143039820664,
      "loss": 0.4941,
      "step": 100400
    },
    {
      "epoch": 0.3111715220776969,
      "grad_norm": 0.0029831244610249996,
      "learning_rate": 0.0002066485433766909,
      "loss": 0.3499,
      "step": 100500
    },
    {
      "epoch": 0.31148114548274936,
      "grad_norm": 0.005044246092438698,
      "learning_rate": 0.00020655565635517518,
      "loss": 0.4595,
      "step": 100600
    },
    {
      "epoch": 0.3117907688878018,
      "grad_norm": 2.633347988128662,
      "learning_rate": 0.00020646276933365944,
      "loss": 0.3667,
      "step": 100700
    },
    {
      "epoch": 0.3121003922928542,
      "grad_norm": 0.0035551481414586306,
      "learning_rate": 0.0002063698823121437,
      "loss": 0.6299,
      "step": 100800
    },
    {
      "epoch": 0.31241001569790666,
      "grad_norm": 0.004049175884574652,
      "learning_rate": 0.000206276995290628,
      "loss": 0.3699,
      "step": 100900
    },
    {
      "epoch": 0.3127196391029591,
      "grad_norm": 0.7190626263618469,
      "learning_rate": 0.00020618410826911225,
      "loss": 0.6469,
      "step": 101000
    },
    {
      "epoch": 0.3130292625080115,
      "grad_norm": 0.003634099615737796,
      "learning_rate": 0.0002060912212475965,
      "loss": 0.3773,
      "step": 101100
    },
    {
      "epoch": 0.3133388859130639,
      "grad_norm": 0.004076614044606686,
      "learning_rate": 0.0002059983342260808,
      "loss": 0.4304,
      "step": 101200
    },
    {
      "epoch": 0.3136485093181164,
      "grad_norm": 0.001027154503390193,
      "learning_rate": 0.00020590544720456506,
      "loss": 0.5132,
      "step": 101300
    },
    {
      "epoch": 0.3139581327231688,
      "grad_norm": 0.005695025436580181,
      "learning_rate": 0.00020581256018304932,
      "loss": 0.6482,
      "step": 101400
    },
    {
      "epoch": 0.3142677561282212,
      "grad_norm": 0.028518421575427055,
      "learning_rate": 0.0002057196731615336,
      "loss": 0.3996,
      "step": 101500
    },
    {
      "epoch": 0.3145773795332737,
      "grad_norm": 0.048168085515499115,
      "learning_rate": 0.00020562678614001787,
      "loss": 0.3815,
      "step": 101600
    },
    {
      "epoch": 0.3148870029383261,
      "grad_norm": 0.05906844511628151,
      "learning_rate": 0.00020553389911850213,
      "loss": 0.3909,
      "step": 101700
    },
    {
      "epoch": 0.3151966263433785,
      "grad_norm": 0.016500908881425858,
      "learning_rate": 0.00020544101209698642,
      "loss": 0.4091,
      "step": 101800
    },
    {
      "epoch": 0.315506249748431,
      "grad_norm": 0.005694693885743618,
      "learning_rate": 0.00020534812507547068,
      "loss": 0.3613,
      "step": 101900
    },
    {
      "epoch": 0.3158158731534834,
      "grad_norm": 0.14058105647563934,
      "learning_rate": 0.00020525523805395494,
      "loss": 0.4563,
      "step": 102000
    },
    {
      "epoch": 0.31612549655853583,
      "grad_norm": 0.0015214624581858516,
      "learning_rate": 0.00020516235103243923,
      "loss": 0.3643,
      "step": 102100
    },
    {
      "epoch": 0.3164351199635883,
      "grad_norm": 0.019973544403910637,
      "learning_rate": 0.00020506946401092349,
      "loss": 0.3906,
      "step": 102200
    },
    {
      "epoch": 0.3167447433686407,
      "grad_norm": 0.016053564846515656,
      "learning_rate": 0.00020497657698940777,
      "loss": 0.4116,
      "step": 102300
    },
    {
      "epoch": 0.31705436677369314,
      "grad_norm": 0.10382739454507828,
      "learning_rate": 0.00020488368996789203,
      "loss": 0.6712,
      "step": 102400
    },
    {
      "epoch": 0.3173639901787456,
      "grad_norm": 0.0006606569513678551,
      "learning_rate": 0.0002047908029463763,
      "loss": 0.6339,
      "step": 102500
    },
    {
      "epoch": 0.31767361358379803,
      "grad_norm": 0.012875843793153763,
      "learning_rate": 0.00020469791592486058,
      "loss": 0.3609,
      "step": 102600
    },
    {
      "epoch": 0.31798323698885045,
      "grad_norm": 0.04941941425204277,
      "learning_rate": 0.00020460502890334484,
      "loss": 0.2964,
      "step": 102700
    },
    {
      "epoch": 0.3182928603939029,
      "grad_norm": 18.687021255493164,
      "learning_rate": 0.0002045121418818291,
      "loss": 0.5707,
      "step": 102800
    },
    {
      "epoch": 0.31860248379895534,
      "grad_norm": 0.0012050921795889735,
      "learning_rate": 0.0002044192548603134,
      "loss": 0.4435,
      "step": 102900
    },
    {
      "epoch": 0.31891210720400776,
      "grad_norm": 0.0022089474368840456,
      "learning_rate": 0.00020432636783879765,
      "loss": 0.4319,
      "step": 103000
    },
    {
      "epoch": 0.3192217306090602,
      "grad_norm": 0.003902086988091469,
      "learning_rate": 0.0002042334808172819,
      "loss": 0.5835,
      "step": 103100
    },
    {
      "epoch": 0.31953135401411265,
      "grad_norm": 0.0007901840726844966,
      "learning_rate": 0.0002041405937957662,
      "loss": 0.393,
      "step": 103200
    },
    {
      "epoch": 0.31984097741916506,
      "grad_norm": 30.902681350708008,
      "learning_rate": 0.00020404770677425046,
      "loss": 0.5175,
      "step": 103300
    },
    {
      "epoch": 0.3201506008242175,
      "grad_norm": 39.60398864746094,
      "learning_rate": 0.00020395481975273472,
      "loss": 0.5572,
      "step": 103400
    },
    {
      "epoch": 0.32046022422926995,
      "grad_norm": 32.23931884765625,
      "learning_rate": 0.000203861932731219,
      "loss": 0.3734,
      "step": 103500
    },
    {
      "epoch": 0.32076984763432237,
      "grad_norm": 0.0017082116100937128,
      "learning_rate": 0.00020376904570970327,
      "loss": 0.317,
      "step": 103600
    },
    {
      "epoch": 0.3210794710393748,
      "grad_norm": 0.004287230782210827,
      "learning_rate": 0.00020367615868818753,
      "loss": 0.4072,
      "step": 103700
    },
    {
      "epoch": 0.32138909444442726,
      "grad_norm": 0.06190719082951546,
      "learning_rate": 0.00020358327166667182,
      "loss": 0.5912,
      "step": 103800
    },
    {
      "epoch": 0.3216987178494797,
      "grad_norm": 0.008773347362875938,
      "learning_rate": 0.00020349038464515608,
      "loss": 0.4024,
      "step": 103900
    },
    {
      "epoch": 0.3220083412545321,
      "grad_norm": 0.004872501362115145,
      "learning_rate": 0.00020339749762364034,
      "loss": 0.4015,
      "step": 104000
    },
    {
      "epoch": 0.32231796465958457,
      "grad_norm": 0.0014983388828113675,
      "learning_rate": 0.00020330461060212462,
      "loss": 0.384,
      "step": 104100
    },
    {
      "epoch": 0.322627588064637,
      "grad_norm": 0.0010326345218345523,
      "learning_rate": 0.00020321172358060889,
      "loss": 0.4132,
      "step": 104200
    },
    {
      "epoch": 0.3229372114696894,
      "grad_norm": 0.0014377455227077007,
      "learning_rate": 0.00020311883655909315,
      "loss": 0.4572,
      "step": 104300
    },
    {
      "epoch": 0.3232468348747419,
      "grad_norm": 44.3740234375,
      "learning_rate": 0.00020302594953757743,
      "loss": 0.3296,
      "step": 104400
    },
    {
      "epoch": 0.3235564582797943,
      "grad_norm": 0.02031220868229866,
      "learning_rate": 0.0002029330625160617,
      "loss": 0.2769,
      "step": 104500
    },
    {
      "epoch": 0.3238660816848467,
      "grad_norm": 0.0015864574816077948,
      "learning_rate": 0.00020284017549454595,
      "loss": 0.3462,
      "step": 104600
    },
    {
      "epoch": 0.3241757050898992,
      "grad_norm": 0.00021857563115190715,
      "learning_rate": 0.00020274728847303024,
      "loss": 0.4417,
      "step": 104700
    },
    {
      "epoch": 0.3244853284949516,
      "grad_norm": 0.11246941238641739,
      "learning_rate": 0.0002026544014515145,
      "loss": 0.2268,
      "step": 104800
    },
    {
      "epoch": 0.324794951900004,
      "grad_norm": 0.006678857374936342,
      "learning_rate": 0.00020256151442999876,
      "loss": 0.4325,
      "step": 104900
    },
    {
      "epoch": 0.3251045753050565,
      "grad_norm": 0.03920409083366394,
      "learning_rate": 0.00020246862740848305,
      "loss": 0.5413,
      "step": 105000
    },
    {
      "epoch": 0.3254141987101089,
      "grad_norm": 1.3236374855041504,
      "learning_rate": 0.0002023757403869673,
      "loss": 0.4738,
      "step": 105100
    },
    {
      "epoch": 0.3257238221151613,
      "grad_norm": 0.014209664426743984,
      "learning_rate": 0.00020228285336545157,
      "loss": 0.4309,
      "step": 105200
    },
    {
      "epoch": 0.32603344552021374,
      "grad_norm": 149.1513214111328,
      "learning_rate": 0.00020218996634393586,
      "loss": 0.421,
      "step": 105300
    },
    {
      "epoch": 0.3263430689252662,
      "grad_norm": 0.012744019739329815,
      "learning_rate": 0.00020209707932242012,
      "loss": 0.7026,
      "step": 105400
    },
    {
      "epoch": 0.32665269233031863,
      "grad_norm": 3.2644007205963135,
      "learning_rate": 0.00020200419230090438,
      "loss": 0.4627,
      "step": 105500
    },
    {
      "epoch": 0.32696231573537105,
      "grad_norm": 0.0011994255473837256,
      "learning_rate": 0.00020191130527938867,
      "loss": 0.3808,
      "step": 105600
    },
    {
      "epoch": 0.3272719391404235,
      "grad_norm": 0.0015182183124125004,
      "learning_rate": 0.00020181841825787293,
      "loss": 0.2982,
      "step": 105700
    },
    {
      "epoch": 0.32758156254547593,
      "grad_norm": 0.08613109588623047,
      "learning_rate": 0.0002017255312363572,
      "loss": 0.3856,
      "step": 105800
    },
    {
      "epoch": 0.32789118595052835,
      "grad_norm": 0.010045035742223263,
      "learning_rate": 0.00020163264421484148,
      "loss": 0.3361,
      "step": 105900
    },
    {
      "epoch": 0.3282008093555808,
      "grad_norm": 0.001314120483584702,
      "learning_rate": 0.00020153975719332574,
      "loss": 0.3364,
      "step": 106000
    },
    {
      "epoch": 0.32851043276063324,
      "grad_norm": 0.012555533088743687,
      "learning_rate": 0.00020144687017181002,
      "loss": 0.3474,
      "step": 106100
    },
    {
      "epoch": 0.32882005616568566,
      "grad_norm": 114.64795684814453,
      "learning_rate": 0.00020135398315029428,
      "loss": 0.6774,
      "step": 106200
    },
    {
      "epoch": 0.32912967957073813,
      "grad_norm": 0.006164755672216415,
      "learning_rate": 0.00020126109612877854,
      "loss": 0.3544,
      "step": 106300
    },
    {
      "epoch": 0.32943930297579055,
      "grad_norm": 0.002485315315425396,
      "learning_rate": 0.00020116820910726283,
      "loss": 0.3327,
      "step": 106400
    },
    {
      "epoch": 0.32974892638084297,
      "grad_norm": 0.22423811256885529,
      "learning_rate": 0.0002010753220857471,
      "loss": 0.4015,
      "step": 106500
    },
    {
      "epoch": 0.33005854978589544,
      "grad_norm": 0.0005284661892801523,
      "learning_rate": 0.00020098243506423135,
      "loss": 0.5083,
      "step": 106600
    },
    {
      "epoch": 0.33036817319094786,
      "grad_norm": 0.0029027285054326057,
      "learning_rate": 0.00020088954804271564,
      "loss": 0.4988,
      "step": 106700
    },
    {
      "epoch": 0.3306777965960003,
      "grad_norm": 0.007530468050390482,
      "learning_rate": 0.0002007966610211999,
      "loss": 0.3456,
      "step": 106800
    },
    {
      "epoch": 0.33098742000105275,
      "grad_norm": 0.01862005889415741,
      "learning_rate": 0.00020070377399968416,
      "loss": 0.283,
      "step": 106900
    },
    {
      "epoch": 0.33129704340610516,
      "grad_norm": 58.02540969848633,
      "learning_rate": 0.00020061088697816845,
      "loss": 0.5105,
      "step": 107000
    },
    {
      "epoch": 0.3316066668111576,
      "grad_norm": 0.08464512974023819,
      "learning_rate": 0.0002005179999566527,
      "loss": 0.5959,
      "step": 107100
    },
    {
      "epoch": 0.33191629021621,
      "grad_norm": 53.2012825012207,
      "learning_rate": 0.00020042511293513697,
      "loss": 0.489,
      "step": 107200
    },
    {
      "epoch": 0.33222591362126247,
      "grad_norm": 0.0032836582977324724,
      "learning_rate": 0.00020033222591362126,
      "loss": 0.5017,
      "step": 107300
    },
    {
      "epoch": 0.3325355370263149,
      "grad_norm": 26.077423095703125,
      "learning_rate": 0.00020023933889210552,
      "loss": 0.7523,
      "step": 107400
    },
    {
      "epoch": 0.3328451604313673,
      "grad_norm": 9.481303215026855,
      "learning_rate": 0.00020014645187058978,
      "loss": 0.3005,
      "step": 107500
    },
    {
      "epoch": 0.3331547838364198,
      "grad_norm": 0.007863502018153667,
      "learning_rate": 0.00020005356484907407,
      "loss": 0.5615,
      "step": 107600
    },
    {
      "epoch": 0.3334644072414722,
      "grad_norm": 0.0003970118996221572,
      "learning_rate": 0.00019996067782755833,
      "loss": 0.3601,
      "step": 107700
    },
    {
      "epoch": 0.3337740306465246,
      "grad_norm": 25.38532829284668,
      "learning_rate": 0.0001998677908060426,
      "loss": 0.3564,
      "step": 107800
    },
    {
      "epoch": 0.3340836540515771,
      "grad_norm": 0.019341440871357918,
      "learning_rate": 0.00019977490378452687,
      "loss": 0.4323,
      "step": 107900
    },
    {
      "epoch": 0.3343932774566295,
      "grad_norm": 0.010516904294490814,
      "learning_rate": 0.00019968201676301114,
      "loss": 0.5186,
      "step": 108000
    },
    {
      "epoch": 0.3347029008616819,
      "grad_norm": 69.0787124633789,
      "learning_rate": 0.0001995891297414954,
      "loss": 0.3607,
      "step": 108100
    },
    {
      "epoch": 0.3350125242667344,
      "grad_norm": 0.004462664946913719,
      "learning_rate": 0.00019949624271997968,
      "loss": 0.3327,
      "step": 108200
    },
    {
      "epoch": 0.3353221476717868,
      "grad_norm": 0.00042618336738087237,
      "learning_rate": 0.00019940335569846394,
      "loss": 0.2925,
      "step": 108300
    },
    {
      "epoch": 0.3356317710768392,
      "grad_norm": 0.0017924014246091247,
      "learning_rate": 0.0001993104686769482,
      "loss": 0.3716,
      "step": 108400
    },
    {
      "epoch": 0.3359413944818917,
      "grad_norm": 5.454592227935791,
      "learning_rate": 0.0001992175816554325,
      "loss": 0.4326,
      "step": 108500
    },
    {
      "epoch": 0.3362510178869441,
      "grad_norm": 0.0019787584897130728,
      "learning_rate": 0.00019912469463391675,
      "loss": 0.4991,
      "step": 108600
    },
    {
      "epoch": 0.33656064129199653,
      "grad_norm": 0.007764696609228849,
      "learning_rate": 0.000199031807612401,
      "loss": 0.5495,
      "step": 108700
    },
    {
      "epoch": 0.336870264697049,
      "grad_norm": 52.54224395751953,
      "learning_rate": 0.0001989389205908853,
      "loss": 0.326,
      "step": 108800
    },
    {
      "epoch": 0.3371798881021014,
      "grad_norm": 0.006386125925928354,
      "learning_rate": 0.00019884603356936956,
      "loss": 0.524,
      "step": 108900
    },
    {
      "epoch": 0.33748951150715384,
      "grad_norm": 7.468971252441406,
      "learning_rate": 0.00019875314654785382,
      "loss": 0.6088,
      "step": 109000
    },
    {
      "epoch": 0.33779913491220626,
      "grad_norm": 42.65898513793945,
      "learning_rate": 0.0001986602595263381,
      "loss": 0.5237,
      "step": 109100
    },
    {
      "epoch": 0.33810875831725873,
      "grad_norm": 0.002126229228451848,
      "learning_rate": 0.00019856737250482237,
      "loss": 0.4132,
      "step": 109200
    },
    {
      "epoch": 0.33841838172231115,
      "grad_norm": 0.004302500747144222,
      "learning_rate": 0.00019847448548330663,
      "loss": 0.5154,
      "step": 109300
    },
    {
      "epoch": 0.33872800512736356,
      "grad_norm": 24.031999588012695,
      "learning_rate": 0.00019838159846179092,
      "loss": 0.5384,
      "step": 109400
    },
    {
      "epoch": 0.33903762853241604,
      "grad_norm": 8.121201515197754,
      "learning_rate": 0.00019828871144027518,
      "loss": 0.4563,
      "step": 109500
    },
    {
      "epoch": 0.33934725193746845,
      "grad_norm": 0.0004178498056717217,
      "learning_rate": 0.00019819582441875944,
      "loss": 0.2901,
      "step": 109600
    },
    {
      "epoch": 0.33965687534252087,
      "grad_norm": 0.009889055974781513,
      "learning_rate": 0.00019810293739724373,
      "loss": 0.278,
      "step": 109700
    },
    {
      "epoch": 0.33996649874757334,
      "grad_norm": 0.008986755274236202,
      "learning_rate": 0.00019801005037572799,
      "loss": 0.4575,
      "step": 109800
    },
    {
      "epoch": 0.34027612215262576,
      "grad_norm": 31.003150939941406,
      "learning_rate": 0.00019791716335421225,
      "loss": 0.4114,
      "step": 109900
    },
    {
      "epoch": 0.3405857455576782,
      "grad_norm": 0.022754337638616562,
      "learning_rate": 0.00019782427633269653,
      "loss": 0.3306,
      "step": 110000
    },
    {
      "epoch": 0.34089536896273065,
      "grad_norm": 0.005015722941607237,
      "learning_rate": 0.0001977313893111808,
      "loss": 0.2104,
      "step": 110100
    },
    {
      "epoch": 0.34120499236778307,
      "grad_norm": 52.795448303222656,
      "learning_rate": 0.00019763850228966508,
      "loss": 0.3778,
      "step": 110200
    },
    {
      "epoch": 0.3415146157728355,
      "grad_norm": 0.03587976098060608,
      "learning_rate": 0.00019754561526814934,
      "loss": 0.4946,
      "step": 110300
    },
    {
      "epoch": 0.34182423917788796,
      "grad_norm": 0.020658526569604874,
      "learning_rate": 0.0001974527282466336,
      "loss": 0.239,
      "step": 110400
    },
    {
      "epoch": 0.3421338625829404,
      "grad_norm": 0.6204400062561035,
      "learning_rate": 0.0001973598412251179,
      "loss": 0.4231,
      "step": 110500
    },
    {
      "epoch": 0.3424434859879928,
      "grad_norm": 38.54445266723633,
      "learning_rate": 0.00019726695420360215,
      "loss": 0.3624,
      "step": 110600
    },
    {
      "epoch": 0.34275310939304526,
      "grad_norm": 0.0015144729986786842,
      "learning_rate": 0.0001971740671820864,
      "loss": 0.3875,
      "step": 110700
    },
    {
      "epoch": 0.3430627327980977,
      "grad_norm": 39.597137451171875,
      "learning_rate": 0.0001970811801605707,
      "loss": 0.5701,
      "step": 110800
    },
    {
      "epoch": 0.3433723562031501,
      "grad_norm": 46.472923278808594,
      "learning_rate": 0.00019698829313905496,
      "loss": 0.4641,
      "step": 110900
    },
    {
      "epoch": 0.34368197960820257,
      "grad_norm": 3.2001454830169678,
      "learning_rate": 0.00019689540611753922,
      "loss": 0.4458,
      "step": 111000
    },
    {
      "epoch": 0.343991603013255,
      "grad_norm": 32.214332580566406,
      "learning_rate": 0.0001968025190960235,
      "loss": 0.4002,
      "step": 111100
    },
    {
      "epoch": 0.3443012264183074,
      "grad_norm": 0.33022013306617737,
      "learning_rate": 0.00019670963207450777,
      "loss": 0.5047,
      "step": 111200
    },
    {
      "epoch": 0.3446108498233598,
      "grad_norm": 0.00017883079999592155,
      "learning_rate": 0.00019661674505299203,
      "loss": 0.4413,
      "step": 111300
    },
    {
      "epoch": 0.3449204732284123,
      "grad_norm": 0.001225986867211759,
      "learning_rate": 0.00019652385803147632,
      "loss": 0.3415,
      "step": 111400
    },
    {
      "epoch": 0.3452300966334647,
      "grad_norm": 0.2515306770801544,
      "learning_rate": 0.00019643097100996058,
      "loss": 0.4265,
      "step": 111500
    },
    {
      "epoch": 0.34553972003851713,
      "grad_norm": 0.08722540736198425,
      "learning_rate": 0.00019633808398844484,
      "loss": 0.6163,
      "step": 111600
    },
    {
      "epoch": 0.3458493434435696,
      "grad_norm": 0.0032787187956273556,
      "learning_rate": 0.00019624519696692912,
      "loss": 0.5939,
      "step": 111700
    },
    {
      "epoch": 0.346158966848622,
      "grad_norm": 0.002399345161393285,
      "learning_rate": 0.00019615230994541338,
      "loss": 0.4444,
      "step": 111800
    },
    {
      "epoch": 0.34646859025367444,
      "grad_norm": 0.047221504151821136,
      "learning_rate": 0.00019605942292389765,
      "loss": 0.342,
      "step": 111900
    },
    {
      "epoch": 0.3467782136587269,
      "grad_norm": 0.001548445550724864,
      "learning_rate": 0.00019596653590238193,
      "loss": 0.472,
      "step": 112000
    },
    {
      "epoch": 0.3470878370637793,
      "grad_norm": 0.0020913260523229837,
      "learning_rate": 0.0001958736488808662,
      "loss": 0.4272,
      "step": 112100
    },
    {
      "epoch": 0.34739746046883174,
      "grad_norm": 0.11776072531938553,
      "learning_rate": 0.00019578076185935045,
      "loss": 0.5119,
      "step": 112200
    },
    {
      "epoch": 0.3477070838738842,
      "grad_norm": 0.0241354051977396,
      "learning_rate": 0.00019568787483783474,
      "loss": 0.4222,
      "step": 112300
    },
    {
      "epoch": 0.34801670727893663,
      "grad_norm": 0.004381733015179634,
      "learning_rate": 0.000195594987816319,
      "loss": 0.3571,
      "step": 112400
    },
    {
      "epoch": 0.34832633068398905,
      "grad_norm": 0.0017663814360275865,
      "learning_rate": 0.00019550210079480326,
      "loss": 0.4341,
      "step": 112500
    },
    {
      "epoch": 0.3486359540890415,
      "grad_norm": 0.01751774549484253,
      "learning_rate": 0.00019540921377328755,
      "loss": 0.2583,
      "step": 112600
    },
    {
      "epoch": 0.34894557749409394,
      "grad_norm": 0.004320881329476833,
      "learning_rate": 0.0001953163267517718,
      "loss": 0.239,
      "step": 112700
    },
    {
      "epoch": 0.34925520089914636,
      "grad_norm": 0.09546137601137161,
      "learning_rate": 0.00019522343973025607,
      "loss": 0.5578,
      "step": 112800
    },
    {
      "epoch": 0.34956482430419883,
      "grad_norm": 0.02446814253926277,
      "learning_rate": 0.00019513055270874036,
      "loss": 0.3635,
      "step": 112900
    },
    {
      "epoch": 0.34987444770925125,
      "grad_norm": 0.02232387103140354,
      "learning_rate": 0.00019503766568722462,
      "loss": 0.6405,
      "step": 113000
    },
    {
      "epoch": 0.35018407111430366,
      "grad_norm": 0.028586426749825478,
      "learning_rate": 0.00019494477866570888,
      "loss": 0.309,
      "step": 113100
    },
    {
      "epoch": 0.3504936945193561,
      "grad_norm": 0.010601680725812912,
      "learning_rate": 0.00019485189164419317,
      "loss": 0.1818,
      "step": 113200
    },
    {
      "epoch": 0.35080331792440855,
      "grad_norm": 0.00029941819957457483,
      "learning_rate": 0.00019475900462267743,
      "loss": 0.3698,
      "step": 113300
    },
    {
      "epoch": 0.35111294132946097,
      "grad_norm": 0.0021585505455732346,
      "learning_rate": 0.0001946661176011617,
      "loss": 0.4064,
      "step": 113400
    },
    {
      "epoch": 0.3514225647345134,
      "grad_norm": 0.00024433762882836163,
      "learning_rate": 0.00019457323057964598,
      "loss": 0.4596,
      "step": 113500
    },
    {
      "epoch": 0.35173218813956586,
      "grad_norm": 0.1331620067358017,
      "learning_rate": 0.00019448034355813024,
      "loss": 0.558,
      "step": 113600
    },
    {
      "epoch": 0.3520418115446183,
      "grad_norm": 48.12134552001953,
      "learning_rate": 0.0001943874565366145,
      "loss": 0.4654,
      "step": 113700
    },
    {
      "epoch": 0.3523514349496707,
      "grad_norm": 0.00023122945276554674,
      "learning_rate": 0.00019429456951509878,
      "loss": 0.48,
      "step": 113800
    },
    {
      "epoch": 0.35266105835472317,
      "grad_norm": 0.08118508756160736,
      "learning_rate": 0.00019420168249358304,
      "loss": 0.3204,
      "step": 113900
    },
    {
      "epoch": 0.3529706817597756,
      "grad_norm": 19.99589729309082,
      "learning_rate": 0.00019410879547206728,
      "loss": 0.4825,
      "step": 114000
    },
    {
      "epoch": 0.353280305164828,
      "grad_norm": 41.86772918701172,
      "learning_rate": 0.0001940159084505516,
      "loss": 0.3953,
      "step": 114100
    },
    {
      "epoch": 0.3535899285698805,
      "grad_norm": 92.60192108154297,
      "learning_rate": 0.00019392302142903585,
      "loss": 0.545,
      "step": 114200
    },
    {
      "epoch": 0.3538995519749329,
      "grad_norm": 0.00817844644188881,
      "learning_rate": 0.00019383013440752014,
      "loss": 0.2924,
      "step": 114300
    },
    {
      "epoch": 0.3542091753799853,
      "grad_norm": 0.010470813140273094,
      "learning_rate": 0.0001937372473860044,
      "loss": 0.5553,
      "step": 114400
    },
    {
      "epoch": 0.3545187987850378,
      "grad_norm": 0.0009635808528400958,
      "learning_rate": 0.00019364436036448866,
      "loss": 0.3213,
      "step": 114500
    },
    {
      "epoch": 0.3548284221900902,
      "grad_norm": 0.0015038053970783949,
      "learning_rate": 0.00019355147334297295,
      "loss": 0.4553,
      "step": 114600
    },
    {
      "epoch": 0.3551380455951426,
      "grad_norm": 0.000894760072696954,
      "learning_rate": 0.0001934585863214572,
      "loss": 0.3323,
      "step": 114700
    },
    {
      "epoch": 0.3554476690001951,
      "grad_norm": 0.00549352215602994,
      "learning_rate": 0.00019336569929994147,
      "loss": 0.4964,
      "step": 114800
    },
    {
      "epoch": 0.3557572924052475,
      "grad_norm": 0.0004858394095208496,
      "learning_rate": 0.00019327281227842576,
      "loss": 0.6136,
      "step": 114900
    },
    {
      "epoch": 0.3560669158102999,
      "grad_norm": 0.002204406773671508,
      "learning_rate": 0.00019317992525691002,
      "loss": 0.298,
      "step": 115000
    },
    {
      "epoch": 0.35637653921535234,
      "grad_norm": 0.0007581252139061689,
      "learning_rate": 0.00019308703823539428,
      "loss": 0.2317,
      "step": 115100
    },
    {
      "epoch": 0.3566861626204048,
      "grad_norm": 0.0014151978539302945,
      "learning_rate": 0.00019299415121387857,
      "loss": 0.2926,
      "step": 115200
    },
    {
      "epoch": 0.35699578602545723,
      "grad_norm": 0.04161966219544411,
      "learning_rate": 0.00019290126419236283,
      "loss": 0.4495,
      "step": 115300
    },
    {
      "epoch": 0.35730540943050965,
      "grad_norm": 1.4096357822418213,
      "learning_rate": 0.00019280837717084706,
      "loss": 0.5755,
      "step": 115400
    },
    {
      "epoch": 0.3576150328355621,
      "grad_norm": 7.211362838745117,
      "learning_rate": 0.00019271549014933137,
      "loss": 0.3334,
      "step": 115500
    },
    {
      "epoch": 0.35792465624061454,
      "grad_norm": 0.0002891901240218431,
      "learning_rate": 0.00019262260312781563,
      "loss": 0.4688,
      "step": 115600
    },
    {
      "epoch": 0.35823427964566695,
      "grad_norm": 0.00032163766445592046,
      "learning_rate": 0.00019252971610629987,
      "loss": 0.4227,
      "step": 115700
    },
    {
      "epoch": 0.3585439030507194,
      "grad_norm": 0.0002752592845354229,
      "learning_rate": 0.00019243682908478418,
      "loss": 0.3843,
      "step": 115800
    },
    {
      "epoch": 0.35885352645577184,
      "grad_norm": 0.004120638128370047,
      "learning_rate": 0.00019234394206326844,
      "loss": 0.3298,
      "step": 115900
    },
    {
      "epoch": 0.35916314986082426,
      "grad_norm": 0.0013494514860212803,
      "learning_rate": 0.00019225105504175268,
      "loss": 0.3879,
      "step": 116000
    },
    {
      "epoch": 0.35947277326587673,
      "grad_norm": 0.0029221533332020044,
      "learning_rate": 0.000192158168020237,
      "loss": 0.4115,
      "step": 116100
    },
    {
      "epoch": 0.35978239667092915,
      "grad_norm": 0.2046993374824524,
      "learning_rate": 0.00019206528099872125,
      "loss": 0.4168,
      "step": 116200
    },
    {
      "epoch": 0.36009202007598157,
      "grad_norm": 0.00203676032833755,
      "learning_rate": 0.00019197239397720549,
      "loss": 0.4539,
      "step": 116300
    },
    {
      "epoch": 0.36040164348103404,
      "grad_norm": 0.0014481376856565475,
      "learning_rate": 0.0001918795069556898,
      "loss": 0.2872,
      "step": 116400
    },
    {
      "epoch": 0.36071126688608646,
      "grad_norm": 0.0016821747412905097,
      "learning_rate": 0.00019178661993417403,
      "loss": 0.5254,
      "step": 116500
    },
    {
      "epoch": 0.3610208902911389,
      "grad_norm": 0.001731865108013153,
      "learning_rate": 0.0001916937329126583,
      "loss": 0.5012,
      "step": 116600
    },
    {
      "epoch": 0.36133051369619135,
      "grad_norm": 10.77453327178955,
      "learning_rate": 0.0001916008458911426,
      "loss": 0.4482,
      "step": 116700
    },
    {
      "epoch": 0.36164013710124376,
      "grad_norm": 0.000999254290945828,
      "learning_rate": 0.00019150795886962684,
      "loss": 0.4957,
      "step": 116800
    },
    {
      "epoch": 0.3619497605062962,
      "grad_norm": 0.003224027808755636,
      "learning_rate": 0.0001914150718481111,
      "loss": 0.2973,
      "step": 116900
    },
    {
      "epoch": 0.36225938391134865,
      "grad_norm": 0.002694575348868966,
      "learning_rate": 0.00019132218482659542,
      "loss": 0.5016,
      "step": 117000
    },
    {
      "epoch": 0.36256900731640107,
      "grad_norm": 0.0007723715389147401,
      "learning_rate": 0.00019122929780507965,
      "loss": 0.3468,
      "step": 117100
    },
    {
      "epoch": 0.3628786307214535,
      "grad_norm": 0.003059388604015112,
      "learning_rate": 0.0001911364107835639,
      "loss": 0.4979,
      "step": 117200
    },
    {
      "epoch": 0.3631882541265059,
      "grad_norm": 0.001477596117183566,
      "learning_rate": 0.00019104352376204823,
      "loss": 0.5485,
      "step": 117300
    },
    {
      "epoch": 0.3634978775315584,
      "grad_norm": 0.0012835976667702198,
      "learning_rate": 0.00019095063674053246,
      "loss": 0.4398,
      "step": 117400
    },
    {
      "epoch": 0.3638075009366108,
      "grad_norm": 151.12445068359375,
      "learning_rate": 0.00019085774971901672,
      "loss": 0.5407,
      "step": 117500
    },
    {
      "epoch": 0.3641171243416632,
      "grad_norm": 0.007819661870598793,
      "learning_rate": 0.000190764862697501,
      "loss": 0.4788,
      "step": 117600
    },
    {
      "epoch": 0.3644267477467157,
      "grad_norm": 17.9765682220459,
      "learning_rate": 0.00019067197567598527,
      "loss": 0.2014,
      "step": 117700
    },
    {
      "epoch": 0.3647363711517681,
      "grad_norm": 0.0006517778965644538,
      "learning_rate": 0.00019057908865446953,
      "loss": 0.4476,
      "step": 117800
    },
    {
      "epoch": 0.3650459945568205,
      "grad_norm": 0.0073731811717152596,
      "learning_rate": 0.00019048620163295381,
      "loss": 0.6836,
      "step": 117900
    },
    {
      "epoch": 0.365355617961873,
      "grad_norm": 0.006051680538803339,
      "learning_rate": 0.00019039331461143808,
      "loss": 0.4075,
      "step": 118000
    },
    {
      "epoch": 0.3656652413669254,
      "grad_norm": 0.014022002927958965,
      "learning_rate": 0.0001903004275899224,
      "loss": 0.2488,
      "step": 118100
    },
    {
      "epoch": 0.3659748647719778,
      "grad_norm": 0.02768663316965103,
      "learning_rate": 0.00019020754056840662,
      "loss": 0.3175,
      "step": 118200
    },
    {
      "epoch": 0.3662844881770303,
      "grad_norm": 0.005359145812690258,
      "learning_rate": 0.00019011465354689088,
      "loss": 0.4796,
      "step": 118300
    },
    {
      "epoch": 0.3665941115820827,
      "grad_norm": 0.6236974000930786,
      "learning_rate": 0.0001900217665253752,
      "loss": 0.5994,
      "step": 118400
    },
    {
      "epoch": 0.36690373498713513,
      "grad_norm": 0.00197702762670815,
      "learning_rate": 0.00018992887950385943,
      "loss": 0.1672,
      "step": 118500
    },
    {
      "epoch": 0.3672133583921876,
      "grad_norm": 123.219482421875,
      "learning_rate": 0.0001898359924823437,
      "loss": 0.3425,
      "step": 118600
    },
    {
      "epoch": 0.36752298179724,
      "grad_norm": 0.018435675650835037,
      "learning_rate": 0.00018974310546082798,
      "loss": 0.4537,
      "step": 118700
    },
    {
      "epoch": 0.36783260520229244,
      "grad_norm": 0.43046799302101135,
      "learning_rate": 0.00018965021843931224,
      "loss": 0.7745,
      "step": 118800
    },
    {
      "epoch": 0.3681422286073449,
      "grad_norm": 108.72857666015625,
      "learning_rate": 0.0001895573314177965,
      "loss": 0.4713,
      "step": 118900
    },
    {
      "epoch": 0.36845185201239733,
      "grad_norm": 4.734999179840088,
      "learning_rate": 0.0001894644443962808,
      "loss": 0.5142,
      "step": 119000
    },
    {
      "epoch": 0.36876147541744975,
      "grad_norm": 0.0028499457985162735,
      "learning_rate": 0.00018937155737476505,
      "loss": 0.4035,
      "step": 119100
    },
    {
      "epoch": 0.36907109882250216,
      "grad_norm": 5.493624687194824,
      "learning_rate": 0.0001892786703532493,
      "loss": 0.5902,
      "step": 119200
    },
    {
      "epoch": 0.36938072222755464,
      "grad_norm": 8.585402488708496,
      "learning_rate": 0.0001891857833317336,
      "loss": 0.2866,
      "step": 119300
    },
    {
      "epoch": 0.36969034563260705,
      "grad_norm": 0.01018279418349266,
      "learning_rate": 0.00018909289631021786,
      "loss": 0.4156,
      "step": 119400
    },
    {
      "epoch": 0.36999996903765947,
      "grad_norm": 0.000851608463563025,
      "learning_rate": 0.00018900000928870212,
      "loss": 0.2803,
      "step": 119500
    },
    {
      "epoch": 0.37030959244271194,
      "grad_norm": 0.014021131210029125,
      "learning_rate": 0.0001889071222671864,
      "loss": 0.4416,
      "step": 119600
    },
    {
      "epoch": 0.37061921584776436,
      "grad_norm": 0.18378973007202148,
      "learning_rate": 0.00018881423524567067,
      "loss": 0.3944,
      "step": 119700
    },
    {
      "epoch": 0.3709288392528168,
      "grad_norm": 0.03539010509848595,
      "learning_rate": 0.00018872134822415493,
      "loss": 0.3163,
      "step": 119800
    },
    {
      "epoch": 0.37123846265786925,
      "grad_norm": 0.0028707035817205906,
      "learning_rate": 0.00018862846120263921,
      "loss": 0.5127,
      "step": 119900
    },
    {
      "epoch": 0.37154808606292167,
      "grad_norm": 0.0004544359107967466,
      "learning_rate": 0.00018853557418112347,
      "loss": 0.2967,
      "step": 120000
    },
    {
      "epoch": 0.3718577094679741,
      "grad_norm": 0.14115244150161743,
      "learning_rate": 0.00018844268715960773,
      "loss": 0.4837,
      "step": 120100
    },
    {
      "epoch": 0.37216733287302656,
      "grad_norm": 47.00848388671875,
      "learning_rate": 0.00018834980013809202,
      "loss": 0.6754,
      "step": 120200
    },
    {
      "epoch": 0.372476956278079,
      "grad_norm": 5.398534297943115,
      "learning_rate": 0.00018825691311657628,
      "loss": 0.4304,
      "step": 120300
    },
    {
      "epoch": 0.3727865796831314,
      "grad_norm": 41.16035079956055,
      "learning_rate": 0.00018816402609506054,
      "loss": 0.2626,
      "step": 120400
    },
    {
      "epoch": 0.37309620308818386,
      "grad_norm": 0.08127418160438538,
      "learning_rate": 0.00018807113907354483,
      "loss": 0.4114,
      "step": 120500
    },
    {
      "epoch": 0.3734058264932363,
      "grad_norm": 0.013631924986839294,
      "learning_rate": 0.0001879782520520291,
      "loss": 0.3485,
      "step": 120600
    },
    {
      "epoch": 0.3737154498982887,
      "grad_norm": 0.003280538832768798,
      "learning_rate": 0.00018788536503051335,
      "loss": 0.2949,
      "step": 120700
    },
    {
      "epoch": 0.37402507330334117,
      "grad_norm": 0.001061206334270537,
      "learning_rate": 0.00018779247800899764,
      "loss": 0.2934,
      "step": 120800
    },
    {
      "epoch": 0.3743346967083936,
      "grad_norm": 0.5007719993591309,
      "learning_rate": 0.0001876995909874819,
      "loss": 0.3024,
      "step": 120900
    },
    {
      "epoch": 0.374644320113446,
      "grad_norm": 0.00023954712378326803,
      "learning_rate": 0.00018760670396596616,
      "loss": 0.416,
      "step": 121000
    },
    {
      "epoch": 0.3749539435184984,
      "grad_norm": 0.004685692954808474,
      "learning_rate": 0.00018751381694445045,
      "loss": 0.294,
      "step": 121100
    },
    {
      "epoch": 0.3752635669235509,
      "grad_norm": 0.0029947629664093256,
      "learning_rate": 0.0001874209299229347,
      "loss": 0.2224,
      "step": 121200
    },
    {
      "epoch": 0.3755731903286033,
      "grad_norm": 0.004890630953013897,
      "learning_rate": 0.00018732804290141897,
      "loss": 0.5245,
      "step": 121300
    },
    {
      "epoch": 0.37588281373365573,
      "grad_norm": 0.00021229121193755418,
      "learning_rate": 0.00018723515587990326,
      "loss": 0.373,
      "step": 121400
    },
    {
      "epoch": 0.3761924371387082,
      "grad_norm": 1.3578130006790161,
      "learning_rate": 0.00018714226885838752,
      "loss": 0.4315,
      "step": 121500
    },
    {
      "epoch": 0.3765020605437606,
      "grad_norm": 0.0026060491800308228,
      "learning_rate": 0.00018704938183687178,
      "loss": 0.5717,
      "step": 121600
    },
    {
      "epoch": 0.37681168394881304,
      "grad_norm": 0.5652070641517639,
      "learning_rate": 0.00018695649481535606,
      "loss": 0.3451,
      "step": 121700
    },
    {
      "epoch": 0.3771213073538655,
      "grad_norm": 0.0028288059402257204,
      "learning_rate": 0.00018686360779384033,
      "loss": 0.4485,
      "step": 121800
    },
    {
      "epoch": 0.3774309307589179,
      "grad_norm": 0.0013049027184024453,
      "learning_rate": 0.00018677072077232459,
      "loss": 0.6918,
      "step": 121900
    },
    {
      "epoch": 0.37774055416397034,
      "grad_norm": 0.0018034563399851322,
      "learning_rate": 0.00018667783375080887,
      "loss": 0.5451,
      "step": 122000
    },
    {
      "epoch": 0.3780501775690228,
      "grad_norm": 30.99909019470215,
      "learning_rate": 0.00018658494672929313,
      "loss": 0.4487,
      "step": 122100
    },
    {
      "epoch": 0.37835980097407523,
      "grad_norm": 0.08084305375814438,
      "learning_rate": 0.00018649205970777742,
      "loss": 0.4282,
      "step": 122200
    },
    {
      "epoch": 0.37866942437912765,
      "grad_norm": 0.0007414044230245054,
      "learning_rate": 0.00018639917268626168,
      "loss": 0.3286,
      "step": 122300
    },
    {
      "epoch": 0.3789790477841801,
      "grad_norm": 0.0005592657253146172,
      "learning_rate": 0.00018630628566474594,
      "loss": 0.4824,
      "step": 122400
    },
    {
      "epoch": 0.37928867118923254,
      "grad_norm": 24.457050323486328,
      "learning_rate": 0.00018621339864323023,
      "loss": 0.4486,
      "step": 122500
    },
    {
      "epoch": 0.37959829459428496,
      "grad_norm": 0.00134271162096411,
      "learning_rate": 0.0001861205116217145,
      "loss": 0.3785,
      "step": 122600
    },
    {
      "epoch": 0.37990791799933743,
      "grad_norm": 0.003567173145711422,
      "learning_rate": 0.00018602762460019875,
      "loss": 0.4491,
      "step": 122700
    },
    {
      "epoch": 0.38021754140438985,
      "grad_norm": 39.90856170654297,
      "learning_rate": 0.00018593473757868304,
      "loss": 0.3973,
      "step": 122800
    },
    {
      "epoch": 0.38052716480944226,
      "grad_norm": 0.49905136227607727,
      "learning_rate": 0.0001858418505571673,
      "loss": 0.4366,
      "step": 122900
    },
    {
      "epoch": 0.38083678821449474,
      "grad_norm": 0.007331620901823044,
      "learning_rate": 0.00018574896353565156,
      "loss": 0.7837,
      "step": 123000
    },
    {
      "epoch": 0.38114641161954715,
      "grad_norm": 5.084874629974365,
      "learning_rate": 0.00018565607651413585,
      "loss": 0.3316,
      "step": 123100
    },
    {
      "epoch": 0.38145603502459957,
      "grad_norm": 12.147278785705566,
      "learning_rate": 0.0001855631894926201,
      "loss": 0.624,
      "step": 123200
    },
    {
      "epoch": 0.381765658429652,
      "grad_norm": 0.00021757000649813563,
      "learning_rate": 0.00018547030247110437,
      "loss": 0.3132,
      "step": 123300
    },
    {
      "epoch": 0.38207528183470446,
      "grad_norm": 0.011347769759595394,
      "learning_rate": 0.00018537741544958866,
      "loss": 0.3572,
      "step": 123400
    },
    {
      "epoch": 0.3823849052397569,
      "grad_norm": 0.0064767515286803246,
      "learning_rate": 0.00018528452842807292,
      "loss": 0.3196,
      "step": 123500
    },
    {
      "epoch": 0.3826945286448093,
      "grad_norm": 14.190756797790527,
      "learning_rate": 0.00018519164140655718,
      "loss": 0.4261,
      "step": 123600
    },
    {
      "epoch": 0.38300415204986177,
      "grad_norm": 0.0122535964474082,
      "learning_rate": 0.00018509875438504146,
      "loss": 0.4026,
      "step": 123700
    },
    {
      "epoch": 0.3833137754549142,
      "grad_norm": 0.004103471525013447,
      "learning_rate": 0.00018500586736352572,
      "loss": 0.4209,
      "step": 123800
    },
    {
      "epoch": 0.3836233988599666,
      "grad_norm": 0.16052907705307007,
      "learning_rate": 0.00018491298034200998,
      "loss": 0.5265,
      "step": 123900
    },
    {
      "epoch": 0.3839330222650191,
      "grad_norm": 0.6594218611717224,
      "learning_rate": 0.00018482009332049427,
      "loss": 0.2868,
      "step": 124000
    },
    {
      "epoch": 0.3842426456700715,
      "grad_norm": 0.005659776274114847,
      "learning_rate": 0.00018472720629897853,
      "loss": 0.2503,
      "step": 124100
    },
    {
      "epoch": 0.3845522690751239,
      "grad_norm": 0.00043712015030905604,
      "learning_rate": 0.0001846343192774628,
      "loss": 0.3448,
      "step": 124200
    },
    {
      "epoch": 0.3848618924801764,
      "grad_norm": 0.0057452102191746235,
      "learning_rate": 0.00018454143225594708,
      "loss": 0.2655,
      "step": 124300
    },
    {
      "epoch": 0.3851715158852288,
      "grad_norm": 0.004762563388794661,
      "learning_rate": 0.00018444854523443134,
      "loss": 0.4309,
      "step": 124400
    },
    {
      "epoch": 0.3854811392902812,
      "grad_norm": 2.224782943725586,
      "learning_rate": 0.0001843556582129156,
      "loss": 0.476,
      "step": 124500
    },
    {
      "epoch": 0.3857907626953337,
      "grad_norm": 0.010652653872966766,
      "learning_rate": 0.0001842627711913999,
      "loss": 0.4165,
      "step": 124600
    },
    {
      "epoch": 0.3861003861003861,
      "grad_norm": 0.2950979173183441,
      "learning_rate": 0.00018416988416988415,
      "loss": 0.2832,
      "step": 124700
    },
    {
      "epoch": 0.3864100095054385,
      "grad_norm": 0.005670551676303148,
      "learning_rate": 0.0001840769971483684,
      "loss": 0.4591,
      "step": 124800
    },
    {
      "epoch": 0.386719632910491,
      "grad_norm": 61.589576721191406,
      "learning_rate": 0.0001839841101268527,
      "loss": 0.3465,
      "step": 124900
    },
    {
      "epoch": 0.3870292563155434,
      "grad_norm": 0.00022362916206475347,
      "learning_rate": 0.00018389122310533696,
      "loss": 0.4939,
      "step": 125000
    },
    {
      "epoch": 0.38733887972059583,
      "grad_norm": 0.0018250048160552979,
      "learning_rate": 0.00018379833608382122,
      "loss": 0.4769,
      "step": 125100
    },
    {
      "epoch": 0.38764850312564825,
      "grad_norm": 32.76881408691406,
      "learning_rate": 0.0001837054490623055,
      "loss": 0.3922,
      "step": 125200
    },
    {
      "epoch": 0.3879581265307007,
      "grad_norm": 0.012992079369723797,
      "learning_rate": 0.00018361256204078977,
      "loss": 0.5069,
      "step": 125300
    },
    {
      "epoch": 0.38826774993575314,
      "grad_norm": 0.00037565059028565884,
      "learning_rate": 0.00018351967501927403,
      "loss": 0.3191,
      "step": 125400
    },
    {
      "epoch": 0.38857737334080555,
      "grad_norm": 0.0008612326346337795,
      "learning_rate": 0.00018342678799775831,
      "loss": 0.2141,
      "step": 125500
    },
    {
      "epoch": 0.388886996745858,
      "grad_norm": 0.0002862859982997179,
      "learning_rate": 0.00018333390097624258,
      "loss": 0.399,
      "step": 125600
    },
    {
      "epoch": 0.38919662015091044,
      "grad_norm": 40.592594146728516,
      "learning_rate": 0.00018324101395472684,
      "loss": 0.4338,
      "step": 125700
    },
    {
      "epoch": 0.38950624355596286,
      "grad_norm": 0.018707051873207092,
      "learning_rate": 0.00018314812693321112,
      "loss": 0.2835,
      "step": 125800
    },
    {
      "epoch": 0.38981586696101533,
      "grad_norm": 68.24292755126953,
      "learning_rate": 0.00018305523991169538,
      "loss": 0.4903,
      "step": 125900
    },
    {
      "epoch": 0.39012549036606775,
      "grad_norm": 0.07047602534294128,
      "learning_rate": 0.00018296235289017964,
      "loss": 0.3392,
      "step": 126000
    },
    {
      "epoch": 0.39043511377112017,
      "grad_norm": 18.120990753173828,
      "learning_rate": 0.00018286946586866393,
      "loss": 0.3326,
      "step": 126100
    },
    {
      "epoch": 0.39074473717617264,
      "grad_norm": 18.756107330322266,
      "learning_rate": 0.0001827765788471482,
      "loss": 0.5589,
      "step": 126200
    },
    {
      "epoch": 0.39105436058122506,
      "grad_norm": 11.445582389831543,
      "learning_rate": 0.00018268369182563248,
      "loss": 0.4586,
      "step": 126300
    },
    {
      "epoch": 0.3913639839862775,
      "grad_norm": 0.0009549296810291708,
      "learning_rate": 0.00018259080480411674,
      "loss": 0.4219,
      "step": 126400
    },
    {
      "epoch": 0.39167360739132995,
      "grad_norm": 0.0008255207212641835,
      "learning_rate": 0.000182497917782601,
      "loss": 0.4766,
      "step": 126500
    },
    {
      "epoch": 0.39198323079638236,
      "grad_norm": 18.957733154296875,
      "learning_rate": 0.0001824050307610853,
      "loss": 0.2077,
      "step": 126600
    },
    {
      "epoch": 0.3922928542014348,
      "grad_norm": 0.009735963307321072,
      "learning_rate": 0.00018231214373956955,
      "loss": 0.5324,
      "step": 126700
    },
    {
      "epoch": 0.39260247760648725,
      "grad_norm": 0.005386590026319027,
      "learning_rate": 0.0001822192567180538,
      "loss": 0.3191,
      "step": 126800
    },
    {
      "epoch": 0.39291210101153967,
      "grad_norm": 102.47559356689453,
      "learning_rate": 0.0001821263696965381,
      "loss": 0.4634,
      "step": 126900
    },
    {
      "epoch": 0.3932217244165921,
      "grad_norm": 0.0013527247356250882,
      "learning_rate": 0.00018203348267502236,
      "loss": 0.3338,
      "step": 127000
    },
    {
      "epoch": 0.3935313478216445,
      "grad_norm": 80.32086181640625,
      "learning_rate": 0.00018194059565350662,
      "loss": 0.3633,
      "step": 127100
    },
    {
      "epoch": 0.393840971226697,
      "grad_norm": 0.012448961846530437,
      "learning_rate": 0.0001818477086319909,
      "loss": 0.2516,
      "step": 127200
    },
    {
      "epoch": 0.3941505946317494,
      "grad_norm": 0.0191695187240839,
      "learning_rate": 0.00018175482161047517,
      "loss": 0.4214,
      "step": 127300
    },
    {
      "epoch": 0.3944602180368018,
      "grad_norm": 0.0009207692346535623,
      "learning_rate": 0.00018166193458895943,
      "loss": 0.2439,
      "step": 127400
    },
    {
      "epoch": 0.3947698414418543,
      "grad_norm": 0.16388578712940216,
      "learning_rate": 0.0001815690475674437,
      "loss": 0.4419,
      "step": 127500
    },
    {
      "epoch": 0.3950794648469067,
      "grad_norm": 0.001173987751826644,
      "learning_rate": 0.00018147616054592797,
      "loss": 0.5684,
      "step": 127600
    },
    {
      "epoch": 0.3953890882519591,
      "grad_norm": 0.08658163249492645,
      "learning_rate": 0.00018138327352441223,
      "loss": 0.4275,
      "step": 127700
    },
    {
      "epoch": 0.3956987116570116,
      "grad_norm": 15.467829704284668,
      "learning_rate": 0.00018129038650289652,
      "loss": 0.5049,
      "step": 127800
    },
    {
      "epoch": 0.396008335062064,
      "grad_norm": 0.15692628920078278,
      "learning_rate": 0.00018119749948138078,
      "loss": 0.3871,
      "step": 127900
    },
    {
      "epoch": 0.3963179584671164,
      "grad_norm": 0.008865186013281345,
      "learning_rate": 0.00018110461245986504,
      "loss": 0.3482,
      "step": 128000
    },
    {
      "epoch": 0.3966275818721689,
      "grad_norm": 0.03982405737042427,
      "learning_rate": 0.00018101172543834933,
      "loss": 0.286,
      "step": 128100
    },
    {
      "epoch": 0.3969372052772213,
      "grad_norm": 32.010074615478516,
      "learning_rate": 0.0001809188384168336,
      "loss": 0.4411,
      "step": 128200
    },
    {
      "epoch": 0.39724682868227373,
      "grad_norm": 0.011149629019200802,
      "learning_rate": 0.00018082595139531785,
      "loss": 0.3492,
      "step": 128300
    },
    {
      "epoch": 0.3975564520873262,
      "grad_norm": 0.001589183579199016,
      "learning_rate": 0.00018073306437380214,
      "loss": 0.4317,
      "step": 128400
    },
    {
      "epoch": 0.3978660754923786,
      "grad_norm": 37.44545364379883,
      "learning_rate": 0.0001806401773522864,
      "loss": 0.3337,
      "step": 128500
    },
    {
      "epoch": 0.39817569889743104,
      "grad_norm": 0.026533812284469604,
      "learning_rate": 0.00018054729033077066,
      "loss": 0.2482,
      "step": 128600
    },
    {
      "epoch": 0.3984853223024835,
      "grad_norm": 0.0024911626242101192,
      "learning_rate": 0.00018045440330925495,
      "loss": 0.3653,
      "step": 128700
    },
    {
      "epoch": 0.39879494570753593,
      "grad_norm": 2.3624093532562256,
      "learning_rate": 0.0001803615162877392,
      "loss": 0.4285,
      "step": 128800
    },
    {
      "epoch": 0.39910456911258835,
      "grad_norm": 0.06446252018213272,
      "learning_rate": 0.00018026862926622347,
      "loss": 0.3998,
      "step": 128900
    },
    {
      "epoch": 0.3994141925176408,
      "grad_norm": 0.0017380556091666222,
      "learning_rate": 0.00018017574224470776,
      "loss": 0.3348,
      "step": 129000
    },
    {
      "epoch": 0.39972381592269324,
      "grad_norm": 0.002239156048744917,
      "learning_rate": 0.00018008285522319202,
      "loss": 0.587,
      "step": 129100
    },
    {
      "epoch": 0.40003343932774565,
      "grad_norm": 0.032996851950883865,
      "learning_rate": 0.00017998996820167628,
      "loss": 0.2724,
      "step": 129200
    },
    {
      "epoch": 0.40034306273279807,
      "grad_norm": 127.63233947753906,
      "learning_rate": 0.00017989708118016056,
      "loss": 0.4318,
      "step": 129300
    },
    {
      "epoch": 0.40065268613785054,
      "grad_norm": 0.0035141429398208857,
      "learning_rate": 0.00017980419415864482,
      "loss": 0.3274,
      "step": 129400
    },
    {
      "epoch": 0.40096230954290296,
      "grad_norm": 22.51018714904785,
      "learning_rate": 0.00017971130713712909,
      "loss": 0.4362,
      "step": 129500
    },
    {
      "epoch": 0.4012719329479554,
      "grad_norm": 0.00019825241179205477,
      "learning_rate": 0.00017961842011561337,
      "loss": 0.2563,
      "step": 129600
    },
    {
      "epoch": 0.40158155635300785,
      "grad_norm": 0.0011017289943993092,
      "learning_rate": 0.00017952553309409763,
      "loss": 0.353,
      "step": 129700
    },
    {
      "epoch": 0.40189117975806027,
      "grad_norm": 6.951376914978027,
      "learning_rate": 0.0001794326460725819,
      "loss": 0.3476,
      "step": 129800
    },
    {
      "epoch": 0.4022008031631127,
      "grad_norm": 0.0005061808042228222,
      "learning_rate": 0.00017933975905106618,
      "loss": 0.3608,
      "step": 129900
    },
    {
      "epoch": 0.40251042656816516,
      "grad_norm": 10.916986465454102,
      "learning_rate": 0.00017924687202955044,
      "loss": 0.373,
      "step": 130000
    },
    {
      "epoch": 0.4028200499732176,
      "grad_norm": 0.0019067684188485146,
      "learning_rate": 0.0001791539850080347,
      "loss": 0.3851,
      "step": 130100
    },
    {
      "epoch": 0.40312967337827,
      "grad_norm": 0.0787515938282013,
      "learning_rate": 0.000179061097986519,
      "loss": 0.4569,
      "step": 130200
    },
    {
      "epoch": 0.40343929678332247,
      "grad_norm": 0.0019596880301833153,
      "learning_rate": 0.00017896821096500325,
      "loss": 0.3526,
      "step": 130300
    },
    {
      "epoch": 0.4037489201883749,
      "grad_norm": 0.010544370859861374,
      "learning_rate": 0.00017887532394348754,
      "loss": 0.2513,
      "step": 130400
    },
    {
      "epoch": 0.4040585435934273,
      "grad_norm": 8.575221061706543,
      "learning_rate": 0.0001787824369219718,
      "loss": 0.4803,
      "step": 130500
    },
    {
      "epoch": 0.40436816699847977,
      "grad_norm": 0.00021593105338979512,
      "learning_rate": 0.00017868954990045606,
      "loss": 0.6069,
      "step": 130600
    },
    {
      "epoch": 0.4046777904035322,
      "grad_norm": 0.10882770270109177,
      "learning_rate": 0.00017859666287894035,
      "loss": 0.2147,
      "step": 130700
    },
    {
      "epoch": 0.4049874138085846,
      "grad_norm": 139.6846160888672,
      "learning_rate": 0.0001785037758574246,
      "loss": 0.4555,
      "step": 130800
    },
    {
      "epoch": 0.4052970372136371,
      "grad_norm": 0.0008581914589740336,
      "learning_rate": 0.00017841088883590887,
      "loss": 0.2467,
      "step": 130900
    },
    {
      "epoch": 0.4056066606186895,
      "grad_norm": 0.0517842173576355,
      "learning_rate": 0.00017831800181439315,
      "loss": 0.6098,
      "step": 131000
    },
    {
      "epoch": 0.4059162840237419,
      "grad_norm": 43.609718322753906,
      "learning_rate": 0.00017822511479287742,
      "loss": 0.5478,
      "step": 131100
    },
    {
      "epoch": 0.40622590742879433,
      "grad_norm": 0.012584302574396133,
      "learning_rate": 0.00017813222777136168,
      "loss": 0.3386,
      "step": 131200
    },
    {
      "epoch": 0.4065355308338468,
      "grad_norm": 0.0009046124177984893,
      "learning_rate": 0.00017803934074984596,
      "loss": 0.4394,
      "step": 131300
    },
    {
      "epoch": 0.4068451542388992,
      "grad_norm": 0.003865644568577409,
      "learning_rate": 0.00017794645372833022,
      "loss": 0.4725,
      "step": 131400
    },
    {
      "epoch": 0.40715477764395164,
      "grad_norm": 0.007200656458735466,
      "learning_rate": 0.00017785356670681448,
      "loss": 0.4258,
      "step": 131500
    },
    {
      "epoch": 0.4074644010490041,
      "grad_norm": 0.03613142669200897,
      "learning_rate": 0.00017776067968529877,
      "loss": 0.5095,
      "step": 131600
    },
    {
      "epoch": 0.4077740244540565,
      "grad_norm": 0.00032854650635272264,
      "learning_rate": 0.00017766779266378303,
      "loss": 0.3817,
      "step": 131700
    },
    {
      "epoch": 0.40808364785910894,
      "grad_norm": 0.539996862411499,
      "learning_rate": 0.0001775749056422673,
      "loss": 0.3252,
      "step": 131800
    },
    {
      "epoch": 0.4083932712641614,
      "grad_norm": 0.004186484031379223,
      "learning_rate": 0.00017748201862075158,
      "loss": 0.4407,
      "step": 131900
    },
    {
      "epoch": 0.40870289466921383,
      "grad_norm": 1.8607505559921265,
      "learning_rate": 0.00017738913159923584,
      "loss": 0.4153,
      "step": 132000
    },
    {
      "epoch": 0.40901251807426625,
      "grad_norm": 0.0032402300275862217,
      "learning_rate": 0.0001772962445777201,
      "loss": 0.3521,
      "step": 132100
    },
    {
      "epoch": 0.4093221414793187,
      "grad_norm": 0.0015256372280418873,
      "learning_rate": 0.0001772033575562044,
      "loss": 0.4902,
      "step": 132200
    },
    {
      "epoch": 0.40963176488437114,
      "grad_norm": 83.33967590332031,
      "learning_rate": 0.00017711047053468865,
      "loss": 0.24,
      "step": 132300
    },
    {
      "epoch": 0.40994138828942356,
      "grad_norm": 0.10891222208738327,
      "learning_rate": 0.0001770175835131729,
      "loss": 0.3979,
      "step": 132400
    },
    {
      "epoch": 0.41025101169447603,
      "grad_norm": 0.000418010022258386,
      "learning_rate": 0.0001769246964916572,
      "loss": 0.2356,
      "step": 132500
    },
    {
      "epoch": 0.41056063509952845,
      "grad_norm": 0.0007305098697543144,
      "learning_rate": 0.00017683180947014146,
      "loss": 0.535,
      "step": 132600
    },
    {
      "epoch": 0.41087025850458087,
      "grad_norm": 0.09174791723489761,
      "learning_rate": 0.00017673892244862572,
      "loss": 0.5471,
      "step": 132700
    },
    {
      "epoch": 0.41117988190963334,
      "grad_norm": 33.152442932128906,
      "learning_rate": 0.00017664603542711,
      "loss": 0.3911,
      "step": 132800
    },
    {
      "epoch": 0.41148950531468576,
      "grad_norm": 10.230374336242676,
      "learning_rate": 0.00017655314840559427,
      "loss": 0.4442,
      "step": 132900
    },
    {
      "epoch": 0.41179912871973817,
      "grad_norm": 0.0013714258093386889,
      "learning_rate": 0.00017646026138407853,
      "loss": 0.5642,
      "step": 133000
    },
    {
      "epoch": 0.41210875212479064,
      "grad_norm": 0.0030367912258952856,
      "learning_rate": 0.00017636737436256281,
      "loss": 0.4073,
      "step": 133100
    },
    {
      "epoch": 0.41241837552984306,
      "grad_norm": 19.043237686157227,
      "learning_rate": 0.00017627448734104707,
      "loss": 0.4137,
      "step": 133200
    },
    {
      "epoch": 0.4127279989348955,
      "grad_norm": 0.0016498262993991375,
      "learning_rate": 0.00017618160031953134,
      "loss": 0.3024,
      "step": 133300
    },
    {
      "epoch": 0.4130376223399479,
      "grad_norm": 0.000742637028452009,
      "learning_rate": 0.00017608871329801562,
      "loss": 0.4088,
      "step": 133400
    },
    {
      "epoch": 0.41334724574500037,
      "grad_norm": 1.0775729417800903,
      "learning_rate": 0.00017599582627649988,
      "loss": 0.4472,
      "step": 133500
    },
    {
      "epoch": 0.4136568691500528,
      "grad_norm": 0.045944757759571075,
      "learning_rate": 0.00017590293925498414,
      "loss": 0.509,
      "step": 133600
    },
    {
      "epoch": 0.4139664925551052,
      "grad_norm": 9.521345138549805,
      "learning_rate": 0.00017581005223346843,
      "loss": 0.2911,
      "step": 133700
    },
    {
      "epoch": 0.4142761159601577,
      "grad_norm": 0.0001763004984240979,
      "learning_rate": 0.0001757171652119527,
      "loss": 0.5449,
      "step": 133800
    },
    {
      "epoch": 0.4145857393652101,
      "grad_norm": 0.00048531952779740095,
      "learning_rate": 0.00017562427819043695,
      "loss": 0.4311,
      "step": 133900
    },
    {
      "epoch": 0.4148953627702625,
      "grad_norm": 82.1119155883789,
      "learning_rate": 0.00017553139116892124,
      "loss": 0.3138,
      "step": 134000
    },
    {
      "epoch": 0.415204986175315,
      "grad_norm": 122.81904602050781,
      "learning_rate": 0.0001754385041474055,
      "loss": 0.467,
      "step": 134100
    },
    {
      "epoch": 0.4155146095803674,
      "grad_norm": 0.026485148817300797,
      "learning_rate": 0.0001753456171258898,
      "loss": 0.4429,
      "step": 134200
    },
    {
      "epoch": 0.4158242329854198,
      "grad_norm": 0.002815054263919592,
      "learning_rate": 0.00017525273010437405,
      "loss": 0.2399,
      "step": 134300
    },
    {
      "epoch": 0.4161338563904723,
      "grad_norm": 0.5634592771530151,
      "learning_rate": 0.0001751598430828583,
      "loss": 0.4072,
      "step": 134400
    },
    {
      "epoch": 0.4164434797955247,
      "grad_norm": 25.83411979675293,
      "learning_rate": 0.0001750669560613426,
      "loss": 0.3185,
      "step": 134500
    },
    {
      "epoch": 0.4167531032005771,
      "grad_norm": 0.042836323380470276,
      "learning_rate": 0.00017497406903982686,
      "loss": 0.3388,
      "step": 134600
    },
    {
      "epoch": 0.4170627266056296,
      "grad_norm": 0.0014968387549743056,
      "learning_rate": 0.00017488118201831112,
      "loss": 0.3932,
      "step": 134700
    },
    {
      "epoch": 0.417372350010682,
      "grad_norm": 0.0024077235721051693,
      "learning_rate": 0.0001747882949967954,
      "loss": 0.3166,
      "step": 134800
    },
    {
      "epoch": 0.41768197341573443,
      "grad_norm": 0.014639354310929775,
      "learning_rate": 0.00017469540797527967,
      "loss": 0.3753,
      "step": 134900
    },
    {
      "epoch": 0.4179915968207869,
      "grad_norm": 0.003690344514325261,
      "learning_rate": 0.00017460252095376393,
      "loss": 0.427,
      "step": 135000
    },
    {
      "epoch": 0.4183012202258393,
      "grad_norm": 0.17434902489185333,
      "learning_rate": 0.0001745096339322482,
      "loss": 0.4011,
      "step": 135100
    },
    {
      "epoch": 0.41861084363089174,
      "grad_norm": 18.6712703704834,
      "learning_rate": 0.00017441674691073247,
      "loss": 0.4269,
      "step": 135200
    },
    {
      "epoch": 0.41892046703594416,
      "grad_norm": 51.38310241699219,
      "learning_rate": 0.00017432385988921673,
      "loss": 0.3137,
      "step": 135300
    },
    {
      "epoch": 0.41923009044099663,
      "grad_norm": 0.0009383525466546416,
      "learning_rate": 0.00017423097286770102,
      "loss": 0.325,
      "step": 135400
    },
    {
      "epoch": 0.41953971384604904,
      "grad_norm": 5.803954601287842,
      "learning_rate": 0.00017413808584618528,
      "loss": 0.2827,
      "step": 135500
    },
    {
      "epoch": 0.41984933725110146,
      "grad_norm": 0.0024697482585906982,
      "learning_rate": 0.00017404519882466954,
      "loss": 0.3141,
      "step": 135600
    },
    {
      "epoch": 0.42015896065615393,
      "grad_norm": 0.002631962997838855,
      "learning_rate": 0.00017395231180315383,
      "loss": 0.3266,
      "step": 135700
    },
    {
      "epoch": 0.42046858406120635,
      "grad_norm": 34.2080192565918,
      "learning_rate": 0.0001738594247816381,
      "loss": 0.3492,
      "step": 135800
    },
    {
      "epoch": 0.42077820746625877,
      "grad_norm": 0.009347108192741871,
      "learning_rate": 0.00017376653776012235,
      "loss": 0.2946,
      "step": 135900
    },
    {
      "epoch": 0.42108783087131124,
      "grad_norm": 0.11709611862897873,
      "learning_rate": 0.00017367365073860664,
      "loss": 0.3143,
      "step": 136000
    },
    {
      "epoch": 0.42139745427636366,
      "grad_norm": 0.0018495699623599648,
      "learning_rate": 0.0001735807637170909,
      "loss": 0.2961,
      "step": 136100
    },
    {
      "epoch": 0.4217070776814161,
      "grad_norm": 0.00866001844406128,
      "learning_rate": 0.00017348787669557513,
      "loss": 0.4128,
      "step": 136200
    },
    {
      "epoch": 0.42201670108646855,
      "grad_norm": 7.462274074554443,
      "learning_rate": 0.00017339498967405945,
      "loss": 0.3984,
      "step": 136300
    },
    {
      "epoch": 0.42232632449152097,
      "grad_norm": 0.010259483940899372,
      "learning_rate": 0.0001733021026525437,
      "loss": 0.5,
      "step": 136400
    },
    {
      "epoch": 0.4226359478965734,
      "grad_norm": 0.0007501162472181022,
      "learning_rate": 0.00017320921563102794,
      "loss": 0.2474,
      "step": 136500
    },
    {
      "epoch": 0.42294557130162586,
      "grad_norm": 151.48008728027344,
      "learning_rate": 0.00017311632860951226,
      "loss": 0.2832,
      "step": 136600
    },
    {
      "epoch": 0.4232551947066783,
      "grad_norm": 0.0016578224021941423,
      "learning_rate": 0.00017302344158799652,
      "loss": 0.4398,
      "step": 136700
    },
    {
      "epoch": 0.4235648181117307,
      "grad_norm": 11.084966659545898,
      "learning_rate": 0.00017293055456648075,
      "loss": 0.5563,
      "step": 136800
    },
    {
      "epoch": 0.42387444151678316,
      "grad_norm": 0.007463349495083094,
      "learning_rate": 0.00017283766754496506,
      "loss": 0.3727,
      "step": 136900
    },
    {
      "epoch": 0.4241840649218356,
      "grad_norm": 0.0016499940538778901,
      "learning_rate": 0.00017274478052344932,
      "loss": 0.3977,
      "step": 137000
    },
    {
      "epoch": 0.424493688326888,
      "grad_norm": 0.9697467088699341,
      "learning_rate": 0.00017265189350193356,
      "loss": 0.2714,
      "step": 137100
    },
    {
      "epoch": 0.4248033117319404,
      "grad_norm": 0.013014761731028557,
      "learning_rate": 0.00017255900648041787,
      "loss": 0.4322,
      "step": 137200
    },
    {
      "epoch": 0.4251129351369929,
      "grad_norm": 57.08792495727539,
      "learning_rate": 0.0001724661194589021,
      "loss": 0.4053,
      "step": 137300
    },
    {
      "epoch": 0.4254225585420453,
      "grad_norm": 0.0025577587075531483,
      "learning_rate": 0.00017237323243738637,
      "loss": 0.3265,
      "step": 137400
    },
    {
      "epoch": 0.4257321819470977,
      "grad_norm": 62.355587005615234,
      "learning_rate": 0.00017228034541587068,
      "loss": 0.3934,
      "step": 137500
    },
    {
      "epoch": 0.4260418053521502,
      "grad_norm": 0.00013048107211943716,
      "learning_rate": 0.00017218745839435491,
      "loss": 0.2967,
      "step": 137600
    },
    {
      "epoch": 0.4263514287572026,
      "grad_norm": 116.16068267822266,
      "learning_rate": 0.00017209457137283917,
      "loss": 0.3621,
      "step": 137700
    },
    {
      "epoch": 0.42666105216225503,
      "grad_norm": 18.36283302307129,
      "learning_rate": 0.0001720016843513235,
      "loss": 0.2021,
      "step": 137800
    },
    {
      "epoch": 0.4269706755673075,
      "grad_norm": 0.00025739811826497316,
      "learning_rate": 0.00017190879732980772,
      "loss": 0.3073,
      "step": 137900
    },
    {
      "epoch": 0.4272802989723599,
      "grad_norm": 0.044116925448179245,
      "learning_rate": 0.00017181591030829198,
      "loss": 0.2929,
      "step": 138000
    },
    {
      "epoch": 0.42758992237741233,
      "grad_norm": 0.0032940220553427935,
      "learning_rate": 0.0001717230232867763,
      "loss": 0.3535,
      "step": 138100
    },
    {
      "epoch": 0.4278995457824648,
      "grad_norm": 0.007237524725496769,
      "learning_rate": 0.00017163013626526053,
      "loss": 0.2502,
      "step": 138200
    },
    {
      "epoch": 0.4282091691875172,
      "grad_norm": 0.0012815955560654402,
      "learning_rate": 0.00017153724924374485,
      "loss": 0.4973,
      "step": 138300
    },
    {
      "epoch": 0.42851879259256964,
      "grad_norm": 0.0018531096866354346,
      "learning_rate": 0.00017144436222222908,
      "loss": 0.2989,
      "step": 138400
    },
    {
      "epoch": 0.4288284159976221,
      "grad_norm": 0.00220690225251019,
      "learning_rate": 0.00017135147520071334,
      "loss": 0.5362,
      "step": 138500
    },
    {
      "epoch": 0.42913803940267453,
      "grad_norm": 0.0028946232050657272,
      "learning_rate": 0.00017125858817919765,
      "loss": 0.3271,
      "step": 138600
    },
    {
      "epoch": 0.42944766280772695,
      "grad_norm": 0.0005610367516055703,
      "learning_rate": 0.0001711657011576819,
      "loss": 0.2954,
      "step": 138700
    },
    {
      "epoch": 0.4297572862127794,
      "grad_norm": 0.003796758595854044,
      "learning_rate": 0.00017107281413616615,
      "loss": 0.4243,
      "step": 138800
    },
    {
      "epoch": 0.43006690961783184,
      "grad_norm": 0.001955052139237523,
      "learning_rate": 0.00017097992711465046,
      "loss": 0.5311,
      "step": 138900
    },
    {
      "epoch": 0.43037653302288426,
      "grad_norm": 8.709885597229004,
      "learning_rate": 0.0001708870400931347,
      "loss": 0.3823,
      "step": 139000
    },
    {
      "epoch": 0.43068615642793673,
      "grad_norm": 0.0002142593584721908,
      "learning_rate": 0.00017079415307161896,
      "loss": 0.4602,
      "step": 139100
    },
    {
      "epoch": 0.43099577983298915,
      "grad_norm": 86.63821411132812,
      "learning_rate": 0.00017070126605010327,
      "loss": 0.4914,
      "step": 139200
    },
    {
      "epoch": 0.43130540323804156,
      "grad_norm": 0.0021303598769009113,
      "learning_rate": 0.0001706083790285875,
      "loss": 0.4234,
      "step": 139300
    },
    {
      "epoch": 0.431615026643094,
      "grad_norm": 0.002310197101905942,
      "learning_rate": 0.00017051549200707177,
      "loss": 0.2763,
      "step": 139400
    },
    {
      "epoch": 0.43192465004814645,
      "grad_norm": 0.0016848992090672255,
      "learning_rate": 0.00017042260498555605,
      "loss": 0.2453,
      "step": 139500
    },
    {
      "epoch": 0.43223427345319887,
      "grad_norm": 21.856979370117188,
      "learning_rate": 0.0001703297179640403,
      "loss": 0.3044,
      "step": 139600
    },
    {
      "epoch": 0.4325438968582513,
      "grad_norm": 0.00012063526810379699,
      "learning_rate": 0.00017023683094252457,
      "loss": 0.4315,
      "step": 139700
    },
    {
      "epoch": 0.43285352026330376,
      "grad_norm": 0.00012215420429129153,
      "learning_rate": 0.00017014394392100886,
      "loss": 0.2484,
      "step": 139800
    },
    {
      "epoch": 0.4331631436683562,
      "grad_norm": 0.044577088207006454,
      "learning_rate": 0.00017005105689949312,
      "loss": 0.3779,
      "step": 139900
    },
    {
      "epoch": 0.4334727670734086,
      "grad_norm": 0.00294869183562696,
      "learning_rate": 0.00016995816987797738,
      "loss": 0.3359,
      "step": 140000
    },
    {
      "epoch": 0.43378239047846107,
      "grad_norm": 28.90298080444336,
      "learning_rate": 0.00016986528285646167,
      "loss": 0.4308,
      "step": 140100
    },
    {
      "epoch": 0.4340920138835135,
      "grad_norm": 0.0018464243039488792,
      "learning_rate": 0.00016977239583494593,
      "loss": 0.4123,
      "step": 140200
    },
    {
      "epoch": 0.4344016372885659,
      "grad_norm": 0.005342813208699226,
      "learning_rate": 0.0001696795088134302,
      "loss": 0.4609,
      "step": 140300
    },
    {
      "epoch": 0.4347112606936184,
      "grad_norm": 0.04677741229534149,
      "learning_rate": 0.00016958662179191448,
      "loss": 0.3868,
      "step": 140400
    },
    {
      "epoch": 0.4350208840986708,
      "grad_norm": 0.004683809820562601,
      "learning_rate": 0.00016949373477039874,
      "loss": 0.3113,
      "step": 140500
    },
    {
      "epoch": 0.4353305075037232,
      "grad_norm": 1.3114433288574219,
      "learning_rate": 0.000169400847748883,
      "loss": 0.4766,
      "step": 140600
    },
    {
      "epoch": 0.4356401309087757,
      "grad_norm": 0.000800823443569243,
      "learning_rate": 0.0001693079607273673,
      "loss": 0.4979,
      "step": 140700
    },
    {
      "epoch": 0.4359497543138281,
      "grad_norm": 0.00039465067675337195,
      "learning_rate": 0.00016921507370585155,
      "loss": 0.3412,
      "step": 140800
    },
    {
      "epoch": 0.4362593777188805,
      "grad_norm": 0.0013649342581629753,
      "learning_rate": 0.0001691221866843358,
      "loss": 0.4689,
      "step": 140900
    },
    {
      "epoch": 0.436569001123933,
      "grad_norm": 0.0015690699219703674,
      "learning_rate": 0.0001690292996628201,
      "loss": 0.2656,
      "step": 141000
    },
    {
      "epoch": 0.4368786245289854,
      "grad_norm": 13.39607048034668,
      "learning_rate": 0.00016893641264130436,
      "loss": 0.3746,
      "step": 141100
    },
    {
      "epoch": 0.4371882479340378,
      "grad_norm": 0.007332446053624153,
      "learning_rate": 0.00016884352561978862,
      "loss": 0.4138,
      "step": 141200
    },
    {
      "epoch": 0.43749787133909024,
      "grad_norm": 0.0011558172991499305,
      "learning_rate": 0.0001687506385982729,
      "loss": 0.5259,
      "step": 141300
    },
    {
      "epoch": 0.4378074947441427,
      "grad_norm": 0.04083353653550148,
      "learning_rate": 0.00016865775157675716,
      "loss": 0.2843,
      "step": 141400
    },
    {
      "epoch": 0.43811711814919513,
      "grad_norm": 0.013771031983196735,
      "learning_rate": 0.00016856486455524142,
      "loss": 0.4086,
      "step": 141500
    },
    {
      "epoch": 0.43842674155424755,
      "grad_norm": 68.88553619384766,
      "learning_rate": 0.0001684719775337257,
      "loss": 0.3895,
      "step": 141600
    },
    {
      "epoch": 0.4387363649593,
      "grad_norm": 0.001317093730904162,
      "learning_rate": 0.00016837909051220997,
      "loss": 0.3611,
      "step": 141700
    },
    {
      "epoch": 0.43904598836435244,
      "grad_norm": 16.675914764404297,
      "learning_rate": 0.00016828620349069423,
      "loss": 0.3064,
      "step": 141800
    },
    {
      "epoch": 0.43935561176940485,
      "grad_norm": 25.08238410949707,
      "learning_rate": 0.00016819331646917852,
      "loss": 0.4076,
      "step": 141900
    },
    {
      "epoch": 0.4396652351744573,
      "grad_norm": 0.0008908071322366595,
      "learning_rate": 0.00016810042944766278,
      "loss": 0.4082,
      "step": 142000
    },
    {
      "epoch": 0.43997485857950974,
      "grad_norm": 0.005310636013746262,
      "learning_rate": 0.00016800754242614704,
      "loss": 0.284,
      "step": 142100
    },
    {
      "epoch": 0.44028448198456216,
      "grad_norm": 0.003637834917753935,
      "learning_rate": 0.00016791465540463133,
      "loss": 0.2584,
      "step": 142200
    },
    {
      "epoch": 0.44059410538961463,
      "grad_norm": 0.0029674237594008446,
      "learning_rate": 0.0001678217683831156,
      "loss": 0.4075,
      "step": 142300
    },
    {
      "epoch": 0.44090372879466705,
      "grad_norm": 0.002958619501441717,
      "learning_rate": 0.00016772888136159988,
      "loss": 0.5104,
      "step": 142400
    },
    {
      "epoch": 0.44121335219971947,
      "grad_norm": 0.005136623978614807,
      "learning_rate": 0.00016763599434008414,
      "loss": 0.3779,
      "step": 142500
    },
    {
      "epoch": 0.44152297560477194,
      "grad_norm": 0.001085642958059907,
      "learning_rate": 0.0001675431073185684,
      "loss": 0.3695,
      "step": 142600
    },
    {
      "epoch": 0.44183259900982436,
      "grad_norm": 0.003371952334418893,
      "learning_rate": 0.00016745022029705269,
      "loss": 0.2538,
      "step": 142700
    },
    {
      "epoch": 0.4421422224148768,
      "grad_norm": 33.80450439453125,
      "learning_rate": 0.00016735733327553695,
      "loss": 0.3431,
      "step": 142800
    },
    {
      "epoch": 0.44245184581992925,
      "grad_norm": 0.0009942323667928576,
      "learning_rate": 0.0001672644462540212,
      "loss": 0.3157,
      "step": 142900
    },
    {
      "epoch": 0.44276146922498166,
      "grad_norm": 0.0004696157411672175,
      "learning_rate": 0.0001671715592325055,
      "loss": 0.3894,
      "step": 143000
    },
    {
      "epoch": 0.4430710926300341,
      "grad_norm": 0.0024167916271835566,
      "learning_rate": 0.00016707867221098975,
      "loss": 0.4778,
      "step": 143100
    },
    {
      "epoch": 0.4433807160350865,
      "grad_norm": 0.0010226945159956813,
      "learning_rate": 0.00016698578518947401,
      "loss": 0.4131,
      "step": 143200
    },
    {
      "epoch": 0.44369033944013897,
      "grad_norm": 20.67059898376465,
      "learning_rate": 0.0001668928981679583,
      "loss": 0.3088,
      "step": 143300
    },
    {
      "epoch": 0.4439999628451914,
      "grad_norm": 0.0011137605179101229,
      "learning_rate": 0.00016680001114644256,
      "loss": 0.3493,
      "step": 143400
    },
    {
      "epoch": 0.4443095862502438,
      "grad_norm": 0.0013958689523860812,
      "learning_rate": 0.00016670712412492682,
      "loss": 0.3204,
      "step": 143500
    },
    {
      "epoch": 0.4446192096552963,
      "grad_norm": 0.004705012310296297,
      "learning_rate": 0.0001666142371034111,
      "loss": 0.2418,
      "step": 143600
    },
    {
      "epoch": 0.4449288330603487,
      "grad_norm": 0.06699372082948685,
      "learning_rate": 0.00016652135008189537,
      "loss": 0.2249,
      "step": 143700
    },
    {
      "epoch": 0.4452384564654011,
      "grad_norm": 0.00114833761472255,
      "learning_rate": 0.00016642846306037963,
      "loss": 0.3574,
      "step": 143800
    },
    {
      "epoch": 0.4455480798704536,
      "grad_norm": 166.0990447998047,
      "learning_rate": 0.00016633557603886392,
      "loss": 0.3235,
      "step": 143900
    },
    {
      "epoch": 0.445857703275506,
      "grad_norm": 0.0008492517517879605,
      "learning_rate": 0.00016624268901734818,
      "loss": 0.2309,
      "step": 144000
    },
    {
      "epoch": 0.4461673266805584,
      "grad_norm": 0.00015405466547235847,
      "learning_rate": 0.00016614980199583244,
      "loss": 0.2874,
      "step": 144100
    },
    {
      "epoch": 0.4464769500856109,
      "grad_norm": 0.01964501291513443,
      "learning_rate": 0.00016605691497431673,
      "loss": 0.2767,
      "step": 144200
    },
    {
      "epoch": 0.4467865734906633,
      "grad_norm": 24.207338333129883,
      "learning_rate": 0.000165964027952801,
      "loss": 0.374,
      "step": 144300
    },
    {
      "epoch": 0.4470961968957157,
      "grad_norm": 0.07547198235988617,
      "learning_rate": 0.00016587114093128525,
      "loss": 0.34,
      "step": 144400
    },
    {
      "epoch": 0.4474058203007682,
      "grad_norm": 0.21493516862392426,
      "learning_rate": 0.00016577825390976954,
      "loss": 0.253,
      "step": 144500
    },
    {
      "epoch": 0.4477154437058206,
      "grad_norm": 0.015736287459731102,
      "learning_rate": 0.0001656853668882538,
      "loss": 0.5816,
      "step": 144600
    },
    {
      "epoch": 0.44802506711087303,
      "grad_norm": 2.672215700149536,
      "learning_rate": 0.00016559247986673806,
      "loss": 0.4215,
      "step": 144700
    },
    {
      "epoch": 0.4483346905159255,
      "grad_norm": 0.061675671488046646,
      "learning_rate": 0.00016549959284522234,
      "loss": 0.3614,
      "step": 144800
    },
    {
      "epoch": 0.4486443139209779,
      "grad_norm": 0.006305343471467495,
      "learning_rate": 0.0001654067058237066,
      "loss": 0.2896,
      "step": 144900
    },
    {
      "epoch": 0.44895393732603034,
      "grad_norm": 122.3914566040039,
      "learning_rate": 0.00016531381880219087,
      "loss": 0.3385,
      "step": 145000
    },
    {
      "epoch": 0.4492635607310828,
      "grad_norm": 12.422801971435547,
      "learning_rate": 0.00016522093178067515,
      "loss": 0.2714,
      "step": 145100
    },
    {
      "epoch": 0.44957318413613523,
      "grad_norm": 0.0028567123226821423,
      "learning_rate": 0.00016512804475915941,
      "loss": 0.31,
      "step": 145200
    },
    {
      "epoch": 0.44988280754118765,
      "grad_norm": 3.221782922744751,
      "learning_rate": 0.00016503515773764367,
      "loss": 0.3343,
      "step": 145300
    },
    {
      "epoch": 0.45019243094624006,
      "grad_norm": 0.0009058452560566366,
      "learning_rate": 0.00016494227071612796,
      "loss": 0.4688,
      "step": 145400
    },
    {
      "epoch": 0.45050205435129254,
      "grad_norm": 127.41818237304688,
      "learning_rate": 0.00016484938369461222,
      "loss": 0.5311,
      "step": 145500
    },
    {
      "epoch": 0.45081167775634495,
      "grad_norm": 0.0003305379650555551,
      "learning_rate": 0.00016475649667309648,
      "loss": 0.4535,
      "step": 145600
    },
    {
      "epoch": 0.45112130116139737,
      "grad_norm": 18.16562271118164,
      "learning_rate": 0.00016466360965158077,
      "loss": 0.362,
      "step": 145700
    },
    {
      "epoch": 0.45143092456644984,
      "grad_norm": 24.552997589111328,
      "learning_rate": 0.00016457072263006503,
      "loss": 0.5401,
      "step": 145800
    },
    {
      "epoch": 0.45174054797150226,
      "grad_norm": 95.43956756591797,
      "learning_rate": 0.0001644778356085493,
      "loss": 0.2275,
      "step": 145900
    },
    {
      "epoch": 0.4520501713765547,
      "grad_norm": 0.02353430911898613,
      "learning_rate": 0.00016438494858703358,
      "loss": 0.2968,
      "step": 146000
    },
    {
      "epoch": 0.45235979478160715,
      "grad_norm": 36.89574432373047,
      "learning_rate": 0.00016429206156551784,
      "loss": 0.2987,
      "step": 146100
    },
    {
      "epoch": 0.45266941818665957,
      "grad_norm": 0.0005502088461071253,
      "learning_rate": 0.00016419917454400213,
      "loss": 0.2379,
      "step": 146200
    },
    {
      "epoch": 0.452979041591712,
      "grad_norm": 12.287332534790039,
      "learning_rate": 0.0001641062875224864,
      "loss": 0.3149,
      "step": 146300
    },
    {
      "epoch": 0.45328866499676446,
      "grad_norm": 0.0006845367606729269,
      "learning_rate": 0.00016401340050097065,
      "loss": 0.2723,
      "step": 146400
    },
    {
      "epoch": 0.4535982884018169,
      "grad_norm": 0.0001353076659142971,
      "learning_rate": 0.00016392051347945494,
      "loss": 0.3944,
      "step": 146500
    },
    {
      "epoch": 0.4539079118068693,
      "grad_norm": 0.31511637568473816,
      "learning_rate": 0.0001638276264579392,
      "loss": 0.4214,
      "step": 146600
    },
    {
      "epoch": 0.45421753521192176,
      "grad_norm": 0.00488023879006505,
      "learning_rate": 0.00016373473943642346,
      "loss": 0.2902,
      "step": 146700
    },
    {
      "epoch": 0.4545271586169742,
      "grad_norm": 0.0014402286615222692,
      "learning_rate": 0.00016364185241490774,
      "loss": 0.3933,
      "step": 146800
    },
    {
      "epoch": 0.4548367820220266,
      "grad_norm": 9.011557267513126e-05,
      "learning_rate": 0.000163548965393392,
      "loss": 0.4955,
      "step": 146900
    },
    {
      "epoch": 0.45514640542707907,
      "grad_norm": 0.00022772957163397223,
      "learning_rate": 0.00016345607837187626,
      "loss": 0.326,
      "step": 147000
    },
    {
      "epoch": 0.4554560288321315,
      "grad_norm": 69.67086029052734,
      "learning_rate": 0.00016336319135036055,
      "loss": 0.5434,
      "step": 147100
    },
    {
      "epoch": 0.4557656522371839,
      "grad_norm": 0.1835843175649643,
      "learning_rate": 0.0001632703043288448,
      "loss": 0.3623,
      "step": 147200
    },
    {
      "epoch": 0.4560752756422363,
      "grad_norm": 7.203486919403076,
      "learning_rate": 0.00016317741730732907,
      "loss": 0.2327,
      "step": 147300
    },
    {
      "epoch": 0.4563848990472888,
      "grad_norm": 0.002124307444319129,
      "learning_rate": 0.00016308453028581336,
      "loss": 0.3634,
      "step": 147400
    },
    {
      "epoch": 0.4566945224523412,
      "grad_norm": 0.00010064394155051559,
      "learning_rate": 0.00016299164326429762,
      "loss": 0.3169,
      "step": 147500
    },
    {
      "epoch": 0.45700414585739363,
      "grad_norm": 0.004394047427922487,
      "learning_rate": 0.00016289875624278188,
      "loss": 0.4299,
      "step": 147600
    },
    {
      "epoch": 0.4573137692624461,
      "grad_norm": 0.005405695177614689,
      "learning_rate": 0.00016280586922126617,
      "loss": 0.2197,
      "step": 147700
    },
    {
      "epoch": 0.4576233926674985,
      "grad_norm": 0.0038759007584303617,
      "learning_rate": 0.00016271298219975043,
      "loss": 0.3454,
      "step": 147800
    },
    {
      "epoch": 0.45793301607255094,
      "grad_norm": 0.0006303720874711871,
      "learning_rate": 0.0001626200951782347,
      "loss": 0.3819,
      "step": 147900
    },
    {
      "epoch": 0.4582426394776034,
      "grad_norm": 0.02207915484905243,
      "learning_rate": 0.00016252720815671898,
      "loss": 0.5603,
      "step": 148000
    },
    {
      "epoch": 0.4585522628826558,
      "grad_norm": 0.0616229772567749,
      "learning_rate": 0.00016243432113520324,
      "loss": 0.348,
      "step": 148100
    },
    {
      "epoch": 0.45886188628770824,
      "grad_norm": 0.0022099411580711603,
      "learning_rate": 0.0001623414341136875,
      "loss": 0.2537,
      "step": 148200
    },
    {
      "epoch": 0.4591715096927607,
      "grad_norm": 0.006154417991638184,
      "learning_rate": 0.00016224854709217179,
      "loss": 0.2982,
      "step": 148300
    },
    {
      "epoch": 0.45948113309781313,
      "grad_norm": 0.030738288536667824,
      "learning_rate": 0.00016215566007065605,
      "loss": 0.344,
      "step": 148400
    },
    {
      "epoch": 0.45979075650286555,
      "grad_norm": 1.446190595626831,
      "learning_rate": 0.0001620627730491403,
      "loss": 0.4621,
      "step": 148500
    },
    {
      "epoch": 0.460100379907918,
      "grad_norm": 0.007519865874201059,
      "learning_rate": 0.0001619698860276246,
      "loss": 0.4044,
      "step": 148600
    },
    {
      "epoch": 0.46041000331297044,
      "grad_norm": 55.66908264160156,
      "learning_rate": 0.00016187699900610886,
      "loss": 0.5724,
      "step": 148700
    },
    {
      "epoch": 0.46071962671802286,
      "grad_norm": 0.005532220005989075,
      "learning_rate": 0.00016178411198459312,
      "loss": 0.2766,
      "step": 148800
    },
    {
      "epoch": 0.46102925012307533,
      "grad_norm": 0.36967089772224426,
      "learning_rate": 0.0001616912249630774,
      "loss": 0.2756,
      "step": 148900
    },
    {
      "epoch": 0.46133887352812775,
      "grad_norm": 0.038026198744773865,
      "learning_rate": 0.00016159833794156166,
      "loss": 0.3231,
      "step": 149000
    },
    {
      "epoch": 0.46164849693318016,
      "grad_norm": 0.007580948527902365,
      "learning_rate": 0.00016150545092004592,
      "loss": 0.3866,
      "step": 149100
    },
    {
      "epoch": 0.4619581203382326,
      "grad_norm": 10.986893653869629,
      "learning_rate": 0.0001614125638985302,
      "loss": 0.4876,
      "step": 149200
    },
    {
      "epoch": 0.46226774374328505,
      "grad_norm": 0.018074553459882736,
      "learning_rate": 0.00016131967687701447,
      "loss": 0.1861,
      "step": 149300
    },
    {
      "epoch": 0.46257736714833747,
      "grad_norm": 0.0014544022269546986,
      "learning_rate": 0.00016122678985549873,
      "loss": 0.2179,
      "step": 149400
    },
    {
      "epoch": 0.4628869905533899,
      "grad_norm": 0.002541876398026943,
      "learning_rate": 0.00016113390283398302,
      "loss": 0.2963,
      "step": 149500
    },
    {
      "epoch": 0.46319661395844236,
      "grad_norm": 24.440187454223633,
      "learning_rate": 0.00016104101581246728,
      "loss": 0.2558,
      "step": 149600
    },
    {
      "epoch": 0.4635062373634948,
      "grad_norm": 0.15892909467220306,
      "learning_rate": 0.00016094812879095154,
      "loss": 0.4275,
      "step": 149700
    },
    {
      "epoch": 0.4638158607685472,
      "grad_norm": 0.0001930315775098279,
      "learning_rate": 0.00016085524176943583,
      "loss": 0.4233,
      "step": 149800
    },
    {
      "epoch": 0.46412548417359967,
      "grad_norm": 0.031943339854478836,
      "learning_rate": 0.0001607623547479201,
      "loss": 0.3886,
      "step": 149900
    },
    {
      "epoch": 0.4644351075786521,
      "grad_norm": 23.002838134765625,
      "learning_rate": 0.00016066946772640435,
      "loss": 0.2339,
      "step": 150000
    },
    {
      "epoch": 0.4647447309837045,
      "grad_norm": 0.0008327066898345947,
      "learning_rate": 0.00016057658070488864,
      "loss": 0.2171,
      "step": 150100
    },
    {
      "epoch": 0.465054354388757,
      "grad_norm": 0.00765126571059227,
      "learning_rate": 0.0001604836936833729,
      "loss": 0.3405,
      "step": 150200
    },
    {
      "epoch": 0.4653639777938094,
      "grad_norm": 10.214447975158691,
      "learning_rate": 0.00016039080666185719,
      "loss": 0.3139,
      "step": 150300
    },
    {
      "epoch": 0.4656736011988618,
      "grad_norm": 0.0001946738047990948,
      "learning_rate": 0.00016029791964034145,
      "loss": 0.2459,
      "step": 150400
    },
    {
      "epoch": 0.4659832246039143,
      "grad_norm": 0.002983503509312868,
      "learning_rate": 0.0001602050326188257,
      "loss": 0.3833,
      "step": 150500
    },
    {
      "epoch": 0.4662928480089667,
      "grad_norm": 0.03072242997586727,
      "learning_rate": 0.00016011214559731,
      "loss": 0.2905,
      "step": 150600
    },
    {
      "epoch": 0.4666024714140191,
      "grad_norm": 1.7092005014419556,
      "learning_rate": 0.00016001925857579425,
      "loss": 0.317,
      "step": 150700
    },
    {
      "epoch": 0.4669120948190716,
      "grad_norm": 0.0010196908842772245,
      "learning_rate": 0.00015992637155427851,
      "loss": 0.3354,
      "step": 150800
    },
    {
      "epoch": 0.467221718224124,
      "grad_norm": 0.0006204968667589128,
      "learning_rate": 0.0001598334845327628,
      "loss": 0.245,
      "step": 150900
    },
    {
      "epoch": 0.4675313416291764,
      "grad_norm": 0.0009612419526092708,
      "learning_rate": 0.00015974059751124706,
      "loss": 0.3813,
      "step": 151000
    },
    {
      "epoch": 0.4678409650342289,
      "grad_norm": 14.696596145629883,
      "learning_rate": 0.00015964771048973132,
      "loss": 0.3405,
      "step": 151100
    },
    {
      "epoch": 0.4681505884392813,
      "grad_norm": 0.0021608967799693346,
      "learning_rate": 0.0001595548234682156,
      "loss": 0.3589,
      "step": 151200
    },
    {
      "epoch": 0.46846021184433373,
      "grad_norm": 0.0003647478006314486,
      "learning_rate": 0.00015946193644669987,
      "loss": 0.3221,
      "step": 151300
    },
    {
      "epoch": 0.46876983524938615,
      "grad_norm": 0.0022249068133533,
      "learning_rate": 0.00015936904942518413,
      "loss": 0.3137,
      "step": 151400
    },
    {
      "epoch": 0.4690794586544386,
      "grad_norm": 0.1206553503870964,
      "learning_rate": 0.00015927616240366842,
      "loss": 0.3759,
      "step": 151500
    },
    {
      "epoch": 0.46938908205949104,
      "grad_norm": 1.3017834424972534,
      "learning_rate": 0.00015918327538215268,
      "loss": 0.4523,
      "step": 151600
    },
    {
      "epoch": 0.46969870546454345,
      "grad_norm": 0.044195372611284256,
      "learning_rate": 0.00015909038836063694,
      "loss": 0.2726,
      "step": 151700
    },
    {
      "epoch": 0.4700083288695959,
      "grad_norm": 0.7546112537384033,
      "learning_rate": 0.00015899750133912123,
      "loss": 0.3811,
      "step": 151800
    },
    {
      "epoch": 0.47031795227464834,
      "grad_norm": 0.0004848651879001409,
      "learning_rate": 0.0001589046143176055,
      "loss": 0.3689,
      "step": 151900
    },
    {
      "epoch": 0.47062757567970076,
      "grad_norm": 0.9803758263587952,
      "learning_rate": 0.00015881172729608975,
      "loss": 0.564,
      "step": 152000
    },
    {
      "epoch": 0.47093719908475323,
      "grad_norm": 0.08627232164144516,
      "learning_rate": 0.00015871884027457404,
      "loss": 0.5113,
      "step": 152100
    },
    {
      "epoch": 0.47124682248980565,
      "grad_norm": 0.04341806471347809,
      "learning_rate": 0.0001586259532530583,
      "loss": 0.3509,
      "step": 152200
    },
    {
      "epoch": 0.47155644589485807,
      "grad_norm": 0.029334206134080887,
      "learning_rate": 0.00015853306623154256,
      "loss": 0.2281,
      "step": 152300
    },
    {
      "epoch": 0.47186606929991054,
      "grad_norm": 0.0033205801155418158,
      "learning_rate": 0.00015844017921002684,
      "loss": 0.384,
      "step": 152400
    },
    {
      "epoch": 0.47217569270496296,
      "grad_norm": 0.008475253358483315,
      "learning_rate": 0.0001583472921885111,
      "loss": 0.3313,
      "step": 152500
    },
    {
      "epoch": 0.4724853161100154,
      "grad_norm": 0.05977330729365349,
      "learning_rate": 0.00015825440516699537,
      "loss": 0.3461,
      "step": 152600
    },
    {
      "epoch": 0.47279493951506785,
      "grad_norm": 174.93943786621094,
      "learning_rate": 0.00015816151814547965,
      "loss": 0.2757,
      "step": 152700
    },
    {
      "epoch": 0.47310456292012026,
      "grad_norm": 0.002477011876180768,
      "learning_rate": 0.0001580686311239639,
      "loss": 0.401,
      "step": 152800
    },
    {
      "epoch": 0.4734141863251727,
      "grad_norm": 0.013789336197078228,
      "learning_rate": 0.00015797574410244817,
      "loss": 0.374,
      "step": 152900
    },
    {
      "epoch": 0.47372380973022515,
      "grad_norm": 0.06595507264137268,
      "learning_rate": 0.00015788285708093246,
      "loss": 0.2505,
      "step": 153000
    },
    {
      "epoch": 0.47403343313527757,
      "grad_norm": 0.0008465113933198154,
      "learning_rate": 0.00015778997005941672,
      "loss": 0.2294,
      "step": 153100
    },
    {
      "epoch": 0.47434305654033,
      "grad_norm": 0.010252970270812511,
      "learning_rate": 0.00015769708303790098,
      "loss": 0.4946,
      "step": 153200
    },
    {
      "epoch": 0.4746526799453824,
      "grad_norm": 0.003080738242715597,
      "learning_rate": 0.00015760419601638527,
      "loss": 0.395,
      "step": 153300
    },
    {
      "epoch": 0.4749623033504349,
      "grad_norm": 0.0037494278512895107,
      "learning_rate": 0.00015751130899486953,
      "loss": 0.3089,
      "step": 153400
    },
    {
      "epoch": 0.4752719267554873,
      "grad_norm": 0.5008969306945801,
      "learning_rate": 0.0001574184219733538,
      "loss": 0.2844,
      "step": 153500
    },
    {
      "epoch": 0.4755815501605397,
      "grad_norm": 0.0007310092332772911,
      "learning_rate": 0.00015732553495183808,
      "loss": 0.3923,
      "step": 153600
    },
    {
      "epoch": 0.4758911735655922,
      "grad_norm": 40.18803787231445,
      "learning_rate": 0.00015723264793032234,
      "loss": 0.2191,
      "step": 153700
    },
    {
      "epoch": 0.4762007969706446,
      "grad_norm": 114.2186279296875,
      "learning_rate": 0.0001571397609088066,
      "loss": 0.3507,
      "step": 153800
    },
    {
      "epoch": 0.476510420375697,
      "grad_norm": 0.016605257987976074,
      "learning_rate": 0.0001570468738872909,
      "loss": 0.3638,
      "step": 153900
    },
    {
      "epoch": 0.4768200437807495,
      "grad_norm": 0.06946513056755066,
      "learning_rate": 0.00015695398686577515,
      "loss": 0.3138,
      "step": 154000
    },
    {
      "epoch": 0.4771296671858019,
      "grad_norm": 0.00019065587548539042,
      "learning_rate": 0.0001568610998442594,
      "loss": 0.2464,
      "step": 154100
    },
    {
      "epoch": 0.4774392905908543,
      "grad_norm": 0.004063237924128771,
      "learning_rate": 0.0001567682128227437,
      "loss": 0.2588,
      "step": 154200
    },
    {
      "epoch": 0.4777489139959068,
      "grad_norm": 0.0005000868113711476,
      "learning_rate": 0.00015667532580122796,
      "loss": 0.2578,
      "step": 154300
    },
    {
      "epoch": 0.4780585374009592,
      "grad_norm": 0.002828843193128705,
      "learning_rate": 0.00015658243877971224,
      "loss": 0.4239,
      "step": 154400
    },
    {
      "epoch": 0.47836816080601163,
      "grad_norm": 30.745920181274414,
      "learning_rate": 0.0001564895517581965,
      "loss": 0.3535,
      "step": 154500
    },
    {
      "epoch": 0.4786777842110641,
      "grad_norm": 0.0011625024490058422,
      "learning_rate": 0.00015639666473668076,
      "loss": 0.3002,
      "step": 154600
    },
    {
      "epoch": 0.4789874076161165,
      "grad_norm": 2.7677738666534424,
      "learning_rate": 0.00015630377771516505,
      "loss": 0.2233,
      "step": 154700
    },
    {
      "epoch": 0.47929703102116894,
      "grad_norm": 0.00023540739493910223,
      "learning_rate": 0.0001562108906936493,
      "loss": 0.4046,
      "step": 154800
    },
    {
      "epoch": 0.4796066544262214,
      "grad_norm": 0.0021421697456389666,
      "learning_rate": 0.00015611800367213357,
      "loss": 0.2587,
      "step": 154900
    },
    {
      "epoch": 0.47991627783127383,
      "grad_norm": 11.051177978515625,
      "learning_rate": 0.00015602511665061786,
      "loss": 0.2677,
      "step": 155000
    },
    {
      "epoch": 0.48022590123632625,
      "grad_norm": 0.025312645360827446,
      "learning_rate": 0.00015593222962910212,
      "loss": 0.4507,
      "step": 155100
    },
    {
      "epoch": 0.48053552464137866,
      "grad_norm": 0.011514686048030853,
      "learning_rate": 0.00015583934260758638,
      "loss": 0.3804,
      "step": 155200
    },
    {
      "epoch": 0.48084514804643114,
      "grad_norm": 10.471465110778809,
      "learning_rate": 0.00015574645558607067,
      "loss": 0.5632,
      "step": 155300
    },
    {
      "epoch": 0.48115477145148355,
      "grad_norm": 0.0003797068784479052,
      "learning_rate": 0.00015565356856455493,
      "loss": 0.3524,
      "step": 155400
    },
    {
      "epoch": 0.48146439485653597,
      "grad_norm": 0.0041638449765741825,
      "learning_rate": 0.0001555606815430392,
      "loss": 0.2825,
      "step": 155500
    },
    {
      "epoch": 0.48177401826158844,
      "grad_norm": 0.0013544573448598385,
      "learning_rate": 0.00015546779452152348,
      "loss": 0.2129,
      "step": 155600
    },
    {
      "epoch": 0.48208364166664086,
      "grad_norm": 0.004576863255351782,
      "learning_rate": 0.00015537490750000774,
      "loss": 0.3651,
      "step": 155700
    },
    {
      "epoch": 0.4823932650716933,
      "grad_norm": 49.98959732055664,
      "learning_rate": 0.000155282020478492,
      "loss": 0.4994,
      "step": 155800
    },
    {
      "epoch": 0.48270288847674575,
      "grad_norm": 0.002200175542384386,
      "learning_rate": 0.00015518913345697629,
      "loss": 0.531,
      "step": 155900
    },
    {
      "epoch": 0.48301251188179817,
      "grad_norm": 0.0006084205233491957,
      "learning_rate": 0.00015509624643546055,
      "loss": 0.4655,
      "step": 156000
    },
    {
      "epoch": 0.4833221352868506,
      "grad_norm": 0.001259919605217874,
      "learning_rate": 0.0001550033594139448,
      "loss": 0.3677,
      "step": 156100
    },
    {
      "epoch": 0.48363175869190306,
      "grad_norm": 0.008845734409987926,
      "learning_rate": 0.0001549104723924291,
      "loss": 0.5515,
      "step": 156200
    },
    {
      "epoch": 0.4839413820969555,
      "grad_norm": 0.10766894370317459,
      "learning_rate": 0.00015481758537091335,
      "loss": 0.2252,
      "step": 156300
    },
    {
      "epoch": 0.4842510055020079,
      "grad_norm": 0.00029121001716703176,
      "learning_rate": 0.00015472469834939762,
      "loss": 0.3506,
      "step": 156400
    },
    {
      "epoch": 0.48456062890706036,
      "grad_norm": 0.26052504777908325,
      "learning_rate": 0.0001546318113278819,
      "loss": 0.352,
      "step": 156500
    },
    {
      "epoch": 0.4848702523121128,
      "grad_norm": 0.000274857971817255,
      "learning_rate": 0.00015453892430636616,
      "loss": 0.2494,
      "step": 156600
    },
    {
      "epoch": 0.4851798757171652,
      "grad_norm": 13.40360164642334,
      "learning_rate": 0.00015444603728485042,
      "loss": 0.2399,
      "step": 156700
    },
    {
      "epoch": 0.48548949912221767,
      "grad_norm": 0.06693564355373383,
      "learning_rate": 0.0001543531502633347,
      "loss": 0.2446,
      "step": 156800
    },
    {
      "epoch": 0.4857991225272701,
      "grad_norm": 8.98274040222168,
      "learning_rate": 0.00015426026324181897,
      "loss": 0.4871,
      "step": 156900
    },
    {
      "epoch": 0.4861087459323225,
      "grad_norm": 0.014323268085718155,
      "learning_rate": 0.0001541673762203032,
      "loss": 0.2455,
      "step": 157000
    },
    {
      "epoch": 0.486418369337375,
      "grad_norm": 11.133016586303711,
      "learning_rate": 0.00015407448919878752,
      "loss": 0.3286,
      "step": 157100
    },
    {
      "epoch": 0.4867279927424274,
      "grad_norm": 0.40422165393829346,
      "learning_rate": 0.00015398160217727178,
      "loss": 0.2573,
      "step": 157200
    },
    {
      "epoch": 0.4870376161474798,
      "grad_norm": 0.0007959816139191389,
      "learning_rate": 0.000153888715155756,
      "loss": 0.5266,
      "step": 157300
    },
    {
      "epoch": 0.48734723955253223,
      "grad_norm": 0.0009071872918866575,
      "learning_rate": 0.00015379582813424033,
      "loss": 0.3259,
      "step": 157400
    },
    {
      "epoch": 0.4876568629575847,
      "grad_norm": 0.09337245672941208,
      "learning_rate": 0.0001537029411127246,
      "loss": 0.3008,
      "step": 157500
    },
    {
      "epoch": 0.4879664863626371,
      "grad_norm": 0.0006930484669283032,
      "learning_rate": 0.00015361005409120882,
      "loss": 0.2018,
      "step": 157600
    },
    {
      "epoch": 0.48827610976768954,
      "grad_norm": 0.8815987706184387,
      "learning_rate": 0.00015351716706969314,
      "loss": 0.3322,
      "step": 157700
    },
    {
      "epoch": 0.488585733172742,
      "grad_norm": 0.001823412487283349,
      "learning_rate": 0.0001534242800481774,
      "loss": 0.468,
      "step": 157800
    },
    {
      "epoch": 0.4888953565777944,
      "grad_norm": 0.6316315531730652,
      "learning_rate": 0.00015333139302666163,
      "loss": 0.2902,
      "step": 157900
    },
    {
      "epoch": 0.48920497998284684,
      "grad_norm": 0.00010797056165756658,
      "learning_rate": 0.00015323850600514595,
      "loss": 0.4047,
      "step": 158000
    },
    {
      "epoch": 0.4895146033878993,
      "grad_norm": 34.386417388916016,
      "learning_rate": 0.00015314561898363018,
      "loss": 0.3977,
      "step": 158100
    },
    {
      "epoch": 0.48982422679295173,
      "grad_norm": 0.0006231313454918563,
      "learning_rate": 0.0001530527319621145,
      "loss": 0.4266,
      "step": 158200
    },
    {
      "epoch": 0.49013385019800415,
      "grad_norm": 0.0007502890075556934,
      "learning_rate": 0.00015295984494059875,
      "loss": 0.3716,
      "step": 158300
    },
    {
      "epoch": 0.4904434736030566,
      "grad_norm": 0.01671317033469677,
      "learning_rate": 0.000152866957919083,
      "loss": 0.313,
      "step": 158400
    },
    {
      "epoch": 0.49075309700810904,
      "grad_norm": 0.00071712612407282,
      "learning_rate": 0.0001527740708975673,
      "loss": 0.2068,
      "step": 158500
    },
    {
      "epoch": 0.49106272041316146,
      "grad_norm": 0.00025655818171799183,
      "learning_rate": 0.00015268118387605156,
      "loss": 0.3385,
      "step": 158600
    },
    {
      "epoch": 0.49137234381821393,
      "grad_norm": 0.00043980334885418415,
      "learning_rate": 0.0001525882968545358,
      "loss": 0.4999,
      "step": 158700
    },
    {
      "epoch": 0.49168196722326635,
      "grad_norm": 54.882938385009766,
      "learning_rate": 0.0001524954098330201,
      "loss": 0.3247,
      "step": 158800
    },
    {
      "epoch": 0.49199159062831876,
      "grad_norm": 0.10809776186943054,
      "learning_rate": 0.00015240252281150437,
      "loss": 0.2497,
      "step": 158900
    },
    {
      "epoch": 0.49230121403337124,
      "grad_norm": 0.0028843432664871216,
      "learning_rate": 0.0001523096357899886,
      "loss": 0.3228,
      "step": 159000
    },
    {
      "epoch": 0.49261083743842365,
      "grad_norm": 0.0033032854553312063,
      "learning_rate": 0.00015221674876847292,
      "loss": 0.2742,
      "step": 159100
    },
    {
      "epoch": 0.49292046084347607,
      "grad_norm": 0.006153986789286137,
      "learning_rate": 0.00015212386174695715,
      "loss": 0.3182,
      "step": 159200
    },
    {
      "epoch": 0.4932300842485285,
      "grad_norm": 1.9809587001800537,
      "learning_rate": 0.0001520309747254414,
      "loss": 0.423,
      "step": 159300
    },
    {
      "epoch": 0.49353970765358096,
      "grad_norm": 0.06338422745466232,
      "learning_rate": 0.00015193808770392573,
      "loss": 0.3772,
      "step": 159400
    },
    {
      "epoch": 0.4938493310586334,
      "grad_norm": 0.0006027355557307601,
      "learning_rate": 0.00015184520068240996,
      "loss": 0.3623,
      "step": 159500
    },
    {
      "epoch": 0.4941589544636858,
      "grad_norm": 0.002710911212489009,
      "learning_rate": 0.00015175231366089422,
      "loss": 0.3425,
      "step": 159600
    },
    {
      "epoch": 0.49446857786873827,
      "grad_norm": 7.178499221801758,
      "learning_rate": 0.00015165942663937854,
      "loss": 0.2496,
      "step": 159700
    },
    {
      "epoch": 0.4947782012737907,
      "grad_norm": 0.0004718954733107239,
      "learning_rate": 0.00015156653961786277,
      "loss": 0.3946,
      "step": 159800
    },
    {
      "epoch": 0.4950878246788431,
      "grad_norm": 12.000800132751465,
      "learning_rate": 0.00015147365259634703,
      "loss": 0.2861,
      "step": 159900
    },
    {
      "epoch": 0.4953974480838956,
      "grad_norm": 0.0012302394025027752,
      "learning_rate": 0.00015138076557483134,
      "loss": 0.2824,
      "step": 160000
    },
    {
      "epoch": 0.495707071488948,
      "grad_norm": 26.62116050720215,
      "learning_rate": 0.00015128787855331558,
      "loss": 0.4868,
      "step": 160100
    },
    {
      "epoch": 0.4960166948940004,
      "grad_norm": 0.010587088763713837,
      "learning_rate": 0.00015119499153179984,
      "loss": 0.2878,
      "step": 160200
    },
    {
      "epoch": 0.4963263182990529,
      "grad_norm": 15.789850234985352,
      "learning_rate": 0.00015110210451028413,
      "loss": 0.2046,
      "step": 160300
    },
    {
      "epoch": 0.4966359417041053,
      "grad_norm": 0.013095336966216564,
      "learning_rate": 0.00015100921748876839,
      "loss": 0.2782,
      "step": 160400
    },
    {
      "epoch": 0.4969455651091577,
      "grad_norm": 0.015086991712450981,
      "learning_rate": 0.00015091633046725265,
      "loss": 0.4116,
      "step": 160500
    },
    {
      "epoch": 0.4972551885142102,
      "grad_norm": 0.004284171387553215,
      "learning_rate": 0.00015082344344573693,
      "loss": 0.3703,
      "step": 160600
    },
    {
      "epoch": 0.4975648119192626,
      "grad_norm": 0.006387142930179834,
      "learning_rate": 0.0001507305564242212,
      "loss": 0.2694,
      "step": 160700
    },
    {
      "epoch": 0.497874435324315,
      "grad_norm": 0.0001439737534383312,
      "learning_rate": 0.00015063766940270545,
      "loss": 0.3498,
      "step": 160800
    },
    {
      "epoch": 0.4981840587293675,
      "grad_norm": 0.004784092772752047,
      "learning_rate": 0.00015054478238118974,
      "loss": 0.1794,
      "step": 160900
    },
    {
      "epoch": 0.4984936821344199,
      "grad_norm": 0.00018875398382078856,
      "learning_rate": 0.000150451895359674,
      "loss": 0.2202,
      "step": 161000
    },
    {
      "epoch": 0.49880330553947233,
      "grad_norm": 0.006445292383432388,
      "learning_rate": 0.00015035900833815826,
      "loss": 0.4048,
      "step": 161100
    },
    {
      "epoch": 0.49911292894452475,
      "grad_norm": 0.0015718525974079967,
      "learning_rate": 0.00015026612131664255,
      "loss": 0.3571,
      "step": 161200
    },
    {
      "epoch": 0.4994225523495772,
      "grad_norm": 0.02674172818660736,
      "learning_rate": 0.0001501732342951268,
      "loss": 0.2719,
      "step": 161300
    },
    {
      "epoch": 0.49973217575462964,
      "grad_norm": 0.14727193117141724,
      "learning_rate": 0.00015008034727361107,
      "loss": 0.5235,
      "step": 161400
    },
    {
      "epoch": 0.500041799159682,
      "grad_norm": 14.6246976852417,
      "learning_rate": 0.00014998746025209536,
      "loss": 0.3721,
      "step": 161500
    },
    {
      "epoch": 0.5003514225647345,
      "grad_norm": 0.16226604580879211,
      "learning_rate": 0.00014989457323057962,
      "loss": 0.2747,
      "step": 161600
    },
    {
      "epoch": 0.500661045969787,
      "grad_norm": 0.011498115956783295,
      "learning_rate": 0.0001498016862090639,
      "loss": 0.2305,
      "step": 161700
    },
    {
      "epoch": 0.5009706693748394,
      "grad_norm": 0.7733200192451477,
      "learning_rate": 0.00014970879918754817,
      "loss": 0.3848,
      "step": 161800
    },
    {
      "epoch": 0.5012802927798918,
      "grad_norm": 20.706308364868164,
      "learning_rate": 0.00014961591216603243,
      "loss": 0.5368,
      "step": 161900
    },
    {
      "epoch": 0.5015899161849443,
      "grad_norm": 0.0016019074246287346,
      "learning_rate": 0.00014952302514451672,
      "loss": 0.3344,
      "step": 162000
    },
    {
      "epoch": 0.5018995395899967,
      "grad_norm": 0.004643431399017572,
      "learning_rate": 0.00014943013812300098,
      "loss": 0.3634,
      "step": 162100
    },
    {
      "epoch": 0.5022091629950491,
      "grad_norm": 9.630787826608866e-05,
      "learning_rate": 0.00014933725110148524,
      "loss": 0.3999,
      "step": 162200
    },
    {
      "epoch": 0.5025187864001015,
      "grad_norm": 1.1071553230285645,
      "learning_rate": 0.00014924436407996952,
      "loss": 0.2779,
      "step": 162300
    },
    {
      "epoch": 0.502828409805154,
      "grad_norm": 0.0021109834779053926,
      "learning_rate": 0.00014915147705845378,
      "loss": 0.1416,
      "step": 162400
    },
    {
      "epoch": 0.5031380332102064,
      "grad_norm": 0.003588220104575157,
      "learning_rate": 0.00014905859003693805,
      "loss": 0.5603,
      "step": 162500
    },
    {
      "epoch": 0.5034476566152588,
      "grad_norm": 0.00035420581116341054,
      "learning_rate": 0.00014896570301542233,
      "loss": 0.2919,
      "step": 162600
    },
    {
      "epoch": 0.5037572800203113,
      "grad_norm": 0.026323040947318077,
      "learning_rate": 0.0001488728159939066,
      "loss": 0.5143,
      "step": 162700
    },
    {
      "epoch": 0.5040669034253638,
      "grad_norm": 13.014383316040039,
      "learning_rate": 0.00014877992897239085,
      "loss": 0.1615,
      "step": 162800
    },
    {
      "epoch": 0.5043765268304161,
      "grad_norm": 0.0010290730278939009,
      "learning_rate": 0.00014868704195087514,
      "loss": 0.5832,
      "step": 162900
    },
    {
      "epoch": 0.5046861502354686,
      "grad_norm": 0.17673128843307495,
      "learning_rate": 0.0001485941549293594,
      "loss": 0.3322,
      "step": 163000
    },
    {
      "epoch": 0.5049957736405211,
      "grad_norm": 0.0024019817356020212,
      "learning_rate": 0.00014850126790784366,
      "loss": 0.3442,
      "step": 163100
    },
    {
      "epoch": 0.5053053970455734,
      "grad_norm": 123.11869812011719,
      "learning_rate": 0.00014840838088632795,
      "loss": 0.3731,
      "step": 163200
    },
    {
      "epoch": 0.5056150204506259,
      "grad_norm": 0.004489627201110125,
      "learning_rate": 0.0001483154938648122,
      "loss": 0.36,
      "step": 163300
    },
    {
      "epoch": 0.5059246438556784,
      "grad_norm": 28.3073787689209,
      "learning_rate": 0.0001482226068432965,
      "loss": 0.4412,
      "step": 163400
    },
    {
      "epoch": 0.5062342672607307,
      "grad_norm": 0.022718610242009163,
      "learning_rate": 0.00014812971982178076,
      "loss": 0.2835,
      "step": 163500
    },
    {
      "epoch": 0.5065438906657832,
      "grad_norm": 21.683841705322266,
      "learning_rate": 0.00014803683280026502,
      "loss": 0.3445,
      "step": 163600
    },
    {
      "epoch": 0.5068535140708357,
      "grad_norm": 90.23880004882812,
      "learning_rate": 0.0001479439457787493,
      "loss": 0.2117,
      "step": 163700
    },
    {
      "epoch": 0.507163137475888,
      "grad_norm": 0.266434907913208,
      "learning_rate": 0.00014785105875723357,
      "loss": 0.2464,
      "step": 163800
    },
    {
      "epoch": 0.5074727608809405,
      "grad_norm": 0.0005264554056338966,
      "learning_rate": 0.00014775817173571783,
      "loss": 0.2872,
      "step": 163900
    },
    {
      "epoch": 0.507782384285993,
      "grad_norm": 0.0053571308963000774,
      "learning_rate": 0.00014766528471420211,
      "loss": 0.3714,
      "step": 164000
    },
    {
      "epoch": 0.5080920076910453,
      "grad_norm": 10.220155715942383,
      "learning_rate": 0.00014757239769268638,
      "loss": 0.4083,
      "step": 164100
    },
    {
      "epoch": 0.5084016310960978,
      "grad_norm": 0.022745074704289436,
      "learning_rate": 0.00014747951067117064,
      "loss": 0.4943,
      "step": 164200
    },
    {
      "epoch": 0.5087112545011503,
      "grad_norm": 0.0008116001263260841,
      "learning_rate": 0.00014738662364965492,
      "loss": 0.1886,
      "step": 164300
    },
    {
      "epoch": 0.5090208779062027,
      "grad_norm": 0.0010884689399972558,
      "learning_rate": 0.00014729373662813918,
      "loss": 0.3109,
      "step": 164400
    },
    {
      "epoch": 0.5093305013112551,
      "grad_norm": 0.0175494235008955,
      "learning_rate": 0.00014720084960662344,
      "loss": 0.3695,
      "step": 164500
    },
    {
      "epoch": 0.5096401247163076,
      "grad_norm": 18.706912994384766,
      "learning_rate": 0.00014710796258510773,
      "loss": 0.3279,
      "step": 164600
    },
    {
      "epoch": 0.50994974812136,
      "grad_norm": 0.868234395980835,
      "learning_rate": 0.000147015075563592,
      "loss": 0.3735,
      "step": 164700
    },
    {
      "epoch": 0.5102593715264124,
      "grad_norm": 0.0035001696087419987,
      "learning_rate": 0.00014692218854207625,
      "loss": 0.2325,
      "step": 164800
    },
    {
      "epoch": 0.5105689949314649,
      "grad_norm": 0.0018572903936728835,
      "learning_rate": 0.00014682930152056054,
      "loss": 0.2134,
      "step": 164900
    },
    {
      "epoch": 0.5108786183365173,
      "grad_norm": 0.0006085769273340702,
      "learning_rate": 0.0001467364144990448,
      "loss": 0.1698,
      "step": 165000
    },
    {
      "epoch": 0.5111882417415697,
      "grad_norm": 1.9471075534820557,
      "learning_rate": 0.00014664352747752906,
      "loss": 0.2177,
      "step": 165100
    },
    {
      "epoch": 0.5114978651466222,
      "grad_norm": 0.0076017798855900764,
      "learning_rate": 0.00014655064045601335,
      "loss": 0.2705,
      "step": 165200
    },
    {
      "epoch": 0.5118074885516746,
      "grad_norm": 25.850845336914062,
      "learning_rate": 0.0001464577534344976,
      "loss": 0.3771,
      "step": 165300
    },
    {
      "epoch": 0.512117111956727,
      "grad_norm": 0.0006267593707889318,
      "learning_rate": 0.00014636486641298187,
      "loss": 0.42,
      "step": 165400
    },
    {
      "epoch": 0.5124267353617795,
      "grad_norm": 0.0023215978872030973,
      "learning_rate": 0.00014627197939146616,
      "loss": 0.2606,
      "step": 165500
    },
    {
      "epoch": 0.5127363587668319,
      "grad_norm": 0.002877742052078247,
      "learning_rate": 0.00014617909236995042,
      "loss": 0.5061,
      "step": 165600
    },
    {
      "epoch": 0.5130459821718844,
      "grad_norm": 0.004234735853970051,
      "learning_rate": 0.00014608620534843468,
      "loss": 0.37,
      "step": 165700
    },
    {
      "epoch": 0.5133556055769368,
      "grad_norm": 0.0030048000626266003,
      "learning_rate": 0.00014599331832691897,
      "loss": 0.3357,
      "step": 165800
    },
    {
      "epoch": 0.5136652289819892,
      "grad_norm": 0.008780001662671566,
      "learning_rate": 0.00014590043130540323,
      "loss": 0.2041,
      "step": 165900
    },
    {
      "epoch": 0.5139748523870417,
      "grad_norm": 0.023701662197709084,
      "learning_rate": 0.0001458075442838875,
      "loss": 0.1352,
      "step": 166000
    },
    {
      "epoch": 0.5142844757920941,
      "grad_norm": 100.8742904663086,
      "learning_rate": 0.00014571465726237177,
      "loss": 0.4023,
      "step": 166100
    },
    {
      "epoch": 0.5145940991971465,
      "grad_norm": 0.0018902558367699385,
      "learning_rate": 0.00014562177024085603,
      "loss": 0.3003,
      "step": 166200
    },
    {
      "epoch": 0.514903722602199,
      "grad_norm": 0.00047193607315421104,
      "learning_rate": 0.0001455288832193403,
      "loss": 0.3629,
      "step": 166300
    },
    {
      "epoch": 0.5152133460072513,
      "grad_norm": 0.0009115541470237076,
      "learning_rate": 0.00014543599619782458,
      "loss": 0.1645,
      "step": 166400
    },
    {
      "epoch": 0.5155229694123038,
      "grad_norm": 0.21562623977661133,
      "learning_rate": 0.00014534310917630884,
      "loss": 0.2243,
      "step": 166500
    },
    {
      "epoch": 0.5158325928173563,
      "grad_norm": 0.06581832468509674,
      "learning_rate": 0.0001452502221547931,
      "loss": 0.3724,
      "step": 166600
    },
    {
      "epoch": 0.5161422162224086,
      "grad_norm": 0.0076252492144703865,
      "learning_rate": 0.0001451573351332774,
      "loss": 0.3475,
      "step": 166700
    },
    {
      "epoch": 0.5164518396274611,
      "grad_norm": 53.264461517333984,
      "learning_rate": 0.00014506444811176165,
      "loss": 0.3634,
      "step": 166800
    },
    {
      "epoch": 0.5167614630325136,
      "grad_norm": 11.307731628417969,
      "learning_rate": 0.0001449715610902459,
      "loss": 0.2999,
      "step": 166900
    },
    {
      "epoch": 0.5170710864375659,
      "grad_norm": 0.0015911477385088801,
      "learning_rate": 0.0001448786740687302,
      "loss": 0.4067,
      "step": 167000
    },
    {
      "epoch": 0.5173807098426184,
      "grad_norm": 0.001487767556682229,
      "learning_rate": 0.00014478578704721446,
      "loss": 0.2681,
      "step": 167100
    },
    {
      "epoch": 0.5176903332476709,
      "grad_norm": 0.034020762890577316,
      "learning_rate": 0.00014469290002569875,
      "loss": 0.3491,
      "step": 167200
    },
    {
      "epoch": 0.5179999566527232,
      "grad_norm": 0.013949533924460411,
      "learning_rate": 0.000144600013004183,
      "loss": 0.2168,
      "step": 167300
    },
    {
      "epoch": 0.5183095800577757,
      "grad_norm": 0.0019107116386294365,
      "learning_rate": 0.00014450712598266727,
      "loss": 0.2509,
      "step": 167400
    },
    {
      "epoch": 0.5186192034628282,
      "grad_norm": 0.024484209716320038,
      "learning_rate": 0.00014441423896115156,
      "loss": 0.4503,
      "step": 167500
    },
    {
      "epoch": 0.5189288268678806,
      "grad_norm": 0.006933833938091993,
      "learning_rate": 0.00014432135193963582,
      "loss": 0.2858,
      "step": 167600
    },
    {
      "epoch": 0.519238450272933,
      "grad_norm": 89.32760620117188,
      "learning_rate": 0.00014422846491812008,
      "loss": 0.1419,
      "step": 167700
    },
    {
      "epoch": 0.5195480736779855,
      "grad_norm": 0.7170017957687378,
      "learning_rate": 0.00014413557789660436,
      "loss": 0.2037,
      "step": 167800
    },
    {
      "epoch": 0.5198576970830379,
      "grad_norm": 0.03774391487240791,
      "learning_rate": 0.00014404269087508862,
      "loss": 0.3247,
      "step": 167900
    },
    {
      "epoch": 0.5201673204880903,
      "grad_norm": 0.028773395344614983,
      "learning_rate": 0.00014394980385357289,
      "loss": 0.2733,
      "step": 168000
    },
    {
      "epoch": 0.5204769438931428,
      "grad_norm": 0.126979798078537,
      "learning_rate": 0.00014385691683205717,
      "loss": 0.3011,
      "step": 168100
    },
    {
      "epoch": 0.5207865672981952,
      "grad_norm": 0.0038656641263514757,
      "learning_rate": 0.00014376402981054143,
      "loss": 0.2994,
      "step": 168200
    },
    {
      "epoch": 0.5210961907032476,
      "grad_norm": 0.000726092781405896,
      "learning_rate": 0.0001436711427890257,
      "loss": 0.3734,
      "step": 168300
    },
    {
      "epoch": 0.5214058141083001,
      "grad_norm": 82.51640319824219,
      "learning_rate": 0.00014357825576750998,
      "loss": 0.496,
      "step": 168400
    },
    {
      "epoch": 0.5217154375133525,
      "grad_norm": 0.0027590689714998007,
      "learning_rate": 0.00014348536874599421,
      "loss": 0.3008,
      "step": 168500
    },
    {
      "epoch": 0.522025060918405,
      "grad_norm": 0.003099062480032444,
      "learning_rate": 0.0001433924817244785,
      "loss": 0.2702,
      "step": 168600
    },
    {
      "epoch": 0.5223346843234574,
      "grad_norm": 3.930626630783081,
      "learning_rate": 0.0001432995947029628,
      "loss": 0.3014,
      "step": 168700
    },
    {
      "epoch": 0.5226443077285098,
      "grad_norm": 0.010083128698170185,
      "learning_rate": 0.00014320670768144702,
      "loss": 0.3955,
      "step": 168800
    },
    {
      "epoch": 0.5229539311335623,
      "grad_norm": 12.498092651367188,
      "learning_rate": 0.0001431138206599313,
      "loss": 0.3781,
      "step": 168900
    },
    {
      "epoch": 0.5232635545386147,
      "grad_norm": 0.002123307203873992,
      "learning_rate": 0.0001430209336384156,
      "loss": 0.4168,
      "step": 169000
    },
    {
      "epoch": 0.5235731779436671,
      "grad_norm": 0.5133791565895081,
      "learning_rate": 0.00014292804661689983,
      "loss": 0.3262,
      "step": 169100
    },
    {
      "epoch": 0.5238828013487196,
      "grad_norm": 0.0012119156308472157,
      "learning_rate": 0.00014283515959538412,
      "loss": 0.3828,
      "step": 169200
    },
    {
      "epoch": 0.524192424753772,
      "grad_norm": 0.2590997517108917,
      "learning_rate": 0.0001427422725738684,
      "loss": 0.2734,
      "step": 169300
    },
    {
      "epoch": 0.5245020481588244,
      "grad_norm": 0.0007471241988241673,
      "learning_rate": 0.00014264938555235267,
      "loss": 0.1191,
      "step": 169400
    },
    {
      "epoch": 0.5248116715638769,
      "grad_norm": 0.13030731678009033,
      "learning_rate": 0.00014255649853083693,
      "loss": 0.2377,
      "step": 169500
    },
    {
      "epoch": 0.5251212949689293,
      "grad_norm": 0.018390171229839325,
      "learning_rate": 0.0001424636115093212,
      "loss": 0.329,
      "step": 169600
    },
    {
      "epoch": 0.5254309183739817,
      "grad_norm": 5.386057376861572,
      "learning_rate": 0.00014237072448780548,
      "loss": 0.2581,
      "step": 169700
    },
    {
      "epoch": 0.5257405417790342,
      "grad_norm": 0.0012546450598165393,
      "learning_rate": 0.00014227783746628974,
      "loss": 0.3888,
      "step": 169800
    },
    {
      "epoch": 0.5260501651840866,
      "grad_norm": 1.0715994834899902,
      "learning_rate": 0.000142184950444774,
      "loss": 0.2064,
      "step": 169900
    },
    {
      "epoch": 0.526359788589139,
      "grad_norm": 0.00021669045963790268,
      "learning_rate": 0.00014209206342325828,
      "loss": 0.2647,
      "step": 170000
    },
    {
      "epoch": 0.5266694119941915,
      "grad_norm": 0.03242342546582222,
      "learning_rate": 0.00014199917640174254,
      "loss": 0.4838,
      "step": 170100
    },
    {
      "epoch": 0.5269790353992438,
      "grad_norm": 0.001551422057673335,
      "learning_rate": 0.0001419062893802268,
      "loss": 0.5611,
      "step": 170200
    },
    {
      "epoch": 0.5272886588042963,
      "grad_norm": 0.009876489639282227,
      "learning_rate": 0.0001418134023587111,
      "loss": 0.3294,
      "step": 170300
    },
    {
      "epoch": 0.5275982822093488,
      "grad_norm": 0.004116371739655733,
      "learning_rate": 0.00014172051533719535,
      "loss": 0.2068,
      "step": 170400
    },
    {
      "epoch": 0.5279079056144012,
      "grad_norm": 0.003300306387245655,
      "learning_rate": 0.00014162762831567961,
      "loss": 0.3302,
      "step": 170500
    },
    {
      "epoch": 0.5282175290194536,
      "grad_norm": 0.0001235662930412218,
      "learning_rate": 0.0001415347412941639,
      "loss": 0.3602,
      "step": 170600
    },
    {
      "epoch": 0.5285271524245061,
      "grad_norm": 0.0006206000107340515,
      "learning_rate": 0.00014144185427264816,
      "loss": 0.2769,
      "step": 170700
    },
    {
      "epoch": 0.5288367758295585,
      "grad_norm": 0.0052589899860322475,
      "learning_rate": 0.00014134896725113242,
      "loss": 0.3678,
      "step": 170800
    },
    {
      "epoch": 0.5291463992346109,
      "grad_norm": 0.024674171581864357,
      "learning_rate": 0.0001412560802296167,
      "loss": 0.3072,
      "step": 170900
    },
    {
      "epoch": 0.5294560226396634,
      "grad_norm": 0.011770844459533691,
      "learning_rate": 0.00014116319320810097,
      "loss": 0.3155,
      "step": 171000
    },
    {
      "epoch": 0.5297656460447158,
      "grad_norm": 0.0022398324217647314,
      "learning_rate": 0.00014107030618658523,
      "loss": 0.3038,
      "step": 171100
    },
    {
      "epoch": 0.5300752694497682,
      "grad_norm": 0.017226584255695343,
      "learning_rate": 0.00014097741916506952,
      "loss": 0.3375,
      "step": 171200
    },
    {
      "epoch": 0.5303848928548207,
      "grad_norm": 0.00030150628299452364,
      "learning_rate": 0.00014088453214355378,
      "loss": 0.3019,
      "step": 171300
    },
    {
      "epoch": 0.5306945162598731,
      "grad_norm": 0.00035971266333945096,
      "learning_rate": 0.00014079164512203804,
      "loss": 0.2448,
      "step": 171400
    },
    {
      "epoch": 0.5310041396649255,
      "grad_norm": 0.22068215906620026,
      "learning_rate": 0.00014069875810052233,
      "loss": 0.2329,
      "step": 171500
    },
    {
      "epoch": 0.531313763069978,
      "grad_norm": 0.004529016558080912,
      "learning_rate": 0.0001406058710790066,
      "loss": 0.3761,
      "step": 171600
    },
    {
      "epoch": 0.5316233864750304,
      "grad_norm": 12.825822830200195,
      "learning_rate": 0.00014051298405749085,
      "loss": 0.2962,
      "step": 171700
    },
    {
      "epoch": 0.5319330098800829,
      "grad_norm": 6.102810382843018,
      "learning_rate": 0.00014042009703597514,
      "loss": 0.3659,
      "step": 171800
    },
    {
      "epoch": 0.5322426332851353,
      "grad_norm": 10.072892189025879,
      "learning_rate": 0.0001403272100144594,
      "loss": 0.2942,
      "step": 171900
    },
    {
      "epoch": 0.5325522566901877,
      "grad_norm": 0.00214894930832088,
      "learning_rate": 0.00014023432299294366,
      "loss": 0.3079,
      "step": 172000
    },
    {
      "epoch": 0.5328618800952402,
      "grad_norm": 2.5588412284851074,
      "learning_rate": 0.00014014143597142794,
      "loss": 0.3283,
      "step": 172100
    },
    {
      "epoch": 0.5331715035002926,
      "grad_norm": 0.001167107606306672,
      "learning_rate": 0.0001400485489499122,
      "loss": 0.1951,
      "step": 172200
    },
    {
      "epoch": 0.533481126905345,
      "grad_norm": 0.004895806312561035,
      "learning_rate": 0.00013995566192839646,
      "loss": 0.41,
      "step": 172300
    },
    {
      "epoch": 0.5337907503103975,
      "grad_norm": 28.23509979248047,
      "learning_rate": 0.00013986277490688075,
      "loss": 0.3154,
      "step": 172400
    },
    {
      "epoch": 0.5341003737154499,
      "grad_norm": 1.1520671844482422,
      "learning_rate": 0.000139769887885365,
      "loss": 0.3023,
      "step": 172500
    },
    {
      "epoch": 0.5344099971205023,
      "grad_norm": 0.0006390323978848755,
      "learning_rate": 0.00013967700086384927,
      "loss": 0.4138,
      "step": 172600
    },
    {
      "epoch": 0.5347196205255548,
      "grad_norm": 0.05054157227277756,
      "learning_rate": 0.00013958411384233356,
      "loss": 0.2721,
      "step": 172700
    },
    {
      "epoch": 0.5350292439306072,
      "grad_norm": 0.0018448455957695842,
      "learning_rate": 0.00013949122682081782,
      "loss": 0.1446,
      "step": 172800
    },
    {
      "epoch": 0.5353388673356596,
      "grad_norm": 0.0003171644057147205,
      "learning_rate": 0.00013939833979930208,
      "loss": 0.4033,
      "step": 172900
    },
    {
      "epoch": 0.5356484907407121,
      "grad_norm": 68.47675323486328,
      "learning_rate": 0.00013930545277778637,
      "loss": 0.5403,
      "step": 173000
    },
    {
      "epoch": 0.5359581141457646,
      "grad_norm": 0.009579810313880444,
      "learning_rate": 0.00013921256575627063,
      "loss": 0.2215,
      "step": 173100
    },
    {
      "epoch": 0.5362677375508169,
      "grad_norm": 0.003579082665964961,
      "learning_rate": 0.00013911967873475492,
      "loss": 0.2109,
      "step": 173200
    },
    {
      "epoch": 0.5365773609558694,
      "grad_norm": 136.06820678710938,
      "learning_rate": 0.00013902679171323918,
      "loss": 0.3285,
      "step": 173300
    },
    {
      "epoch": 0.5368869843609219,
      "grad_norm": 0.0016529253916814923,
      "learning_rate": 0.00013893390469172344,
      "loss": 0.2293,
      "step": 173400
    },
    {
      "epoch": 0.5371966077659742,
      "grad_norm": 35.667179107666016,
      "learning_rate": 0.00013884101767020773,
      "loss": 0.4112,
      "step": 173500
    },
    {
      "epoch": 0.5375062311710267,
      "grad_norm": 0.001653907005675137,
      "learning_rate": 0.00013874813064869199,
      "loss": 0.226,
      "step": 173600
    },
    {
      "epoch": 0.5378158545760792,
      "grad_norm": 0.0025561212096363306,
      "learning_rate": 0.00013865524362717625,
      "loss": 0.1921,
      "step": 173700
    },
    {
      "epoch": 0.5381254779811315,
      "grad_norm": 0.0018327416619285941,
      "learning_rate": 0.00013856235660566053,
      "loss": 0.292,
      "step": 173800
    },
    {
      "epoch": 0.538435101386184,
      "grad_norm": 0.009460299275815487,
      "learning_rate": 0.0001384694695841448,
      "loss": 0.3078,
      "step": 173900
    },
    {
      "epoch": 0.5387447247912365,
      "grad_norm": 27.999393463134766,
      "learning_rate": 0.00013837658256262906,
      "loss": 0.1326,
      "step": 174000
    },
    {
      "epoch": 0.5390543481962888,
      "grad_norm": 0.0003112786798737943,
      "learning_rate": 0.00013828369554111334,
      "loss": 0.3576,
      "step": 174100
    },
    {
      "epoch": 0.5393639716013413,
      "grad_norm": 0.0007901472854427993,
      "learning_rate": 0.0001381908085195976,
      "loss": 0.4127,
      "step": 174200
    },
    {
      "epoch": 0.5396735950063937,
      "grad_norm": 44.91901397705078,
      "learning_rate": 0.00013809792149808186,
      "loss": 0.3556,
      "step": 174300
    },
    {
      "epoch": 0.5399832184114461,
      "grad_norm": 0.03152385354042053,
      "learning_rate": 0.00013800503447656615,
      "loss": 0.2651,
      "step": 174400
    },
    {
      "epoch": 0.5402928418164986,
      "grad_norm": 4.800487041473389,
      "learning_rate": 0.0001379121474550504,
      "loss": 0.1706,
      "step": 174500
    },
    {
      "epoch": 0.540602465221551,
      "grad_norm": 1.5959635972976685,
      "learning_rate": 0.00013781926043353467,
      "loss": 0.3073,
      "step": 174600
    },
    {
      "epoch": 0.5409120886266034,
      "grad_norm": 118.687744140625,
      "learning_rate": 0.00013772637341201896,
      "loss": 0.3737,
      "step": 174700
    },
    {
      "epoch": 0.5412217120316559,
      "grad_norm": 0.000686115468852222,
      "learning_rate": 0.00013763348639050322,
      "loss": 0.1481,
      "step": 174800
    },
    {
      "epoch": 0.5415313354367083,
      "grad_norm": 15.833333015441895,
      "learning_rate": 0.00013754059936898748,
      "loss": 0.2269,
      "step": 174900
    },
    {
      "epoch": 0.5418409588417608,
      "grad_norm": 0.0009861699072644114,
      "learning_rate": 0.00013744771234747177,
      "loss": 0.3569,
      "step": 175000
    },
    {
      "epoch": 0.5421505822468132,
      "grad_norm": 0.017256923019886017,
      "learning_rate": 0.00013735482532595603,
      "loss": 0.3041,
      "step": 175100
    },
    {
      "epoch": 0.5424602056518656,
      "grad_norm": 77.05870819091797,
      "learning_rate": 0.0001372619383044403,
      "loss": 0.4164,
      "step": 175200
    },
    {
      "epoch": 0.5427698290569181,
      "grad_norm": 0.003294858615845442,
      "learning_rate": 0.00013716905128292458,
      "loss": 0.2807,
      "step": 175300
    },
    {
      "epoch": 0.5430794524619705,
      "grad_norm": 0.033418383449316025,
      "learning_rate": 0.00013707616426140884,
      "loss": 0.5059,
      "step": 175400
    },
    {
      "epoch": 0.5433890758670229,
      "grad_norm": 0.0006940152961760759,
      "learning_rate": 0.0001369832772398931,
      "loss": 0.2038,
      "step": 175500
    },
    {
      "epoch": 0.5436986992720754,
      "grad_norm": 0.0027518838178366423,
      "learning_rate": 0.00013689039021837738,
      "loss": 0.3756,
      "step": 175600
    },
    {
      "epoch": 0.5440083226771278,
      "grad_norm": 0.472663551568985,
      "learning_rate": 0.00013679750319686165,
      "loss": 0.25,
      "step": 175700
    },
    {
      "epoch": 0.5443179460821802,
      "grad_norm": 0.018172243610024452,
      "learning_rate": 0.0001367046161753459,
      "loss": 0.4194,
      "step": 175800
    },
    {
      "epoch": 0.5446275694872327,
      "grad_norm": 45.99102783203125,
      "learning_rate": 0.0001366117291538302,
      "loss": 0.3529,
      "step": 175900
    },
    {
      "epoch": 0.5449371928922851,
      "grad_norm": 0.0007416486041620374,
      "learning_rate": 0.00013651884213231445,
      "loss": 0.329,
      "step": 176000
    },
    {
      "epoch": 0.5452468162973375,
      "grad_norm": 0.0048799398355185986,
      "learning_rate": 0.00013642595511079871,
      "loss": 0.2866,
      "step": 176100
    },
    {
      "epoch": 0.54555643970239,
      "grad_norm": 0.00032681194716133177,
      "learning_rate": 0.000136333068089283,
      "loss": 0.2765,
      "step": 176200
    },
    {
      "epoch": 0.5458660631074425,
      "grad_norm": 28.371803283691406,
      "learning_rate": 0.00013624018106776726,
      "loss": 0.24,
      "step": 176300
    },
    {
      "epoch": 0.5461756865124948,
      "grad_norm": 0.0004906003014184535,
      "learning_rate": 0.00013614729404625152,
      "loss": 0.2789,
      "step": 176400
    },
    {
      "epoch": 0.5464853099175473,
      "grad_norm": 1.0797679424285889,
      "learning_rate": 0.0001360544070247358,
      "loss": 0.1476,
      "step": 176500
    },
    {
      "epoch": 0.5467949333225998,
      "grad_norm": 0.000560964981559664,
      "learning_rate": 0.00013596152000322007,
      "loss": 0.2453,
      "step": 176600
    },
    {
      "epoch": 0.5471045567276521,
      "grad_norm": 0.006670013070106506,
      "learning_rate": 0.00013586863298170433,
      "loss": 0.3017,
      "step": 176700
    },
    {
      "epoch": 0.5474141801327046,
      "grad_norm": 0.1822003722190857,
      "learning_rate": 0.00013577574596018862,
      "loss": 0.3111,
      "step": 176800
    },
    {
      "epoch": 0.5477238035377571,
      "grad_norm": 0.0023975383955985308,
      "learning_rate": 0.00013568285893867288,
      "loss": 0.3484,
      "step": 176900
    },
    {
      "epoch": 0.5480334269428094,
      "grad_norm": 0.0010219693649560213,
      "learning_rate": 0.00013558997191715714,
      "loss": 0.3653,
      "step": 177000
    },
    {
      "epoch": 0.5483430503478619,
      "grad_norm": 0.02427753619849682,
      "learning_rate": 0.00013549708489564143,
      "loss": 0.3128,
      "step": 177100
    },
    {
      "epoch": 0.5486526737529144,
      "grad_norm": 0.0036472578067332506,
      "learning_rate": 0.0001354041978741257,
      "loss": 0.2736,
      "step": 177200
    },
    {
      "epoch": 0.5489622971579667,
      "grad_norm": 3.709745168685913,
      "learning_rate": 0.00013531131085260998,
      "loss": 0.4391,
      "step": 177300
    },
    {
      "epoch": 0.5492719205630192,
      "grad_norm": 0.0024975670967251062,
      "learning_rate": 0.00013521842383109424,
      "loss": 0.3092,
      "step": 177400
    },
    {
      "epoch": 0.5495815439680717,
      "grad_norm": 0.002272737445309758,
      "learning_rate": 0.0001351255368095785,
      "loss": 0.4048,
      "step": 177500
    },
    {
      "epoch": 0.549891167373124,
      "grad_norm": 0.0013521364890038967,
      "learning_rate": 0.00013503264978806278,
      "loss": 0.3418,
      "step": 177600
    },
    {
      "epoch": 0.5502007907781765,
      "grad_norm": 77.56175994873047,
      "learning_rate": 0.00013493976276654704,
      "loss": 0.3615,
      "step": 177700
    },
    {
      "epoch": 0.550510414183229,
      "grad_norm": 0.0035388183314353228,
      "learning_rate": 0.0001348468757450313,
      "loss": 0.2892,
      "step": 177800
    },
    {
      "epoch": 0.5508200375882814,
      "grad_norm": 0.003483785782009363,
      "learning_rate": 0.0001347539887235156,
      "loss": 0.2779,
      "step": 177900
    },
    {
      "epoch": 0.5511296609933338,
      "grad_norm": 0.007471018470823765,
      "learning_rate": 0.00013466110170199985,
      "loss": 0.3005,
      "step": 178000
    },
    {
      "epoch": 0.5514392843983863,
      "grad_norm": 0.00016481985221616924,
      "learning_rate": 0.0001345682146804841,
      "loss": 0.0972,
      "step": 178100
    },
    {
      "epoch": 0.5517489078034387,
      "grad_norm": 0.009029155597090721,
      "learning_rate": 0.0001344753276589684,
      "loss": 0.2214,
      "step": 178200
    },
    {
      "epoch": 0.5520585312084911,
      "grad_norm": 0.002553664380684495,
      "learning_rate": 0.00013438244063745266,
      "loss": 0.2004,
      "step": 178300
    },
    {
      "epoch": 0.5523681546135435,
      "grad_norm": 0.005362614989280701,
      "learning_rate": 0.00013428955361593692,
      "loss": 0.3569,
      "step": 178400
    },
    {
      "epoch": 0.552677778018596,
      "grad_norm": 0.0006858680862933397,
      "learning_rate": 0.0001341966665944212,
      "loss": 0.1755,
      "step": 178500
    },
    {
      "epoch": 0.5529874014236484,
      "grad_norm": 0.20283065736293793,
      "learning_rate": 0.00013410377957290547,
      "loss": 0.3472,
      "step": 178600
    },
    {
      "epoch": 0.5532970248287008,
      "grad_norm": 0.003406392876058817,
      "learning_rate": 0.00013401089255138973,
      "loss": 0.5029,
      "step": 178700
    },
    {
      "epoch": 0.5536066482337533,
      "grad_norm": 10.462170600891113,
      "learning_rate": 0.00013391800552987402,
      "loss": 0.3011,
      "step": 178800
    },
    {
      "epoch": 0.5539162716388057,
      "grad_norm": 0.0012587402015924454,
      "learning_rate": 0.00013382511850835825,
      "loss": 0.2032,
      "step": 178900
    },
    {
      "epoch": 0.5542258950438581,
      "grad_norm": 0.001186281442642212,
      "learning_rate": 0.00013373223148684254,
      "loss": 0.3198,
      "step": 179000
    },
    {
      "epoch": 0.5545355184489106,
      "grad_norm": 0.014884712174534798,
      "learning_rate": 0.00013363934446532683,
      "loss": 0.3021,
      "step": 179100
    },
    {
      "epoch": 0.554845141853963,
      "grad_norm": 0.0014375578612089157,
      "learning_rate": 0.0001335464574438111,
      "loss": 0.2142,
      "step": 179200
    },
    {
      "epoch": 0.5551547652590154,
      "grad_norm": 0.0025427979417145252,
      "learning_rate": 0.00013345357042229535,
      "loss": 0.3515,
      "step": 179300
    },
    {
      "epoch": 0.5554643886640679,
      "grad_norm": 0.0011545784072950482,
      "learning_rate": 0.00013336068340077963,
      "loss": 0.2038,
      "step": 179400
    },
    {
      "epoch": 0.5557740120691204,
      "grad_norm": 10.56753158569336,
      "learning_rate": 0.0001332677963792639,
      "loss": 0.3507,
      "step": 179500
    },
    {
      "epoch": 0.5560836354741727,
      "grad_norm": 0.017784476280212402,
      "learning_rate": 0.00013317490935774816,
      "loss": 0.3928,
      "step": 179600
    },
    {
      "epoch": 0.5563932588792252,
      "grad_norm": 0.013882487080991268,
      "learning_rate": 0.00013308202233623244,
      "loss": 0.3356,
      "step": 179700
    },
    {
      "epoch": 0.5567028822842777,
      "grad_norm": 1.6126350164413452,
      "learning_rate": 0.0001329891353147167,
      "loss": 0.2067,
      "step": 179800
    },
    {
      "epoch": 0.55701250568933,
      "grad_norm": 0.0020409738644957542,
      "learning_rate": 0.00013289624829320096,
      "loss": 0.2363,
      "step": 179900
    },
    {
      "epoch": 0.5573221290943825,
      "grad_norm": 0.0029865854885429144,
      "learning_rate": 0.00013280336127168522,
      "loss": 0.1513,
      "step": 180000
    },
    {
      "epoch": 0.557631752499435,
      "grad_norm": 0.00020069367019459605,
      "learning_rate": 0.0001327104742501695,
      "loss": 0.3417,
      "step": 180100
    },
    {
      "epoch": 0.5579413759044873,
      "grad_norm": 0.029925459995865822,
      "learning_rate": 0.00013261758722865377,
      "loss": 0.2309,
      "step": 180200
    },
    {
      "epoch": 0.5582509993095398,
      "grad_norm": 0.01625162549316883,
      "learning_rate": 0.00013252470020713803,
      "loss": 0.4501,
      "step": 180300
    },
    {
      "epoch": 0.5585606227145923,
      "grad_norm": 0.007951146923005581,
      "learning_rate": 0.00013243181318562232,
      "loss": 0.27,
      "step": 180400
    },
    {
      "epoch": 0.5588702461196446,
      "grad_norm": 0.0005239839665591717,
      "learning_rate": 0.00013233892616410658,
      "loss": 0.1694,
      "step": 180500
    },
    {
      "epoch": 0.5591798695246971,
      "grad_norm": 0.002630264963954687,
      "learning_rate": 0.00013224603914259084,
      "loss": 0.2291,
      "step": 180600
    },
    {
      "epoch": 0.5594894929297496,
      "grad_norm": 37.97432327270508,
      "learning_rate": 0.00013215315212107513,
      "loss": 0.2054,
      "step": 180700
    },
    {
      "epoch": 0.559799116334802,
      "grad_norm": 0.02831888571381569,
      "learning_rate": 0.0001320602650995594,
      "loss": 0.3597,
      "step": 180800
    },
    {
      "epoch": 0.5601087397398544,
      "grad_norm": 0.002279426669701934,
      "learning_rate": 0.00013196737807804365,
      "loss": 0.3903,
      "step": 180900
    },
    {
      "epoch": 0.5604183631449069,
      "grad_norm": 0.004286410287022591,
      "learning_rate": 0.00013187449105652794,
      "loss": 0.4071,
      "step": 181000
    },
    {
      "epoch": 0.5607279865499593,
      "grad_norm": 0.008757228963077068,
      "learning_rate": 0.0001317816040350122,
      "loss": 0.3255,
      "step": 181100
    },
    {
      "epoch": 0.5610376099550117,
      "grad_norm": 0.002707801293581724,
      "learning_rate": 0.00013168871701349646,
      "loss": 0.2663,
      "step": 181200
    },
    {
      "epoch": 0.5613472333600642,
      "grad_norm": 0.0008073790231719613,
      "learning_rate": 0.00013159582999198075,
      "loss": 0.2467,
      "step": 181300
    },
    {
      "epoch": 0.5616568567651166,
      "grad_norm": 0.03285660222172737,
      "learning_rate": 0.000131502942970465,
      "loss": 0.3638,
      "step": 181400
    },
    {
      "epoch": 0.561966480170169,
      "grad_norm": 0.00204604584723711,
      "learning_rate": 0.00013141005594894927,
      "loss": 0.3667,
      "step": 181500
    },
    {
      "epoch": 0.5622761035752215,
      "grad_norm": 30.986385345458984,
      "learning_rate": 0.00013131716892743355,
      "loss": 0.2356,
      "step": 181600
    },
    {
      "epoch": 0.5625857269802739,
      "grad_norm": 0.00017663303879089653,
      "learning_rate": 0.00013122428190591782,
      "loss": 0.2508,
      "step": 181700
    },
    {
      "epoch": 0.5628953503853263,
      "grad_norm": 0.0005494048818945885,
      "learning_rate": 0.00013113139488440208,
      "loss": 0.3127,
      "step": 181800
    },
    {
      "epoch": 0.5632049737903788,
      "grad_norm": 0.0029322102200239897,
      "learning_rate": 0.00013103850786288636,
      "loss": 0.2675,
      "step": 181900
    },
    {
      "epoch": 0.5635145971954312,
      "grad_norm": 0.00017964896687772125,
      "learning_rate": 0.00013094562084137062,
      "loss": 0.2352,
      "step": 182000
    },
    {
      "epoch": 0.5638242206004836,
      "grad_norm": 39.179134368896484,
      "learning_rate": 0.00013085273381985488,
      "loss": 0.3028,
      "step": 182100
    },
    {
      "epoch": 0.564133844005536,
      "grad_norm": 0.011532372795045376,
      "learning_rate": 0.00013075984679833917,
      "loss": 0.3445,
      "step": 182200
    },
    {
      "epoch": 0.5644434674105885,
      "grad_norm": 2.3767569065093994,
      "learning_rate": 0.00013066695977682343,
      "loss": 0.2057,
      "step": 182300
    },
    {
      "epoch": 0.564753090815641,
      "grad_norm": 0.0968751460313797,
      "learning_rate": 0.0001305740727553077,
      "loss": 0.4267,
      "step": 182400
    },
    {
      "epoch": 0.5650627142206933,
      "grad_norm": 0.0017989405896514654,
      "learning_rate": 0.00013048118573379198,
      "loss": 0.1915,
      "step": 182500
    },
    {
      "epoch": 0.5653723376257458,
      "grad_norm": 58.42329788208008,
      "learning_rate": 0.00013038829871227624,
      "loss": 0.187,
      "step": 182600
    },
    {
      "epoch": 0.5656819610307983,
      "grad_norm": 0.08219503611326218,
      "learning_rate": 0.0001302954116907605,
      "loss": 0.3734,
      "step": 182700
    },
    {
      "epoch": 0.5659915844358506,
      "grad_norm": 92.5615005493164,
      "learning_rate": 0.0001302025246692448,
      "loss": 0.3005,
      "step": 182800
    },
    {
      "epoch": 0.5663012078409031,
      "grad_norm": 101.59895324707031,
      "learning_rate": 0.00013010963764772905,
      "loss": 0.2882,
      "step": 182900
    },
    {
      "epoch": 0.5666108312459556,
      "grad_norm": 0.01502135768532753,
      "learning_rate": 0.0001300167506262133,
      "loss": 0.3249,
      "step": 183000
    },
    {
      "epoch": 0.5669204546510079,
      "grad_norm": 0.003245003754273057,
      "learning_rate": 0.0001299238636046976,
      "loss": 0.3887,
      "step": 183100
    },
    {
      "epoch": 0.5672300780560604,
      "grad_norm": 13.835184097290039,
      "learning_rate": 0.00012983097658318186,
      "loss": 0.2607,
      "step": 183200
    },
    {
      "epoch": 0.5675397014611129,
      "grad_norm": 0.0009237201884388924,
      "learning_rate": 0.00012973808956166615,
      "loss": 0.3993,
      "step": 183300
    },
    {
      "epoch": 0.5678493248661652,
      "grad_norm": 0.005246907006949186,
      "learning_rate": 0.0001296452025401504,
      "loss": 0.1118,
      "step": 183400
    },
    {
      "epoch": 0.5681589482712177,
      "grad_norm": 0.000373915332602337,
      "learning_rate": 0.00012955231551863467,
      "loss": 0.4481,
      "step": 183500
    },
    {
      "epoch": 0.5684685716762702,
      "grad_norm": 0.12639012932777405,
      "learning_rate": 0.00012945942849711895,
      "loss": 0.2,
      "step": 183600
    },
    {
      "epoch": 0.5687781950813225,
      "grad_norm": 0.001731598749756813,
      "learning_rate": 0.00012936654147560321,
      "loss": 0.2193,
      "step": 183700
    },
    {
      "epoch": 0.569087818486375,
      "grad_norm": 24.26197052001953,
      "learning_rate": 0.00012927365445408747,
      "loss": 0.3645,
      "step": 183800
    },
    {
      "epoch": 0.5693974418914275,
      "grad_norm": 14.576606750488281,
      "learning_rate": 0.00012918076743257176,
      "loss": 0.2958,
      "step": 183900
    },
    {
      "epoch": 0.5697070652964799,
      "grad_norm": 0.0005080068367533386,
      "learning_rate": 0.00012908788041105602,
      "loss": 0.1985,
      "step": 184000
    },
    {
      "epoch": 0.5700166887015323,
      "grad_norm": 0.00021910681971348822,
      "learning_rate": 0.00012899499338954028,
      "loss": 0.2449,
      "step": 184100
    },
    {
      "epoch": 0.5703263121065848,
      "grad_norm": 0.011872392147779465,
      "learning_rate": 0.00012890210636802457,
      "loss": 0.3211,
      "step": 184200
    },
    {
      "epoch": 0.5706359355116372,
      "grad_norm": 0.0004616311052814126,
      "learning_rate": 0.00012880921934650883,
      "loss": 0.1625,
      "step": 184300
    },
    {
      "epoch": 0.5709455589166896,
      "grad_norm": 2.5970804691314697,
      "learning_rate": 0.0001287163323249931,
      "loss": 0.3095,
      "step": 184400
    },
    {
      "epoch": 0.5712551823217421,
      "grad_norm": 9.28654956817627,
      "learning_rate": 0.00012862344530347738,
      "loss": 0.2819,
      "step": 184500
    },
    {
      "epoch": 0.5715648057267945,
      "grad_norm": 0.0001971402089111507,
      "learning_rate": 0.00012853055828196164,
      "loss": 0.1739,
      "step": 184600
    },
    {
      "epoch": 0.5718744291318469,
      "grad_norm": 0.0013259039260447025,
      "learning_rate": 0.0001284376712604459,
      "loss": 0.3271,
      "step": 184700
    },
    {
      "epoch": 0.5721840525368994,
      "grad_norm": 0.004357355646789074,
      "learning_rate": 0.0001283447842389302,
      "loss": 0.3427,
      "step": 184800
    },
    {
      "epoch": 0.5724936759419518,
      "grad_norm": 0.0016585871344432235,
      "learning_rate": 0.00012825189721741445,
      "loss": 0.2437,
      "step": 184900
    },
    {
      "epoch": 0.5728032993470042,
      "grad_norm": 0.0176912359893322,
      "learning_rate": 0.0001281590101958987,
      "loss": 0.3165,
      "step": 185000
    },
    {
      "epoch": 0.5731129227520567,
      "grad_norm": 0.04001225531101227,
      "learning_rate": 0.000128066123174383,
      "loss": 0.3325,
      "step": 185100
    },
    {
      "epoch": 0.5734225461571091,
      "grad_norm": 0.006303978618234396,
      "learning_rate": 0.00012797323615286726,
      "loss": 0.248,
      "step": 185200
    },
    {
      "epoch": 0.5737321695621616,
      "grad_norm": 0.008244404569268227,
      "learning_rate": 0.00012788034913135152,
      "loss": 0.4489,
      "step": 185300
    },
    {
      "epoch": 0.574041792967214,
      "grad_norm": 18.465917587280273,
      "learning_rate": 0.0001277874621098358,
      "loss": 0.1835,
      "step": 185400
    },
    {
      "epoch": 0.5743514163722664,
      "grad_norm": 0.002033424796536565,
      "learning_rate": 0.00012769457508832006,
      "loss": 0.1913,
      "step": 185500
    },
    {
      "epoch": 0.5746610397773189,
      "grad_norm": 0.0006829175399616361,
      "learning_rate": 0.00012760168806680433,
      "loss": 0.1358,
      "step": 185600
    },
    {
      "epoch": 0.5749706631823713,
      "grad_norm": 0.00019083775987382978,
      "learning_rate": 0.0001275088010452886,
      "loss": 0.1938,
      "step": 185700
    },
    {
      "epoch": 0.5752802865874237,
      "grad_norm": 0.051292259246110916,
      "learning_rate": 0.00012741591402377287,
      "loss": 0.2659,
      "step": 185800
    },
    {
      "epoch": 0.5755899099924762,
      "grad_norm": 12.131329536437988,
      "learning_rate": 0.00012732302700225713,
      "loss": 0.4333,
      "step": 185900
    },
    {
      "epoch": 0.5758995333975286,
      "grad_norm": 0.0044891987927258015,
      "learning_rate": 0.00012723013998074142,
      "loss": 0.259,
      "step": 186000
    },
    {
      "epoch": 0.576209156802581,
      "grad_norm": 0.0035727268550544977,
      "learning_rate": 0.00012713725295922568,
      "loss": 0.4412,
      "step": 186100
    },
    {
      "epoch": 0.5765187802076335,
      "grad_norm": 0.00016851606778800488,
      "learning_rate": 0.00012704436593770994,
      "loss": 0.3962,
      "step": 186200
    },
    {
      "epoch": 0.5768284036126858,
      "grad_norm": 13.070778846740723,
      "learning_rate": 0.00012695147891619423,
      "loss": 0.4252,
      "step": 186300
    },
    {
      "epoch": 0.5771380270177383,
      "grad_norm": 44.02815246582031,
      "learning_rate": 0.0001268585918946785,
      "loss": 0.3243,
      "step": 186400
    },
    {
      "epoch": 0.5774476504227908,
      "grad_norm": 0.009142017923295498,
      "learning_rate": 0.00012676570487316275,
      "loss": 0.3908,
      "step": 186500
    },
    {
      "epoch": 0.5777572738278431,
      "grad_norm": 4.263068199157715,
      "learning_rate": 0.00012667281785164704,
      "loss": 0.2328,
      "step": 186600
    },
    {
      "epoch": 0.5780668972328956,
      "grad_norm": 0.07792041450738907,
      "learning_rate": 0.0001265799308301313,
      "loss": 0.3368,
      "step": 186700
    },
    {
      "epoch": 0.5783765206379481,
      "grad_norm": 0.0018181154737249017,
      "learning_rate": 0.00012648704380861556,
      "loss": 0.3442,
      "step": 186800
    },
    {
      "epoch": 0.5786861440430004,
      "grad_norm": 0.00460292212665081,
      "learning_rate": 0.00012639415678709985,
      "loss": 0.2908,
      "step": 186900
    },
    {
      "epoch": 0.5789957674480529,
      "grad_norm": 0.7897033095359802,
      "learning_rate": 0.0001263012697655841,
      "loss": 0.2924,
      "step": 187000
    },
    {
      "epoch": 0.5793053908531054,
      "grad_norm": 0.002486703684553504,
      "learning_rate": 0.00012620838274406837,
      "loss": 0.6713,
      "step": 187100
    },
    {
      "epoch": 0.5796150142581578,
      "grad_norm": 59.140750885009766,
      "learning_rate": 0.00012611549572255266,
      "loss": 0.3882,
      "step": 187200
    },
    {
      "epoch": 0.5799246376632102,
      "grad_norm": 39.151275634765625,
      "learning_rate": 0.00012602260870103692,
      "loss": 0.2616,
      "step": 187300
    },
    {
      "epoch": 0.5802342610682627,
      "grad_norm": 0.0002754847228061408,
      "learning_rate": 0.0001259297216795212,
      "loss": 0.298,
      "step": 187400
    },
    {
      "epoch": 0.5805438844733151,
      "grad_norm": 0.0023039509542286396,
      "learning_rate": 0.00012583683465800546,
      "loss": 0.3745,
      "step": 187500
    },
    {
      "epoch": 0.5808535078783675,
      "grad_norm": 0.0015788348391652107,
      "learning_rate": 0.00012574394763648972,
      "loss": 0.2911,
      "step": 187600
    },
    {
      "epoch": 0.58116313128342,
      "grad_norm": 0.8879139423370361,
      "learning_rate": 0.000125651060614974,
      "loss": 0.3649,
      "step": 187700
    },
    {
      "epoch": 0.5814727546884724,
      "grad_norm": 0.006880484521389008,
      "learning_rate": 0.00012555817359345827,
      "loss": 0.2931,
      "step": 187800
    },
    {
      "epoch": 0.5817823780935248,
      "grad_norm": 0.0014014901826158166,
      "learning_rate": 0.00012546528657194253,
      "loss": 0.3738,
      "step": 187900
    },
    {
      "epoch": 0.5820920014985773,
      "grad_norm": 0.001213008537888527,
      "learning_rate": 0.00012537239955042682,
      "loss": 0.2154,
      "step": 188000
    },
    {
      "epoch": 0.5824016249036297,
      "grad_norm": 0.0008207247592508793,
      "learning_rate": 0.00012527951252891108,
      "loss": 0.1593,
      "step": 188100
    },
    {
      "epoch": 0.5827112483086822,
      "grad_norm": 0.0022110098507255316,
      "learning_rate": 0.00012518662550739534,
      "loss": 0.2516,
      "step": 188200
    },
    {
      "epoch": 0.5830208717137346,
      "grad_norm": 0.0017107854364439845,
      "learning_rate": 0.00012509373848587963,
      "loss": 0.2628,
      "step": 188300
    },
    {
      "epoch": 0.583330495118787,
      "grad_norm": 0.000480620568851009,
      "learning_rate": 0.0001250008514643639,
      "loss": 0.2277,
      "step": 188400
    },
    {
      "epoch": 0.5836401185238395,
      "grad_norm": 0.0035690865479409695,
      "learning_rate": 0.00012490796444284815,
      "loss": 0.3315,
      "step": 188500
    },
    {
      "epoch": 0.5839497419288919,
      "grad_norm": 17.141357421875,
      "learning_rate": 0.00012481507742133244,
      "loss": 0.3948,
      "step": 188600
    },
    {
      "epoch": 0.5842593653339443,
      "grad_norm": 0.006897724233567715,
      "learning_rate": 0.0001247221903998167,
      "loss": 0.4319,
      "step": 188700
    },
    {
      "epoch": 0.5845689887389968,
      "grad_norm": 0.4145641326904297,
      "learning_rate": 0.00012462930337830096,
      "loss": 0.2731,
      "step": 188800
    },
    {
      "epoch": 0.5848786121440492,
      "grad_norm": 0.006118114572018385,
      "learning_rate": 0.00012453641635678525,
      "loss": 0.2989,
      "step": 188900
    },
    {
      "epoch": 0.5851882355491016,
      "grad_norm": 0.6484097242355347,
      "learning_rate": 0.0001244435293352695,
      "loss": 0.3718,
      "step": 189000
    },
    {
      "epoch": 0.5854978589541541,
      "grad_norm": 0.0015514177503064275,
      "learning_rate": 0.00012435064231375377,
      "loss": 0.1904,
      "step": 189100
    },
    {
      "epoch": 0.5858074823592065,
      "grad_norm": 58.5396842956543,
      "learning_rate": 0.00012425775529223805,
      "loss": 0.4883,
      "step": 189200
    },
    {
      "epoch": 0.5861171057642589,
      "grad_norm": 0.000702508317772299,
      "learning_rate": 0.00012416486827072231,
      "loss": 0.3837,
      "step": 189300
    },
    {
      "epoch": 0.5864267291693114,
      "grad_norm": 0.005459275096654892,
      "learning_rate": 0.00012407198124920658,
      "loss": 0.3052,
      "step": 189400
    },
    {
      "epoch": 0.5867363525743639,
      "grad_norm": 0.0034587527625262737,
      "learning_rate": 0.00012397909422769086,
      "loss": 0.3274,
      "step": 189500
    },
    {
      "epoch": 0.5870459759794162,
      "grad_norm": 0.010617467574775219,
      "learning_rate": 0.00012388620720617512,
      "loss": 0.2499,
      "step": 189600
    },
    {
      "epoch": 0.5873555993844687,
      "grad_norm": 0.018315445631742477,
      "learning_rate": 0.00012379332018465938,
      "loss": 0.4331,
      "step": 189700
    },
    {
      "epoch": 0.5876652227895212,
      "grad_norm": 0.0006202885997481644,
      "learning_rate": 0.00012370043316314367,
      "loss": 0.1744,
      "step": 189800
    },
    {
      "epoch": 0.5879748461945735,
      "grad_norm": 0.0004025342932436615,
      "learning_rate": 0.00012360754614162793,
      "loss": 0.3134,
      "step": 189900
    },
    {
      "epoch": 0.588284469599626,
      "grad_norm": 0.0012142882915213704,
      "learning_rate": 0.0001235146591201122,
      "loss": 0.2808,
      "step": 190000
    },
    {
      "epoch": 0.5885940930046785,
      "grad_norm": 0.006984622683376074,
      "learning_rate": 0.00012342177209859648,
      "loss": 0.2692,
      "step": 190100
    },
    {
      "epoch": 0.5889037164097308,
      "grad_norm": 0.002878996543586254,
      "learning_rate": 0.00012332888507708074,
      "loss": 0.2354,
      "step": 190200
    },
    {
      "epoch": 0.5892133398147833,
      "grad_norm": 0.0005971842911094427,
      "learning_rate": 0.000123235998055565,
      "loss": 0.277,
      "step": 190300
    },
    {
      "epoch": 0.5895229632198357,
      "grad_norm": 0.049126036465168,
      "learning_rate": 0.00012314311103404926,
      "loss": 0.3032,
      "step": 190400
    },
    {
      "epoch": 0.5898325866248881,
      "grad_norm": 0.49258795380592346,
      "learning_rate": 0.00012305022401253355,
      "loss": 0.2366,
      "step": 190500
    },
    {
      "epoch": 0.5901422100299406,
      "grad_norm": 0.01678019016981125,
      "learning_rate": 0.0001229573369910178,
      "loss": 0.2759,
      "step": 190600
    },
    {
      "epoch": 0.590451833434993,
      "grad_norm": 0.00043826919863931835,
      "learning_rate": 0.00012286444996950207,
      "loss": 0.29,
      "step": 190700
    },
    {
      "epoch": 0.5907614568400454,
      "grad_norm": 12.02450180053711,
      "learning_rate": 0.00012277156294798636,
      "loss": 0.2556,
      "step": 190800
    },
    {
      "epoch": 0.5910710802450979,
      "grad_norm": 111.24392700195312,
      "learning_rate": 0.00012267867592647062,
      "loss": 0.3292,
      "step": 190900
    },
    {
      "epoch": 0.5913807036501503,
      "grad_norm": 0.0010429325047880411,
      "learning_rate": 0.00012258578890495488,
      "loss": 0.2407,
      "step": 191000
    },
    {
      "epoch": 0.5916903270552027,
      "grad_norm": 0.0019543690141290426,
      "learning_rate": 0.00012249290188343917,
      "loss": 0.1671,
      "step": 191100
    },
    {
      "epoch": 0.5919999504602552,
      "grad_norm": 0.4963741898536682,
      "learning_rate": 0.00012240001486192345,
      "loss": 0.3647,
      "step": 191200
    },
    {
      "epoch": 0.5923095738653076,
      "grad_norm": 0.00019910564878955483,
      "learning_rate": 0.00012230712784040769,
      "loss": 0.2728,
      "step": 191300
    },
    {
      "epoch": 0.59261919727036,
      "grad_norm": 0.00020313580171205103,
      "learning_rate": 0.00012221424081889197,
      "loss": 0.1452,
      "step": 191400
    },
    {
      "epoch": 0.5929288206754125,
      "grad_norm": 0.022165756672620773,
      "learning_rate": 0.00012212135379737623,
      "loss": 0.2877,
      "step": 191500
    },
    {
      "epoch": 0.5932384440804649,
      "grad_norm": 0.06667659431695938,
      "learning_rate": 0.00012202846677586051,
      "loss": 0.2509,
      "step": 191600
    },
    {
      "epoch": 0.5935480674855174,
      "grad_norm": 75.88113403320312,
      "learning_rate": 0.00012193557975434478,
      "loss": 0.3862,
      "step": 191700
    },
    {
      "epoch": 0.5938576908905698,
      "grad_norm": 10.7846097946167,
      "learning_rate": 0.00012184269273282906,
      "loss": 0.1082,
      "step": 191800
    },
    {
      "epoch": 0.5941673142956222,
      "grad_norm": 1.0114562511444092,
      "learning_rate": 0.00012174980571131332,
      "loss": 0.2524,
      "step": 191900
    },
    {
      "epoch": 0.5944769377006747,
      "grad_norm": 0.022156180813908577,
      "learning_rate": 0.00012165691868979759,
      "loss": 0.119,
      "step": 192000
    },
    {
      "epoch": 0.5947865611057271,
      "grad_norm": 0.0008647990762256086,
      "learning_rate": 0.00012156403166828186,
      "loss": 0.2979,
      "step": 192100
    },
    {
      "epoch": 0.5950961845107795,
      "grad_norm": 30.707006454467773,
      "learning_rate": 0.00012147114464676613,
      "loss": 0.3113,
      "step": 192200
    },
    {
      "epoch": 0.595405807915832,
      "grad_norm": 0.00738173583522439,
      "learning_rate": 0.0001213782576252504,
      "loss": 0.3933,
      "step": 192300
    },
    {
      "epoch": 0.5957154313208844,
      "grad_norm": 0.0013314272509887815,
      "learning_rate": 0.00012128537060373467,
      "loss": 0.5143,
      "step": 192400
    },
    {
      "epoch": 0.5960250547259368,
      "grad_norm": 0.6131274700164795,
      "learning_rate": 0.00012119248358221893,
      "loss": 0.266,
      "step": 192500
    },
    {
      "epoch": 0.5963346781309893,
      "grad_norm": 0.0009528415976092219,
      "learning_rate": 0.00012109959656070321,
      "loss": 0.1723,
      "step": 192600
    },
    {
      "epoch": 0.5966443015360418,
      "grad_norm": 0.0002690694818738848,
      "learning_rate": 0.00012100670953918748,
      "loss": 0.1937,
      "step": 192700
    },
    {
      "epoch": 0.5969539249410941,
      "grad_norm": 2.72957181930542,
      "learning_rate": 0.00012091382251767174,
      "loss": 0.2359,
      "step": 192800
    },
    {
      "epoch": 0.5972635483461466,
      "grad_norm": 11.023266792297363,
      "learning_rate": 0.00012082093549615602,
      "loss": 0.5754,
      "step": 192900
    },
    {
      "epoch": 0.5975731717511991,
      "grad_norm": 2.242056369781494,
      "learning_rate": 0.00012072804847464029,
      "loss": 0.316,
      "step": 193000
    },
    {
      "epoch": 0.5978827951562514,
      "grad_norm": 3.697819948196411,
      "learning_rate": 0.00012063516145312455,
      "loss": 0.2881,
      "step": 193100
    },
    {
      "epoch": 0.5981924185613039,
      "grad_norm": 25.983104705810547,
      "learning_rate": 0.00012054227443160882,
      "loss": 0.1934,
      "step": 193200
    },
    {
      "epoch": 0.5985020419663564,
      "grad_norm": 0.00043106311932206154,
      "learning_rate": 0.0001204493874100931,
      "loss": 0.3345,
      "step": 193300
    },
    {
      "epoch": 0.5988116653714087,
      "grad_norm": 0.0003815061936620623,
      "learning_rate": 0.00012035650038857737,
      "loss": 0.219,
      "step": 193400
    },
    {
      "epoch": 0.5991212887764612,
      "grad_norm": 0.0012052772799506783,
      "learning_rate": 0.00012026361336706163,
      "loss": 0.2959,
      "step": 193500
    },
    {
      "epoch": 0.5994309121815137,
      "grad_norm": 0.01970032788813114,
      "learning_rate": 0.00012017072634554591,
      "loss": 0.2759,
      "step": 193600
    },
    {
      "epoch": 0.599740535586566,
      "grad_norm": 2.6374847888946533,
      "learning_rate": 0.00012007783932403018,
      "loss": 0.2062,
      "step": 193700
    },
    {
      "epoch": 0.6000501589916185,
      "grad_norm": 0.0011384565150365233,
      "learning_rate": 0.00011998495230251444,
      "loss": 0.2525,
      "step": 193800
    },
    {
      "epoch": 0.600359782396671,
      "grad_norm": 0.7894988059997559,
      "learning_rate": 0.00011989206528099872,
      "loss": 0.2651,
      "step": 193900
    },
    {
      "epoch": 0.6006694058017233,
      "grad_norm": 0.003574331058189273,
      "learning_rate": 0.00011979917825948299,
      "loss": 0.1377,
      "step": 194000
    },
    {
      "epoch": 0.6009790292067758,
      "grad_norm": 0.0008496057125739753,
      "learning_rate": 0.00011970629123796725,
      "loss": 0.1795,
      "step": 194100
    },
    {
      "epoch": 0.6012886526118282,
      "grad_norm": 0.00031185883563011885,
      "learning_rate": 0.00011961340421645152,
      "loss": 0.2669,
      "step": 194200
    },
    {
      "epoch": 0.6015982760168807,
      "grad_norm": 0.017823152244091034,
      "learning_rate": 0.0001195205171949358,
      "loss": 0.2933,
      "step": 194300
    },
    {
      "epoch": 0.6019078994219331,
      "grad_norm": 0.00026863309904001653,
      "learning_rate": 0.00011942763017342006,
      "loss": 0.1881,
      "step": 194400
    },
    {
      "epoch": 0.6022175228269855,
      "grad_norm": 0.0010905981762334704,
      "learning_rate": 0.00011933474315190433,
      "loss": 0.5726,
      "step": 194500
    },
    {
      "epoch": 0.602527146232038,
      "grad_norm": 9.95859432220459,
      "learning_rate": 0.00011924185613038861,
      "loss": 0.2591,
      "step": 194600
    },
    {
      "epoch": 0.6028367696370904,
      "grad_norm": 10.273874282836914,
      "learning_rate": 0.00011914896910887285,
      "loss": 0.319,
      "step": 194700
    },
    {
      "epoch": 0.6031463930421428,
      "grad_norm": 0.012565640732645988,
      "learning_rate": 0.00011905608208735714,
      "loss": 0.3054,
      "step": 194800
    },
    {
      "epoch": 0.6034560164471953,
      "grad_norm": 9.424382209777832,
      "learning_rate": 0.00011896319506584142,
      "loss": 0.2692,
      "step": 194900
    },
    {
      "epoch": 0.6037656398522477,
      "grad_norm": 0.21545936167240143,
      "learning_rate": 0.00011887030804432566,
      "loss": 0.1472,
      "step": 195000
    },
    {
      "epoch": 0.6040752632573001,
      "grad_norm": 50.832767486572266,
      "learning_rate": 0.00011877742102280995,
      "loss": 0.5714,
      "step": 195100
    },
    {
      "epoch": 0.6043848866623526,
      "grad_norm": 0.0313023142516613,
      "learning_rate": 0.00011868453400129422,
      "loss": 0.2768,
      "step": 195200
    },
    {
      "epoch": 0.604694510067405,
      "grad_norm": 0.0007320816512219608,
      "learning_rate": 0.0001185916469797785,
      "loss": 0.3705,
      "step": 195300
    },
    {
      "epoch": 0.6050041334724574,
      "grad_norm": 0.030020831152796745,
      "learning_rate": 0.00011849875995826274,
      "loss": 0.289,
      "step": 195400
    },
    {
      "epoch": 0.6053137568775099,
      "grad_norm": 0.0009729114826768637,
      "learning_rate": 0.00011840587293674703,
      "loss": 0.2479,
      "step": 195500
    },
    {
      "epoch": 0.6056233802825624,
      "grad_norm": 0.0015729109290987253,
      "learning_rate": 0.0001183129859152313,
      "loss": 0.2333,
      "step": 195600
    },
    {
      "epoch": 0.6059330036876147,
      "grad_norm": 0.006127961911261082,
      "learning_rate": 0.00011822009889371555,
      "loss": 0.4315,
      "step": 195700
    },
    {
      "epoch": 0.6062426270926672,
      "grad_norm": 0.004448430612683296,
      "learning_rate": 0.00011812721187219983,
      "loss": 0.2427,
      "step": 195800
    },
    {
      "epoch": 0.6065522504977197,
      "grad_norm": 0.0005995517130941153,
      "learning_rate": 0.00011803432485068411,
      "loss": 0.3043,
      "step": 195900
    },
    {
      "epoch": 0.606861873902772,
      "grad_norm": 0.013217589817941189,
      "learning_rate": 0.00011794143782916836,
      "loss": 0.3411,
      "step": 196000
    },
    {
      "epoch": 0.6071714973078245,
      "grad_norm": 0.000879959377925843,
      "learning_rate": 0.00011784855080765264,
      "loss": 0.4288,
      "step": 196100
    },
    {
      "epoch": 0.607481120712877,
      "grad_norm": 1.2425099611282349,
      "learning_rate": 0.00011775566378613692,
      "loss": 0.3622,
      "step": 196200
    },
    {
      "epoch": 0.6077907441179293,
      "grad_norm": 0.05069353058934212,
      "learning_rate": 0.00011766277676462117,
      "loss": 0.2988,
      "step": 196300
    },
    {
      "epoch": 0.6081003675229818,
      "grad_norm": 0.005495462566614151,
      "learning_rate": 0.00011756988974310544,
      "loss": 0.1142,
      "step": 196400
    },
    {
      "epoch": 0.6084099909280343,
      "grad_norm": 10.323256492614746,
      "learning_rate": 0.00011747700272158972,
      "loss": 0.2834,
      "step": 196500
    },
    {
      "epoch": 0.6087196143330866,
      "grad_norm": 0.00044448571861721575,
      "learning_rate": 0.00011738411570007398,
      "loss": 0.2807,
      "step": 196600
    },
    {
      "epoch": 0.6090292377381391,
      "grad_norm": 0.0012570173712447286,
      "learning_rate": 0.00011729122867855825,
      "loss": 0.2825,
      "step": 196700
    },
    {
      "epoch": 0.6093388611431916,
      "grad_norm": 0.0005043710698373616,
      "learning_rate": 0.00011719834165704253,
      "loss": 0.1786,
      "step": 196800
    },
    {
      "epoch": 0.6096484845482439,
      "grad_norm": 0.00040835951222106814,
      "learning_rate": 0.00011710545463552679,
      "loss": 0.2134,
      "step": 196900
    },
    {
      "epoch": 0.6099581079532964,
      "grad_norm": 0.02343636378645897,
      "learning_rate": 0.00011701256761401106,
      "loss": 0.3296,
      "step": 197000
    },
    {
      "epoch": 0.6102677313583489,
      "grad_norm": 0.0027654978912323713,
      "learning_rate": 0.00011691968059249534,
      "loss": 0.3246,
      "step": 197100
    },
    {
      "epoch": 0.6105773547634012,
      "grad_norm": 2.0128602981567383,
      "learning_rate": 0.00011682679357097961,
      "loss": 0.3121,
      "step": 197200
    },
    {
      "epoch": 0.6108869781684537,
      "grad_norm": 1.487807035446167,
      "learning_rate": 0.00011673390654946387,
      "loss": 0.0991,
      "step": 197300
    },
    {
      "epoch": 0.6111966015735062,
      "grad_norm": 0.000455364934168756,
      "learning_rate": 0.00011664101952794814,
      "loss": 0.3816,
      "step": 197400
    },
    {
      "epoch": 0.6115062249785586,
      "grad_norm": 0.0013216719962656498,
      "learning_rate": 0.00011654813250643242,
      "loss": 0.2665,
      "step": 197500
    },
    {
      "epoch": 0.611815848383611,
      "grad_norm": 122.77568054199219,
      "learning_rate": 0.00011645524548491668,
      "loss": 0.3114,
      "step": 197600
    },
    {
      "epoch": 0.6121254717886635,
      "grad_norm": 64.99510955810547,
      "learning_rate": 0.00011636235846340095,
      "loss": 0.2981,
      "step": 197700
    },
    {
      "epoch": 0.6124350951937159,
      "grad_norm": 69.13766479492188,
      "learning_rate": 0.00011626947144188523,
      "loss": 0.2387,
      "step": 197800
    },
    {
      "epoch": 0.6127447185987683,
      "grad_norm": 0.0003461528103798628,
      "learning_rate": 0.00011617658442036949,
      "loss": 0.2185,
      "step": 197900
    },
    {
      "epoch": 0.6130543420038208,
      "grad_norm": 0.0016436866717413068,
      "learning_rate": 0.00011608369739885376,
      "loss": 0.2419,
      "step": 198000
    },
    {
      "epoch": 0.6133639654088732,
      "grad_norm": 0.005813506431877613,
      "learning_rate": 0.00011599081037733803,
      "loss": 0.3361,
      "step": 198100
    },
    {
      "epoch": 0.6136735888139256,
      "grad_norm": 0.34394925832748413,
      "learning_rate": 0.0001158979233558223,
      "loss": 0.299,
      "step": 198200
    },
    {
      "epoch": 0.613983212218978,
      "grad_norm": 2.7196741104125977,
      "learning_rate": 0.00011580503633430657,
      "loss": 0.3389,
      "step": 198300
    },
    {
      "epoch": 0.6142928356240305,
      "grad_norm": 0.001762846135534346,
      "learning_rate": 0.00011571214931279084,
      "loss": 0.194,
      "step": 198400
    },
    {
      "epoch": 0.614602459029083,
      "grad_norm": 0.0930606797337532,
      "learning_rate": 0.0001156192622912751,
      "loss": 0.1517,
      "step": 198500
    },
    {
      "epoch": 0.6149120824341353,
      "grad_norm": 9.401286125183105,
      "learning_rate": 0.00011552637526975938,
      "loss": 0.2854,
      "step": 198600
    },
    {
      "epoch": 0.6152217058391878,
      "grad_norm": 0.0029105027206242085,
      "learning_rate": 0.00011543348824824365,
      "loss": 0.1403,
      "step": 198700
    },
    {
      "epoch": 0.6155313292442403,
      "grad_norm": 2.3321189880371094,
      "learning_rate": 0.00011534060122672791,
      "loss": 0.2498,
      "step": 198800
    },
    {
      "epoch": 0.6158409526492926,
      "grad_norm": 0.05725253373384476,
      "learning_rate": 0.00011524771420521219,
      "loss": 0.3478,
      "step": 198900
    },
    {
      "epoch": 0.6161505760543451,
      "grad_norm": 0.002302235923707485,
      "learning_rate": 0.00011515482718369646,
      "loss": 0.3649,
      "step": 199000
    },
    {
      "epoch": 0.6164601994593976,
      "grad_norm": 0.03980535268783569,
      "learning_rate": 0.00011506194016218072,
      "loss": 0.1937,
      "step": 199100
    },
    {
      "epoch": 0.6167698228644499,
      "grad_norm": 0.7989278435707092,
      "learning_rate": 0.000114969053140665,
      "loss": 0.2798,
      "step": 199200
    },
    {
      "epoch": 0.6170794462695024,
      "grad_norm": 0.0001717023696983233,
      "learning_rate": 0.00011487616611914927,
      "loss": 0.2819,
      "step": 199300
    },
    {
      "epoch": 0.6173890696745549,
      "grad_norm": 0.00034830052754841745,
      "learning_rate": 0.00011478327909763354,
      "loss": 0.4653,
      "step": 199400
    },
    {
      "epoch": 0.6176986930796072,
      "grad_norm": 0.0004856147861573845,
      "learning_rate": 0.0001146903920761178,
      "loss": 0.2331,
      "step": 199500
    },
    {
      "epoch": 0.6180083164846597,
      "grad_norm": 42.453216552734375,
      "learning_rate": 0.00011459750505460208,
      "loss": 0.2438,
      "step": 199600
    },
    {
      "epoch": 0.6183179398897122,
      "grad_norm": 0.001987427705898881,
      "learning_rate": 0.00011450461803308635,
      "loss": 0.1913,
      "step": 199700
    },
    {
      "epoch": 0.6186275632947645,
      "grad_norm": 0.004445066209882498,
      "learning_rate": 0.00011441173101157061,
      "loss": 0.3187,
      "step": 199800
    },
    {
      "epoch": 0.618937186699817,
      "grad_norm": 0.0002813686733134091,
      "learning_rate": 0.00011431884399005489,
      "loss": 0.2514,
      "step": 199900
    },
    {
      "epoch": 0.6192468101048695,
      "grad_norm": 0.011848308145999908,
      "learning_rate": 0.00011422595696853916,
      "loss": 0.2111,
      "step": 200000
    },
    {
      "epoch": 0.6195564335099218,
      "grad_norm": 1.2945555448532104,
      "learning_rate": 0.00011413306994702342,
      "loss": 0.1075,
      "step": 200100
    },
    {
      "epoch": 0.6198660569149743,
      "grad_norm": 0.004544815979897976,
      "learning_rate": 0.0001140401829255077,
      "loss": 0.1684,
      "step": 200200
    },
    {
      "epoch": 0.6201756803200268,
      "grad_norm": 0.008988477289676666,
      "learning_rate": 0.00011394729590399197,
      "loss": 0.3452,
      "step": 200300
    },
    {
      "epoch": 0.6204853037250792,
      "grad_norm": 0.0029741632752120495,
      "learning_rate": 0.00011385440888247623,
      "loss": 0.2162,
      "step": 200400
    },
    {
      "epoch": 0.6207949271301316,
      "grad_norm": 0.005141159053891897,
      "learning_rate": 0.0001137615218609605,
      "loss": 0.3918,
      "step": 200500
    },
    {
      "epoch": 0.6211045505351841,
      "grad_norm": 0.007429942488670349,
      "learning_rate": 0.00011366863483944478,
      "loss": 0.3252,
      "step": 200600
    },
    {
      "epoch": 0.6214141739402365,
      "grad_norm": 0.0011478267842903733,
      "learning_rate": 0.00011357574781792904,
      "loss": 0.1773,
      "step": 200700
    },
    {
      "epoch": 0.6217237973452889,
      "grad_norm": 0.00039690081030130386,
      "learning_rate": 0.00011348286079641331,
      "loss": 0.3321,
      "step": 200800
    },
    {
      "epoch": 0.6220334207503414,
      "grad_norm": 16.008228302001953,
      "learning_rate": 0.00011338997377489758,
      "loss": 0.467,
      "step": 200900
    },
    {
      "epoch": 0.6223430441553938,
      "grad_norm": 0.005610757041722536,
      "learning_rate": 0.00011329708675338185,
      "loss": 0.2754,
      "step": 201000
    },
    {
      "epoch": 0.6226526675604462,
      "grad_norm": 0.001112255617044866,
      "learning_rate": 0.00011320419973186612,
      "loss": 0.2219,
      "step": 201100
    },
    {
      "epoch": 0.6229622909654987,
      "grad_norm": 0.022761598229408264,
      "learning_rate": 0.0001131113127103504,
      "loss": 0.3734,
      "step": 201200
    },
    {
      "epoch": 0.6232719143705511,
      "grad_norm": 0.0019873541314154863,
      "learning_rate": 0.00011301842568883467,
      "loss": 0.1366,
      "step": 201300
    },
    {
      "epoch": 0.6235815377756035,
      "grad_norm": 0.016008848324418068,
      "learning_rate": 0.00011292553866731893,
      "loss": 0.303,
      "step": 201400
    },
    {
      "epoch": 0.623891161180656,
      "grad_norm": 25.131805419921875,
      "learning_rate": 0.0001128326516458032,
      "loss": 0.3939,
      "step": 201500
    },
    {
      "epoch": 0.6242007845857084,
      "grad_norm": 0.0013446095399558544,
      "learning_rate": 0.00011273976462428748,
      "loss": 0.3959,
      "step": 201600
    },
    {
      "epoch": 0.6245104079907609,
      "grad_norm": 53.164737701416016,
      "learning_rate": 0.00011264687760277174,
      "loss": 0.3865,
      "step": 201700
    },
    {
      "epoch": 0.6248200313958133,
      "grad_norm": 0.0024268904235213995,
      "learning_rate": 0.00011255399058125601,
      "loss": 0.2253,
      "step": 201800
    },
    {
      "epoch": 0.6251296548008657,
      "grad_norm": 0.0037849140353500843,
      "learning_rate": 0.00011246110355974028,
      "loss": 0.203,
      "step": 201900
    },
    {
      "epoch": 0.6254392782059182,
      "grad_norm": 46.584476470947266,
      "learning_rate": 0.00011236821653822454,
      "loss": 0.3724,
      "step": 202000
    },
    {
      "epoch": 0.6257489016109706,
      "grad_norm": 0.0023088129237294197,
      "learning_rate": 0.00011227532951670882,
      "loss": 0.3307,
      "step": 202100
    },
    {
      "epoch": 0.626058525016023,
      "grad_norm": 68.38398742675781,
      "learning_rate": 0.00011218244249519309,
      "loss": 0.3693,
      "step": 202200
    },
    {
      "epoch": 0.6263681484210755,
      "grad_norm": 0.0011066357837989926,
      "learning_rate": 0.00011208955547367735,
      "loss": 0.5098,
      "step": 202300
    },
    {
      "epoch": 0.6266777718261278,
      "grad_norm": 0.0022833640687167645,
      "learning_rate": 0.00011199666845216163,
      "loss": 0.2863,
      "step": 202400
    },
    {
      "epoch": 0.6269873952311803,
      "grad_norm": 0.00263246544636786,
      "learning_rate": 0.0001119037814306459,
      "loss": 0.2821,
      "step": 202500
    },
    {
      "epoch": 0.6272970186362328,
      "grad_norm": 0.0007559664081782103,
      "learning_rate": 0.00011181089440913016,
      "loss": 0.2953,
      "step": 202600
    },
    {
      "epoch": 0.6276066420412851,
      "grad_norm": 0.0005454153288155794,
      "learning_rate": 0.00011171800738761444,
      "loss": 0.1978,
      "step": 202700
    },
    {
      "epoch": 0.6279162654463376,
      "grad_norm": 0.0010318269487470388,
      "learning_rate": 0.00011162512036609871,
      "loss": 0.2187,
      "step": 202800
    },
    {
      "epoch": 0.6282258888513901,
      "grad_norm": 0.002661454724147916,
      "learning_rate": 0.00011153223334458297,
      "loss": 0.3927,
      "step": 202900
    },
    {
      "epoch": 0.6285355122564424,
      "grad_norm": 0.007706083822995424,
      "learning_rate": 0.00011143934632306724,
      "loss": 0.3603,
      "step": 203000
    },
    {
      "epoch": 0.6288451356614949,
      "grad_norm": 0.0014704257482662797,
      "learning_rate": 0.00011134645930155152,
      "loss": 0.3852,
      "step": 203100
    },
    {
      "epoch": 0.6291547590665474,
      "grad_norm": 1.0180881023406982,
      "learning_rate": 0.00011125357228003579,
      "loss": 0.3209,
      "step": 203200
    },
    {
      "epoch": 0.6294643824715997,
      "grad_norm": 47.20135498046875,
      "learning_rate": 0.00011116068525852005,
      "loss": 0.3361,
      "step": 203300
    },
    {
      "epoch": 0.6297740058766522,
      "grad_norm": 0.012442727573215961,
      "learning_rate": 0.00011106779823700433,
      "loss": 0.2256,
      "step": 203400
    },
    {
      "epoch": 0.6300836292817047,
      "grad_norm": 0.006283456925302744,
      "learning_rate": 0.0001109749112154886,
      "loss": 0.3042,
      "step": 203500
    },
    {
      "epoch": 0.630393252686757,
      "grad_norm": 5.934100045124069e-05,
      "learning_rate": 0.00011088202419397286,
      "loss": 0.3015,
      "step": 203600
    },
    {
      "epoch": 0.6307028760918095,
      "grad_norm": 0.006616991478949785,
      "learning_rate": 0.00011078913717245714,
      "loss": 0.2998,
      "step": 203700
    },
    {
      "epoch": 0.631012499496862,
      "grad_norm": 18.216827392578125,
      "learning_rate": 0.00011069625015094141,
      "loss": 0.4535,
      "step": 203800
    },
    {
      "epoch": 0.6313221229019144,
      "grad_norm": 0.008622965775430202,
      "learning_rate": 0.00011060336312942567,
      "loss": 0.325,
      "step": 203900
    },
    {
      "epoch": 0.6316317463069668,
      "grad_norm": 0.003285941667854786,
      "learning_rate": 0.00011051047610790994,
      "loss": 0.2529,
      "step": 204000
    },
    {
      "epoch": 0.6319413697120193,
      "grad_norm": 0.002003669971600175,
      "learning_rate": 0.00011041758908639422,
      "loss": 0.3116,
      "step": 204100
    },
    {
      "epoch": 0.6322509931170717,
      "grad_norm": 3.5749456882476807,
      "learning_rate": 0.00011032470206487848,
      "loss": 0.3366,
      "step": 204200
    },
    {
      "epoch": 0.6325606165221241,
      "grad_norm": 0.0009091203100979328,
      "learning_rate": 0.00011023181504336275,
      "loss": 0.1681,
      "step": 204300
    },
    {
      "epoch": 0.6328702399271766,
      "grad_norm": 10.142525672912598,
      "learning_rate": 0.00011013892802184703,
      "loss": 0.1648,
      "step": 204400
    },
    {
      "epoch": 0.633179863332229,
      "grad_norm": 23.62123680114746,
      "learning_rate": 0.00011004604100033129,
      "loss": 0.1848,
      "step": 204500
    },
    {
      "epoch": 0.6334894867372814,
      "grad_norm": 0.0014436969067901373,
      "learning_rate": 0.00010995315397881556,
      "loss": 0.1763,
      "step": 204600
    },
    {
      "epoch": 0.6337991101423339,
      "grad_norm": 0.8006513714790344,
      "learning_rate": 0.00010986026695729983,
      "loss": 0.3863,
      "step": 204700
    },
    {
      "epoch": 0.6341087335473863,
      "grad_norm": 0.0037689439486712217,
      "learning_rate": 0.0001097673799357841,
      "loss": 0.2754,
      "step": 204800
    },
    {
      "epoch": 0.6344183569524388,
      "grad_norm": 0.0012071274686604738,
      "learning_rate": 0.00010967449291426837,
      "loss": 0.2113,
      "step": 204900
    },
    {
      "epoch": 0.6347279803574912,
      "grad_norm": 0.005073350388556719,
      "learning_rate": 0.00010958160589275264,
      "loss": 0.282,
      "step": 205000
    },
    {
      "epoch": 0.6350376037625436,
      "grad_norm": 0.017933223396539688,
      "learning_rate": 0.00010948871887123689,
      "loss": 0.1271,
      "step": 205100
    },
    {
      "epoch": 0.6353472271675961,
      "grad_norm": 12.19930362701416,
      "learning_rate": 0.00010939583184972118,
      "loss": 0.3264,
      "step": 205200
    },
    {
      "epoch": 0.6356568505726485,
      "grad_norm": 0.13680174946784973,
      "learning_rate": 0.00010930294482820545,
      "loss": 0.2616,
      "step": 205300
    },
    {
      "epoch": 0.6359664739777009,
      "grad_norm": 0.011914734728634357,
      "learning_rate": 0.00010921005780668973,
      "loss": 0.1668,
      "step": 205400
    },
    {
      "epoch": 0.6362760973827534,
      "grad_norm": 0.0019331759540364146,
      "learning_rate": 0.00010911717078517399,
      "loss": 0.1519,
      "step": 205500
    },
    {
      "epoch": 0.6365857207878058,
      "grad_norm": 0.001256256364285946,
      "learning_rate": 0.00010902428376365826,
      "loss": 0.2276,
      "step": 205600
    },
    {
      "epoch": 0.6368953441928582,
      "grad_norm": 0.0005285939550958574,
      "learning_rate": 0.00010893139674214253,
      "loss": 0.2068,
      "step": 205700
    },
    {
      "epoch": 0.6372049675979107,
      "grad_norm": 0.002158824121579528,
      "learning_rate": 0.00010883850972062678,
      "loss": 0.2131,
      "step": 205800
    },
    {
      "epoch": 0.6375145910029631,
      "grad_norm": 0.0015881770523265004,
      "learning_rate": 0.00010874562269911107,
      "loss": 0.2183,
      "step": 205900
    },
    {
      "epoch": 0.6378242144080155,
      "grad_norm": 0.003203223692253232,
      "learning_rate": 0.00010865273567759534,
      "loss": 0.2967,
      "step": 206000
    },
    {
      "epoch": 0.638133837813068,
      "grad_norm": 0.0008914548670873046,
      "learning_rate": 0.00010855984865607959,
      "loss": 0.2591,
      "step": 206100
    },
    {
      "epoch": 0.6384434612181203,
      "grad_norm": 0.0016388705698773265,
      "learning_rate": 0.00010846696163456388,
      "loss": 0.3924,
      "step": 206200
    },
    {
      "epoch": 0.6387530846231728,
      "grad_norm": 0.003046283731237054,
      "learning_rate": 0.00010837407461304815,
      "loss": 0.3005,
      "step": 206300
    },
    {
      "epoch": 0.6390627080282253,
      "grad_norm": 0.0032529958989471197,
      "learning_rate": 0.0001082811875915324,
      "loss": 0.2603,
      "step": 206400
    },
    {
      "epoch": 0.6393723314332777,
      "grad_norm": 0.6113865971565247,
      "learning_rate": 0.00010818830057001667,
      "loss": 0.0965,
      "step": 206500
    },
    {
      "epoch": 0.6396819548383301,
      "grad_norm": 0.02010834962129593,
      "learning_rate": 0.00010809541354850096,
      "loss": 0.2194,
      "step": 206600
    },
    {
      "epoch": 0.6399915782433826,
      "grad_norm": 0.005484126973897219,
      "learning_rate": 0.0001080025265269852,
      "loss": 0.3746,
      "step": 206700
    },
    {
      "epoch": 0.640301201648435,
      "grad_norm": 0.3094926178455353,
      "learning_rate": 0.00010790963950546948,
      "loss": 0.2767,
      "step": 206800
    },
    {
      "epoch": 0.6406108250534874,
      "grad_norm": 0.001610667910426855,
      "learning_rate": 0.00010781675248395375,
      "loss": 0.3127,
      "step": 206900
    },
    {
      "epoch": 0.6409204484585399,
      "grad_norm": 0.18813163042068481,
      "learning_rate": 0.00010772386546243801,
      "loss": 0.2131,
      "step": 207000
    },
    {
      "epoch": 0.6412300718635923,
      "grad_norm": 0.0015481910668313503,
      "learning_rate": 0.00010763097844092229,
      "loss": 0.509,
      "step": 207100
    },
    {
      "epoch": 0.6415396952686447,
      "grad_norm": 0.00035938943619839847,
      "learning_rate": 0.00010753809141940656,
      "loss": 0.2011,
      "step": 207200
    },
    {
      "epoch": 0.6418493186736972,
      "grad_norm": 0.005478595849126577,
      "learning_rate": 0.00010744520439789085,
      "loss": 0.1225,
      "step": 207300
    },
    {
      "epoch": 0.6421589420787496,
      "grad_norm": 0.0008423903491348028,
      "learning_rate": 0.0001073523173763751,
      "loss": 0.1445,
      "step": 207400
    },
    {
      "epoch": 0.642468565483802,
      "grad_norm": 0.002445251913741231,
      "learning_rate": 0.00010725943035485937,
      "loss": 0.2612,
      "step": 207500
    },
    {
      "epoch": 0.6427781888888545,
      "grad_norm": 0.0003322865522932261,
      "learning_rate": 0.00010716654333334365,
      "loss": 0.2989,
      "step": 207600
    },
    {
      "epoch": 0.6430878122939069,
      "grad_norm": 9.674955368041992,
      "learning_rate": 0.0001070736563118279,
      "loss": 0.2966,
      "step": 207700
    },
    {
      "epoch": 0.6433974356989594,
      "grad_norm": 0.0007962121162563562,
      "learning_rate": 0.00010698076929031218,
      "loss": 0.3084,
      "step": 207800
    },
    {
      "epoch": 0.6437070591040118,
      "grad_norm": 0.0006526861106976867,
      "learning_rate": 0.00010688788226879645,
      "loss": 0.34,
      "step": 207900
    },
    {
      "epoch": 0.6440166825090642,
      "grad_norm": 0.2073875069618225,
      "learning_rate": 0.00010679499524728071,
      "loss": 0.3087,
      "step": 208000
    },
    {
      "epoch": 0.6443263059141167,
      "grad_norm": 0.0004896353930234909,
      "learning_rate": 0.00010670210822576499,
      "loss": 0.1768,
      "step": 208100
    },
    {
      "epoch": 0.6446359293191691,
      "grad_norm": 0.0009744432172738016,
      "learning_rate": 0.00010660922120424926,
      "loss": 0.1951,
      "step": 208200
    },
    {
      "epoch": 0.6449455527242215,
      "grad_norm": 0.0040328409522771835,
      "learning_rate": 0.00010651633418273352,
      "loss": 0.076,
      "step": 208300
    },
    {
      "epoch": 0.645255176129274,
      "grad_norm": 0.0034146031830459833,
      "learning_rate": 0.0001064234471612178,
      "loss": 0.2988,
      "step": 208400
    },
    {
      "epoch": 0.6455647995343264,
      "grad_norm": 0.0019956878386437893,
      "learning_rate": 0.00010633056013970207,
      "loss": 0.1981,
      "step": 208500
    },
    {
      "epoch": 0.6458744229393788,
      "grad_norm": 5.778881549835205,
      "learning_rate": 0.00010623767311818633,
      "loss": 0.3899,
      "step": 208600
    },
    {
      "epoch": 0.6461840463444313,
      "grad_norm": 0.0032628478948026896,
      "learning_rate": 0.0001061447860966706,
      "loss": 0.2571,
      "step": 208700
    },
    {
      "epoch": 0.6464936697494837,
      "grad_norm": 0.00663644028827548,
      "learning_rate": 0.00010605189907515488,
      "loss": 0.2602,
      "step": 208800
    },
    {
      "epoch": 0.6468032931545361,
      "grad_norm": 0.00010550322622293606,
      "learning_rate": 0.00010595901205363914,
      "loss": 0.3566,
      "step": 208900
    },
    {
      "epoch": 0.6471129165595886,
      "grad_norm": 32.57292556762695,
      "learning_rate": 0.00010586612503212341,
      "loss": 0.3135,
      "step": 209000
    },
    {
      "epoch": 0.647422539964641,
      "grad_norm": 0.0009231492877006531,
      "learning_rate": 0.00010577323801060769,
      "loss": 0.2338,
      "step": 209100
    },
    {
      "epoch": 0.6477321633696934,
      "grad_norm": 0.0028086560778319836,
      "learning_rate": 0.00010568035098909196,
      "loss": 0.3685,
      "step": 209200
    },
    {
      "epoch": 0.6480417867747459,
      "grad_norm": 0.006456243339926004,
      "learning_rate": 0.00010558746396757622,
      "loss": 0.3396,
      "step": 209300
    },
    {
      "epoch": 0.6483514101797984,
      "grad_norm": 0.0009436347172595561,
      "learning_rate": 0.0001054945769460605,
      "loss": 0.2205,
      "step": 209400
    },
    {
      "epoch": 0.6486610335848507,
      "grad_norm": 0.2640451490879059,
      "learning_rate": 0.00010540168992454477,
      "loss": 0.2323,
      "step": 209500
    },
    {
      "epoch": 0.6489706569899032,
      "grad_norm": 0.00036587470094673336,
      "learning_rate": 0.00010530880290302903,
      "loss": 0.3754,
      "step": 209600
    },
    {
      "epoch": 0.6492802803949557,
      "grad_norm": 145.47109985351562,
      "learning_rate": 0.0001052159158815133,
      "loss": 0.4354,
      "step": 209700
    },
    {
      "epoch": 0.649589903800008,
      "grad_norm": 0.0016936727333813906,
      "learning_rate": 0.00010512302885999758,
      "loss": 0.2248,
      "step": 209800
    },
    {
      "epoch": 0.6498995272050605,
      "grad_norm": 114.03053283691406,
      "learning_rate": 0.00010503014183848184,
      "loss": 0.285,
      "step": 209900
    },
    {
      "epoch": 0.650209150610113,
      "grad_norm": 0.3284957706928253,
      "learning_rate": 0.00010493725481696611,
      "loss": 0.3056,
      "step": 210000
    },
    {
      "epoch": 0.6505187740151653,
      "grad_norm": 0.023036999627947807,
      "learning_rate": 0.00010484436779545039,
      "loss": 0.3173,
      "step": 210100
    },
    {
      "epoch": 0.6508283974202178,
      "grad_norm": 0.007487475872039795,
      "learning_rate": 0.00010475148077393465,
      "loss": 0.2295,
      "step": 210200
    },
    {
      "epoch": 0.6511380208252702,
      "grad_norm": 1.071792721748352,
      "learning_rate": 0.00010465859375241892,
      "loss": 0.1065,
      "step": 210300
    },
    {
      "epoch": 0.6514476442303226,
      "grad_norm": 0.006157076917588711,
      "learning_rate": 0.0001045657067309032,
      "loss": 0.1937,
      "step": 210400
    },
    {
      "epoch": 0.6517572676353751,
      "grad_norm": 0.0016524854581803083,
      "learning_rate": 0.00010447281970938746,
      "loss": 0.2067,
      "step": 210500
    },
    {
      "epoch": 0.6520668910404275,
      "grad_norm": 0.000769929145462811,
      "learning_rate": 0.00010437993268787173,
      "loss": 0.2157,
      "step": 210600
    },
    {
      "epoch": 0.65237651444548,
      "grad_norm": 0.001299429452046752,
      "learning_rate": 0.000104287045666356,
      "loss": 0.2248,
      "step": 210700
    },
    {
      "epoch": 0.6526861378505324,
      "grad_norm": 0.0011232595425099134,
      "learning_rate": 0.00010419415864484026,
      "loss": 0.2749,
      "step": 210800
    },
    {
      "epoch": 0.6529957612555848,
      "grad_norm": 0.00018885202007368207,
      "learning_rate": 0.00010410127162332454,
      "loss": 0.1316,
      "step": 210900
    },
    {
      "epoch": 0.6533053846606373,
      "grad_norm": 0.01679537072777748,
      "learning_rate": 0.00010400838460180881,
      "loss": 0.2925,
      "step": 211000
    },
    {
      "epoch": 0.6536150080656897,
      "grad_norm": 0.002204171847552061,
      "learning_rate": 0.00010391549758029307,
      "loss": 0.29,
      "step": 211100
    },
    {
      "epoch": 0.6539246314707421,
      "grad_norm": 12.442761421203613,
      "learning_rate": 0.00010382261055877735,
      "loss": 0.1372,
      "step": 211200
    },
    {
      "epoch": 0.6542342548757946,
      "grad_norm": 0.030882084742188454,
      "learning_rate": 0.00010372972353726162,
      "loss": 0.1981,
      "step": 211300
    },
    {
      "epoch": 0.654543878280847,
      "grad_norm": 0.2922859489917755,
      "learning_rate": 0.0001036368365157459,
      "loss": 0.2929,
      "step": 211400
    },
    {
      "epoch": 0.6548535016858994,
      "grad_norm": 0.00019421013712417334,
      "learning_rate": 0.00010354394949423016,
      "loss": 0.5037,
      "step": 211500
    },
    {
      "epoch": 0.6551631250909519,
      "grad_norm": 117.15813446044922,
      "learning_rate": 0.00010345106247271443,
      "loss": 0.388,
      "step": 211600
    },
    {
      "epoch": 0.6554727484960043,
      "grad_norm": 23.343324661254883,
      "learning_rate": 0.0001033581754511987,
      "loss": 0.3909,
      "step": 211700
    },
    {
      "epoch": 0.6557823719010567,
      "grad_norm": 0.13740059733390808,
      "learning_rate": 0.00010326528842968296,
      "loss": 0.393,
      "step": 211800
    },
    {
      "epoch": 0.6560919953061092,
      "grad_norm": 0.0016936088213697076,
      "learning_rate": 0.00010317240140816724,
      "loss": 0.4199,
      "step": 211900
    },
    {
      "epoch": 0.6564016187111616,
      "grad_norm": 0.02925713174045086,
      "learning_rate": 0.00010307951438665151,
      "loss": 0.2565,
      "step": 212000
    },
    {
      "epoch": 0.656711242116214,
      "grad_norm": 1.3494935035705566,
      "learning_rate": 0.00010298662736513577,
      "loss": 0.3426,
      "step": 212100
    },
    {
      "epoch": 0.6570208655212665,
      "grad_norm": 17.85242462158203,
      "learning_rate": 0.00010289374034362005,
      "loss": 0.2236,
      "step": 212200
    },
    {
      "epoch": 0.657330488926319,
      "grad_norm": 0.03674367442727089,
      "learning_rate": 0.00010280085332210432,
      "loss": 0.2927,
      "step": 212300
    },
    {
      "epoch": 0.6576401123313713,
      "grad_norm": 0.0031276775989681482,
      "learning_rate": 0.00010270796630058858,
      "loss": 0.2211,
      "step": 212400
    },
    {
      "epoch": 0.6579497357364238,
      "grad_norm": 0.001655028434470296,
      "learning_rate": 0.00010261507927907286,
      "loss": 0.124,
      "step": 212500
    },
    {
      "epoch": 0.6582593591414763,
      "grad_norm": 0.0035289637744426727,
      "learning_rate": 0.00010252219225755713,
      "loss": 0.225,
      "step": 212600
    },
    {
      "epoch": 0.6585689825465286,
      "grad_norm": 0.002854034071788192,
      "learning_rate": 0.00010242930523604139,
      "loss": 0.3995,
      "step": 212700
    },
    {
      "epoch": 0.6588786059515811,
      "grad_norm": 0.0008043613051995635,
      "learning_rate": 0.00010233641821452566,
      "loss": 0.2777,
      "step": 212800
    },
    {
      "epoch": 0.6591882293566336,
      "grad_norm": 0.002667308086529374,
      "learning_rate": 0.00010224353119300994,
      "loss": 0.287,
      "step": 212900
    },
    {
      "epoch": 0.6594978527616859,
      "grad_norm": 0.0005136372055858374,
      "learning_rate": 0.0001021506441714942,
      "loss": 0.1205,
      "step": 213000
    },
    {
      "epoch": 0.6598074761667384,
      "grad_norm": 16.95502471923828,
      "learning_rate": 0.00010205775714997847,
      "loss": 0.2729,
      "step": 213100
    },
    {
      "epoch": 0.6601170995717909,
      "grad_norm": 100.39083099365234,
      "learning_rate": 0.00010196487012846275,
      "loss": 0.3614,
      "step": 213200
    },
    {
      "epoch": 0.6604267229768432,
      "grad_norm": 0.000683092512190342,
      "learning_rate": 0.00010187198310694702,
      "loss": 0.1307,
      "step": 213300
    },
    {
      "epoch": 0.6607363463818957,
      "grad_norm": 0.0004377486475277692,
      "learning_rate": 0.00010177909608543128,
      "loss": 0.2724,
      "step": 213400
    },
    {
      "epoch": 0.6610459697869482,
      "grad_norm": 74.47604370117188,
      "learning_rate": 0.00010168620906391555,
      "loss": 0.28,
      "step": 213500
    },
    {
      "epoch": 0.6613555931920005,
      "grad_norm": 0.17061442136764526,
      "learning_rate": 0.00010159332204239983,
      "loss": 0.1635,
      "step": 213600
    },
    {
      "epoch": 0.661665216597053,
      "grad_norm": 0.001554984482936561,
      "learning_rate": 0.00010150043502088409,
      "loss": 0.2986,
      "step": 213700
    },
    {
      "epoch": 0.6619748400021055,
      "grad_norm": 0.033278148621320724,
      "learning_rate": 0.00010140754799936836,
      "loss": 0.0777,
      "step": 213800
    },
    {
      "epoch": 0.6622844634071579,
      "grad_norm": 0.0038106769789010286,
      "learning_rate": 0.00010131466097785264,
      "loss": 0.3358,
      "step": 213900
    },
    {
      "epoch": 0.6625940868122103,
      "grad_norm": 0.0008068095194175839,
      "learning_rate": 0.0001012217739563369,
      "loss": 0.3184,
      "step": 214000
    },
    {
      "epoch": 0.6629037102172628,
      "grad_norm": 0.0006141682388260961,
      "learning_rate": 0.00010112888693482117,
      "loss": 0.2737,
      "step": 214100
    },
    {
      "epoch": 0.6632133336223152,
      "grad_norm": 0.0036360209342092276,
      "learning_rate": 0.00010103599991330545,
      "loss": 0.2627,
      "step": 214200
    },
    {
      "epoch": 0.6635229570273676,
      "grad_norm": 0.002400761004537344,
      "learning_rate": 0.0001009431128917897,
      "loss": 0.1224,
      "step": 214300
    },
    {
      "epoch": 0.66383258043242,
      "grad_norm": 0.004884965252131224,
      "learning_rate": 0.00010085022587027398,
      "loss": 0.3258,
      "step": 214400
    },
    {
      "epoch": 0.6641422038374725,
      "grad_norm": 0.0006479833391495049,
      "learning_rate": 0.00010075733884875825,
      "loss": 0.3611,
      "step": 214500
    },
    {
      "epoch": 0.6644518272425249,
      "grad_norm": 0.002033888828009367,
      "learning_rate": 0.00010066445182724251,
      "loss": 0.1664,
      "step": 214600
    },
    {
      "epoch": 0.6647614506475773,
      "grad_norm": 0.001794822164811194,
      "learning_rate": 0.00010057156480572679,
      "loss": 0.1447,
      "step": 214700
    },
    {
      "epoch": 0.6650710740526298,
      "grad_norm": 0.1704394519329071,
      "learning_rate": 0.00010047867778421106,
      "loss": 0.3255,
      "step": 214800
    },
    {
      "epoch": 0.6653806974576822,
      "grad_norm": 0.020478637889027596,
      "learning_rate": 0.00010038579076269532,
      "loss": 0.0857,
      "step": 214900
    },
    {
      "epoch": 0.6656903208627346,
      "grad_norm": 0.000853982986882329,
      "learning_rate": 0.0001002929037411796,
      "loss": 0.2379,
      "step": 215000
    },
    {
      "epoch": 0.6659999442677871,
      "grad_norm": 0.0008639026782475412,
      "learning_rate": 0.00010020001671966387,
      "loss": 0.1563,
      "step": 215100
    },
    {
      "epoch": 0.6663095676728396,
      "grad_norm": 0.009700006805360317,
      "learning_rate": 0.00010010712969814815,
      "loss": 0.1906,
      "step": 215200
    },
    {
      "epoch": 0.6666191910778919,
      "grad_norm": 0.0013668142491951585,
      "learning_rate": 0.0001000142426766324,
      "loss": 0.3681,
      "step": 215300
    },
    {
      "epoch": 0.6669288144829444,
      "grad_norm": 0.004050320014357567,
      "learning_rate": 9.992135565511668e-05,
      "loss": 0.1703,
      "step": 215400
    },
    {
      "epoch": 0.6672384378879969,
      "grad_norm": 99.66353607177734,
      "learning_rate": 9.982846863360095e-05,
      "loss": 0.181,
      "step": 215500
    },
    {
      "epoch": 0.6675480612930492,
      "grad_norm": 0.0036628027446568012,
      "learning_rate": 9.973558161208521e-05,
      "loss": 0.2551,
      "step": 215600
    },
    {
      "epoch": 0.6678576846981017,
      "grad_norm": 0.0009708403376862407,
      "learning_rate": 9.964269459056949e-05,
      "loss": 0.227,
      "step": 215700
    },
    {
      "epoch": 0.6681673081031542,
      "grad_norm": 0.0016981327207759023,
      "learning_rate": 9.954980756905376e-05,
      "loss": 0.2416,
      "step": 215800
    },
    {
      "epoch": 0.6684769315082065,
      "grad_norm": 0.0038423093501478434,
      "learning_rate": 9.945692054753802e-05,
      "loss": 0.3205,
      "step": 215900
    },
    {
      "epoch": 0.668786554913259,
      "grad_norm": 0.04167358577251434,
      "learning_rate": 9.93640335260223e-05,
      "loss": 0.2727,
      "step": 216000
    },
    {
      "epoch": 0.6690961783183115,
      "grad_norm": 0.0007337684510275722,
      "learning_rate": 9.927114650450657e-05,
      "loss": 0.3116,
      "step": 216100
    },
    {
      "epoch": 0.6694058017233638,
      "grad_norm": 0.0007402721676044166,
      "learning_rate": 9.917825948299082e-05,
      "loss": 0.2478,
      "step": 216200
    },
    {
      "epoch": 0.6697154251284163,
      "grad_norm": 0.3780035376548767,
      "learning_rate": 9.90853724614751e-05,
      "loss": 0.2169,
      "step": 216300
    },
    {
      "epoch": 0.6700250485334688,
      "grad_norm": 0.014185542240738869,
      "learning_rate": 9.899248543995938e-05,
      "loss": 0.1953,
      "step": 216400
    },
    {
      "epoch": 0.6703346719385211,
      "grad_norm": 64.93321990966797,
      "learning_rate": 9.889959841844363e-05,
      "loss": 0.1623,
      "step": 216500
    },
    {
      "epoch": 0.6706442953435736,
      "grad_norm": 14.089445114135742,
      "learning_rate": 9.880671139692791e-05,
      "loss": 0.1423,
      "step": 216600
    },
    {
      "epoch": 0.6709539187486261,
      "grad_norm": 0.0015391770284622908,
      "learning_rate": 9.871382437541219e-05,
      "loss": 0.3385,
      "step": 216700
    },
    {
      "epoch": 0.6712635421536784,
      "grad_norm": 0.01237184926867485,
      "learning_rate": 9.862093735389643e-05,
      "loss": 0.3995,
      "step": 216800
    },
    {
      "epoch": 0.6715731655587309,
      "grad_norm": 0.10452583432197571,
      "learning_rate": 9.852805033238071e-05,
      "loss": 0.3355,
      "step": 216900
    },
    {
      "epoch": 0.6718827889637834,
      "grad_norm": 0.0028029128443449736,
      "learning_rate": 9.8435163310865e-05,
      "loss": 0.1488,
      "step": 217000
    },
    {
      "epoch": 0.6721924123688358,
      "grad_norm": 8.077728271484375,
      "learning_rate": 9.834227628934924e-05,
      "loss": 0.4172,
      "step": 217100
    },
    {
      "epoch": 0.6725020357738882,
      "grad_norm": 0.0004914621240459383,
      "learning_rate": 9.824938926783352e-05,
      "loss": 0.2984,
      "step": 217200
    },
    {
      "epoch": 0.6728116591789407,
      "grad_norm": 0.001584774348884821,
      "learning_rate": 9.815650224631779e-05,
      "loss": 0.4671,
      "step": 217300
    },
    {
      "epoch": 0.6731212825839931,
      "grad_norm": 0.0011372965527698398,
      "learning_rate": 9.806361522480208e-05,
      "loss": 0.2683,
      "step": 217400
    },
    {
      "epoch": 0.6734309059890455,
      "grad_norm": 0.003910163417458534,
      "learning_rate": 9.797072820328633e-05,
      "loss": 0.1371,
      "step": 217500
    },
    {
      "epoch": 0.673740529394098,
      "grad_norm": 0.0009200638742186129,
      "learning_rate": 9.78778411817706e-05,
      "loss": 0.3014,
      "step": 217600
    },
    {
      "epoch": 0.6740501527991504,
      "grad_norm": 0.07393108308315277,
      "learning_rate": 9.778495416025489e-05,
      "loss": 0.2536,
      "step": 217700
    },
    {
      "epoch": 0.6743597762042028,
      "grad_norm": 0.02564655989408493,
      "learning_rate": 9.769206713873913e-05,
      "loss": 0.2883,
      "step": 217800
    },
    {
      "epoch": 0.6746693996092553,
      "grad_norm": 0.0026669991202652454,
      "learning_rate": 9.759918011722341e-05,
      "loss": 0.1284,
      "step": 217900
    },
    {
      "epoch": 0.6749790230143077,
      "grad_norm": 0.0019087055698037148,
      "learning_rate": 9.750629309570768e-05,
      "loss": 0.4714,
      "step": 218000
    },
    {
      "epoch": 0.6752886464193602,
      "grad_norm": 0.0012796616647392511,
      "learning_rate": 9.741340607419194e-05,
      "loss": 0.2691,
      "step": 218100
    },
    {
      "epoch": 0.6755982698244125,
      "grad_norm": 0.0012192110298201442,
      "learning_rate": 9.732051905267622e-05,
      "loss": 0.1568,
      "step": 218200
    },
    {
      "epoch": 0.675907893229465,
      "grad_norm": 0.0018436179962009192,
      "learning_rate": 9.722763203116049e-05,
      "loss": 0.3181,
      "step": 218300
    },
    {
      "epoch": 0.6762175166345175,
      "grad_norm": 0.6239413619041443,
      "learning_rate": 9.713474500964475e-05,
      "loss": 0.3658,
      "step": 218400
    },
    {
      "epoch": 0.6765271400395698,
      "grad_norm": 0.00010622899571899325,
      "learning_rate": 9.704185798812902e-05,
      "loss": 0.2666,
      "step": 218500
    },
    {
      "epoch": 0.6768367634446223,
      "grad_norm": 13.565896034240723,
      "learning_rate": 9.69489709666133e-05,
      "loss": 0.3531,
      "step": 218600
    },
    {
      "epoch": 0.6771463868496748,
      "grad_norm": 11.0474214553833,
      "learning_rate": 9.685608394509756e-05,
      "loss": 0.1082,
      "step": 218700
    },
    {
      "epoch": 0.6774560102547271,
      "grad_norm": 0.0007913689478300512,
      "learning_rate": 9.676319692358183e-05,
      "loss": 0.2571,
      "step": 218800
    },
    {
      "epoch": 0.6777656336597796,
      "grad_norm": 18.68745231628418,
      "learning_rate": 9.667030990206611e-05,
      "loss": 0.2362,
      "step": 218900
    },
    {
      "epoch": 0.6780752570648321,
      "grad_norm": 0.0009449806530028582,
      "learning_rate": 9.657742288055037e-05,
      "loss": 0.2286,
      "step": 219000
    },
    {
      "epoch": 0.6783848804698844,
      "grad_norm": 0.0006543744821101427,
      "learning_rate": 9.648453585903464e-05,
      "loss": 0.227,
      "step": 219100
    },
    {
      "epoch": 0.6786945038749369,
      "grad_norm": 0.0013507503317669034,
      "learning_rate": 9.639164883751892e-05,
      "loss": 0.0931,
      "step": 219200
    },
    {
      "epoch": 0.6790041272799894,
      "grad_norm": 0.013879998587071896,
      "learning_rate": 9.629876181600319e-05,
      "loss": 0.1577,
      "step": 219300
    },
    {
      "epoch": 0.6793137506850417,
      "grad_norm": 0.1612905114889145,
      "learning_rate": 9.620587479448745e-05,
      "loss": 0.4018,
      "step": 219400
    },
    {
      "epoch": 0.6796233740900942,
      "grad_norm": 0.0006515145651064813,
      "learning_rate": 9.611298777297172e-05,
      "loss": 0.3136,
      "step": 219500
    },
    {
      "epoch": 0.6799329974951467,
      "grad_norm": 2.5773048400878906,
      "learning_rate": 9.6020100751456e-05,
      "loss": 0.2372,
      "step": 219600
    },
    {
      "epoch": 0.680242620900199,
      "grad_norm": 0.0005282151396386325,
      "learning_rate": 9.592721372994026e-05,
      "loss": 0.2909,
      "step": 219700
    },
    {
      "epoch": 0.6805522443052515,
      "grad_norm": 0.0005745439557358623,
      "learning_rate": 9.583432670842453e-05,
      "loss": 0.4045,
      "step": 219800
    },
    {
      "epoch": 0.680861867710304,
      "grad_norm": 0.000864460424054414,
      "learning_rate": 9.57414396869088e-05,
      "loss": 0.2014,
      "step": 219900
    },
    {
      "epoch": 0.6811714911153564,
      "grad_norm": 0.003220914164558053,
      "learning_rate": 9.564855266539307e-05,
      "loss": 0.4365,
      "step": 220000
    },
    {
      "epoch": 0.6814811145204088,
      "grad_norm": 0.0034289644099771976,
      "learning_rate": 9.555566564387734e-05,
      "loss": 0.3344,
      "step": 220100
    },
    {
      "epoch": 0.6817907379254613,
      "grad_norm": 0.0021406621672213078,
      "learning_rate": 9.546277862236162e-05,
      "loss": 0.2152,
      "step": 220200
    },
    {
      "epoch": 0.6821003613305137,
      "grad_norm": 0.0007324328180402517,
      "learning_rate": 9.536989160084588e-05,
      "loss": 0.3065,
      "step": 220300
    },
    {
      "epoch": 0.6824099847355661,
      "grad_norm": 0.0021080004516988993,
      "learning_rate": 9.527700457933015e-05,
      "loss": 0.2519,
      "step": 220400
    },
    {
      "epoch": 0.6827196081406186,
      "grad_norm": 0.0012458822457119823,
      "learning_rate": 9.518411755781442e-05,
      "loss": 0.1453,
      "step": 220500
    },
    {
      "epoch": 0.683029231545671,
      "grad_norm": 0.006305012386292219,
      "learning_rate": 9.509123053629868e-05,
      "loss": 0.3102,
      "step": 220600
    },
    {
      "epoch": 0.6833388549507234,
      "grad_norm": 4.8021403927123174e-05,
      "learning_rate": 9.499834351478296e-05,
      "loss": 0.2817,
      "step": 220700
    },
    {
      "epoch": 0.6836484783557759,
      "grad_norm": 0.003580321092158556,
      "learning_rate": 9.490545649326723e-05,
      "loss": 0.1799,
      "step": 220800
    },
    {
      "epoch": 0.6839581017608283,
      "grad_norm": 0.0034518272150307894,
      "learning_rate": 9.481256947175149e-05,
      "loss": 0.2366,
      "step": 220900
    },
    {
      "epoch": 0.6842677251658807,
      "grad_norm": 0.0033836376387625933,
      "learning_rate": 9.471968245023577e-05,
      "loss": 0.193,
      "step": 221000
    },
    {
      "epoch": 0.6845773485709332,
      "grad_norm": 0.008153633214533329,
      "learning_rate": 9.462679542872004e-05,
      "loss": 0.189,
      "step": 221100
    },
    {
      "epoch": 0.6848869719759856,
      "grad_norm": 3.2933287620544434,
      "learning_rate": 9.453390840720431e-05,
      "loss": 0.2789,
      "step": 221200
    },
    {
      "epoch": 0.685196595381038,
      "grad_norm": 0.002938559977337718,
      "learning_rate": 9.444102138568858e-05,
      "loss": 0.2601,
      "step": 221300
    },
    {
      "epoch": 0.6855062187860905,
      "grad_norm": 0.005317145958542824,
      "learning_rate": 9.434813436417285e-05,
      "loss": 0.3009,
      "step": 221400
    },
    {
      "epoch": 0.6858158421911429,
      "grad_norm": 105.23780059814453,
      "learning_rate": 9.425524734265712e-05,
      "loss": 0.2536,
      "step": 221500
    },
    {
      "epoch": 0.6861254655961954,
      "grad_norm": 4.622503280639648,
      "learning_rate": 9.416236032114138e-05,
      "loss": 0.2821,
      "step": 221600
    },
    {
      "epoch": 0.6864350890012478,
      "grad_norm": 1.3919527530670166,
      "learning_rate": 9.406947329962566e-05,
      "loss": 0.2056,
      "step": 221700
    },
    {
      "epoch": 0.6867447124063002,
      "grad_norm": 12.31626033782959,
      "learning_rate": 9.397658627810993e-05,
      "loss": 0.2356,
      "step": 221800
    },
    {
      "epoch": 0.6870543358113527,
      "grad_norm": 0.01157151348888874,
      "learning_rate": 9.388369925659419e-05,
      "loss": 0.2888,
      "step": 221900
    },
    {
      "epoch": 0.6873639592164051,
      "grad_norm": 0.03144388645887375,
      "learning_rate": 9.379081223507847e-05,
      "loss": 0.288,
      "step": 222000
    },
    {
      "epoch": 0.6876735826214575,
      "grad_norm": 10.296919822692871,
      "learning_rate": 9.369792521356274e-05,
      "loss": 0.2843,
      "step": 222100
    },
    {
      "epoch": 0.68798320602651,
      "grad_norm": 0.0006150563713163137,
      "learning_rate": 9.3605038192047e-05,
      "loss": 0.1878,
      "step": 222200
    },
    {
      "epoch": 0.6882928294315623,
      "grad_norm": 0.0014208074426278472,
      "learning_rate": 9.351215117053127e-05,
      "loss": 0.2757,
      "step": 222300
    },
    {
      "epoch": 0.6886024528366148,
      "grad_norm": 0.0002843133988790214,
      "learning_rate": 9.341926414901555e-05,
      "loss": 0.2328,
      "step": 222400
    },
    {
      "epoch": 0.6889120762416673,
      "grad_norm": 0.0009696388733573258,
      "learning_rate": 9.332637712749981e-05,
      "loss": 0.2414,
      "step": 222500
    },
    {
      "epoch": 0.6892216996467196,
      "grad_norm": 0.00020559903350658715,
      "learning_rate": 9.323349010598408e-05,
      "loss": 0.0963,
      "step": 222600
    },
    {
      "epoch": 0.6895313230517721,
      "grad_norm": 9.929471015930176,
      "learning_rate": 9.314060308446836e-05,
      "loss": 0.2526,
      "step": 222700
    },
    {
      "epoch": 0.6898409464568246,
      "grad_norm": 0.00107828329782933,
      "learning_rate": 9.304771606295262e-05,
      "loss": 0.2794,
      "step": 222800
    },
    {
      "epoch": 0.690150569861877,
      "grad_norm": 0.02338033728301525,
      "learning_rate": 9.295482904143689e-05,
      "loss": 0.1542,
      "step": 222900
    },
    {
      "epoch": 0.6904601932669294,
      "grad_norm": 0.0028425634372979403,
      "learning_rate": 9.286194201992117e-05,
      "loss": 0.2902,
      "step": 223000
    },
    {
      "epoch": 0.6907698166719819,
      "grad_norm": 0.003890284802764654,
      "learning_rate": 9.276905499840543e-05,
      "loss": 0.2283,
      "step": 223100
    },
    {
      "epoch": 0.6910794400770343,
      "grad_norm": 0.0019679570104926825,
      "learning_rate": 9.26761679768897e-05,
      "loss": 0.1956,
      "step": 223200
    },
    {
      "epoch": 0.6913890634820867,
      "grad_norm": 0.01958695612847805,
      "learning_rate": 9.258328095537397e-05,
      "loss": 0.3277,
      "step": 223300
    },
    {
      "epoch": 0.6916986868871392,
      "grad_norm": 52.786434173583984,
      "learning_rate": 9.249039393385825e-05,
      "loss": 0.3048,
      "step": 223400
    },
    {
      "epoch": 0.6920083102921916,
      "grad_norm": 0.003429349046200514,
      "learning_rate": 9.239750691234251e-05,
      "loss": 0.1255,
      "step": 223500
    },
    {
      "epoch": 0.692317933697244,
      "grad_norm": 0.00015502564201597124,
      "learning_rate": 9.230461989082678e-05,
      "loss": 0.3804,
      "step": 223600
    },
    {
      "epoch": 0.6926275571022965,
      "grad_norm": 0.00031854931148700416,
      "learning_rate": 9.221173286931106e-05,
      "loss": 0.3389,
      "step": 223700
    },
    {
      "epoch": 0.6929371805073489,
      "grad_norm": 0.00015173236897680908,
      "learning_rate": 9.211884584779532e-05,
      "loss": 0.4535,
      "step": 223800
    },
    {
      "epoch": 0.6932468039124013,
      "grad_norm": 0.0016357945278286934,
      "learning_rate": 9.202595882627959e-05,
      "loss": 0.2408,
      "step": 223900
    },
    {
      "epoch": 0.6935564273174538,
      "grad_norm": 0.0002554701641201973,
      "learning_rate": 9.193307180476387e-05,
      "loss": 0.2163,
      "step": 224000
    },
    {
      "epoch": 0.6938660507225062,
      "grad_norm": 0.010808762162923813,
      "learning_rate": 9.184018478324813e-05,
      "loss": 0.2341,
      "step": 224100
    },
    {
      "epoch": 0.6941756741275587,
      "grad_norm": 8.970990180969238,
      "learning_rate": 9.17472977617324e-05,
      "loss": 0.2544,
      "step": 224200
    },
    {
      "epoch": 0.6944852975326111,
      "grad_norm": 0.047006335109472275,
      "learning_rate": 9.165441074021667e-05,
      "loss": 0.2088,
      "step": 224300
    },
    {
      "epoch": 0.6947949209376635,
      "grad_norm": 0.00014941682456992567,
      "learning_rate": 9.156152371870093e-05,
      "loss": 0.2429,
      "step": 224400
    },
    {
      "epoch": 0.695104544342716,
      "grad_norm": 22.488780975341797,
      "learning_rate": 9.146863669718521e-05,
      "loss": 0.3093,
      "step": 224500
    },
    {
      "epoch": 0.6954141677477684,
      "grad_norm": 0.0008708862005732954,
      "learning_rate": 9.137574967566948e-05,
      "loss": 0.2317,
      "step": 224600
    },
    {
      "epoch": 0.6957237911528208,
      "grad_norm": 0.028634900227189064,
      "learning_rate": 9.128286265415374e-05,
      "loss": 0.2534,
      "step": 224700
    },
    {
      "epoch": 0.6960334145578733,
      "grad_norm": 0.00025079832994379103,
      "learning_rate": 9.118997563263802e-05,
      "loss": 0.3329,
      "step": 224800
    },
    {
      "epoch": 0.6963430379629257,
      "grad_norm": 0.0009594139992259443,
      "learning_rate": 9.109708861112229e-05,
      "loss": 0.2065,
      "step": 224900
    },
    {
      "epoch": 0.6966526613679781,
      "grad_norm": 0.011669413186609745,
      "learning_rate": 9.100420158960655e-05,
      "loss": 0.2561,
      "step": 225000
    },
    {
      "epoch": 0.6969622847730306,
      "grad_norm": 0.00044157623779028654,
      "learning_rate": 9.091131456809082e-05,
      "loss": 0.2502,
      "step": 225100
    },
    {
      "epoch": 0.697271908178083,
      "grad_norm": 0.0009103139163926244,
      "learning_rate": 9.08184275465751e-05,
      "loss": 0.2561,
      "step": 225200
    },
    {
      "epoch": 0.6975815315831354,
      "grad_norm": 0.6547539234161377,
      "learning_rate": 9.072554052505937e-05,
      "loss": 0.3469,
      "step": 225300
    },
    {
      "epoch": 0.6978911549881879,
      "grad_norm": 0.0009917051065713167,
      "learning_rate": 9.063265350354363e-05,
      "loss": 0.254,
      "step": 225400
    },
    {
      "epoch": 0.6982007783932404,
      "grad_norm": 0.002668782602995634,
      "learning_rate": 9.053976648202791e-05,
      "loss": 0.3229,
      "step": 225500
    },
    {
      "epoch": 0.6985104017982927,
      "grad_norm": 0.0006256874185055494,
      "learning_rate": 9.044687946051218e-05,
      "loss": 0.2273,
      "step": 225600
    },
    {
      "epoch": 0.6988200252033452,
      "grad_norm": 0.0005236717406660318,
      "learning_rate": 9.035399243899644e-05,
      "loss": 0.3127,
      "step": 225700
    },
    {
      "epoch": 0.6991296486083977,
      "grad_norm": 0.0012333785416558385,
      "learning_rate": 9.026110541748072e-05,
      "loss": 0.3705,
      "step": 225800
    },
    {
      "epoch": 0.69943927201345,
      "grad_norm": 0.0059973024763166904,
      "learning_rate": 9.016821839596499e-05,
      "loss": 0.2338,
      "step": 225900
    },
    {
      "epoch": 0.6997488954185025,
      "grad_norm": 0.0020387587137520313,
      "learning_rate": 9.007533137444925e-05,
      "loss": 0.2029,
      "step": 226000
    },
    {
      "epoch": 0.700058518823555,
      "grad_norm": 0.00412394292652607,
      "learning_rate": 8.998244435293352e-05,
      "loss": 0.0986,
      "step": 226100
    },
    {
      "epoch": 0.7003681422286073,
      "grad_norm": 0.000938577635679394,
      "learning_rate": 8.98895573314178e-05,
      "loss": 0.1172,
      "step": 226200
    },
    {
      "epoch": 0.7006777656336598,
      "grad_norm": 0.0007703036535531282,
      "learning_rate": 8.979667030990206e-05,
      "loss": 0.3279,
      "step": 226300
    },
    {
      "epoch": 0.7009873890387122,
      "grad_norm": 0.13861051201820374,
      "learning_rate": 8.970378328838633e-05,
      "loss": 0.28,
      "step": 226400
    },
    {
      "epoch": 0.7012970124437646,
      "grad_norm": 0.019917547702789307,
      "learning_rate": 8.961089626687061e-05,
      "loss": 0.2218,
      "step": 226500
    },
    {
      "epoch": 0.7016066358488171,
      "grad_norm": 0.001994933933019638,
      "learning_rate": 8.951800924535485e-05,
      "loss": 0.1761,
      "step": 226600
    },
    {
      "epoch": 0.7019162592538695,
      "grad_norm": 0.013572127558290958,
      "learning_rate": 8.942512222383914e-05,
      "loss": 0.1755,
      "step": 226700
    },
    {
      "epoch": 0.7022258826589219,
      "grad_norm": 6.966624736785889,
      "learning_rate": 8.933223520232342e-05,
      "loss": 0.2535,
      "step": 226800
    },
    {
      "epoch": 0.7025355060639744,
      "grad_norm": 0.006514017004519701,
      "learning_rate": 8.923934818080766e-05,
      "loss": 0.1251,
      "step": 226900
    },
    {
      "epoch": 0.7028451294690268,
      "grad_norm": 0.0001521189115010202,
      "learning_rate": 8.914646115929195e-05,
      "loss": 0.2281,
      "step": 227000
    },
    {
      "epoch": 0.7031547528740792,
      "grad_norm": 0.0029736487194895744,
      "learning_rate": 8.905357413777622e-05,
      "loss": 0.1201,
      "step": 227100
    },
    {
      "epoch": 0.7034643762791317,
      "grad_norm": 0.00472386134788394,
      "learning_rate": 8.89606871162605e-05,
      "loss": 0.2229,
      "step": 227200
    },
    {
      "epoch": 0.7037739996841841,
      "grad_norm": 0.0007130971644073725,
      "learning_rate": 8.886780009474474e-05,
      "loss": 0.2514,
      "step": 227300
    },
    {
      "epoch": 0.7040836230892366,
      "grad_norm": 7.610477769048885e-05,
      "learning_rate": 8.877491307322903e-05,
      "loss": 0.1867,
      "step": 227400
    },
    {
      "epoch": 0.704393246494289,
      "grad_norm": 0.0011396640911698341,
      "learning_rate": 8.86820260517133e-05,
      "loss": 0.3555,
      "step": 227500
    },
    {
      "epoch": 0.7047028698993414,
      "grad_norm": 0.11903384327888489,
      "learning_rate": 8.858913903019755e-05,
      "loss": 0.1165,
      "step": 227600
    },
    {
      "epoch": 0.7050124933043939,
      "grad_norm": 0.0022054393775761127,
      "learning_rate": 8.849625200868183e-05,
      "loss": 0.2921,
      "step": 227700
    },
    {
      "epoch": 0.7053221167094463,
      "grad_norm": 0.00015369900211226195,
      "learning_rate": 8.840336498716611e-05,
      "loss": 0.2306,
      "step": 227800
    },
    {
      "epoch": 0.7056317401144987,
      "grad_norm": 0.0013739918358623981,
      "learning_rate": 8.831047796565036e-05,
      "loss": 0.3023,
      "step": 227900
    },
    {
      "epoch": 0.7059413635195512,
      "grad_norm": 0.0004902236978523433,
      "learning_rate": 8.821759094413464e-05,
      "loss": 0.159,
      "step": 228000
    },
    {
      "epoch": 0.7062509869246036,
      "grad_norm": 0.0032081634271889925,
      "learning_rate": 8.812470392261892e-05,
      "loss": 0.1658,
      "step": 228100
    },
    {
      "epoch": 0.706560610329656,
      "grad_norm": 0.006293964106589556,
      "learning_rate": 8.803181690110317e-05,
      "loss": 0.3586,
      "step": 228200
    },
    {
      "epoch": 0.7068702337347085,
      "grad_norm": 0.0033933420199900866,
      "learning_rate": 8.793892987958744e-05,
      "loss": 0.3389,
      "step": 228300
    },
    {
      "epoch": 0.707179857139761,
      "grad_norm": 0.0002573172969277948,
      "learning_rate": 8.784604285807172e-05,
      "loss": 0.4079,
      "step": 228400
    },
    {
      "epoch": 0.7074894805448133,
      "grad_norm": 0.001442977343685925,
      "learning_rate": 8.775315583655598e-05,
      "loss": 0.2903,
      "step": 228500
    },
    {
      "epoch": 0.7077991039498658,
      "grad_norm": 0.00046315963845700026,
      "learning_rate": 8.766026881504025e-05,
      "loss": 0.2226,
      "step": 228600
    },
    {
      "epoch": 0.7081087273549183,
      "grad_norm": 0.00047032005386427045,
      "learning_rate": 8.756738179352453e-05,
      "loss": 0.2575,
      "step": 228700
    },
    {
      "epoch": 0.7084183507599706,
      "grad_norm": 0.0057855513878166676,
      "learning_rate": 8.747449477200879e-05,
      "loss": 0.2069,
      "step": 228800
    },
    {
      "epoch": 0.7087279741650231,
      "grad_norm": 46.83280563354492,
      "learning_rate": 8.738160775049306e-05,
      "loss": 0.256,
      "step": 228900
    },
    {
      "epoch": 0.7090375975700756,
      "grad_norm": 0.004333633463829756,
      "learning_rate": 8.728872072897734e-05,
      "loss": 0.2103,
      "step": 229000
    },
    {
      "epoch": 0.7093472209751279,
      "grad_norm": 2.409529685974121,
      "learning_rate": 8.71958337074616e-05,
      "loss": 0.2308,
      "step": 229100
    },
    {
      "epoch": 0.7096568443801804,
      "grad_norm": 0.0005616257549263537,
      "learning_rate": 8.710294668594587e-05,
      "loss": 0.2485,
      "step": 229200
    },
    {
      "epoch": 0.7099664677852329,
      "grad_norm": 0.004008509684354067,
      "learning_rate": 8.701005966443014e-05,
      "loss": 0.3148,
      "step": 229300
    },
    {
      "epoch": 0.7102760911902852,
      "grad_norm": 0.03103022277355194,
      "learning_rate": 8.691717264291442e-05,
      "loss": 0.2912,
      "step": 229400
    },
    {
      "epoch": 0.7105857145953377,
      "grad_norm": 0.0013223461573943496,
      "learning_rate": 8.682428562139868e-05,
      "loss": 0.1623,
      "step": 229500
    },
    {
      "epoch": 0.7108953380003902,
      "grad_norm": 0.0017029405571520329,
      "learning_rate": 8.673139859988295e-05,
      "loss": 0.2896,
      "step": 229600
    },
    {
      "epoch": 0.7112049614054425,
      "grad_norm": 0.0006824005395174026,
      "learning_rate": 8.663851157836723e-05,
      "loss": 0.2674,
      "step": 229700
    },
    {
      "epoch": 0.711514584810495,
      "grad_norm": 0.22232282161712646,
      "learning_rate": 8.654562455685149e-05,
      "loss": 0.2252,
      "step": 229800
    },
    {
      "epoch": 0.7118242082155475,
      "grad_norm": 0.0011161785805597901,
      "learning_rate": 8.645273753533576e-05,
      "loss": 0.2189,
      "step": 229900
    },
    {
      "epoch": 0.7121338316205998,
      "grad_norm": 108.11824035644531,
      "learning_rate": 8.635985051382003e-05,
      "loss": 0.1998,
      "step": 230000
    },
    {
      "epoch": 0.7124434550256523,
      "grad_norm": 0.002983913291245699,
      "learning_rate": 8.62669634923043e-05,
      "loss": 0.2206,
      "step": 230100
    },
    {
      "epoch": 0.7127530784307047,
      "grad_norm": 0.0010560344671830535,
      "learning_rate": 8.617407647078857e-05,
      "loss": 0.125,
      "step": 230200
    },
    {
      "epoch": 0.7130627018357572,
      "grad_norm": 11.874015808105469,
      "learning_rate": 8.608118944927284e-05,
      "loss": 0.3801,
      "step": 230300
    },
    {
      "epoch": 0.7133723252408096,
      "grad_norm": 0.008419917896389961,
      "learning_rate": 8.59883024277571e-05,
      "loss": 0.4748,
      "step": 230400
    },
    {
      "epoch": 0.713681948645862,
      "grad_norm": 0.0032785844523459673,
      "learning_rate": 8.589541540624138e-05,
      "loss": 0.2478,
      "step": 230500
    },
    {
      "epoch": 0.7139915720509145,
      "grad_norm": 0.0010363520123064518,
      "learning_rate": 8.580252838472565e-05,
      "loss": 0.2525,
      "step": 230600
    },
    {
      "epoch": 0.7143011954559669,
      "grad_norm": 0.0077516064047813416,
      "learning_rate": 8.570964136320991e-05,
      "loss": 0.1237,
      "step": 230700
    },
    {
      "epoch": 0.7146108188610193,
      "grad_norm": 0.0019939965568482876,
      "learning_rate": 8.561675434169419e-05,
      "loss": 0.3578,
      "step": 230800
    },
    {
      "epoch": 0.7149204422660718,
      "grad_norm": 7.854346767999232e-05,
      "learning_rate": 8.552386732017846e-05,
      "loss": 0.2026,
      "step": 230900
    },
    {
      "epoch": 0.7152300656711242,
      "grad_norm": 0.003533991752192378,
      "learning_rate": 8.543098029866272e-05,
      "loss": 0.261,
      "step": 231000
    },
    {
      "epoch": 0.7155396890761766,
      "grad_norm": 0.0009112622356042266,
      "learning_rate": 8.5338093277147e-05,
      "loss": 0.1697,
      "step": 231100
    },
    {
      "epoch": 0.7158493124812291,
      "grad_norm": 0.000791720871347934,
      "learning_rate": 8.524520625563127e-05,
      "loss": 0.1754,
      "step": 231200
    },
    {
      "epoch": 0.7161589358862815,
      "grad_norm": 0.0012823535362258554,
      "learning_rate": 8.515231923411554e-05,
      "loss": 0.2439,
      "step": 231300
    },
    {
      "epoch": 0.7164685592913339,
      "grad_norm": 39.64750671386719,
      "learning_rate": 8.50594322125998e-05,
      "loss": 0.2559,
      "step": 231400
    },
    {
      "epoch": 0.7167781826963864,
      "grad_norm": 0.0003436697297729552,
      "learning_rate": 8.496654519108408e-05,
      "loss": 0.3006,
      "step": 231500
    },
    {
      "epoch": 0.7170878061014389,
      "grad_norm": 0.000307584268739447,
      "learning_rate": 8.487365816956835e-05,
      "loss": 0.1461,
      "step": 231600
    },
    {
      "epoch": 0.7173974295064912,
      "grad_norm": 0.9268057942390442,
      "learning_rate": 8.478077114805261e-05,
      "loss": 0.2149,
      "step": 231700
    },
    {
      "epoch": 0.7177070529115437,
      "grad_norm": 41.29633331298828,
      "learning_rate": 8.468788412653689e-05,
      "loss": 0.1976,
      "step": 231800
    },
    {
      "epoch": 0.7180166763165962,
      "grad_norm": 0.42632296681404114,
      "learning_rate": 8.459499710502116e-05,
      "loss": 0.2186,
      "step": 231900
    },
    {
      "epoch": 0.7183262997216485,
      "grad_norm": 0.00023803771182429045,
      "learning_rate": 8.450211008350542e-05,
      "loss": 0.1767,
      "step": 232000
    },
    {
      "epoch": 0.718635923126701,
      "grad_norm": 0.0005788615089841187,
      "learning_rate": 8.44092230619897e-05,
      "loss": 0.1809,
      "step": 232100
    },
    {
      "epoch": 0.7189455465317535,
      "grad_norm": 0.03161204978823662,
      "learning_rate": 8.431633604047397e-05,
      "loss": 0.3237,
      "step": 232200
    },
    {
      "epoch": 0.7192551699368058,
      "grad_norm": 56.585445404052734,
      "learning_rate": 8.422344901895823e-05,
      "loss": 0.3296,
      "step": 232300
    },
    {
      "epoch": 0.7195647933418583,
      "grad_norm": 0.0010081194341182709,
      "learning_rate": 8.41305619974425e-05,
      "loss": 0.1845,
      "step": 232400
    },
    {
      "epoch": 0.7198744167469108,
      "grad_norm": 0.00021485662728082389,
      "learning_rate": 8.403767497592678e-05,
      "loss": 0.2469,
      "step": 232500
    },
    {
      "epoch": 0.7201840401519631,
      "grad_norm": 0.0004841124464292079,
      "learning_rate": 8.394478795441104e-05,
      "loss": 0.1351,
      "step": 232600
    },
    {
      "epoch": 0.7204936635570156,
      "grad_norm": 0.0033814674243330956,
      "learning_rate": 8.385190093289531e-05,
      "loss": 0.2929,
      "step": 232700
    },
    {
      "epoch": 0.7208032869620681,
      "grad_norm": 0.000355237047187984,
      "learning_rate": 8.375901391137958e-05,
      "loss": 0.3217,
      "step": 232800
    },
    {
      "epoch": 0.7211129103671204,
      "grad_norm": 0.007142210379242897,
      "learning_rate": 8.366612688986385e-05,
      "loss": 0.1272,
      "step": 232900
    },
    {
      "epoch": 0.7214225337721729,
      "grad_norm": 0.0016099016647785902,
      "learning_rate": 8.357323986834812e-05,
      "loss": 0.065,
      "step": 233000
    },
    {
      "epoch": 0.7217321571772254,
      "grad_norm": 14.662631034851074,
      "learning_rate": 8.34803528468324e-05,
      "loss": 0.3686,
      "step": 233100
    },
    {
      "epoch": 0.7220417805822777,
      "grad_norm": 0.0005336130270734429,
      "learning_rate": 8.338746582531667e-05,
      "loss": 0.1947,
      "step": 233200
    },
    {
      "epoch": 0.7223514039873302,
      "grad_norm": 0.0007149541052058339,
      "learning_rate": 8.329457880380093e-05,
      "loss": 0.2126,
      "step": 233300
    },
    {
      "epoch": 0.7226610273923827,
      "grad_norm": 0.0011144924210384488,
      "learning_rate": 8.32016917822852e-05,
      "loss": 0.2819,
      "step": 233400
    },
    {
      "epoch": 0.722970650797435,
      "grad_norm": 0.0010615917854011059,
      "learning_rate": 8.310880476076948e-05,
      "loss": 0.3266,
      "step": 233500
    },
    {
      "epoch": 0.7232802742024875,
      "grad_norm": 0.0002729525149334222,
      "learning_rate": 8.301591773925374e-05,
      "loss": 0.4322,
      "step": 233600
    },
    {
      "epoch": 0.72358989760754,
      "grad_norm": 0.0014766132226213813,
      "learning_rate": 8.292303071773801e-05,
      "loss": 0.1386,
      "step": 233700
    },
    {
      "epoch": 0.7238995210125924,
      "grad_norm": 0.0005731892888434231,
      "learning_rate": 8.283014369622228e-05,
      "loss": 0.2671,
      "step": 233800
    },
    {
      "epoch": 0.7242091444176448,
      "grad_norm": 0.0013497936306521297,
      "learning_rate": 8.273725667470654e-05,
      "loss": 0.2362,
      "step": 233900
    },
    {
      "epoch": 0.7245187678226973,
      "grad_norm": 0.0006841055583208799,
      "learning_rate": 8.264436965319082e-05,
      "loss": 0.2509,
      "step": 234000
    },
    {
      "epoch": 0.7248283912277497,
      "grad_norm": 0.0015231845900416374,
      "learning_rate": 8.255148263167509e-05,
      "loss": 0.1946,
      "step": 234100
    },
    {
      "epoch": 0.7251380146328021,
      "grad_norm": 0.006032720673829317,
      "learning_rate": 8.245859561015935e-05,
      "loss": 0.21,
      "step": 234200
    },
    {
      "epoch": 0.7254476380378545,
      "grad_norm": 0.004282274283468723,
      "learning_rate": 8.236570858864363e-05,
      "loss": 0.3744,
      "step": 234300
    },
    {
      "epoch": 0.725757261442907,
      "grad_norm": 0.00011909683234989643,
      "learning_rate": 8.22728215671279e-05,
      "loss": 0.2803,
      "step": 234400
    },
    {
      "epoch": 0.7260668848479594,
      "grad_norm": 0.005126835312694311,
      "learning_rate": 8.217993454561216e-05,
      "loss": 0.0988,
      "step": 234500
    },
    {
      "epoch": 0.7263765082530118,
      "grad_norm": 2.4465012550354004,
      "learning_rate": 8.208704752409644e-05,
      "loss": 0.1279,
      "step": 234600
    },
    {
      "epoch": 0.7266861316580643,
      "grad_norm": 11.959648132324219,
      "learning_rate": 8.199416050258071e-05,
      "loss": 0.2843,
      "step": 234700
    },
    {
      "epoch": 0.7269957550631168,
      "grad_norm": 0.0002778809575829655,
      "learning_rate": 8.190127348106497e-05,
      "loss": 0.2412,
      "step": 234800
    },
    {
      "epoch": 0.7273053784681691,
      "grad_norm": 0.004761025309562683,
      "learning_rate": 8.180838645954924e-05,
      "loss": 0.2817,
      "step": 234900
    },
    {
      "epoch": 0.7276150018732216,
      "grad_norm": 0.0011324352817609906,
      "learning_rate": 8.171549943803352e-05,
      "loss": 0.2803,
      "step": 235000
    },
    {
      "epoch": 0.7279246252782741,
      "grad_norm": 0.011115954257547855,
      "learning_rate": 8.162261241651778e-05,
      "loss": 0.2638,
      "step": 235100
    },
    {
      "epoch": 0.7282342486833264,
      "grad_norm": 43.20692443847656,
      "learning_rate": 8.152972539500205e-05,
      "loss": 0.1844,
      "step": 235200
    },
    {
      "epoch": 0.7285438720883789,
      "grad_norm": 0.002040107734501362,
      "learning_rate": 8.143683837348633e-05,
      "loss": 0.3591,
      "step": 235300
    },
    {
      "epoch": 0.7288534954934314,
      "grad_norm": 0.0014398489147424698,
      "learning_rate": 8.13439513519706e-05,
      "loss": 0.1976,
      "step": 235400
    },
    {
      "epoch": 0.7291631188984837,
      "grad_norm": 41.6076774597168,
      "learning_rate": 8.125106433045486e-05,
      "loss": 0.2183,
      "step": 235500
    },
    {
      "epoch": 0.7294727423035362,
      "grad_norm": 13.856557846069336,
      "learning_rate": 8.115817730893914e-05,
      "loss": 0.1772,
      "step": 235600
    },
    {
      "epoch": 0.7297823657085887,
      "grad_norm": 0.5698521137237549,
      "learning_rate": 8.106529028742341e-05,
      "loss": 0.1739,
      "step": 235700
    },
    {
      "epoch": 0.730091989113641,
      "grad_norm": 0.0025179963558912277,
      "learning_rate": 8.097240326590767e-05,
      "loss": 0.238,
      "step": 235800
    },
    {
      "epoch": 0.7304016125186935,
      "grad_norm": 11.843611717224121,
      "learning_rate": 8.087951624439194e-05,
      "loss": 0.3321,
      "step": 235900
    },
    {
      "epoch": 0.730711235923746,
      "grad_norm": 0.005957877729088068,
      "learning_rate": 8.078662922287622e-05,
      "loss": 0.2874,
      "step": 236000
    },
    {
      "epoch": 0.7310208593287983,
      "grad_norm": 0.003439161693677306,
      "learning_rate": 8.069374220136048e-05,
      "loss": 0.1631,
      "step": 236100
    },
    {
      "epoch": 0.7313304827338508,
      "grad_norm": 0.0002214970299974084,
      "learning_rate": 8.060085517984475e-05,
      "loss": 0.275,
      "step": 236200
    },
    {
      "epoch": 0.7316401061389033,
      "grad_norm": 0.018700161948800087,
      "learning_rate": 8.050796815832903e-05,
      "loss": 0.306,
      "step": 236300
    },
    {
      "epoch": 0.7319497295439557,
      "grad_norm": 0.0062287189066410065,
      "learning_rate": 8.041508113681329e-05,
      "loss": 0.1382,
      "step": 236400
    },
    {
      "epoch": 0.7322593529490081,
      "grad_norm": 0.002474147127941251,
      "learning_rate": 8.032219411529756e-05,
      "loss": 0.3038,
      "step": 236500
    },
    {
      "epoch": 0.7325689763540606,
      "grad_norm": 0.00010750150249805301,
      "learning_rate": 8.022930709378183e-05,
      "loss": 0.1842,
      "step": 236600
    },
    {
      "epoch": 0.732878599759113,
      "grad_norm": 35.69261169433594,
      "learning_rate": 8.01364200722661e-05,
      "loss": 0.3174,
      "step": 236700
    },
    {
      "epoch": 0.7331882231641654,
      "grad_norm": 0.0037516369484364986,
      "learning_rate": 8.004353305075037e-05,
      "loss": 0.2399,
      "step": 236800
    },
    {
      "epoch": 0.7334978465692179,
      "grad_norm": 0.0025435490533709526,
      "learning_rate": 7.995064602923464e-05,
      "loss": 0.1217,
      "step": 236900
    },
    {
      "epoch": 0.7338074699742703,
      "grad_norm": 0.6692049503326416,
      "learning_rate": 7.985775900771889e-05,
      "loss": 0.1468,
      "step": 237000
    },
    {
      "epoch": 0.7341170933793227,
      "grad_norm": 0.0011212937533855438,
      "learning_rate": 7.976487198620318e-05,
      "loss": 0.2868,
      "step": 237100
    },
    {
      "epoch": 0.7344267167843752,
      "grad_norm": 0.0013506493996828794,
      "learning_rate": 7.967198496468745e-05,
      "loss": 0.2134,
      "step": 237200
    },
    {
      "epoch": 0.7347363401894276,
      "grad_norm": 0.0015935497358441353,
      "learning_rate": 7.957909794317173e-05,
      "loss": 0.2894,
      "step": 237300
    },
    {
      "epoch": 0.73504596359448,
      "grad_norm": 0.0013323442544788122,
      "learning_rate": 7.948621092165599e-05,
      "loss": 0.1638,
      "step": 237400
    },
    {
      "epoch": 0.7353555869995325,
      "grad_norm": 0.005051019601523876,
      "learning_rate": 7.939332390014026e-05,
      "loss": 0.2317,
      "step": 237500
    },
    {
      "epoch": 0.7356652104045849,
      "grad_norm": 0.0035060583613812923,
      "learning_rate": 7.930043687862453e-05,
      "loss": 0.2323,
      "step": 237600
    },
    {
      "epoch": 0.7359748338096374,
      "grad_norm": 0.0022453104611486197,
      "learning_rate": 7.920754985710878e-05,
      "loss": 0.3442,
      "step": 237700
    },
    {
      "epoch": 0.7362844572146898,
      "grad_norm": 0.1462956815958023,
      "learning_rate": 7.911466283559307e-05,
      "loss": 0.2591,
      "step": 237800
    },
    {
      "epoch": 0.7365940806197422,
      "grad_norm": 35.546531677246094,
      "learning_rate": 7.902177581407734e-05,
      "loss": 0.214,
      "step": 237900
    },
    {
      "epoch": 0.7369037040247947,
      "grad_norm": 0.00020777026657015085,
      "learning_rate": 7.892888879256159e-05,
      "loss": 0.179,
      "step": 238000
    },
    {
      "epoch": 0.7372133274298471,
      "grad_norm": 0.00034952795249409974,
      "learning_rate": 7.883600177104586e-05,
      "loss": 0.1354,
      "step": 238100
    },
    {
      "epoch": 0.7375229508348995,
      "grad_norm": 0.005678401328623295,
      "learning_rate": 7.874311474953015e-05,
      "loss": 0.2383,
      "step": 238200
    },
    {
      "epoch": 0.737832574239952,
      "grad_norm": 12.862836837768555,
      "learning_rate": 7.86502277280144e-05,
      "loss": 0.3866,
      "step": 238300
    },
    {
      "epoch": 0.7381421976450043,
      "grad_norm": 0.008214309811592102,
      "learning_rate": 7.855734070649867e-05,
      "loss": 0.2619,
      "step": 238400
    },
    {
      "epoch": 0.7384518210500568,
      "grad_norm": 0.01281948946416378,
      "learning_rate": 7.846445368498296e-05,
      "loss": 0.2779,
      "step": 238500
    },
    {
      "epoch": 0.7387614444551093,
      "grad_norm": 0.039947692304849625,
      "learning_rate": 7.83715666634672e-05,
      "loss": 0.433,
      "step": 238600
    },
    {
      "epoch": 0.7390710678601616,
      "grad_norm": 0.00014971746713854373,
      "learning_rate": 7.827867964195148e-05,
      "loss": 0.2892,
      "step": 238700
    },
    {
      "epoch": 0.7393806912652141,
      "grad_norm": 0.005164204631000757,
      "learning_rate": 7.818579262043575e-05,
      "loss": 0.2596,
      "step": 238800
    },
    {
      "epoch": 0.7396903146702666,
      "grad_norm": 0.08335141092538834,
      "learning_rate": 7.809290559892002e-05,
      "loss": 0.2483,
      "step": 238900
    },
    {
      "epoch": 0.7399999380753189,
      "grad_norm": 0.10689648240804672,
      "learning_rate": 7.800001857740429e-05,
      "loss": 0.1817,
      "step": 239000
    },
    {
      "epoch": 0.7403095614803714,
      "grad_norm": 0.0008040895336307585,
      "learning_rate": 7.790713155588856e-05,
      "loss": 0.1942,
      "step": 239100
    },
    {
      "epoch": 0.7406191848854239,
      "grad_norm": 0.0005630024243146181,
      "learning_rate": 7.781424453437284e-05,
      "loss": 0.2996,
      "step": 239200
    },
    {
      "epoch": 0.7409288082904762,
      "grad_norm": 0.003915035631507635,
      "learning_rate": 7.77213575128571e-05,
      "loss": 0.1741,
      "step": 239300
    },
    {
      "epoch": 0.7412384316955287,
      "grad_norm": 0.0007317616255022585,
      "learning_rate": 7.762847049134137e-05,
      "loss": 0.2925,
      "step": 239400
    },
    {
      "epoch": 0.7415480551005812,
      "grad_norm": 12.908609390258789,
      "learning_rate": 7.753558346982565e-05,
      "loss": 0.1798,
      "step": 239500
    },
    {
      "epoch": 0.7418576785056336,
      "grad_norm": 0.6886650323867798,
      "learning_rate": 7.74426964483099e-05,
      "loss": 0.2491,
      "step": 239600
    },
    {
      "epoch": 0.742167301910686,
      "grad_norm": 0.013724512420594692,
      "learning_rate": 7.734980942679418e-05,
      "loss": 0.2428,
      "step": 239700
    },
    {
      "epoch": 0.7424769253157385,
      "grad_norm": 0.003577540395781398,
      "learning_rate": 7.725692240527845e-05,
      "loss": 0.1445,
      "step": 239800
    },
    {
      "epoch": 0.7427865487207909,
      "grad_norm": 0.0018789505120366812,
      "learning_rate": 7.716403538376271e-05,
      "loss": 0.4215,
      "step": 239900
    },
    {
      "epoch": 0.7430961721258433,
      "grad_norm": 0.0009173310245387256,
      "learning_rate": 7.707114836224699e-05,
      "loss": 0.1614,
      "step": 240000
    },
    {
      "epoch": 0.7434057955308958,
      "grad_norm": 0.012604432180523872,
      "learning_rate": 7.697826134073126e-05,
      "loss": 0.18,
      "step": 240100
    },
    {
      "epoch": 0.7437154189359482,
      "grad_norm": 0.00028633172041736543,
      "learning_rate": 7.688537431921552e-05,
      "loss": 0.1712,
      "step": 240200
    },
    {
      "epoch": 0.7440250423410006,
      "grad_norm": 0.014560899697244167,
      "learning_rate": 7.67924872976998e-05,
      "loss": 0.3442,
      "step": 240300
    },
    {
      "epoch": 0.7443346657460531,
      "grad_norm": 0.000930227164644748,
      "learning_rate": 7.669960027618407e-05,
      "loss": 0.2688,
      "step": 240400
    },
    {
      "epoch": 0.7446442891511055,
      "grad_norm": 0.0022682244889438152,
      "learning_rate": 7.660671325466833e-05,
      "loss": 0.121,
      "step": 240500
    },
    {
      "epoch": 0.744953912556158,
      "grad_norm": 0.0018888101913034916,
      "learning_rate": 7.65138262331526e-05,
      "loss": 0.3119,
      "step": 240600
    },
    {
      "epoch": 0.7452635359612104,
      "grad_norm": 0.4429941177368164,
      "learning_rate": 7.642093921163688e-05,
      "loss": 0.2196,
      "step": 240700
    },
    {
      "epoch": 0.7455731593662628,
      "grad_norm": 0.008044450543820858,
      "learning_rate": 7.632805219012114e-05,
      "loss": 0.2457,
      "step": 240800
    },
    {
      "epoch": 0.7458827827713153,
      "grad_norm": 14.933979034423828,
      "learning_rate": 7.623516516860541e-05,
      "loss": 0.3272,
      "step": 240900
    },
    {
      "epoch": 0.7461924061763677,
      "grad_norm": 2.477954694768414e-05,
      "learning_rate": 7.614227814708969e-05,
      "loss": 0.1678,
      "step": 241000
    },
    {
      "epoch": 0.7465020295814201,
      "grad_norm": 0.0008300611516460776,
      "learning_rate": 7.604939112557395e-05,
      "loss": 0.2411,
      "step": 241100
    },
    {
      "epoch": 0.7468116529864726,
      "grad_norm": 0.004844616167247295,
      "learning_rate": 7.595650410405822e-05,
      "loss": 0.2323,
      "step": 241200
    },
    {
      "epoch": 0.747121276391525,
      "grad_norm": 0.0018391319317743182,
      "learning_rate": 7.58636170825425e-05,
      "loss": 0.2182,
      "step": 241300
    },
    {
      "epoch": 0.7474308997965774,
      "grad_norm": 0.9930770397186279,
      "learning_rate": 7.577073006102677e-05,
      "loss": 0.2696,
      "step": 241400
    },
    {
      "epoch": 0.7477405232016299,
      "grad_norm": 0.0005988472839817405,
      "learning_rate": 7.567784303951103e-05,
      "loss": 0.2969,
      "step": 241500
    },
    {
      "epoch": 0.7480501466066823,
      "grad_norm": 0.0008354384917765856,
      "learning_rate": 7.55849560179953e-05,
      "loss": 0.3146,
      "step": 241600
    },
    {
      "epoch": 0.7483597700117347,
      "grad_norm": 0.003099264344200492,
      "learning_rate": 7.549206899647958e-05,
      "loss": 0.3162,
      "step": 241700
    },
    {
      "epoch": 0.7486693934167872,
      "grad_norm": 0.0043937331065535545,
      "learning_rate": 7.539918197496384e-05,
      "loss": 0.1129,
      "step": 241800
    },
    {
      "epoch": 0.7489790168218396,
      "grad_norm": 1.2096456289291382,
      "learning_rate": 7.530629495344811e-05,
      "loss": 0.1183,
      "step": 241900
    },
    {
      "epoch": 0.749288640226892,
      "grad_norm": 7.692239761352539,
      "learning_rate": 7.521340793193239e-05,
      "loss": 0.2852,
      "step": 242000
    },
    {
      "epoch": 0.7495982636319445,
      "grad_norm": 0.5418381690979004,
      "learning_rate": 7.512052091041665e-05,
      "loss": 0.2933,
      "step": 242100
    },
    {
      "epoch": 0.7499078870369968,
      "grad_norm": 7.3994011878967285,
      "learning_rate": 7.502763388890092e-05,
      "loss": 0.1369,
      "step": 242200
    },
    {
      "epoch": 0.7502175104420493,
      "grad_norm": 0.00012488561333157122,
      "learning_rate": 7.49347468673852e-05,
      "loss": 0.2385,
      "step": 242300
    },
    {
      "epoch": 0.7505271338471018,
      "grad_norm": 0.0031841478776186705,
      "learning_rate": 7.484185984586947e-05,
      "loss": 0.2685,
      "step": 242400
    },
    {
      "epoch": 0.7508367572521542,
      "grad_norm": 0.0798196867108345,
      "learning_rate": 7.474897282435373e-05,
      "loss": 0.2126,
      "step": 242500
    },
    {
      "epoch": 0.7511463806572066,
      "grad_norm": 0.0016752582741901278,
      "learning_rate": 7.4656085802838e-05,
      "loss": 0.2377,
      "step": 242600
    },
    {
      "epoch": 0.7514560040622591,
      "grad_norm": 9.346934530185536e-05,
      "learning_rate": 7.456319878132228e-05,
      "loss": 0.3289,
      "step": 242700
    },
    {
      "epoch": 0.7517656274673115,
      "grad_norm": 0.011639425531029701,
      "learning_rate": 7.447031175980654e-05,
      "loss": 0.1747,
      "step": 242800
    },
    {
      "epoch": 0.7520752508723639,
      "grad_norm": 0.010708385147154331,
      "learning_rate": 7.437742473829081e-05,
      "loss": 0.1872,
      "step": 242900
    },
    {
      "epoch": 0.7523848742774164,
      "grad_norm": 11.59067153930664,
      "learning_rate": 7.428453771677509e-05,
      "loss": 0.3425,
      "step": 243000
    },
    {
      "epoch": 0.7526944976824688,
      "grad_norm": 0.0003831431095022708,
      "learning_rate": 7.419165069525935e-05,
      "loss": 0.3434,
      "step": 243100
    },
    {
      "epoch": 0.7530041210875212,
      "grad_norm": 0.0016942286165431142,
      "learning_rate": 7.409876367374362e-05,
      "loss": 0.2395,
      "step": 243200
    },
    {
      "epoch": 0.7533137444925737,
      "grad_norm": 0.009358408860862255,
      "learning_rate": 7.400587665222788e-05,
      "loss": 0.1252,
      "step": 243300
    },
    {
      "epoch": 0.7536233678976261,
      "grad_norm": 0.0014856952475383878,
      "learning_rate": 7.391298963071216e-05,
      "loss": 0.1935,
      "step": 243400
    },
    {
      "epoch": 0.7539329913026785,
      "grad_norm": 60.50698471069336,
      "learning_rate": 7.382010260919643e-05,
      "loss": 0.2387,
      "step": 243500
    },
    {
      "epoch": 0.754242614707731,
      "grad_norm": 0.00024879915872588754,
      "learning_rate": 7.372721558768069e-05,
      "loss": 0.168,
      "step": 243600
    },
    {
      "epoch": 0.7545522381127834,
      "grad_norm": 19.435293197631836,
      "learning_rate": 7.363432856616498e-05,
      "loss": 0.3298,
      "step": 243700
    },
    {
      "epoch": 0.7548618615178359,
      "grad_norm": 0.0027443033177405596,
      "learning_rate": 7.354144154464924e-05,
      "loss": 0.2181,
      "step": 243800
    },
    {
      "epoch": 0.7551714849228883,
      "grad_norm": 18.204544067382812,
      "learning_rate": 7.34485545231335e-05,
      "loss": 0.1302,
      "step": 243900
    },
    {
      "epoch": 0.7554811083279407,
      "grad_norm": 19.32545280456543,
      "learning_rate": 7.335566750161777e-05,
      "loss": 0.2086,
      "step": 244000
    },
    {
      "epoch": 0.7557907317329932,
      "grad_norm": 0.0004930492723360658,
      "learning_rate": 7.326278048010205e-05,
      "loss": 0.1522,
      "step": 244100
    },
    {
      "epoch": 0.7561003551380456,
      "grad_norm": 0.00020613332162611187,
      "learning_rate": 7.316989345858631e-05,
      "loss": 0.2727,
      "step": 244200
    },
    {
      "epoch": 0.756409978543098,
      "grad_norm": 0.00022443824855145067,
      "learning_rate": 7.307700643707058e-05,
      "loss": 0.3529,
      "step": 244300
    },
    {
      "epoch": 0.7567196019481505,
      "grad_norm": 0.004406196530908346,
      "learning_rate": 7.298411941555486e-05,
      "loss": 0.2442,
      "step": 244400
    },
    {
      "epoch": 0.7570292253532029,
      "grad_norm": 0.002138713141903281,
      "learning_rate": 7.289123239403912e-05,
      "loss": 0.1444,
      "step": 244500
    },
    {
      "epoch": 0.7573388487582553,
      "grad_norm": 0.001366154756397009,
      "learning_rate": 7.279834537252339e-05,
      "loss": 0.126,
      "step": 244600
    },
    {
      "epoch": 0.7576484721633078,
      "grad_norm": 0.1277562975883484,
      "learning_rate": 7.270545835100766e-05,
      "loss": 0.1361,
      "step": 244700
    },
    {
      "epoch": 0.7579580955683602,
      "grad_norm": 8.630644798278809,
      "learning_rate": 7.261257132949194e-05,
      "loss": 0.3708,
      "step": 244800
    },
    {
      "epoch": 0.7582677189734126,
      "grad_norm": 38.4786262512207,
      "learning_rate": 7.25196843079762e-05,
      "loss": 0.1465,
      "step": 244900
    },
    {
      "epoch": 0.7585773423784651,
      "grad_norm": 0.4192458689212799,
      "learning_rate": 7.242679728646047e-05,
      "loss": 0.1639,
      "step": 245000
    },
    {
      "epoch": 0.7588869657835176,
      "grad_norm": 0.024763628840446472,
      "learning_rate": 7.233391026494475e-05,
      "loss": 0.1255,
      "step": 245100
    },
    {
      "epoch": 0.7591965891885699,
      "grad_norm": 0.0002766404941212386,
      "learning_rate": 7.2241023243429e-05,
      "loss": 0.2431,
      "step": 245200
    },
    {
      "epoch": 0.7595062125936224,
      "grad_norm": 0.0017754407599568367,
      "learning_rate": 7.214813622191328e-05,
      "loss": 0.4221,
      "step": 245300
    },
    {
      "epoch": 0.7598158359986749,
      "grad_norm": 173.46685791015625,
      "learning_rate": 7.205524920039755e-05,
      "loss": 0.3113,
      "step": 245400
    },
    {
      "epoch": 0.7601254594037272,
      "grad_norm": 0.000894675322342664,
      "learning_rate": 7.196236217888182e-05,
      "loss": 0.1432,
      "step": 245500
    },
    {
      "epoch": 0.7604350828087797,
      "grad_norm": 0.008555851876735687,
      "learning_rate": 7.186947515736609e-05,
      "loss": 0.1562,
      "step": 245600
    },
    {
      "epoch": 0.7607447062138322,
      "grad_norm": 0.08360373973846436,
      "learning_rate": 7.177658813585036e-05,
      "loss": 0.4199,
      "step": 245700
    },
    {
      "epoch": 0.7610543296188845,
      "grad_norm": 1.2399345636367798,
      "learning_rate": 7.168370111433462e-05,
      "loss": 0.271,
      "step": 245800
    },
    {
      "epoch": 0.761363953023937,
      "grad_norm": 0.00021023719455115497,
      "learning_rate": 7.15908140928189e-05,
      "loss": 0.1731,
      "step": 245900
    },
    {
      "epoch": 0.7616735764289895,
      "grad_norm": 0.0008949596085585654,
      "learning_rate": 7.149792707130317e-05,
      "loss": 0.3712,
      "step": 246000
    },
    {
      "epoch": 0.7619831998340418,
      "grad_norm": 6.762417615391314e-05,
      "learning_rate": 7.140504004978743e-05,
      "loss": 0.233,
      "step": 246100
    },
    {
      "epoch": 0.7622928232390943,
      "grad_norm": 0.014255322515964508,
      "learning_rate": 7.13121530282717e-05,
      "loss": 0.1151,
      "step": 246200
    },
    {
      "epoch": 0.7626024466441467,
      "grad_norm": 15.348200798034668,
      "learning_rate": 7.121926600675598e-05,
      "loss": 0.2143,
      "step": 246300
    },
    {
      "epoch": 0.7629120700491991,
      "grad_norm": 0.0006151295965537429,
      "learning_rate": 7.112637898524024e-05,
      "loss": 0.2385,
      "step": 246400
    },
    {
      "epoch": 0.7632216934542516,
      "grad_norm": 0.001332503859885037,
      "learning_rate": 7.103349196372451e-05,
      "loss": 0.2906,
      "step": 246500
    },
    {
      "epoch": 0.763531316859304,
      "grad_norm": 0.0018359047826379538,
      "learning_rate": 7.094060494220879e-05,
      "loss": 0.1519,
      "step": 246600
    },
    {
      "epoch": 0.7638409402643564,
      "grad_norm": 0.007785154040902853,
      "learning_rate": 7.084771792069306e-05,
      "loss": 0.3168,
      "step": 246700
    },
    {
      "epoch": 0.7641505636694089,
      "grad_norm": 17.819860458374023,
      "learning_rate": 7.075483089917732e-05,
      "loss": 0.1294,
      "step": 246800
    },
    {
      "epoch": 0.7644601870744613,
      "grad_norm": 0.000898559286724776,
      "learning_rate": 7.06619438776616e-05,
      "loss": 0.2101,
      "step": 246900
    },
    {
      "epoch": 0.7647698104795138,
      "grad_norm": 0.00013861653860658407,
      "learning_rate": 7.056905685614587e-05,
      "loss": 0.2136,
      "step": 247000
    },
    {
      "epoch": 0.7650794338845662,
      "grad_norm": 0.00035646831383928657,
      "learning_rate": 7.047616983463013e-05,
      "loss": 0.3696,
      "step": 247100
    },
    {
      "epoch": 0.7653890572896186,
      "grad_norm": 0.0002640178136061877,
      "learning_rate": 7.03832828131144e-05,
      "loss": 0.2816,
      "step": 247200
    },
    {
      "epoch": 0.7656986806946711,
      "grad_norm": 0.0028196657076478004,
      "learning_rate": 7.029039579159868e-05,
      "loss": 0.1327,
      "step": 247300
    },
    {
      "epoch": 0.7660083040997235,
      "grad_norm": 0.00037542724749073386,
      "learning_rate": 7.019750877008294e-05,
      "loss": 0.2393,
      "step": 247400
    },
    {
      "epoch": 0.7663179275047759,
      "grad_norm": 0.007557826116681099,
      "learning_rate": 7.010462174856721e-05,
      "loss": 0.216,
      "step": 247500
    },
    {
      "epoch": 0.7666275509098284,
      "grad_norm": 0.004510972183197737,
      "learning_rate": 7.001173472705149e-05,
      "loss": 0.2564,
      "step": 247600
    },
    {
      "epoch": 0.7669371743148808,
      "grad_norm": 0.00019006477668881416,
      "learning_rate": 6.991884770553575e-05,
      "loss": 0.1939,
      "step": 247700
    },
    {
      "epoch": 0.7672467977199332,
      "grad_norm": 0.0010124287800863385,
      "learning_rate": 6.982596068402002e-05,
      "loss": 0.1532,
      "step": 247800
    },
    {
      "epoch": 0.7675564211249857,
      "grad_norm": 0.0029242855962365866,
      "learning_rate": 6.97330736625043e-05,
      "loss": 0.2193,
      "step": 247900
    },
    {
      "epoch": 0.7678660445300381,
      "grad_norm": 0.0008491937769576907,
      "learning_rate": 6.964018664098856e-05,
      "loss": 0.1581,
      "step": 248000
    },
    {
      "epoch": 0.7681756679350905,
      "grad_norm": 0.0002749223494902253,
      "learning_rate": 6.954729961947283e-05,
      "loss": 0.2878,
      "step": 248100
    },
    {
      "epoch": 0.768485291340143,
      "grad_norm": 12.591745376586914,
      "learning_rate": 6.94544125979571e-05,
      "loss": 0.2675,
      "step": 248200
    },
    {
      "epoch": 0.7687949147451955,
      "grad_norm": 0.0005665577482432127,
      "learning_rate": 6.936152557644137e-05,
      "loss": 0.1809,
      "step": 248300
    },
    {
      "epoch": 0.7691045381502478,
      "grad_norm": 0.015069629997015,
      "learning_rate": 6.926863855492564e-05,
      "loss": 0.1735,
      "step": 248400
    },
    {
      "epoch": 0.7694141615553003,
      "grad_norm": 0.009260104037821293,
      "learning_rate": 6.91757515334099e-05,
      "loss": 0.1798,
      "step": 248500
    },
    {
      "epoch": 0.7697237849603528,
      "grad_norm": 0.002250505145639181,
      "learning_rate": 6.908286451189417e-05,
      "loss": 0.1155,
      "step": 248600
    },
    {
      "epoch": 0.7700334083654051,
      "grad_norm": 0.00037010989035479724,
      "learning_rate": 6.898997749037845e-05,
      "loss": 0.1944,
      "step": 248700
    },
    {
      "epoch": 0.7703430317704576,
      "grad_norm": 0.011922420933842659,
      "learning_rate": 6.889709046886271e-05,
      "loss": 0.2619,
      "step": 248800
    },
    {
      "epoch": 0.7706526551755101,
      "grad_norm": 0.007153937127441168,
      "learning_rate": 6.8804203447347e-05,
      "loss": 0.3738,
      "step": 248900
    },
    {
      "epoch": 0.7709622785805624,
      "grad_norm": 0.00043328857282176614,
      "learning_rate": 6.871131642583126e-05,
      "loss": 0.3452,
      "step": 249000
    },
    {
      "epoch": 0.7712719019856149,
      "grad_norm": 0.0004836683510802686,
      "learning_rate": 6.861842940431552e-05,
      "loss": 0.3429,
      "step": 249100
    },
    {
      "epoch": 0.7715815253906674,
      "grad_norm": 0.0018319262890145183,
      "learning_rate": 6.852554238279979e-05,
      "loss": 0.2487,
      "step": 249200
    },
    {
      "epoch": 0.7718911487957197,
      "grad_norm": 15.461654663085938,
      "learning_rate": 6.843265536128406e-05,
      "loss": 0.2907,
      "step": 249300
    },
    {
      "epoch": 0.7722007722007722,
      "grad_norm": 0.00025271112099289894,
      "learning_rate": 6.833976833976833e-05,
      "loss": 0.1859,
      "step": 249400
    },
    {
      "epoch": 0.7725103956058247,
      "grad_norm": 0.00035211117938160896,
      "learning_rate": 6.82468813182526e-05,
      "loss": 0.1698,
      "step": 249500
    },
    {
      "epoch": 0.772820019010877,
      "grad_norm": 0.005852337460964918,
      "learning_rate": 6.815399429673687e-05,
      "loss": 0.131,
      "step": 249600
    },
    {
      "epoch": 0.7731296424159295,
      "grad_norm": 0.0009882096201181412,
      "learning_rate": 6.806110727522115e-05,
      "loss": 0.1919,
      "step": 249700
    },
    {
      "epoch": 0.773439265820982,
      "grad_norm": 7.411526166833937e-05,
      "learning_rate": 6.796822025370541e-05,
      "loss": 0.2688,
      "step": 249800
    },
    {
      "epoch": 0.7737488892260344,
      "grad_norm": 17.924394607543945,
      "learning_rate": 6.787533323218968e-05,
      "loss": 0.2425,
      "step": 249900
    },
    {
      "epoch": 0.7740585126310868,
      "grad_norm": 0.003703874535858631,
      "learning_rate": 6.778244621067396e-05,
      "loss": 0.2074,
      "step": 250000
    },
    {
      "epoch": 0.7743681360361393,
      "grad_norm": 73.6462173461914,
      "learning_rate": 6.768955918915822e-05,
      "loss": 0.1761,
      "step": 250100
    },
    {
      "epoch": 0.7746777594411917,
      "grad_norm": 0.00024258439952973276,
      "learning_rate": 6.759667216764249e-05,
      "loss": 0.1952,
      "step": 250200
    },
    {
      "epoch": 0.7749873828462441,
      "grad_norm": 0.5584880113601685,
      "learning_rate": 6.750378514612676e-05,
      "loss": 0.1534,
      "step": 250300
    },
    {
      "epoch": 0.7752970062512965,
      "grad_norm": 3.782191038131714,
      "learning_rate": 6.741089812461102e-05,
      "loss": 0.2985,
      "step": 250400
    },
    {
      "epoch": 0.775606629656349,
      "grad_norm": 0.004882059525698423,
      "learning_rate": 6.73180111030953e-05,
      "loss": 0.2658,
      "step": 250500
    },
    {
      "epoch": 0.7759162530614014,
      "grad_norm": 0.002528225304558873,
      "learning_rate": 6.722512408157957e-05,
      "loss": 0.2053,
      "step": 250600
    },
    {
      "epoch": 0.7762258764664538,
      "grad_norm": 0.40695586800575256,
      "learning_rate": 6.713223706006383e-05,
      "loss": 0.1147,
      "step": 250700
    },
    {
      "epoch": 0.7765354998715063,
      "grad_norm": 0.03167352452874184,
      "learning_rate": 6.703935003854811e-05,
      "loss": 0.4005,
      "step": 250800
    },
    {
      "epoch": 0.7768451232765587,
      "grad_norm": 0.00014274788554757833,
      "learning_rate": 6.694646301703238e-05,
      "loss": 0.3014,
      "step": 250900
    },
    {
      "epoch": 0.7771547466816111,
      "grad_norm": 0.00022363467724062502,
      "learning_rate": 6.685357599551664e-05,
      "loss": 0.2351,
      "step": 251000
    },
    {
      "epoch": 0.7774643700866636,
      "grad_norm": 0.0024981137830764055,
      "learning_rate": 6.676068897400092e-05,
      "loss": 0.1549,
      "step": 251100
    },
    {
      "epoch": 0.777773993491716,
      "grad_norm": 12.23487663269043,
      "learning_rate": 6.666780195248519e-05,
      "loss": 0.2954,
      "step": 251200
    },
    {
      "epoch": 0.7780836168967684,
      "grad_norm": 0.0008368816343136132,
      "learning_rate": 6.657491493096945e-05,
      "loss": 0.3996,
      "step": 251300
    },
    {
      "epoch": 0.7783932403018209,
      "grad_norm": 0.0003954663116019219,
      "learning_rate": 6.648202790945372e-05,
      "loss": 0.4067,
      "step": 251400
    },
    {
      "epoch": 0.7787028637068734,
      "grad_norm": 0.0013713142834603786,
      "learning_rate": 6.6389140887938e-05,
      "loss": 0.2484,
      "step": 251500
    },
    {
      "epoch": 0.7790124871119257,
      "grad_norm": 0.001066052820533514,
      "learning_rate": 6.629625386642226e-05,
      "loss": 0.2251,
      "step": 251600
    },
    {
      "epoch": 0.7793221105169782,
      "grad_norm": 0.0007623574347235262,
      "learning_rate": 6.620336684490653e-05,
      "loss": 0.1377,
      "step": 251700
    },
    {
      "epoch": 0.7796317339220307,
      "grad_norm": 20.688507080078125,
      "learning_rate": 6.611047982339081e-05,
      "loss": 0.2471,
      "step": 251800
    },
    {
      "epoch": 0.779941357327083,
      "grad_norm": 0.00011280260514467955,
      "learning_rate": 6.601759280187508e-05,
      "loss": 0.129,
      "step": 251900
    },
    {
      "epoch": 0.7802509807321355,
      "grad_norm": 0.011193457990884781,
      "learning_rate": 6.592470578035934e-05,
      "loss": 0.1853,
      "step": 252000
    },
    {
      "epoch": 0.780560604137188,
      "grad_norm": 0.0062151942402124405,
      "learning_rate": 6.583181875884362e-05,
      "loss": 0.2123,
      "step": 252100
    },
    {
      "epoch": 0.7808702275422403,
      "grad_norm": 0.0014749389374628663,
      "learning_rate": 6.573893173732789e-05,
      "loss": 0.251,
      "step": 252200
    },
    {
      "epoch": 0.7811798509472928,
      "grad_norm": 0.0008715947042219341,
      "learning_rate": 6.564604471581215e-05,
      "loss": 0.3189,
      "step": 252300
    },
    {
      "epoch": 0.7814894743523453,
      "grad_norm": 0.3403390645980835,
      "learning_rate": 6.555315769429642e-05,
      "loss": 0.2256,
      "step": 252400
    },
    {
      "epoch": 0.7817990977573976,
      "grad_norm": 0.0043082148768007755,
      "learning_rate": 6.54602706727807e-05,
      "loss": 0.1769,
      "step": 252500
    },
    {
      "epoch": 0.7821087211624501,
      "grad_norm": 0.00041417431202717125,
      "learning_rate": 6.536738365126496e-05,
      "loss": 0.1147,
      "step": 252600
    },
    {
      "epoch": 0.7824183445675026,
      "grad_norm": 0.0008114739321172237,
      "learning_rate": 6.527449662974923e-05,
      "loss": 0.319,
      "step": 252700
    },
    {
      "epoch": 0.782727967972555,
      "grad_norm": 0.0004634213983081281,
      "learning_rate": 6.51816096082335e-05,
      "loss": 0.1715,
      "step": 252800
    },
    {
      "epoch": 0.7830375913776074,
      "grad_norm": 7.548904977738857e-05,
      "learning_rate": 6.508872258671777e-05,
      "loss": 0.1721,
      "step": 252900
    },
    {
      "epoch": 0.7833472147826599,
      "grad_norm": 0.37594664096832275,
      "learning_rate": 6.499583556520204e-05,
      "loss": 0.3098,
      "step": 253000
    },
    {
      "epoch": 0.7836568381877123,
      "grad_norm": 0.0020646078046411276,
      "learning_rate": 6.490294854368631e-05,
      "loss": 0.3399,
      "step": 253100
    },
    {
      "epoch": 0.7839664615927647,
      "grad_norm": 0.005420851055532694,
      "learning_rate": 6.481006152217058e-05,
      "loss": 0.1264,
      "step": 253200
    },
    {
      "epoch": 0.7842760849978172,
      "grad_norm": 0.006419995799660683,
      "learning_rate": 6.471717450065485e-05,
      "loss": 0.088,
      "step": 253300
    },
    {
      "epoch": 0.7845857084028696,
      "grad_norm": 0.0018199111800640821,
      "learning_rate": 6.462428747913912e-05,
      "loss": 0.2165,
      "step": 253400
    },
    {
      "epoch": 0.784895331807922,
      "grad_norm": 0.00018893249216489494,
      "learning_rate": 6.453140045762338e-05,
      "loss": 0.1726,
      "step": 253500
    },
    {
      "epoch": 0.7852049552129745,
      "grad_norm": 0.004677609074860811,
      "learning_rate": 6.443851343610766e-05,
      "loss": 0.2145,
      "step": 253600
    },
    {
      "epoch": 0.7855145786180269,
      "grad_norm": 0.04975443705916405,
      "learning_rate": 6.434562641459192e-05,
      "loss": 0.2965,
      "step": 253700
    },
    {
      "epoch": 0.7858242020230793,
      "grad_norm": 0.022359665483236313,
      "learning_rate": 6.42527393930762e-05,
      "loss": 0.1976,
      "step": 253800
    },
    {
      "epoch": 0.7861338254281318,
      "grad_norm": 0.00024758948711678386,
      "learning_rate": 6.415985237156047e-05,
      "loss": 0.2404,
      "step": 253900
    },
    {
      "epoch": 0.7864434488331842,
      "grad_norm": 0.042192280292510986,
      "learning_rate": 6.406696535004473e-05,
      "loss": 0.1796,
      "step": 254000
    },
    {
      "epoch": 0.7867530722382367,
      "grad_norm": 17.787193298339844,
      "learning_rate": 6.397407832852901e-05,
      "loss": 0.237,
      "step": 254100
    },
    {
      "epoch": 0.787062695643289,
      "grad_norm": 0.004155784845352173,
      "learning_rate": 6.388119130701327e-05,
      "loss": 0.243,
      "step": 254200
    },
    {
      "epoch": 0.7873723190483415,
      "grad_norm": 0.0030407593585550785,
      "learning_rate": 6.378830428549754e-05,
      "loss": 0.2606,
      "step": 254300
    },
    {
      "epoch": 0.787681942453394,
      "grad_norm": 5.383639290812425e-05,
      "learning_rate": 6.369541726398181e-05,
      "loss": 0.108,
      "step": 254400
    },
    {
      "epoch": 0.7879915658584463,
      "grad_norm": 0.003995888400822878,
      "learning_rate": 6.360253024246608e-05,
      "loss": 0.1403,
      "step": 254500
    },
    {
      "epoch": 0.7883011892634988,
      "grad_norm": 93.59944915771484,
      "learning_rate": 6.350964322095034e-05,
      "loss": 0.2395,
      "step": 254600
    },
    {
      "epoch": 0.7886108126685513,
      "grad_norm": 0.0018540140008553863,
      "learning_rate": 6.341675619943462e-05,
      "loss": 0.1369,
      "step": 254700
    },
    {
      "epoch": 0.7889204360736036,
      "grad_norm": 6.580320358276367,
      "learning_rate": 6.332386917791889e-05,
      "loss": 0.265,
      "step": 254800
    },
    {
      "epoch": 0.7892300594786561,
      "grad_norm": 0.0017201803857460618,
      "learning_rate": 6.323098215640317e-05,
      "loss": 0.141,
      "step": 254900
    },
    {
      "epoch": 0.7895396828837086,
      "grad_norm": 59.88825225830078,
      "learning_rate": 6.313809513488743e-05,
      "loss": 0.1548,
      "step": 255000
    },
    {
      "epoch": 0.7898493062887609,
      "grad_norm": 27.94142723083496,
      "learning_rate": 6.30452081133717e-05,
      "loss": 0.2415,
      "step": 255100
    },
    {
      "epoch": 0.7901589296938134,
      "grad_norm": 0.8519670963287354,
      "learning_rate": 6.295232109185597e-05,
      "loss": 0.1912,
      "step": 255200
    },
    {
      "epoch": 0.7904685530988659,
      "grad_norm": 0.0015537937870249152,
      "learning_rate": 6.285943407034023e-05,
      "loss": 0.2196,
      "step": 255300
    },
    {
      "epoch": 0.7907781765039182,
      "grad_norm": 0.0008055434445850551,
      "learning_rate": 6.276654704882451e-05,
      "loss": 0.3006,
      "step": 255400
    },
    {
      "epoch": 0.7910877999089707,
      "grad_norm": 0.0002317101025255397,
      "learning_rate": 6.267366002730878e-05,
      "loss": 0.1702,
      "step": 255500
    },
    {
      "epoch": 0.7913974233140232,
      "grad_norm": 0.001047969446517527,
      "learning_rate": 6.258077300579304e-05,
      "loss": 0.3504,
      "step": 255600
    },
    {
      "epoch": 0.7917070467190755,
      "grad_norm": 0.0003161683853249997,
      "learning_rate": 6.248788598427732e-05,
      "loss": 0.1737,
      "step": 255700
    },
    {
      "epoch": 0.792016670124128,
      "grad_norm": 50.224029541015625,
      "learning_rate": 6.239499896276159e-05,
      "loss": 0.2674,
      "step": 255800
    },
    {
      "epoch": 0.7923262935291805,
      "grad_norm": 0.0011966887395828962,
      "learning_rate": 6.230211194124585e-05,
      "loss": 0.1852,
      "step": 255900
    },
    {
      "epoch": 0.7926359169342329,
      "grad_norm": 0.0030645125079900026,
      "learning_rate": 6.220922491973013e-05,
      "loss": 0.1328,
      "step": 256000
    },
    {
      "epoch": 0.7929455403392853,
      "grad_norm": 0.000709487299900502,
      "learning_rate": 6.21163378982144e-05,
      "loss": 0.2115,
      "step": 256100
    },
    {
      "epoch": 0.7932551637443378,
      "grad_norm": 0.00012416385288815945,
      "learning_rate": 6.202345087669866e-05,
      "loss": 0.1916,
      "step": 256200
    },
    {
      "epoch": 0.7935647871493902,
      "grad_norm": 0.0002769512648228556,
      "learning_rate": 6.193056385518293e-05,
      "loss": 0.322,
      "step": 256300
    },
    {
      "epoch": 0.7938744105544426,
      "grad_norm": 13.595852851867676,
      "learning_rate": 6.183767683366721e-05,
      "loss": 0.3486,
      "step": 256400
    },
    {
      "epoch": 0.7941840339594951,
      "grad_norm": 0.02084858901798725,
      "learning_rate": 6.174478981215147e-05,
      "loss": 0.199,
      "step": 256500
    },
    {
      "epoch": 0.7944936573645475,
      "grad_norm": 0.00015858735423535109,
      "learning_rate": 6.165190279063574e-05,
      "loss": 0.2,
      "step": 256600
    },
    {
      "epoch": 0.7948032807695999,
      "grad_norm": 0.0026112517807632685,
      "learning_rate": 6.155901576912002e-05,
      "loss": 0.2166,
      "step": 256700
    },
    {
      "epoch": 0.7951129041746524,
      "grad_norm": 0.0016589548904448748,
      "learning_rate": 6.146612874760429e-05,
      "loss": 0.1828,
      "step": 256800
    },
    {
      "epoch": 0.7954225275797048,
      "grad_norm": 0.0004408464301377535,
      "learning_rate": 6.137324172608855e-05,
      "loss": 0.3084,
      "step": 256900
    },
    {
      "epoch": 0.7957321509847572,
      "grad_norm": 0.00029078847728669643,
      "learning_rate": 6.128035470457282e-05,
      "loss": 0.2646,
      "step": 257000
    },
    {
      "epoch": 0.7960417743898097,
      "grad_norm": 0.0003811631177086383,
      "learning_rate": 6.11874676830571e-05,
      "loss": 0.2131,
      "step": 257100
    },
    {
      "epoch": 0.7963513977948621,
      "grad_norm": 0.05862351134419441,
      "learning_rate": 6.109458066154136e-05,
      "loss": 0.275,
      "step": 257200
    },
    {
      "epoch": 0.7966610211999146,
      "grad_norm": 0.004512123763561249,
      "learning_rate": 6.100169364002563e-05,
      "loss": 0.1193,
      "step": 257300
    },
    {
      "epoch": 0.796970644604967,
      "grad_norm": 0.0002974179806187749,
      "learning_rate": 6.09088066185099e-05,
      "loss": 0.1974,
      "step": 257400
    },
    {
      "epoch": 0.7972802680100194,
      "grad_norm": 0.0015836728271096945,
      "learning_rate": 6.081591959699417e-05,
      "loss": 0.1678,
      "step": 257500
    },
    {
      "epoch": 0.7975898914150719,
      "grad_norm": 0.00022505268862005323,
      "learning_rate": 6.0723032575478435e-05,
      "loss": 0.2328,
      "step": 257600
    },
    {
      "epoch": 0.7978995148201243,
      "grad_norm": 0.0035745087079703808,
      "learning_rate": 6.063014555396271e-05,
      "loss": 0.3215,
      "step": 257700
    },
    {
      "epoch": 0.7982091382251767,
      "grad_norm": 0.0005992045626044273,
      "learning_rate": 6.0537258532446976e-05,
      "loss": 0.1678,
      "step": 257800
    },
    {
      "epoch": 0.7985187616302292,
      "grad_norm": 0.00018509574874769896,
      "learning_rate": 6.044437151093125e-05,
      "loss": 0.1767,
      "step": 257900
    },
    {
      "epoch": 0.7988283850352816,
      "grad_norm": 0.00021651304268743843,
      "learning_rate": 6.035148448941552e-05,
      "loss": 0.1193,
      "step": 258000
    },
    {
      "epoch": 0.799138008440334,
      "grad_norm": 0.002477560890838504,
      "learning_rate": 6.0258597467899785e-05,
      "loss": 0.2151,
      "step": 258100
    },
    {
      "epoch": 0.7994476318453865,
      "grad_norm": 11.55831527709961,
      "learning_rate": 6.016571044638406e-05,
      "loss": 0.2473,
      "step": 258200
    },
    {
      "epoch": 0.7997572552504388,
      "grad_norm": 0.0002618840953800827,
      "learning_rate": 6.0072823424868326e-05,
      "loss": 0.2364,
      "step": 258300
    },
    {
      "epoch": 0.8000668786554913,
      "grad_norm": 1.046103596687317,
      "learning_rate": 5.997993640335259e-05,
      "loss": 0.3209,
      "step": 258400
    },
    {
      "epoch": 0.8003765020605438,
      "grad_norm": 0.00037635042099282146,
      "learning_rate": 5.988704938183687e-05,
      "loss": 0.3411,
      "step": 258500
    },
    {
      "epoch": 0.8006861254655961,
      "grad_norm": 0.005771254189312458,
      "learning_rate": 5.9794162360321135e-05,
      "loss": 0.1999,
      "step": 258600
    },
    {
      "epoch": 0.8009957488706486,
      "grad_norm": 0.003792844945564866,
      "learning_rate": 5.97012753388054e-05,
      "loss": 0.2068,
      "step": 258700
    },
    {
      "epoch": 0.8013053722757011,
      "grad_norm": 0.0001357706933049485,
      "learning_rate": 5.9608388317289676e-05,
      "loss": 0.2377,
      "step": 258800
    },
    {
      "epoch": 0.8016149956807535,
      "grad_norm": 0.11808905005455017,
      "learning_rate": 5.951550129577394e-05,
      "loss": 0.2277,
      "step": 258900
    },
    {
      "epoch": 0.8019246190858059,
      "grad_norm": 26.79158592224121,
      "learning_rate": 5.942261427425822e-05,
      "loss": 0.1619,
      "step": 259000
    },
    {
      "epoch": 0.8022342424908584,
      "grad_norm": 0.047701526433229446,
      "learning_rate": 5.9329727252742484e-05,
      "loss": 0.2608,
      "step": 259100
    },
    {
      "epoch": 0.8025438658959108,
      "grad_norm": 0.00019601218809839338,
      "learning_rate": 5.923684023122675e-05,
      "loss": 0.1706,
      "step": 259200
    },
    {
      "epoch": 0.8028534893009632,
      "grad_norm": 104.70919036865234,
      "learning_rate": 5.9143953209711026e-05,
      "loss": 0.1582,
      "step": 259300
    },
    {
      "epoch": 0.8031631127060157,
      "grad_norm": 0.012546942569315434,
      "learning_rate": 5.905106618819529e-05,
      "loss": 0.1598,
      "step": 259400
    },
    {
      "epoch": 0.8034727361110681,
      "grad_norm": 4.981645584106445,
      "learning_rate": 5.895817916667956e-05,
      "loss": 0.1891,
      "step": 259500
    },
    {
      "epoch": 0.8037823595161205,
      "grad_norm": 0.00031476409640163183,
      "learning_rate": 5.8865292145163834e-05,
      "loss": 0.1797,
      "step": 259600
    },
    {
      "epoch": 0.804091982921173,
      "grad_norm": 0.0014929795870557427,
      "learning_rate": 5.87724051236481e-05,
      "loss": 0.2893,
      "step": 259700
    },
    {
      "epoch": 0.8044016063262254,
      "grad_norm": 0.0002766380785033107,
      "learning_rate": 5.8679518102132375e-05,
      "loss": 0.2502,
      "step": 259800
    },
    {
      "epoch": 0.8047112297312778,
      "grad_norm": 20.574981689453125,
      "learning_rate": 5.858663108061664e-05,
      "loss": 0.1922,
      "step": 259900
    },
    {
      "epoch": 0.8050208531363303,
      "grad_norm": 0.0003142612986266613,
      "learning_rate": 5.849374405910091e-05,
      "loss": 0.2003,
      "step": 260000
    },
    {
      "epoch": 0.8053304765413827,
      "grad_norm": 0.0009828392649069428,
      "learning_rate": 5.8400857037585184e-05,
      "loss": 0.1298,
      "step": 260100
    },
    {
      "epoch": 0.8056400999464352,
      "grad_norm": 0.0005613847752101719,
      "learning_rate": 5.830797001606945e-05,
      "loss": 0.2651,
      "step": 260200
    },
    {
      "epoch": 0.8059497233514876,
      "grad_norm": 0.002483937656506896,
      "learning_rate": 5.821508299455372e-05,
      "loss": 0.1448,
      "step": 260300
    },
    {
      "epoch": 0.80625934675654,
      "grad_norm": 4.352986812591553,
      "learning_rate": 5.812219597303799e-05,
      "loss": 0.3579,
      "step": 260400
    },
    {
      "epoch": 0.8065689701615925,
      "grad_norm": 0.020022841170430183,
      "learning_rate": 5.802930895152226e-05,
      "loss": 0.1829,
      "step": 260500
    },
    {
      "epoch": 0.8068785935666449,
      "grad_norm": 1.255580264114542e-05,
      "learning_rate": 5.793642193000653e-05,
      "loss": 0.1934,
      "step": 260600
    },
    {
      "epoch": 0.8071882169716973,
      "grad_norm": 0.00018206622917205095,
      "learning_rate": 5.78435349084908e-05,
      "loss": 0.2047,
      "step": 260700
    },
    {
      "epoch": 0.8074978403767498,
      "grad_norm": 0.0006995238945819438,
      "learning_rate": 5.775064788697507e-05,
      "loss": 0.1933,
      "step": 260800
    },
    {
      "epoch": 0.8078074637818022,
      "grad_norm": 0.0066966768354177475,
      "learning_rate": 5.765776086545934e-05,
      "loss": 0.1143,
      "step": 260900
    },
    {
      "epoch": 0.8081170871868546,
      "grad_norm": 12.11917495727539,
      "learning_rate": 5.756487384394361e-05,
      "loss": 0.1576,
      "step": 261000
    },
    {
      "epoch": 0.8084267105919071,
      "grad_norm": 0.00013062907964922488,
      "learning_rate": 5.7471986822427876e-05,
      "loss": 0.1934,
      "step": 261100
    },
    {
      "epoch": 0.8087363339969595,
      "grad_norm": 0.004638976883143187,
      "learning_rate": 5.737909980091215e-05,
      "loss": 0.2894,
      "step": 261200
    },
    {
      "epoch": 0.8090459574020119,
      "grad_norm": 0.0010224648285657167,
      "learning_rate": 5.728621277939642e-05,
      "loss": 0.2748,
      "step": 261300
    },
    {
      "epoch": 0.8093555808070644,
      "grad_norm": 0.0021414957009255886,
      "learning_rate": 5.719332575788068e-05,
      "loss": 0.1386,
      "step": 261400
    },
    {
      "epoch": 0.8096652042121169,
      "grad_norm": 0.0012281023664399981,
      "learning_rate": 5.710043873636496e-05,
      "loss": 0.2314,
      "step": 261500
    },
    {
      "epoch": 0.8099748276171692,
      "grad_norm": 38.72101974487305,
      "learning_rate": 5.700755171484922e-05,
      "loss": 0.2491,
      "step": 261600
    },
    {
      "epoch": 0.8102844510222217,
      "grad_norm": 0.008397908881306648,
      "learning_rate": 5.691466469333349e-05,
      "loss": 0.1519,
      "step": 261700
    },
    {
      "epoch": 0.8105940744272742,
      "grad_norm": 0.0006901842425577343,
      "learning_rate": 5.682177767181777e-05,
      "loss": 0.2336,
      "step": 261800
    },
    {
      "epoch": 0.8109036978323265,
      "grad_norm": 2.4189303076127544e-05,
      "learning_rate": 5.672889065030203e-05,
      "loss": 0.2727,
      "step": 261900
    },
    {
      "epoch": 0.811213321237379,
      "grad_norm": 0.0005458214436657727,
      "learning_rate": 5.663600362878631e-05,
      "loss": 0.2297,
      "step": 262000
    },
    {
      "epoch": 0.8115229446424315,
      "grad_norm": 0.0005270700785331428,
      "learning_rate": 5.654311660727057e-05,
      "loss": 0.2463,
      "step": 262100
    },
    {
      "epoch": 0.8118325680474838,
      "grad_norm": 79.40894317626953,
      "learning_rate": 5.6450229585754836e-05,
      "loss": 0.2932,
      "step": 262200
    },
    {
      "epoch": 0.8121421914525363,
      "grad_norm": 0.0008887759177014232,
      "learning_rate": 5.635734256423911e-05,
      "loss": 0.2046,
      "step": 262300
    },
    {
      "epoch": 0.8124518148575887,
      "grad_norm": 0.00023473866167478263,
      "learning_rate": 5.626445554272338e-05,
      "loss": 0.2829,
      "step": 262400
    },
    {
      "epoch": 0.8127614382626411,
      "grad_norm": 0.07408745586872101,
      "learning_rate": 5.6171568521207645e-05,
      "loss": 0.219,
      "step": 262500
    },
    {
      "epoch": 0.8130710616676936,
      "grad_norm": 0.0004546625423245132,
      "learning_rate": 5.607868149969192e-05,
      "loss": 0.1632,
      "step": 262600
    },
    {
      "epoch": 0.813380685072746,
      "grad_norm": 10.401236534118652,
      "learning_rate": 5.5985794478176186e-05,
      "loss": 0.1166,
      "step": 262700
    },
    {
      "epoch": 0.8136903084777984,
      "grad_norm": 0.0031238903757184744,
      "learning_rate": 5.589290745666046e-05,
      "loss": 0.1638,
      "step": 262800
    },
    {
      "epoch": 0.8139999318828509,
      "grad_norm": 0.7076862454414368,
      "learning_rate": 5.580002043514473e-05,
      "loss": 0.1958,
      "step": 262900
    },
    {
      "epoch": 0.8143095552879033,
      "grad_norm": 0.0004024204099550843,
      "learning_rate": 5.5707133413628995e-05,
      "loss": 0.1979,
      "step": 263000
    },
    {
      "epoch": 0.8146191786929557,
      "grad_norm": 0.0005557987606152892,
      "learning_rate": 5.561424639211327e-05,
      "loss": 0.1875,
      "step": 263100
    },
    {
      "epoch": 0.8149288020980082,
      "grad_norm": 0.00010599739471217617,
      "learning_rate": 5.5521359370597536e-05,
      "loss": 0.2654,
      "step": 263200
    },
    {
      "epoch": 0.8152384255030606,
      "grad_norm": 0.0017438102513551712,
      "learning_rate": 5.54284723490818e-05,
      "loss": 0.1843,
      "step": 263300
    },
    {
      "epoch": 0.815548048908113,
      "grad_norm": 0.0031760470010340214,
      "learning_rate": 5.533558532756608e-05,
      "loss": 0.3076,
      "step": 263400
    },
    {
      "epoch": 0.8158576723131655,
      "grad_norm": 0.0018084704643115401,
      "learning_rate": 5.5242698306050344e-05,
      "loss": 0.1599,
      "step": 263500
    },
    {
      "epoch": 0.8161672957182179,
      "grad_norm": 0.0033894050866365433,
      "learning_rate": 5.514981128453461e-05,
      "loss": 0.121,
      "step": 263600
    },
    {
      "epoch": 0.8164769191232704,
      "grad_norm": 0.005273515824228525,
      "learning_rate": 5.5056924263018886e-05,
      "loss": 0.14,
      "step": 263700
    },
    {
      "epoch": 0.8167865425283228,
      "grad_norm": 0.0013734749518334866,
      "learning_rate": 5.496403724150315e-05,
      "loss": 0.266,
      "step": 263800
    },
    {
      "epoch": 0.8170961659333752,
      "grad_norm": 2.2809014320373535,
      "learning_rate": 5.487115021998743e-05,
      "loss": 0.2705,
      "step": 263900
    },
    {
      "epoch": 0.8174057893384277,
      "grad_norm": 0.0004255323437973857,
      "learning_rate": 5.4778263198471694e-05,
      "loss": 0.2493,
      "step": 264000
    },
    {
      "epoch": 0.8177154127434801,
      "grad_norm": 32.382408142089844,
      "learning_rate": 5.468537617695596e-05,
      "loss": 0.1366,
      "step": 264100
    },
    {
      "epoch": 0.8180250361485325,
      "grad_norm": 0.0005008145817555487,
      "learning_rate": 5.4592489155440235e-05,
      "loss": 0.2859,
      "step": 264200
    },
    {
      "epoch": 0.818334659553585,
      "grad_norm": 0.0022806318011134863,
      "learning_rate": 5.44996021339245e-05,
      "loss": 0.2429,
      "step": 264300
    },
    {
      "epoch": 0.8186442829586374,
      "grad_norm": 0.0006473861285485327,
      "learning_rate": 5.440671511240877e-05,
      "loss": 0.2267,
      "step": 264400
    },
    {
      "epoch": 0.8189539063636898,
      "grad_norm": 0.0010268581099808216,
      "learning_rate": 5.4313828090893044e-05,
      "loss": 0.0946,
      "step": 264500
    },
    {
      "epoch": 0.8192635297687423,
      "grad_norm": 0.0010209734318777919,
      "learning_rate": 5.422094106937731e-05,
      "loss": 0.1914,
      "step": 264600
    },
    {
      "epoch": 0.8195731531737948,
      "grad_norm": 0.0071730902418494225,
      "learning_rate": 5.412805404786158e-05,
      "loss": 0.244,
      "step": 264700
    },
    {
      "epoch": 0.8198827765788471,
      "grad_norm": 0.0007739563588984311,
      "learning_rate": 5.403516702634585e-05,
      "loss": 0.2853,
      "step": 264800
    },
    {
      "epoch": 0.8201923999838996,
      "grad_norm": 0.00500950962305069,
      "learning_rate": 5.394228000483012e-05,
      "loss": 0.2266,
      "step": 264900
    },
    {
      "epoch": 0.8205020233889521,
      "grad_norm": 0.10713773965835571,
      "learning_rate": 5.3849392983314393e-05,
      "loss": 0.2709,
      "step": 265000
    },
    {
      "epoch": 0.8208116467940044,
      "grad_norm": 0.05485493317246437,
      "learning_rate": 5.375650596179866e-05,
      "loss": 0.2413,
      "step": 265100
    },
    {
      "epoch": 0.8211212701990569,
      "grad_norm": 0.003348244819790125,
      "learning_rate": 5.366361894028293e-05,
      "loss": 0.0917,
      "step": 265200
    },
    {
      "epoch": 0.8214308936041094,
      "grad_norm": 0.001074381754733622,
      "learning_rate": 5.35707319187672e-05,
      "loss": 0.2175,
      "step": 265300
    },
    {
      "epoch": 0.8217405170091617,
      "grad_norm": 0.00125809945166111,
      "learning_rate": 5.347784489725147e-05,
      "loss": 0.1108,
      "step": 265400
    },
    {
      "epoch": 0.8220501404142142,
      "grad_norm": 0.08283494412899017,
      "learning_rate": 5.3384957875735736e-05,
      "loss": 0.1652,
      "step": 265500
    },
    {
      "epoch": 0.8223597638192667,
      "grad_norm": 0.0002591031661722809,
      "learning_rate": 5.329207085422001e-05,
      "loss": 0.1991,
      "step": 265600
    },
    {
      "epoch": 0.822669387224319,
      "grad_norm": 0.9296978712081909,
      "learning_rate": 5.319918383270428e-05,
      "loss": 0.0971,
      "step": 265700
    },
    {
      "epoch": 0.8229790106293715,
      "grad_norm": 0.0024900652933865786,
      "learning_rate": 5.310629681118855e-05,
      "loss": 0.2953,
      "step": 265800
    },
    {
      "epoch": 0.823288634034424,
      "grad_norm": 0.00026902472018264234,
      "learning_rate": 5.301340978967282e-05,
      "loss": 0.1038,
      "step": 265900
    },
    {
      "epoch": 0.8235982574394763,
      "grad_norm": 0.001221574959345162,
      "learning_rate": 5.2920522768157086e-05,
      "loss": 0.2162,
      "step": 266000
    },
    {
      "epoch": 0.8239078808445288,
      "grad_norm": 0.0008469054009765387,
      "learning_rate": 5.282763574664136e-05,
      "loss": 0.175,
      "step": 266100
    },
    {
      "epoch": 0.8242175042495813,
      "grad_norm": 0.00020010289154015481,
      "learning_rate": 5.273474872512563e-05,
      "loss": 0.2175,
      "step": 266200
    },
    {
      "epoch": 0.8245271276546337,
      "grad_norm": 0.002315928228199482,
      "learning_rate": 5.2641861703609895e-05,
      "loss": 0.1612,
      "step": 266300
    },
    {
      "epoch": 0.8248367510596861,
      "grad_norm": 0.00024588947417214513,
      "learning_rate": 5.254897468209417e-05,
      "loss": 0.2621,
      "step": 266400
    },
    {
      "epoch": 0.8251463744647385,
      "grad_norm": 82.10871124267578,
      "learning_rate": 5.2456087660578436e-05,
      "loss": 0.1833,
      "step": 266500
    },
    {
      "epoch": 0.825455997869791,
      "grad_norm": 0.00027277719345875084,
      "learning_rate": 5.2363200639062696e-05,
      "loss": 0.2207,
      "step": 266600
    },
    {
      "epoch": 0.8257656212748434,
      "grad_norm": 0.21237725019454956,
      "learning_rate": 5.227031361754698e-05,
      "loss": 0.2275,
      "step": 266700
    },
    {
      "epoch": 0.8260752446798958,
      "grad_norm": 0.00015135497960727662,
      "learning_rate": 5.217742659603124e-05,
      "loss": 0.1911,
      "step": 266800
    },
    {
      "epoch": 0.8263848680849483,
      "grad_norm": 0.0005244848434813321,
      "learning_rate": 5.208453957451552e-05,
      "loss": 0.1215,
      "step": 266900
    },
    {
      "epoch": 0.8266944914900007,
      "grad_norm": 0.016848810017108917,
      "learning_rate": 5.1991652552999786e-05,
      "loss": 0.2948,
      "step": 267000
    },
    {
      "epoch": 0.8270041148950531,
      "grad_norm": 0.0019180572126060724,
      "learning_rate": 5.1898765531484046e-05,
      "loss": 0.1155,
      "step": 267100
    },
    {
      "epoch": 0.8273137383001056,
      "grad_norm": 0.17803603410720825,
      "learning_rate": 5.180587850996833e-05,
      "loss": 0.2942,
      "step": 267200
    },
    {
      "epoch": 0.827623361705158,
      "grad_norm": 0.0012382841669023037,
      "learning_rate": 5.171299148845259e-05,
      "loss": 0.208,
      "step": 267300
    },
    {
      "epoch": 0.8279329851102104,
      "grad_norm": 6.155236721038818,
      "learning_rate": 5.1620104466936855e-05,
      "loss": 0.1368,
      "step": 267400
    },
    {
      "epoch": 0.8282426085152629,
      "grad_norm": 62.611568450927734,
      "learning_rate": 5.152721744542113e-05,
      "loss": 0.2505,
      "step": 267500
    },
    {
      "epoch": 0.8285522319203154,
      "grad_norm": 0.0006351476185955107,
      "learning_rate": 5.1434330423905396e-05,
      "loss": 0.1114,
      "step": 267600
    },
    {
      "epoch": 0.8288618553253677,
      "grad_norm": 0.0013550110161304474,
      "learning_rate": 5.134144340238966e-05,
      "loss": 0.191,
      "step": 267700
    },
    {
      "epoch": 0.8291714787304202,
      "grad_norm": 0.0008848341531120241,
      "learning_rate": 5.124855638087394e-05,
      "loss": 0.4207,
      "step": 267800
    },
    {
      "epoch": 0.8294811021354727,
      "grad_norm": 20.754150390625,
      "learning_rate": 5.1155669359358204e-05,
      "loss": 0.2119,
      "step": 267900
    },
    {
      "epoch": 0.829790725540525,
      "grad_norm": 0.0005353227024897933,
      "learning_rate": 5.106278233784248e-05,
      "loss": 0.1508,
      "step": 268000
    },
    {
      "epoch": 0.8301003489455775,
      "grad_norm": 0.00021971769456285983,
      "learning_rate": 5.0969895316326746e-05,
      "loss": 0.173,
      "step": 268100
    },
    {
      "epoch": 0.83040997235063,
      "grad_norm": 0.0034016091376543045,
      "learning_rate": 5.087700829481101e-05,
      "loss": 0.0892,
      "step": 268200
    },
    {
      "epoch": 0.8307195957556823,
      "grad_norm": 0.00044134538620710373,
      "learning_rate": 5.078412127329529e-05,
      "loss": 0.3293,
      "step": 268300
    },
    {
      "epoch": 0.8310292191607348,
      "grad_norm": 0.0005551912472583354,
      "learning_rate": 5.0691234251779554e-05,
      "loss": 0.3218,
      "step": 268400
    },
    {
      "epoch": 0.8313388425657873,
      "grad_norm": 0.00044540021917782724,
      "learning_rate": 5.059834723026382e-05,
      "loss": 0.0996,
      "step": 268500
    },
    {
      "epoch": 0.8316484659708396,
      "grad_norm": 0.0002327784604858607,
      "learning_rate": 5.0505460208748095e-05,
      "loss": 0.3528,
      "step": 268600
    },
    {
      "epoch": 0.8319580893758921,
      "grad_norm": 0.001007961924187839,
      "learning_rate": 5.041257318723236e-05,
      "loss": 0.1133,
      "step": 268700
    },
    {
      "epoch": 0.8322677127809446,
      "grad_norm": 0.0003514373383950442,
      "learning_rate": 5.0319686165716637e-05,
      "loss": 0.1928,
      "step": 268800
    },
    {
      "epoch": 0.8325773361859969,
      "grad_norm": 0.0005359716597013175,
      "learning_rate": 5.0226799144200904e-05,
      "loss": 0.1804,
      "step": 268900
    },
    {
      "epoch": 0.8328869595910494,
      "grad_norm": 11.395440101623535,
      "learning_rate": 5.013391212268517e-05,
      "loss": 0.1389,
      "step": 269000
    },
    {
      "epoch": 0.8331965829961019,
      "grad_norm": 0.0011797068873420358,
      "learning_rate": 5.0041025101169445e-05,
      "loss": 0.3011,
      "step": 269100
    },
    {
      "epoch": 0.8335062064011542,
      "grad_norm": 17.222990036010742,
      "learning_rate": 4.994813807965371e-05,
      "loss": 0.389,
      "step": 269200
    },
    {
      "epoch": 0.8338158298062067,
      "grad_norm": 0.0009399125119671226,
      "learning_rate": 4.985525105813798e-05,
      "loss": 0.1133,
      "step": 269300
    },
    {
      "epoch": 0.8341254532112592,
      "grad_norm": 0.8602943420410156,
      "learning_rate": 4.9762364036622253e-05,
      "loss": 0.2259,
      "step": 269400
    },
    {
      "epoch": 0.8344350766163116,
      "grad_norm": 109.16181182861328,
      "learning_rate": 4.966947701510652e-05,
      "loss": 0.2306,
      "step": 269500
    },
    {
      "epoch": 0.834744700021364,
      "grad_norm": 0.0027932985685765743,
      "learning_rate": 4.957658999359079e-05,
      "loss": 0.2459,
      "step": 269600
    },
    {
      "epoch": 0.8350543234264165,
      "grad_norm": 0.0021726868581026793,
      "learning_rate": 4.948370297207506e-05,
      "loss": 0.1742,
      "step": 269700
    },
    {
      "epoch": 0.8353639468314689,
      "grad_norm": 0.000595844176132232,
      "learning_rate": 4.939081595055933e-05,
      "loss": 0.1925,
      "step": 269800
    },
    {
      "epoch": 0.8356735702365213,
      "grad_norm": 0.03488822653889656,
      "learning_rate": 4.92979289290436e-05,
      "loss": 0.2674,
      "step": 269900
    },
    {
      "epoch": 0.8359831936415738,
      "grad_norm": 0.003727141534909606,
      "learning_rate": 4.920504190752787e-05,
      "loss": 0.3295,
      "step": 270000
    },
    {
      "epoch": 0.8362928170466262,
      "grad_norm": 0.0010054451413452625,
      "learning_rate": 4.911215488601214e-05,
      "loss": 0.1609,
      "step": 270100
    },
    {
      "epoch": 0.8366024404516786,
      "grad_norm": 0.0010436531156301498,
      "learning_rate": 4.901926786449641e-05,
      "loss": 0.1539,
      "step": 270200
    },
    {
      "epoch": 0.836912063856731,
      "grad_norm": 24.7147216796875,
      "learning_rate": 4.892638084298068e-05,
      "loss": 0.2277,
      "step": 270300
    },
    {
      "epoch": 0.8372216872617835,
      "grad_norm": 0.004141602665185928,
      "learning_rate": 4.8833493821464946e-05,
      "loss": 0.237,
      "step": 270400
    },
    {
      "epoch": 0.837531310666836,
      "grad_norm": 6.453665264416486e-05,
      "learning_rate": 4.874060679994922e-05,
      "loss": 0.2995,
      "step": 270500
    },
    {
      "epoch": 0.8378409340718883,
      "grad_norm": 1.2546476125717163,
      "learning_rate": 4.864771977843349e-05,
      "loss": 0.0847,
      "step": 270600
    },
    {
      "epoch": 0.8381505574769408,
      "grad_norm": 0.0005915221991017461,
      "learning_rate": 4.8554832756917755e-05,
      "loss": 0.2061,
      "step": 270700
    },
    {
      "epoch": 0.8384601808819933,
      "grad_norm": 0.0008782569202594459,
      "learning_rate": 4.846194573540203e-05,
      "loss": 0.3099,
      "step": 270800
    },
    {
      "epoch": 0.8387698042870456,
      "grad_norm": 0.0010533035965636373,
      "learning_rate": 4.8369058713886296e-05,
      "loss": 0.0836,
      "step": 270900
    },
    {
      "epoch": 0.8390794276920981,
      "grad_norm": 0.0007346855709329247,
      "learning_rate": 4.827617169237057e-05,
      "loss": 0.174,
      "step": 271000
    },
    {
      "epoch": 0.8393890510971506,
      "grad_norm": 0.05837337672710419,
      "learning_rate": 4.818328467085484e-05,
      "loss": 0.2401,
      "step": 271100
    },
    {
      "epoch": 0.8396986745022029,
      "grad_norm": 0.00032375010778196156,
      "learning_rate": 4.8090397649339104e-05,
      "loss": 0.2451,
      "step": 271200
    },
    {
      "epoch": 0.8400082979072554,
      "grad_norm": 0.00048370924196206033,
      "learning_rate": 4.799751062782338e-05,
      "loss": 0.1676,
      "step": 271300
    },
    {
      "epoch": 0.8403179213123079,
      "grad_norm": 0.0005740788183175027,
      "learning_rate": 4.7904623606307646e-05,
      "loss": 0.2343,
      "step": 271400
    },
    {
      "epoch": 0.8406275447173602,
      "grad_norm": 0.00023736836737953126,
      "learning_rate": 4.781173658479191e-05,
      "loss": 0.1883,
      "step": 271500
    },
    {
      "epoch": 0.8409371681224127,
      "grad_norm": 0.0004888353287242353,
      "learning_rate": 4.771884956327619e-05,
      "loss": 0.1668,
      "step": 271600
    },
    {
      "epoch": 0.8412467915274652,
      "grad_norm": 0.0007958274218253791,
      "learning_rate": 4.7625962541760454e-05,
      "loss": 0.1394,
      "step": 271700
    },
    {
      "epoch": 0.8415564149325175,
      "grad_norm": 7.342876434326172,
      "learning_rate": 4.753307552024473e-05,
      "loss": 0.2919,
      "step": 271800
    },
    {
      "epoch": 0.84186603833757,
      "grad_norm": 0.0008580363937653601,
      "learning_rate": 4.7440188498728995e-05,
      "loss": 0.3547,
      "step": 271900
    },
    {
      "epoch": 0.8421756617426225,
      "grad_norm": 12.91145133972168,
      "learning_rate": 4.7347301477213256e-05,
      "loss": 0.3013,
      "step": 272000
    },
    {
      "epoch": 0.8424852851476748,
      "grad_norm": 11.519936561584473,
      "learning_rate": 4.7254414455697537e-05,
      "loss": 0.2076,
      "step": 272100
    },
    {
      "epoch": 0.8427949085527273,
      "grad_norm": 0.002491624793037772,
      "learning_rate": 4.7161527434181804e-05,
      "loss": 0.1995,
      "step": 272200
    },
    {
      "epoch": 0.8431045319577798,
      "grad_norm": 0.0002966395113617182,
      "learning_rate": 4.7068640412666064e-05,
      "loss": 0.3191,
      "step": 272300
    },
    {
      "epoch": 0.8434141553628322,
      "grad_norm": 0.0016639309469610453,
      "learning_rate": 4.6975753391150345e-05,
      "loss": 0.2106,
      "step": 272400
    },
    {
      "epoch": 0.8437237787678846,
      "grad_norm": 51.92089080810547,
      "learning_rate": 4.6882866369634606e-05,
      "loss": 0.2367,
      "step": 272500
    },
    {
      "epoch": 0.8440334021729371,
      "grad_norm": 0.00040547942626290023,
      "learning_rate": 4.678997934811887e-05,
      "loss": 0.2423,
      "step": 272600
    },
    {
      "epoch": 0.8443430255779895,
      "grad_norm": 0.0011611066292971373,
      "learning_rate": 4.669709232660315e-05,
      "loss": 0.1427,
      "step": 272700
    },
    {
      "epoch": 0.8446526489830419,
      "grad_norm": 7.519585778936744e-05,
      "learning_rate": 4.6604205305087414e-05,
      "loss": 0.1797,
      "step": 272800
    },
    {
      "epoch": 0.8449622723880944,
      "grad_norm": 0.0012803489807993174,
      "learning_rate": 4.651131828357169e-05,
      "loss": 0.183,
      "step": 272900
    },
    {
      "epoch": 0.8452718957931468,
      "grad_norm": 14.639209747314453,
      "learning_rate": 4.6418431262055955e-05,
      "loss": 0.1848,
      "step": 273000
    },
    {
      "epoch": 0.8455815191981992,
      "grad_norm": 0.00014985151938162744,
      "learning_rate": 4.632554424054022e-05,
      "loss": 0.2721,
      "step": 273100
    },
    {
      "epoch": 0.8458911426032517,
      "grad_norm": 0.00331838708370924,
      "learning_rate": 4.6232657219024496e-05,
      "loss": 0.424,
      "step": 273200
    },
    {
      "epoch": 0.8462007660083041,
      "grad_norm": 0.000640557031147182,
      "learning_rate": 4.6139770197508764e-05,
      "loss": 0.1351,
      "step": 273300
    },
    {
      "epoch": 0.8465103894133565,
      "grad_norm": 0.0011925922008231282,
      "learning_rate": 4.604688317599303e-05,
      "loss": 0.3052,
      "step": 273400
    },
    {
      "epoch": 0.846820012818409,
      "grad_norm": 0.000679841497913003,
      "learning_rate": 4.5953996154477305e-05,
      "loss": 0.1116,
      "step": 273500
    },
    {
      "epoch": 0.8471296362234614,
      "grad_norm": 0.0002973303780891001,
      "learning_rate": 4.586110913296157e-05,
      "loss": 0.1177,
      "step": 273600
    },
    {
      "epoch": 0.8474392596285139,
      "grad_norm": 0.008642966859042645,
      "learning_rate": 4.576822211144584e-05,
      "loss": 0.2275,
      "step": 273700
    },
    {
      "epoch": 0.8477488830335663,
      "grad_norm": 0.0004342223983258009,
      "learning_rate": 4.5675335089930113e-05,
      "loss": 0.1891,
      "step": 273800
    },
    {
      "epoch": 0.8480585064386187,
      "grad_norm": 8.261848415713757e-05,
      "learning_rate": 4.558244806841438e-05,
      "loss": 0.2028,
      "step": 273900
    },
    {
      "epoch": 0.8483681298436712,
      "grad_norm": 0.0017524344148114324,
      "learning_rate": 4.5489561046898655e-05,
      "loss": 0.1583,
      "step": 274000
    },
    {
      "epoch": 0.8486777532487236,
      "grad_norm": 15.346671104431152,
      "learning_rate": 4.539667402538292e-05,
      "loss": 0.1441,
      "step": 274100
    },
    {
      "epoch": 0.848987376653776,
      "grad_norm": 0.0014562624273821712,
      "learning_rate": 4.530378700386719e-05,
      "loss": 0.2165,
      "step": 274200
    },
    {
      "epoch": 0.8492970000588285,
      "grad_norm": 0.0001517789496574551,
      "learning_rate": 4.521089998235146e-05,
      "loss": 0.1674,
      "step": 274300
    },
    {
      "epoch": 0.8496066234638808,
      "grad_norm": 0.001981397159397602,
      "learning_rate": 4.511801296083573e-05,
      "loss": 0.2078,
      "step": 274400
    },
    {
      "epoch": 0.8499162468689333,
      "grad_norm": 0.00026636067195795476,
      "learning_rate": 4.502512593932e-05,
      "loss": 0.1495,
      "step": 274500
    },
    {
      "epoch": 0.8502258702739858,
      "grad_norm": 0.00014934047067072242,
      "learning_rate": 4.493223891780427e-05,
      "loss": 0.319,
      "step": 274600
    },
    {
      "epoch": 0.8505354936790381,
      "grad_norm": 42.265804290771484,
      "learning_rate": 4.483935189628854e-05,
      "loss": 0.1225,
      "step": 274700
    },
    {
      "epoch": 0.8508451170840906,
      "grad_norm": 0.003091614693403244,
      "learning_rate": 4.474646487477281e-05,
      "loss": 0.1553,
      "step": 274800
    },
    {
      "epoch": 0.8511547404891431,
      "grad_norm": 0.002357110846787691,
      "learning_rate": 4.465357785325708e-05,
      "loss": 0.1895,
      "step": 274900
    },
    {
      "epoch": 0.8514643638941954,
      "grad_norm": 0.003155832877382636,
      "learning_rate": 4.456069083174135e-05,
      "loss": 0.282,
      "step": 275000
    },
    {
      "epoch": 0.8517739872992479,
      "grad_norm": 11.4283447265625,
      "learning_rate": 4.446780381022562e-05,
      "loss": 0.3318,
      "step": 275100
    },
    {
      "epoch": 0.8520836107043004,
      "grad_norm": 0.0005337229231372476,
      "learning_rate": 4.437491678870989e-05,
      "loss": 0.1016,
      "step": 275200
    },
    {
      "epoch": 0.8523932341093527,
      "grad_norm": 0.0007660093833692372,
      "learning_rate": 4.4282029767194156e-05,
      "loss": 0.2245,
      "step": 275300
    },
    {
      "epoch": 0.8527028575144052,
      "grad_norm": 1.0749316215515137,
      "learning_rate": 4.418914274567843e-05,
      "loss": 0.2716,
      "step": 275400
    },
    {
      "epoch": 0.8530124809194577,
      "grad_norm": 0.005960364826023579,
      "learning_rate": 4.40962557241627e-05,
      "loss": 0.2475,
      "step": 275500
    },
    {
      "epoch": 0.8533221043245101,
      "grad_norm": 30.485416412353516,
      "learning_rate": 4.4003368702646964e-05,
      "loss": 0.1066,
      "step": 275600
    },
    {
      "epoch": 0.8536317277295625,
      "grad_norm": 0.0004030811251141131,
      "learning_rate": 4.391048168113124e-05,
      "loss": 0.3889,
      "step": 275700
    },
    {
      "epoch": 0.853941351134615,
      "grad_norm": 0.009349503554403782,
      "learning_rate": 4.3817594659615506e-05,
      "loss": 0.1901,
      "step": 275800
    },
    {
      "epoch": 0.8542509745396674,
      "grad_norm": 23.455812454223633,
      "learning_rate": 4.372470763809978e-05,
      "loss": 0.274,
      "step": 275900
    },
    {
      "epoch": 0.8545605979447198,
      "grad_norm": 0.0021992602851241827,
      "learning_rate": 4.363182061658405e-05,
      "loss": 0.2215,
      "step": 276000
    },
    {
      "epoch": 0.8548702213497723,
      "grad_norm": 0.0019443478668108582,
      "learning_rate": 4.3538933595068314e-05,
      "loss": 0.2528,
      "step": 276100
    },
    {
      "epoch": 0.8551798447548247,
      "grad_norm": 10.907697677612305,
      "learning_rate": 4.344604657355259e-05,
      "loss": 0.2201,
      "step": 276200
    },
    {
      "epoch": 0.8554894681598771,
      "grad_norm": 0.004214680753648281,
      "learning_rate": 4.3353159552036855e-05,
      "loss": 0.1081,
      "step": 276300
    },
    {
      "epoch": 0.8557990915649296,
      "grad_norm": 0.0010139300720766187,
      "learning_rate": 4.326027253052112e-05,
      "loss": 0.1922,
      "step": 276400
    },
    {
      "epoch": 0.856108714969982,
      "grad_norm": 0.009193245321512222,
      "learning_rate": 4.3167385509005397e-05,
      "loss": 0.2627,
      "step": 276500
    },
    {
      "epoch": 0.8564183383750344,
      "grad_norm": 0.0018895595567300916,
      "learning_rate": 4.3074498487489664e-05,
      "loss": 0.2326,
      "step": 276600
    },
    {
      "epoch": 0.8567279617800869,
      "grad_norm": 0.008015268482267857,
      "learning_rate": 4.298161146597393e-05,
      "loss": 0.1197,
      "step": 276700
    },
    {
      "epoch": 0.8570375851851393,
      "grad_norm": 0.002978043630719185,
      "learning_rate": 4.2888724444458205e-05,
      "loss": 0.148,
      "step": 276800
    },
    {
      "epoch": 0.8573472085901918,
      "grad_norm": 0.00029233063105493784,
      "learning_rate": 4.279583742294247e-05,
      "loss": 0.1422,
      "step": 276900
    },
    {
      "epoch": 0.8576568319952442,
      "grad_norm": 0.00043677969370037317,
      "learning_rate": 4.2702950401426746e-05,
      "loss": 0.1837,
      "step": 277000
    },
    {
      "epoch": 0.8579664554002966,
      "grad_norm": 0.00023467121354769915,
      "learning_rate": 4.2610063379911014e-05,
      "loss": 0.2205,
      "step": 277100
    },
    {
      "epoch": 0.8582760788053491,
      "grad_norm": 0.0005519716651178896,
      "learning_rate": 4.251717635839528e-05,
      "loss": 0.136,
      "step": 277200
    },
    {
      "epoch": 0.8585857022104015,
      "grad_norm": 0.001196806551888585,
      "learning_rate": 4.2424289336879555e-05,
      "loss": 0.1482,
      "step": 277300
    },
    {
      "epoch": 0.8588953256154539,
      "grad_norm": 68.1003646850586,
      "learning_rate": 4.233140231536382e-05,
      "loss": 0.273,
      "step": 277400
    },
    {
      "epoch": 0.8592049490205064,
      "grad_norm": 0.0007218052051030099,
      "learning_rate": 4.223851529384808e-05,
      "loss": 0.1474,
      "step": 277500
    },
    {
      "epoch": 0.8595145724255588,
      "grad_norm": 0.0005435775383375585,
      "learning_rate": 4.214562827233236e-05,
      "loss": 0.1304,
      "step": 277600
    },
    {
      "epoch": 0.8598241958306112,
      "grad_norm": 0.00011867617286043242,
      "learning_rate": 4.2052741250816624e-05,
      "loss": 0.0235,
      "step": 277700
    },
    {
      "epoch": 0.8601338192356637,
      "grad_norm": 0.0002337932091904804,
      "learning_rate": 4.1959854229300904e-05,
      "loss": 0.1188,
      "step": 277800
    },
    {
      "epoch": 0.8604434426407161,
      "grad_norm": 0.00039979733992367983,
      "learning_rate": 4.1866967207785165e-05,
      "loss": 0.1202,
      "step": 277900
    },
    {
      "epoch": 0.8607530660457685,
      "grad_norm": 0.00034005517954938114,
      "learning_rate": 4.177408018626943e-05,
      "loss": 0.1566,
      "step": 278000
    },
    {
      "epoch": 0.861062689450821,
      "grad_norm": 0.0001309906510869041,
      "learning_rate": 4.1681193164753706e-05,
      "loss": 0.2226,
      "step": 278100
    },
    {
      "epoch": 0.8613723128558735,
      "grad_norm": 0.002330164425075054,
      "learning_rate": 4.1588306143237973e-05,
      "loss": 0.1231,
      "step": 278200
    },
    {
      "epoch": 0.8616819362609258,
      "grad_norm": 0.00013371146633289754,
      "learning_rate": 4.149541912172224e-05,
      "loss": 0.2072,
      "step": 278300
    },
    {
      "epoch": 0.8619915596659783,
      "grad_norm": 0.002176397480070591,
      "learning_rate": 4.1402532100206515e-05,
      "loss": 0.1919,
      "step": 278400
    },
    {
      "epoch": 0.8623011830710307,
      "grad_norm": 0.0007294861134141684,
      "learning_rate": 4.130964507869078e-05,
      "loss": 0.1334,
      "step": 278500
    },
    {
      "epoch": 0.8626108064760831,
      "grad_norm": 0.0005038641393184662,
      "learning_rate": 4.121675805717505e-05,
      "loss": 0.2526,
      "step": 278600
    },
    {
      "epoch": 0.8629204298811356,
      "grad_norm": 0.0024156137369573116,
      "learning_rate": 4.112387103565932e-05,
      "loss": 0.1768,
      "step": 278700
    },
    {
      "epoch": 0.863230053286188,
      "grad_norm": 6.799654511269182e-05,
      "learning_rate": 4.103098401414359e-05,
      "loss": 0.2093,
      "step": 278800
    },
    {
      "epoch": 0.8635396766912404,
      "grad_norm": 0.002715308917686343,
      "learning_rate": 4.0938096992627864e-05,
      "loss": 0.2347,
      "step": 278900
    },
    {
      "epoch": 0.8638493000962929,
      "grad_norm": 0.0003648086276371032,
      "learning_rate": 4.084520997111213e-05,
      "loss": 0.1117,
      "step": 279000
    },
    {
      "epoch": 0.8641589235013453,
      "grad_norm": 0.004801720380783081,
      "learning_rate": 4.07523229495964e-05,
      "loss": 0.1402,
      "step": 279100
    },
    {
      "epoch": 0.8644685469063977,
      "grad_norm": 0.013347245752811432,
      "learning_rate": 4.065943592808067e-05,
      "loss": 0.1659,
      "step": 279200
    },
    {
      "epoch": 0.8647781703114502,
      "grad_norm": 19.313270568847656,
      "learning_rate": 4.056654890656494e-05,
      "loss": 0.1436,
      "step": 279300
    },
    {
      "epoch": 0.8650877937165026,
      "grad_norm": 0.5750904679298401,
      "learning_rate": 4.047366188504921e-05,
      "loss": 0.246,
      "step": 279400
    },
    {
      "epoch": 0.865397417121555,
      "grad_norm": 0.0017034607008099556,
      "learning_rate": 4.038077486353348e-05,
      "loss": 0.1672,
      "step": 279500
    },
    {
      "epoch": 0.8657070405266075,
      "grad_norm": 0.00022267999884206802,
      "learning_rate": 4.028788784201775e-05,
      "loss": 0.0837,
      "step": 279600
    },
    {
      "epoch": 0.8660166639316599,
      "grad_norm": 0.0005532073555514216,
      "learning_rate": 4.0195000820502016e-05,
      "loss": 0.1576,
      "step": 279700
    },
    {
      "epoch": 0.8663262873367124,
      "grad_norm": 0.00029261779855005443,
      "learning_rate": 4.010211379898629e-05,
      "loss": 0.2873,
      "step": 279800
    },
    {
      "epoch": 0.8666359107417648,
      "grad_norm": 0.00027905049500986934,
      "learning_rate": 4.000922677747056e-05,
      "loss": 0.2144,
      "step": 279900
    },
    {
      "epoch": 0.8669455341468172,
      "grad_norm": 98.7761001586914,
      "learning_rate": 3.991633975595483e-05,
      "loss": 0.1413,
      "step": 280000
    },
    {
      "epoch": 0.8672551575518697,
      "grad_norm": 0.00046416837722063065,
      "learning_rate": 3.98234527344391e-05,
      "loss": 0.1291,
      "step": 280100
    },
    {
      "epoch": 0.8675647809569221,
      "grad_norm": 0.00040615134639665484,
      "learning_rate": 3.9730565712923366e-05,
      "loss": 0.1531,
      "step": 280200
    },
    {
      "epoch": 0.8678744043619745,
      "grad_norm": 0.001667607924900949,
      "learning_rate": 3.963767869140764e-05,
      "loss": 0.1298,
      "step": 280300
    },
    {
      "epoch": 0.868184027767027,
      "grad_norm": 0.000938511686399579,
      "learning_rate": 3.954479166989191e-05,
      "loss": 0.2404,
      "step": 280400
    },
    {
      "epoch": 0.8684936511720794,
      "grad_norm": 0.0014234912814572453,
      "learning_rate": 3.9451904648376174e-05,
      "loss": 0.1404,
      "step": 280500
    },
    {
      "epoch": 0.8688032745771318,
      "grad_norm": 0.0006497345748357475,
      "learning_rate": 3.935901762686045e-05,
      "loss": 0.2046,
      "step": 280600
    },
    {
      "epoch": 0.8691128979821843,
      "grad_norm": 0.0015647017862647772,
      "learning_rate": 3.9266130605344715e-05,
      "loss": 0.2199,
      "step": 280700
    },
    {
      "epoch": 0.8694225213872367,
      "grad_norm": 0.15242809057235718,
      "learning_rate": 3.917324358382899e-05,
      "loss": 0.1409,
      "step": 280800
    },
    {
      "epoch": 0.8697321447922891,
      "grad_norm": 1.1250380277633667,
      "learning_rate": 3.9080356562313257e-05,
      "loss": 0.2587,
      "step": 280900
    },
    {
      "epoch": 0.8700417681973416,
      "grad_norm": 0.1856217086315155,
      "learning_rate": 3.8987469540797524e-05,
      "loss": 0.3414,
      "step": 281000
    },
    {
      "epoch": 0.870351391602394,
      "grad_norm": 57.338008880615234,
      "learning_rate": 3.88945825192818e-05,
      "loss": 0.2267,
      "step": 281100
    },
    {
      "epoch": 0.8706610150074464,
      "grad_norm": 0.0010908428812399507,
      "learning_rate": 3.8801695497766065e-05,
      "loss": 0.2751,
      "step": 281200
    },
    {
      "epoch": 0.8709706384124989,
      "grad_norm": 0.2086946815252304,
      "learning_rate": 3.870880847625033e-05,
      "loss": 0.1962,
      "step": 281300
    },
    {
      "epoch": 0.8712802618175514,
      "grad_norm": 0.0003386114549357444,
      "learning_rate": 3.8615921454734606e-05,
      "loss": 0.1587,
      "step": 281400
    },
    {
      "epoch": 0.8715898852226037,
      "grad_norm": 9.400181770324707,
      "learning_rate": 3.8523034433218873e-05,
      "loss": 0.198,
      "step": 281500
    },
    {
      "epoch": 0.8718995086276562,
      "grad_norm": 39.814613342285156,
      "learning_rate": 3.843014741170314e-05,
      "loss": 0.164,
      "step": 281600
    },
    {
      "epoch": 0.8722091320327087,
      "grad_norm": 0.0001837792806327343,
      "learning_rate": 3.8337260390187415e-05,
      "loss": 0.2607,
      "step": 281700
    },
    {
      "epoch": 0.872518755437761,
      "grad_norm": 0.0009303834522143006,
      "learning_rate": 3.824437336867168e-05,
      "loss": 0.2483,
      "step": 281800
    },
    {
      "epoch": 0.8728283788428135,
      "grad_norm": 0.0006587438401766121,
      "learning_rate": 3.8151486347155956e-05,
      "loss": 0.2769,
      "step": 281900
    },
    {
      "epoch": 0.873138002247866,
      "grad_norm": 13.10294246673584,
      "learning_rate": 3.805859932564022e-05,
      "loss": 0.1358,
      "step": 282000
    },
    {
      "epoch": 0.8734476256529183,
      "grad_norm": 0.0007242721039801836,
      "learning_rate": 3.796571230412449e-05,
      "loss": 0.1892,
      "step": 282100
    },
    {
      "epoch": 0.8737572490579708,
      "grad_norm": 0.00019632531621027738,
      "learning_rate": 3.7872825282608764e-05,
      "loss": 0.2224,
      "step": 282200
    },
    {
      "epoch": 0.8740668724630232,
      "grad_norm": 0.0011537062237039208,
      "learning_rate": 3.777993826109303e-05,
      "loss": 0.2093,
      "step": 282300
    },
    {
      "epoch": 0.8743764958680756,
      "grad_norm": 0.22205127775669098,
      "learning_rate": 3.76870512395773e-05,
      "loss": 0.2759,
      "step": 282400
    },
    {
      "epoch": 0.8746861192731281,
      "grad_norm": 0.0002845881099347025,
      "learning_rate": 3.759416421806157e-05,
      "loss": 0.2358,
      "step": 282500
    },
    {
      "epoch": 0.8749957426781805,
      "grad_norm": 0.013469464145600796,
      "learning_rate": 3.750127719654584e-05,
      "loss": 0.1474,
      "step": 282600
    },
    {
      "epoch": 0.875305366083233,
      "grad_norm": 0.0007757442654110491,
      "learning_rate": 3.740839017503011e-05,
      "loss": 0.1359,
      "step": 282700
    },
    {
      "epoch": 0.8756149894882854,
      "grad_norm": 2.0548433894873597e-05,
      "learning_rate": 3.731550315351438e-05,
      "loss": 0.2382,
      "step": 282800
    },
    {
      "epoch": 0.8759246128933378,
      "grad_norm": 0.000880274164956063,
      "learning_rate": 3.722261613199864e-05,
      "loss": 0.1674,
      "step": 282900
    },
    {
      "epoch": 0.8762342362983903,
      "grad_norm": 9.826735913520679e-05,
      "learning_rate": 3.7129729110482916e-05,
      "loss": 0.2038,
      "step": 283000
    },
    {
      "epoch": 0.8765438597034427,
      "grad_norm": 0.0009844343876466155,
      "learning_rate": 3.703684208896718e-05,
      "loss": 0.242,
      "step": 283100
    },
    {
      "epoch": 0.8768534831084951,
      "grad_norm": 0.0017365230014547706,
      "learning_rate": 3.694395506745146e-05,
      "loss": 0.2745,
      "step": 283200
    },
    {
      "epoch": 0.8771631065135476,
      "grad_norm": 99.16459655761719,
      "learning_rate": 3.6851068045935724e-05,
      "loss": 0.2347,
      "step": 283300
    },
    {
      "epoch": 0.8774727299186,
      "grad_norm": 0.0003231263253837824,
      "learning_rate": 3.675818102441999e-05,
      "loss": 0.1013,
      "step": 283400
    },
    {
      "epoch": 0.8777823533236524,
      "grad_norm": 0.00024275764008052647,
      "learning_rate": 3.6665294002904266e-05,
      "loss": 0.1187,
      "step": 283500
    },
    {
      "epoch": 0.8780919767287049,
      "grad_norm": 0.0008428795845247805,
      "learning_rate": 3.657240698138853e-05,
      "loss": 0.154,
      "step": 283600
    },
    {
      "epoch": 0.8784016001337573,
      "grad_norm": 0.0013173738261684775,
      "learning_rate": 3.64795199598728e-05,
      "loss": 0.1584,
      "step": 283700
    },
    {
      "epoch": 0.8787112235388097,
      "grad_norm": 138.08847045898438,
      "learning_rate": 3.6386632938357074e-05,
      "loss": 0.1495,
      "step": 283800
    },
    {
      "epoch": 0.8790208469438622,
      "grad_norm": 0.0006090177921578288,
      "learning_rate": 3.629374591684134e-05,
      "loss": 0.1054,
      "step": 283900
    },
    {
      "epoch": 0.8793304703489147,
      "grad_norm": 9.794574725674465e-05,
      "learning_rate": 3.6200858895325615e-05,
      "loss": 0.2048,
      "step": 284000
    },
    {
      "epoch": 0.879640093753967,
      "grad_norm": 0.0020459883380681276,
      "learning_rate": 3.610797187380988e-05,
      "loss": 0.2542,
      "step": 284100
    },
    {
      "epoch": 0.8799497171590195,
      "grad_norm": 0.00023879625950939953,
      "learning_rate": 3.601508485229415e-05,
      "loss": 0.151,
      "step": 284200
    },
    {
      "epoch": 0.880259340564072,
      "grad_norm": 0.0009353744680993259,
      "learning_rate": 3.5922197830778424e-05,
      "loss": 0.3011,
      "step": 284300
    },
    {
      "epoch": 0.8805689639691243,
      "grad_norm": 0.10338325798511505,
      "learning_rate": 3.582931080926269e-05,
      "loss": 0.1627,
      "step": 284400
    },
    {
      "epoch": 0.8808785873741768,
      "grad_norm": 0.008698259480297565,
      "learning_rate": 3.573642378774696e-05,
      "loss": 0.1856,
      "step": 284500
    },
    {
      "epoch": 0.8811882107792293,
      "grad_norm": 0.014022867195308208,
      "learning_rate": 3.564353676623123e-05,
      "loss": 0.1505,
      "step": 284600
    },
    {
      "epoch": 0.8814978341842816,
      "grad_norm": 0.0007048259722068906,
      "learning_rate": 3.55506497447155e-05,
      "loss": 0.1861,
      "step": 284700
    },
    {
      "epoch": 0.8818074575893341,
      "grad_norm": 2.3764638900756836,
      "learning_rate": 3.545776272319977e-05,
      "loss": 0.145,
      "step": 284800
    },
    {
      "epoch": 0.8821170809943866,
      "grad_norm": 0.0024999629240483046,
      "learning_rate": 3.536487570168404e-05,
      "loss": 0.2135,
      "step": 284900
    },
    {
      "epoch": 0.8824267043994389,
      "grad_norm": 0.6387905478477478,
      "learning_rate": 3.527198868016831e-05,
      "loss": 0.1969,
      "step": 285000
    },
    {
      "epoch": 0.8827363278044914,
      "grad_norm": 0.00014225527411326766,
      "learning_rate": 3.517910165865258e-05,
      "loss": 0.2988,
      "step": 285100
    },
    {
      "epoch": 0.8830459512095439,
      "grad_norm": 0.004391167312860489,
      "learning_rate": 3.508621463713685e-05,
      "loss": 0.2425,
      "step": 285200
    },
    {
      "epoch": 0.8833555746145962,
      "grad_norm": 0.005196296609938145,
      "learning_rate": 3.4993327615621116e-05,
      "loss": 0.3385,
      "step": 285300
    },
    {
      "epoch": 0.8836651980196487,
      "grad_norm": 0.00011725756485247985,
      "learning_rate": 3.490044059410539e-05,
      "loss": 0.3616,
      "step": 285400
    },
    {
      "epoch": 0.8839748214247012,
      "grad_norm": 0.0013685703743249178,
      "learning_rate": 3.480755357258966e-05,
      "loss": 0.1469,
      "step": 285500
    },
    {
      "epoch": 0.8842844448297535,
      "grad_norm": 0.09901167452335358,
      "learning_rate": 3.4714666551073925e-05,
      "loss": 0.2737,
      "step": 285600
    },
    {
      "epoch": 0.884594068234806,
      "grad_norm": 8.227970829466358e-05,
      "learning_rate": 3.462177952955819e-05,
      "loss": 0.3153,
      "step": 285700
    },
    {
      "epoch": 0.8849036916398585,
      "grad_norm": 0.0005850867019034922,
      "learning_rate": 3.4528892508042466e-05,
      "loss": 0.1374,
      "step": 285800
    },
    {
      "epoch": 0.8852133150449109,
      "grad_norm": 0.003655873704701662,
      "learning_rate": 3.4436005486526733e-05,
      "loss": 0.0765,
      "step": 285900
    },
    {
      "epoch": 0.8855229384499633,
      "grad_norm": 0.006240175571292639,
      "learning_rate": 3.4343118465011e-05,
      "loss": 0.2346,
      "step": 286000
    },
    {
      "epoch": 0.8858325618550158,
      "grad_norm": 4.983347389497794e-05,
      "learning_rate": 3.4250231443495275e-05,
      "loss": 0.2808,
      "step": 286100
    },
    {
      "epoch": 0.8861421852600682,
      "grad_norm": 0.0015713528264313936,
      "learning_rate": 3.415734442197954e-05,
      "loss": 0.1367,
      "step": 286200
    },
    {
      "epoch": 0.8864518086651206,
      "grad_norm": 0.0016025181394070387,
      "learning_rate": 3.406445740046381e-05,
      "loss": 0.149,
      "step": 286300
    },
    {
      "epoch": 0.886761432070173,
      "grad_norm": 0.002874103607609868,
      "learning_rate": 3.397157037894808e-05,
      "loss": 0.2243,
      "step": 286400
    },
    {
      "epoch": 0.8870710554752255,
      "grad_norm": 5.2141218185424805,
      "learning_rate": 3.387868335743235e-05,
      "loss": 0.1718,
      "step": 286500
    },
    {
      "epoch": 0.8873806788802779,
      "grad_norm": 0.00010225325968349352,
      "learning_rate": 3.3785796335916624e-05,
      "loss": 0.2314,
      "step": 286600
    },
    {
      "epoch": 0.8876903022853303,
      "grad_norm": 0.3856232464313507,
      "learning_rate": 3.369290931440089e-05,
      "loss": 0.1054,
      "step": 286700
    },
    {
      "epoch": 0.8879999256903828,
      "grad_norm": 0.00021021379507146776,
      "learning_rate": 3.360002229288516e-05,
      "loss": 0.16,
      "step": 286800
    },
    {
      "epoch": 0.8883095490954352,
      "grad_norm": 6.388275505742058e-05,
      "learning_rate": 3.350713527136943e-05,
      "loss": 0.2054,
      "step": 286900
    },
    {
      "epoch": 0.8886191725004876,
      "grad_norm": 0.00045351992594078183,
      "learning_rate": 3.34142482498537e-05,
      "loss": 0.1811,
      "step": 287000
    },
    {
      "epoch": 0.8889287959055401,
      "grad_norm": 0.00025176070630550385,
      "learning_rate": 3.332136122833797e-05,
      "loss": 0.1756,
      "step": 287100
    },
    {
      "epoch": 0.8892384193105926,
      "grad_norm": 0.06407933682203293,
      "learning_rate": 3.322847420682224e-05,
      "loss": 0.1036,
      "step": 287200
    },
    {
      "epoch": 0.8895480427156449,
      "grad_norm": 0.0004456844471860677,
      "learning_rate": 3.313558718530651e-05,
      "loss": 0.1995,
      "step": 287300
    },
    {
      "epoch": 0.8898576661206974,
      "grad_norm": 0.0006600970518775284,
      "learning_rate": 3.3042700163790776e-05,
      "loss": 0.1339,
      "step": 287400
    },
    {
      "epoch": 0.8901672895257499,
      "grad_norm": 0.0020166458562016487,
      "learning_rate": 3.294981314227505e-05,
      "loss": 0.2718,
      "step": 287500
    },
    {
      "epoch": 0.8904769129308022,
      "grad_norm": 0.000324244232615456,
      "learning_rate": 3.285692612075932e-05,
      "loss": 0.1189,
      "step": 287600
    },
    {
      "epoch": 0.8907865363358547,
      "grad_norm": 0.0006050625815987587,
      "learning_rate": 3.276403909924359e-05,
      "loss": 0.3007,
      "step": 287700
    },
    {
      "epoch": 0.8910961597409072,
      "grad_norm": 0.0001524485560366884,
      "learning_rate": 3.267115207772786e-05,
      "loss": 0.26,
      "step": 287800
    },
    {
      "epoch": 0.8914057831459595,
      "grad_norm": 0.0005920192343182862,
      "learning_rate": 3.2578265056212126e-05,
      "loss": 0.2985,
      "step": 287900
    },
    {
      "epoch": 0.891715406551012,
      "grad_norm": 9.516654972685501e-05,
      "learning_rate": 3.24853780346964e-05,
      "loss": 0.4243,
      "step": 288000
    },
    {
      "epoch": 0.8920250299560645,
      "grad_norm": 0.0009453691309317946,
      "learning_rate": 3.239249101318067e-05,
      "loss": 0.2197,
      "step": 288100
    },
    {
      "epoch": 0.8923346533611168,
      "grad_norm": 0.00010912887228187174,
      "learning_rate": 3.2299603991664934e-05,
      "loss": 0.2763,
      "step": 288200
    },
    {
      "epoch": 0.8926442767661693,
      "grad_norm": 0.0027800281532108784,
      "learning_rate": 3.22067169701492e-05,
      "loss": 0.2285,
      "step": 288300
    },
    {
      "epoch": 0.8929539001712218,
      "grad_norm": 0.0035273809917271137,
      "learning_rate": 3.2113829948633475e-05,
      "loss": 0.2949,
      "step": 288400
    },
    {
      "epoch": 0.8932635235762741,
      "grad_norm": 0.0015567454975098372,
      "learning_rate": 3.202094292711774e-05,
      "loss": 0.1782,
      "step": 288500
    },
    {
      "epoch": 0.8935731469813266,
      "grad_norm": 0.0010793597903102636,
      "learning_rate": 3.192805590560201e-05,
      "loss": 0.1988,
      "step": 288600
    },
    {
      "epoch": 0.8938827703863791,
      "grad_norm": 0.001842208905145526,
      "learning_rate": 3.1835168884086284e-05,
      "loss": 0.2201,
      "step": 288700
    },
    {
      "epoch": 0.8941923937914315,
      "grad_norm": 0.0020790412090718746,
      "learning_rate": 3.174228186257055e-05,
      "loss": 0.2168,
      "step": 288800
    },
    {
      "epoch": 0.8945020171964839,
      "grad_norm": 0.0006604529335163534,
      "learning_rate": 3.164939484105482e-05,
      "loss": 0.1598,
      "step": 288900
    },
    {
      "epoch": 0.8948116406015364,
      "grad_norm": 0.0017203050665557384,
      "learning_rate": 3.155650781953909e-05,
      "loss": 0.1193,
      "step": 289000
    },
    {
      "epoch": 0.8951212640065888,
      "grad_norm": 0.005757199600338936,
      "learning_rate": 3.146362079802336e-05,
      "loss": 0.1288,
      "step": 289100
    },
    {
      "epoch": 0.8954308874116412,
      "grad_norm": 0.7192729115486145,
      "learning_rate": 3.1370733776507634e-05,
      "loss": 0.1972,
      "step": 289200
    },
    {
      "epoch": 0.8957405108166937,
      "grad_norm": 0.0010003476636484265,
      "learning_rate": 3.12778467549919e-05,
      "loss": 0.1142,
      "step": 289300
    },
    {
      "epoch": 0.8960501342217461,
      "grad_norm": 0.0004564372356981039,
      "learning_rate": 3.118495973347617e-05,
      "loss": 0.2304,
      "step": 289400
    },
    {
      "epoch": 0.8963597576267985,
      "grad_norm": 0.002741975476965308,
      "learning_rate": 3.109207271196044e-05,
      "loss": 0.2419,
      "step": 289500
    },
    {
      "epoch": 0.896669381031851,
      "grad_norm": 0.0003581139026209712,
      "learning_rate": 3.099918569044471e-05,
      "loss": 0.1378,
      "step": 289600
    },
    {
      "epoch": 0.8969790044369034,
      "grad_norm": 0.00013696658425033092,
      "learning_rate": 3.0906298668928976e-05,
      "loss": 0.0739,
      "step": 289700
    },
    {
      "epoch": 0.8972886278419558,
      "grad_norm": 9.622322082519531,
      "learning_rate": 3.081341164741325e-05,
      "loss": 0.166,
      "step": 289800
    },
    {
      "epoch": 0.8975982512470083,
      "grad_norm": 1.0108755826950073,
      "learning_rate": 3.072052462589752e-05,
      "loss": 0.2959,
      "step": 289900
    },
    {
      "epoch": 0.8979078746520607,
      "grad_norm": 0.0007945221150293946,
      "learning_rate": 3.062763760438179e-05,
      "loss": 0.2488,
      "step": 290000
    },
    {
      "epoch": 0.8982174980571132,
      "grad_norm": 0.0009659594506956637,
      "learning_rate": 3.053475058286606e-05,
      "loss": 0.1469,
      "step": 290100
    },
    {
      "epoch": 0.8985271214621656,
      "grad_norm": 0.00023415261239279062,
      "learning_rate": 3.0441863561350326e-05,
      "loss": 0.2658,
      "step": 290200
    },
    {
      "epoch": 0.898836744867218,
      "grad_norm": 0.0007695865351706743,
      "learning_rate": 3.0348976539834597e-05,
      "loss": 0.1455,
      "step": 290300
    },
    {
      "epoch": 0.8991463682722705,
      "grad_norm": 0.00076145154889673,
      "learning_rate": 3.0256089518318864e-05,
      "loss": 0.2549,
      "step": 290400
    },
    {
      "epoch": 0.8994559916773228,
      "grad_norm": 0.00037323468131944537,
      "learning_rate": 3.0163202496803135e-05,
      "loss": 0.1093,
      "step": 290500
    },
    {
      "epoch": 0.8997656150823753,
      "grad_norm": 0.0008141365833580494,
      "learning_rate": 3.0070315475287405e-05,
      "loss": 0.0425,
      "step": 290600
    },
    {
      "epoch": 0.9000752384874278,
      "grad_norm": 0.000336495169904083,
      "learning_rate": 2.9977428453771676e-05,
      "loss": 0.1882,
      "step": 290700
    },
    {
      "epoch": 0.9003848618924801,
      "grad_norm": 0.005044266581535339,
      "learning_rate": 2.9884541432255943e-05,
      "loss": 0.1543,
      "step": 290800
    },
    {
      "epoch": 0.9006944852975326,
      "grad_norm": 8.127149339998141e-05,
      "learning_rate": 2.9791654410740214e-05,
      "loss": 0.1185,
      "step": 290900
    },
    {
      "epoch": 0.9010041087025851,
      "grad_norm": 0.00021199850016273558,
      "learning_rate": 2.9698767389224484e-05,
      "loss": 0.2865,
      "step": 291000
    },
    {
      "epoch": 0.9013137321076374,
      "grad_norm": 0.008019013330340385,
      "learning_rate": 2.9605880367708755e-05,
      "loss": 0.1911,
      "step": 291100
    },
    {
      "epoch": 0.9016233555126899,
      "grad_norm": 0.01338797714561224,
      "learning_rate": 2.9512993346193022e-05,
      "loss": 0.1984,
      "step": 291200
    },
    {
      "epoch": 0.9019329789177424,
      "grad_norm": 51.50084686279297,
      "learning_rate": 2.9420106324677293e-05,
      "loss": 0.3477,
      "step": 291300
    },
    {
      "epoch": 0.9022426023227947,
      "grad_norm": 92.24483489990234,
      "learning_rate": 2.9327219303161563e-05,
      "loss": 0.1118,
      "step": 291400
    },
    {
      "epoch": 0.9025522257278472,
      "grad_norm": 0.006306059192866087,
      "learning_rate": 2.9234332281645834e-05,
      "loss": 0.1673,
      "step": 291500
    },
    {
      "epoch": 0.9028618491328997,
      "grad_norm": 79.35270690917969,
      "learning_rate": 2.91414452601301e-05,
      "loss": 0.275,
      "step": 291600
    },
    {
      "epoch": 0.903171472537952,
      "grad_norm": 0.0017131931381300092,
      "learning_rate": 2.9048558238614372e-05,
      "loss": 0.1106,
      "step": 291700
    },
    {
      "epoch": 0.9034810959430045,
      "grad_norm": 0.0028937789611518383,
      "learning_rate": 2.8955671217098643e-05,
      "loss": 0.1071,
      "step": 291800
    },
    {
      "epoch": 0.903790719348057,
      "grad_norm": 47.12952423095703,
      "learning_rate": 2.8862784195582906e-05,
      "loss": 0.03,
      "step": 291900
    },
    {
      "epoch": 0.9041003427531094,
      "grad_norm": 0.0002862787223421037,
      "learning_rate": 2.876989717406718e-05,
      "loss": 0.3363,
      "step": 292000
    },
    {
      "epoch": 0.9044099661581618,
      "grad_norm": 1.0338383913040161,
      "learning_rate": 2.867701015255145e-05,
      "loss": 0.3168,
      "step": 292100
    },
    {
      "epoch": 0.9047195895632143,
      "grad_norm": 0.001382152666337788,
      "learning_rate": 2.8584123131035722e-05,
      "loss": 0.1991,
      "step": 292200
    },
    {
      "epoch": 0.9050292129682667,
      "grad_norm": 0.0013316298136487603,
      "learning_rate": 2.8491236109519986e-05,
      "loss": 0.2836,
      "step": 292300
    },
    {
      "epoch": 0.9053388363733191,
      "grad_norm": 0.0005346130346879363,
      "learning_rate": 2.8398349088004256e-05,
      "loss": 0.2559,
      "step": 292400
    },
    {
      "epoch": 0.9056484597783716,
      "grad_norm": 0.002271356526762247,
      "learning_rate": 2.8305462066488527e-05,
      "loss": 0.119,
      "step": 292500
    },
    {
      "epoch": 0.905958083183424,
      "grad_norm": 11.81298828125,
      "learning_rate": 2.8212575044972797e-05,
      "loss": 0.1019,
      "step": 292600
    },
    {
      "epoch": 0.9062677065884764,
      "grad_norm": 0.0003997722815256566,
      "learning_rate": 2.8119688023457065e-05,
      "loss": 0.1109,
      "step": 292700
    },
    {
      "epoch": 0.9065773299935289,
      "grad_norm": 0.0005484613939188421,
      "learning_rate": 2.8026801001941335e-05,
      "loss": 0.1561,
      "step": 292800
    },
    {
      "epoch": 0.9068869533985813,
      "grad_norm": 26.563674926757812,
      "learning_rate": 2.7933913980425606e-05,
      "loss": 0.1657,
      "step": 292900
    },
    {
      "epoch": 0.9071965768036337,
      "grad_norm": 2.2449712560046464e-05,
      "learning_rate": 2.7841026958909877e-05,
      "loss": 0.139,
      "step": 293000
    },
    {
      "epoch": 0.9075062002086862,
      "grad_norm": 0.0006701665697619319,
      "learning_rate": 2.7748139937394144e-05,
      "loss": 0.1456,
      "step": 293100
    },
    {
      "epoch": 0.9078158236137386,
      "grad_norm": 23.503080368041992,
      "learning_rate": 2.7655252915878414e-05,
      "loss": 0.2883,
      "step": 293200
    },
    {
      "epoch": 0.908125447018791,
      "grad_norm": 0.000904739135876298,
      "learning_rate": 2.7562365894362685e-05,
      "loss": 0.1239,
      "step": 293300
    },
    {
      "epoch": 0.9084350704238435,
      "grad_norm": 0.0008180653676390648,
      "learning_rate": 2.7469478872846952e-05,
      "loss": 0.2587,
      "step": 293400
    },
    {
      "epoch": 0.9087446938288959,
      "grad_norm": 0.020387327298521996,
      "learning_rate": 2.7376591851331223e-05,
      "loss": 0.2144,
      "step": 293500
    },
    {
      "epoch": 0.9090543172339484,
      "grad_norm": 0.0006467688363045454,
      "learning_rate": 2.7283704829815493e-05,
      "loss": 0.1582,
      "step": 293600
    },
    {
      "epoch": 0.9093639406390008,
      "grad_norm": 6.779860973358154,
      "learning_rate": 2.7190817808299764e-05,
      "loss": 0.2671,
      "step": 293700
    },
    {
      "epoch": 0.9096735640440532,
      "grad_norm": 16.082168579101562,
      "learning_rate": 2.709793078678403e-05,
      "loss": 0.1997,
      "step": 293800
    },
    {
      "epoch": 0.9099831874491057,
      "grad_norm": 0.008227107115089893,
      "learning_rate": 2.7005043765268302e-05,
      "loss": 0.135,
      "step": 293900
    },
    {
      "epoch": 0.9102928108541581,
      "grad_norm": 0.001265002298168838,
      "learning_rate": 2.6912156743752573e-05,
      "loss": 0.1756,
      "step": 294000
    },
    {
      "epoch": 0.9106024342592105,
      "grad_norm": 0.000585368659812957,
      "learning_rate": 2.6819269722236843e-05,
      "loss": 0.118,
      "step": 294100
    },
    {
      "epoch": 0.910912057664263,
      "grad_norm": 0.7097948789596558,
      "learning_rate": 2.672638270072111e-05,
      "loss": 0.1045,
      "step": 294200
    },
    {
      "epoch": 0.9112216810693153,
      "grad_norm": 0.002033006399869919,
      "learning_rate": 2.663349567920538e-05,
      "loss": 0.1717,
      "step": 294300
    },
    {
      "epoch": 0.9115313044743678,
      "grad_norm": 0.0018615167355164886,
      "learning_rate": 2.654060865768965e-05,
      "loss": 0.1947,
      "step": 294400
    },
    {
      "epoch": 0.9118409278794203,
      "grad_norm": 0.002406001789495349,
      "learning_rate": 2.6447721636173922e-05,
      "loss": 0.2665,
      "step": 294500
    },
    {
      "epoch": 0.9121505512844726,
      "grad_norm": 11.283791542053223,
      "learning_rate": 2.635483461465819e-05,
      "loss": 0.223,
      "step": 294600
    },
    {
      "epoch": 0.9124601746895251,
      "grad_norm": 3.0386028811335564e-05,
      "learning_rate": 2.626194759314246e-05,
      "loss": 0.1513,
      "step": 294700
    },
    {
      "epoch": 0.9127697980945776,
      "grad_norm": 13.929707527160645,
      "learning_rate": 2.616906057162673e-05,
      "loss": 0.2416,
      "step": 294800
    },
    {
      "epoch": 0.91307942149963,
      "grad_norm": 0.0010757434647530317,
      "learning_rate": 2.6076173550110995e-05,
      "loss": 0.1952,
      "step": 294900
    },
    {
      "epoch": 0.9133890449046824,
      "grad_norm": 0.00624628271907568,
      "learning_rate": 2.5983286528595265e-05,
      "loss": 0.2477,
      "step": 295000
    },
    {
      "epoch": 0.9136986683097349,
      "grad_norm": 0.00014545847079716623,
      "learning_rate": 2.5890399507079536e-05,
      "loss": 0.1591,
      "step": 295100
    },
    {
      "epoch": 0.9140082917147873,
      "grad_norm": 0.6949731707572937,
      "learning_rate": 2.5797512485563807e-05,
      "loss": 0.217,
      "step": 295200
    },
    {
      "epoch": 0.9143179151198397,
      "grad_norm": 0.00034956453600898385,
      "learning_rate": 2.5704625464048074e-05,
      "loss": 0.1726,
      "step": 295300
    },
    {
      "epoch": 0.9146275385248922,
      "grad_norm": 0.0004904746310785413,
      "learning_rate": 2.5611738442532344e-05,
      "loss": 0.2556,
      "step": 295400
    },
    {
      "epoch": 0.9149371619299446,
      "grad_norm": 0.005320830270648003,
      "learning_rate": 2.5518851421016615e-05,
      "loss": 0.159,
      "step": 295500
    },
    {
      "epoch": 0.915246785334997,
      "grad_norm": 0.011950759217143059,
      "learning_rate": 2.5425964399500886e-05,
      "loss": 0.2386,
      "step": 295600
    },
    {
      "epoch": 0.9155564087400495,
      "grad_norm": 0.0006812181090936065,
      "learning_rate": 2.5333077377985153e-05,
      "loss": 0.1112,
      "step": 295700
    },
    {
      "epoch": 0.9158660321451019,
      "grad_norm": 0.0005836172495037317,
      "learning_rate": 2.5240190356469423e-05,
      "loss": 0.1917,
      "step": 295800
    },
    {
      "epoch": 0.9161756555501543,
      "grad_norm": 0.004828725941479206,
      "learning_rate": 2.5147303334953694e-05,
      "loss": 0.284,
      "step": 295900
    },
    {
      "epoch": 0.9164852789552068,
      "grad_norm": 0.008983336389064789,
      "learning_rate": 2.5054416313437965e-05,
      "loss": 0.2357,
      "step": 296000
    },
    {
      "epoch": 0.9167949023602592,
      "grad_norm": 0.0007756227860227227,
      "learning_rate": 2.4961529291922232e-05,
      "loss": 0.1793,
      "step": 296100
    },
    {
      "epoch": 0.9171045257653117,
      "grad_norm": 0.00018135162827093154,
      "learning_rate": 2.4868642270406503e-05,
      "loss": 0.156,
      "step": 296200
    },
    {
      "epoch": 0.9174141491703641,
      "grad_norm": 0.004159309435635805,
      "learning_rate": 2.4775755248890773e-05,
      "loss": 0.2595,
      "step": 296300
    },
    {
      "epoch": 0.9177237725754165,
      "grad_norm": 0.0011348957195878029,
      "learning_rate": 2.468286822737504e-05,
      "loss": 0.2654,
      "step": 296400
    },
    {
      "epoch": 0.918033395980469,
      "grad_norm": 0.0008472864865325391,
      "learning_rate": 2.458998120585931e-05,
      "loss": 0.2356,
      "step": 296500
    },
    {
      "epoch": 0.9183430193855214,
      "grad_norm": 0.0028565225657075644,
      "learning_rate": 2.449709418434358e-05,
      "loss": 0.2126,
      "step": 296600
    },
    {
      "epoch": 0.9186526427905738,
      "grad_norm": 4.9830498695373535,
      "learning_rate": 2.4404207162827852e-05,
      "loss": 0.2311,
      "step": 296700
    },
    {
      "epoch": 0.9189622661956263,
      "grad_norm": 0.003111203433945775,
      "learning_rate": 2.431132014131212e-05,
      "loss": 0.2453,
      "step": 296800
    },
    {
      "epoch": 0.9192718896006787,
      "grad_norm": 0.00026745579089038074,
      "learning_rate": 2.421843311979639e-05,
      "loss": 0.1451,
      "step": 296900
    },
    {
      "epoch": 0.9195815130057311,
      "grad_norm": 15.143457412719727,
      "learning_rate": 2.412554609828066e-05,
      "loss": 0.0912,
      "step": 297000
    },
    {
      "epoch": 0.9198911364107836,
      "grad_norm": 0.002754214219748974,
      "learning_rate": 2.403265907676493e-05,
      "loss": 0.1809,
      "step": 297100
    },
    {
      "epoch": 0.920200759815836,
      "grad_norm": 0.0004982987302355468,
      "learning_rate": 2.39397720552492e-05,
      "loss": 0.1696,
      "step": 297200
    },
    {
      "epoch": 0.9205103832208884,
      "grad_norm": 0.0025707255117595196,
      "learning_rate": 2.384688503373347e-05,
      "loss": 0.1316,
      "step": 297300
    },
    {
      "epoch": 0.9208200066259409,
      "grad_norm": 0.006457896437495947,
      "learning_rate": 2.375399801221774e-05,
      "loss": 0.2411,
      "step": 297400
    },
    {
      "epoch": 0.9211296300309934,
      "grad_norm": 0.0017603881424292922,
      "learning_rate": 2.366111099070201e-05,
      "loss": 0.1142,
      "step": 297500
    },
    {
      "epoch": 0.9214392534360457,
      "grad_norm": 0.0007870433619245887,
      "learning_rate": 2.3568223969186274e-05,
      "loss": 0.2312,
      "step": 297600
    },
    {
      "epoch": 0.9217488768410982,
      "grad_norm": 0.0003875810944009572,
      "learning_rate": 2.3475336947670545e-05,
      "loss": 0.2226,
      "step": 297700
    },
    {
      "epoch": 0.9220585002461507,
      "grad_norm": 0.00012805958976969123,
      "learning_rate": 2.3382449926154816e-05,
      "loss": 0.2037,
      "step": 297800
    },
    {
      "epoch": 0.922368123651203,
      "grad_norm": 0.00032945870771072805,
      "learning_rate": 2.3289562904639083e-05,
      "loss": 0.169,
      "step": 297900
    },
    {
      "epoch": 0.9226777470562555,
      "grad_norm": 0.0002883845299948007,
      "learning_rate": 2.3196675883123353e-05,
      "loss": 0.1823,
      "step": 298000
    },
    {
      "epoch": 0.922987370461308,
      "grad_norm": 0.08767910301685333,
      "learning_rate": 2.3103788861607624e-05,
      "loss": 0.194,
      "step": 298100
    },
    {
      "epoch": 0.9232969938663603,
      "grad_norm": 0.00038087935536168516,
      "learning_rate": 2.3010901840091895e-05,
      "loss": 0.1434,
      "step": 298200
    },
    {
      "epoch": 0.9236066172714128,
      "grad_norm": 0.0036349657457321882,
      "learning_rate": 2.2918014818576162e-05,
      "loss": 0.2343,
      "step": 298300
    },
    {
      "epoch": 0.9239162406764652,
      "grad_norm": 0.04283074662089348,
      "learning_rate": 2.2825127797060433e-05,
      "loss": 0.1973,
      "step": 298400
    },
    {
      "epoch": 0.9242258640815176,
      "grad_norm": 0.0014183854218572378,
      "learning_rate": 2.2732240775544703e-05,
      "loss": 0.1893,
      "step": 298500
    },
    {
      "epoch": 0.9245354874865701,
      "grad_norm": 0.0020241255406290293,
      "learning_rate": 2.2639353754028974e-05,
      "loss": 0.2569,
      "step": 298600
    },
    {
      "epoch": 0.9248451108916225,
      "grad_norm": 0.0004330674419179559,
      "learning_rate": 2.254646673251324e-05,
      "loss": 0.141,
      "step": 298700
    },
    {
      "epoch": 0.9251547342966749,
      "grad_norm": 0.0023611413780599833,
      "learning_rate": 2.245357971099751e-05,
      "loss": 0.1164,
      "step": 298800
    },
    {
      "epoch": 0.9254643577017274,
      "grad_norm": 0.0009727560682222247,
      "learning_rate": 2.2360692689481782e-05,
      "loss": 0.1111,
      "step": 298900
    },
    {
      "epoch": 0.9257739811067798,
      "grad_norm": 0.000543676083907485,
      "learning_rate": 2.2267805667966053e-05,
      "loss": 0.1491,
      "step": 299000
    },
    {
      "epoch": 0.9260836045118322,
      "grad_norm": 0.00011009824083885178,
      "learning_rate": 2.217491864645032e-05,
      "loss": 0.221,
      "step": 299100
    },
    {
      "epoch": 0.9263932279168847,
      "grad_norm": 0.0007438540924340487,
      "learning_rate": 2.208203162493459e-05,
      "loss": 0.2115,
      "step": 299200
    },
    {
      "epoch": 0.9267028513219371,
      "grad_norm": 0.004378865007311106,
      "learning_rate": 2.198914460341886e-05,
      "loss": 0.1652,
      "step": 299300
    },
    {
      "epoch": 0.9270124747269896,
      "grad_norm": 0.0005070183542557061,
      "learning_rate": 2.189625758190313e-05,
      "loss": 0.1856,
      "step": 299400
    },
    {
      "epoch": 0.927322098132042,
      "grad_norm": 0.002539046574383974,
      "learning_rate": 2.18033705603874e-05,
      "loss": 0.2524,
      "step": 299500
    },
    {
      "epoch": 0.9276317215370944,
      "grad_norm": 0.003978454042226076,
      "learning_rate": 2.171048353887167e-05,
      "loss": 0.2473,
      "step": 299600
    },
    {
      "epoch": 0.9279413449421469,
      "grad_norm": 0.0022446243092417717,
      "learning_rate": 2.161759651735594e-05,
      "loss": 0.2081,
      "step": 299700
    },
    {
      "epoch": 0.9282509683471993,
      "grad_norm": 0.001113066915422678,
      "learning_rate": 2.1524709495840208e-05,
      "loss": 0.1699,
      "step": 299800
    },
    {
      "epoch": 0.9285605917522517,
      "grad_norm": 0.0015099955489858985,
      "learning_rate": 2.143182247432448e-05,
      "loss": 0.2224,
      "step": 299900
    },
    {
      "epoch": 0.9288702151573042,
      "grad_norm": 0.0010435469448566437,
      "learning_rate": 2.133893545280875e-05,
      "loss": 0.3584,
      "step": 300000
    },
    {
      "epoch": 0.9291798385623566,
      "grad_norm": 40.91252517700195,
      "learning_rate": 2.124604843129302e-05,
      "loss": 0.2141,
      "step": 300100
    },
    {
      "epoch": 0.929489461967409,
      "grad_norm": 54.19454574584961,
      "learning_rate": 2.1153161409777283e-05,
      "loss": 0.2819,
      "step": 300200
    },
    {
      "epoch": 0.9297990853724615,
      "grad_norm": 0.0006867934134788811,
      "learning_rate": 2.1060274388261554e-05,
      "loss": 0.1968,
      "step": 300300
    },
    {
      "epoch": 0.930108708777514,
      "grad_norm": 0.005863051395863295,
      "learning_rate": 2.0967387366745825e-05,
      "loss": 0.12,
      "step": 300400
    },
    {
      "epoch": 0.9304183321825663,
      "grad_norm": 0.08100416511297226,
      "learning_rate": 2.0874500345230095e-05,
      "loss": 0.1422,
      "step": 300500
    },
    {
      "epoch": 0.9307279555876188,
      "grad_norm": 0.001050504157319665,
      "learning_rate": 2.0781613323714363e-05,
      "loss": 0.0745,
      "step": 300600
    },
    {
      "epoch": 0.9310375789926713,
      "grad_norm": 0.0004570856399368495,
      "learning_rate": 2.0688726302198633e-05,
      "loss": 0.2184,
      "step": 300700
    },
    {
      "epoch": 0.9313472023977236,
      "grad_norm": 20.21123695373535,
      "learning_rate": 2.0595839280682904e-05,
      "loss": 0.114,
      "step": 300800
    },
    {
      "epoch": 0.9316568258027761,
      "grad_norm": 0.0003554849827196449,
      "learning_rate": 2.050295225916717e-05,
      "loss": 0.1174,
      "step": 300900
    },
    {
      "epoch": 0.9319664492078286,
      "grad_norm": 0.0028314203955233097,
      "learning_rate": 2.041006523765144e-05,
      "loss": 0.1658,
      "step": 301000
    },
    {
      "epoch": 0.9322760726128809,
      "grad_norm": 0.0015408073086291552,
      "learning_rate": 2.0317178216135712e-05,
      "loss": 0.126,
      "step": 301100
    },
    {
      "epoch": 0.9325856960179334,
      "grad_norm": 0.030072903260588646,
      "learning_rate": 2.0224291194619983e-05,
      "loss": 0.16,
      "step": 301200
    },
    {
      "epoch": 0.9328953194229859,
      "grad_norm": 0.0006077224388718605,
      "learning_rate": 2.013140417310425e-05,
      "loss": 0.2385,
      "step": 301300
    },
    {
      "epoch": 0.9332049428280382,
      "grad_norm": 0.08273199200630188,
      "learning_rate": 2.003851715158852e-05,
      "loss": 0.1991,
      "step": 301400
    },
    {
      "epoch": 0.9335145662330907,
      "grad_norm": 0.0005384233663789928,
      "learning_rate": 1.994563013007279e-05,
      "loss": 0.1668,
      "step": 301500
    },
    {
      "epoch": 0.9338241896381432,
      "grad_norm": 0.001484689419157803,
      "learning_rate": 1.9852743108557062e-05,
      "loss": 0.1094,
      "step": 301600
    },
    {
      "epoch": 0.9341338130431955,
      "grad_norm": 0.0027465717867016792,
      "learning_rate": 1.975985608704133e-05,
      "loss": 0.1317,
      "step": 301700
    },
    {
      "epoch": 0.934443436448248,
      "grad_norm": 0.0023468786384910345,
      "learning_rate": 1.96669690655256e-05,
      "loss": 0.243,
      "step": 301800
    },
    {
      "epoch": 0.9347530598533005,
      "grad_norm": 1.8523389371694066e-05,
      "learning_rate": 1.957408204400987e-05,
      "loss": 0.2261,
      "step": 301900
    },
    {
      "epoch": 0.9350626832583528,
      "grad_norm": 7.101541519165039,
      "learning_rate": 1.948119502249414e-05,
      "loss": 0.2975,
      "step": 302000
    },
    {
      "epoch": 0.9353723066634053,
      "grad_norm": 0.0010521358344703913,
      "learning_rate": 1.938830800097841e-05,
      "loss": 0.0911,
      "step": 302100
    },
    {
      "epoch": 0.9356819300684578,
      "grad_norm": 0.00018916846602223814,
      "learning_rate": 1.929542097946268e-05,
      "loss": 0.2191,
      "step": 302200
    },
    {
      "epoch": 0.9359915534735102,
      "grad_norm": 0.0008413665927946568,
      "learning_rate": 1.920253395794695e-05,
      "loss": 0.1785,
      "step": 302300
    },
    {
      "epoch": 0.9363011768785626,
      "grad_norm": 0.002453249879181385,
      "learning_rate": 1.9109646936431217e-05,
      "loss": 0.2209,
      "step": 302400
    },
    {
      "epoch": 0.936610800283615,
      "grad_norm": 0.0024330110754817724,
      "learning_rate": 1.9016759914915487e-05,
      "loss": 0.3368,
      "step": 302500
    },
    {
      "epoch": 0.9369204236886675,
      "grad_norm": 1.516723394393921,
      "learning_rate": 1.8923872893399758e-05,
      "loss": 0.2131,
      "step": 302600
    },
    {
      "epoch": 0.9372300470937199,
      "grad_norm": 0.0007507328991778195,
      "learning_rate": 1.883098587188403e-05,
      "loss": 0.2256,
      "step": 302700
    },
    {
      "epoch": 0.9375396704987723,
      "grad_norm": 0.0021494412794709206,
      "learning_rate": 1.8738098850368296e-05,
      "loss": 0.1182,
      "step": 302800
    },
    {
      "epoch": 0.9378492939038248,
      "grad_norm": 0.0009629239211790264,
      "learning_rate": 1.8645211828852563e-05,
      "loss": 0.1355,
      "step": 302900
    },
    {
      "epoch": 0.9381589173088772,
      "grad_norm": 10.864354133605957,
      "learning_rate": 1.8552324807336834e-05,
      "loss": 0.1383,
      "step": 303000
    },
    {
      "epoch": 0.9384685407139296,
      "grad_norm": 0.0016566298436373472,
      "learning_rate": 1.8459437785821104e-05,
      "loss": 0.1911,
      "step": 303100
    },
    {
      "epoch": 0.9387781641189821,
      "grad_norm": 0.0001644113945076242,
      "learning_rate": 1.8366550764305375e-05,
      "loss": 0.0766,
      "step": 303200
    },
    {
      "epoch": 0.9390877875240345,
      "grad_norm": 0.00039446860319003463,
      "learning_rate": 1.8273663742789642e-05,
      "loss": 0.1566,
      "step": 303300
    },
    {
      "epoch": 0.9393974109290869,
      "grad_norm": 0.0010136949131265283,
      "learning_rate": 1.8180776721273913e-05,
      "loss": 0.1986,
      "step": 303400
    },
    {
      "epoch": 0.9397070343341394,
      "grad_norm": 0.0012706407578662038,
      "learning_rate": 1.8087889699758184e-05,
      "loss": 0.1744,
      "step": 303500
    },
    {
      "epoch": 0.9400166577391919,
      "grad_norm": 0.00043278958764858544,
      "learning_rate": 1.799500267824245e-05,
      "loss": 0.1224,
      "step": 303600
    },
    {
      "epoch": 0.9403262811442442,
      "grad_norm": 0.1512780338525772,
      "learning_rate": 1.790211565672672e-05,
      "loss": 0.2961,
      "step": 303700
    },
    {
      "epoch": 0.9406359045492967,
      "grad_norm": 0.00563792884349823,
      "learning_rate": 1.7809228635210992e-05,
      "loss": 0.0803,
      "step": 303800
    },
    {
      "epoch": 0.9409455279543492,
      "grad_norm": 0.00015032007650006562,
      "learning_rate": 1.7716341613695263e-05,
      "loss": 0.0312,
      "step": 303900
    },
    {
      "epoch": 0.9412551513594015,
      "grad_norm": 16.023616790771484,
      "learning_rate": 1.762345459217953e-05,
      "loss": 0.1409,
      "step": 304000
    },
    {
      "epoch": 0.941564774764454,
      "grad_norm": 0.5871698260307312,
      "learning_rate": 1.75305675706638e-05,
      "loss": 0.2525,
      "step": 304100
    },
    {
      "epoch": 0.9418743981695065,
      "grad_norm": 0.001128469710238278,
      "learning_rate": 1.7437680549148068e-05,
      "loss": 0.2196,
      "step": 304200
    },
    {
      "epoch": 0.9421840215745588,
      "grad_norm": 0.03153650090098381,
      "learning_rate": 1.734479352763234e-05,
      "loss": 0.0877,
      "step": 304300
    },
    {
      "epoch": 0.9424936449796113,
      "grad_norm": 133.43731689453125,
      "learning_rate": 1.725190650611661e-05,
      "loss": 0.3038,
      "step": 304400
    },
    {
      "epoch": 0.9428032683846638,
      "grad_norm": 0.009279130026698112,
      "learning_rate": 1.715901948460088e-05,
      "loss": 0.1607,
      "step": 304500
    },
    {
      "epoch": 0.9431128917897161,
      "grad_norm": 17.587446212768555,
      "learning_rate": 1.7066132463085147e-05,
      "loss": 0.1355,
      "step": 304600
    },
    {
      "epoch": 0.9434225151947686,
      "grad_norm": 0.872348964214325,
      "learning_rate": 1.6973245441569417e-05,
      "loss": 0.3378,
      "step": 304700
    },
    {
      "epoch": 0.9437321385998211,
      "grad_norm": 0.0013631735928356647,
      "learning_rate": 1.6880358420053688e-05,
      "loss": 0.2316,
      "step": 304800
    },
    {
      "epoch": 0.9440417620048734,
      "grad_norm": 0.0011273688869550824,
      "learning_rate": 1.678747139853796e-05,
      "loss": 0.1386,
      "step": 304900
    },
    {
      "epoch": 0.9443513854099259,
      "grad_norm": 0.001972772413864732,
      "learning_rate": 1.6694584377022226e-05,
      "loss": 0.1269,
      "step": 305000
    },
    {
      "epoch": 0.9446610088149784,
      "grad_norm": 0.001232794369570911,
      "learning_rate": 1.6601697355506497e-05,
      "loss": 0.2323,
      "step": 305100
    },
    {
      "epoch": 0.9449706322200307,
      "grad_norm": 0.0011945010628551245,
      "learning_rate": 1.6508810333990767e-05,
      "loss": 0.1897,
      "step": 305200
    },
    {
      "epoch": 0.9452802556250832,
      "grad_norm": 0.005423473194241524,
      "learning_rate": 1.6415923312475034e-05,
      "loss": 0.2248,
      "step": 305300
    },
    {
      "epoch": 0.9455898790301357,
      "grad_norm": 0.0010845023207366467,
      "learning_rate": 1.6323036290959305e-05,
      "loss": 0.3052,
      "step": 305400
    },
    {
      "epoch": 0.945899502435188,
      "grad_norm": 0.0001162180706160143,
      "learning_rate": 1.6230149269443572e-05,
      "loss": 0.1318,
      "step": 305500
    },
    {
      "epoch": 0.9462091258402405,
      "grad_norm": 0.00037168015842325985,
      "learning_rate": 1.6137262247927843e-05,
      "loss": 0.1144,
      "step": 305600
    },
    {
      "epoch": 0.946518749245293,
      "grad_norm": 18.56038475036621,
      "learning_rate": 1.6044375226412113e-05,
      "loss": 0.1565,
      "step": 305700
    },
    {
      "epoch": 0.9468283726503454,
      "grad_norm": 0.0002454879286233336,
      "learning_rate": 1.5951488204896384e-05,
      "loss": 0.1347,
      "step": 305800
    },
    {
      "epoch": 0.9471379960553978,
      "grad_norm": 0.001547474879771471,
      "learning_rate": 1.585860118338065e-05,
      "loss": 0.3049,
      "step": 305900
    },
    {
      "epoch": 0.9474476194604503,
      "grad_norm": 0.00010837251466000453,
      "learning_rate": 1.5765714161864922e-05,
      "loss": 0.1997,
      "step": 306000
    },
    {
      "epoch": 0.9477572428655027,
      "grad_norm": 0.0002939258120022714,
      "learning_rate": 1.5672827140349193e-05,
      "loss": 0.2385,
      "step": 306100
    },
    {
      "epoch": 0.9480668662705551,
      "grad_norm": 0.00197223131544888,
      "learning_rate": 1.5579940118833463e-05,
      "loss": 0.1318,
      "step": 306200
    },
    {
      "epoch": 0.9483764896756075,
      "grad_norm": 0.0012176469899713993,
      "learning_rate": 1.548705309731773e-05,
      "loss": 0.2307,
      "step": 306300
    },
    {
      "epoch": 0.94868611308066,
      "grad_norm": 0.015927055850625038,
      "learning_rate": 1.5394166075802e-05,
      "loss": 0.2237,
      "step": 306400
    },
    {
      "epoch": 0.9489957364857124,
      "grad_norm": 0.06026342883706093,
      "learning_rate": 1.5301279054286272e-05,
      "loss": 0.1373,
      "step": 306500
    },
    {
      "epoch": 0.9493053598907648,
      "grad_norm": 0.00030556577257812023,
      "learning_rate": 1.5208392032770539e-05,
      "loss": 0.0962,
      "step": 306600
    },
    {
      "epoch": 0.9496149832958173,
      "grad_norm": 0.00038149862666614354,
      "learning_rate": 1.511550501125481e-05,
      "loss": 0.2931,
      "step": 306700
    },
    {
      "epoch": 0.9499246067008698,
      "grad_norm": 0.0006960860337130725,
      "learning_rate": 1.5022617989739078e-05,
      "loss": 0.2092,
      "step": 306800
    },
    {
      "epoch": 0.9502342301059221,
      "grad_norm": 0.001068047364242375,
      "learning_rate": 1.4929730968223349e-05,
      "loss": 0.1045,
      "step": 306900
    },
    {
      "epoch": 0.9505438535109746,
      "grad_norm": 1.0540196895599365,
      "learning_rate": 1.4836843946707618e-05,
      "loss": 0.2714,
      "step": 307000
    },
    {
      "epoch": 0.9508534769160271,
      "grad_norm": 0.0009040668373927474,
      "learning_rate": 1.4743956925191889e-05,
      "loss": 0.1537,
      "step": 307100
    },
    {
      "epoch": 0.9511631003210794,
      "grad_norm": 0.0002942240098491311,
      "learning_rate": 1.4651069903676158e-05,
      "loss": 0.3022,
      "step": 307200
    },
    {
      "epoch": 0.9514727237261319,
      "grad_norm": 3.208843469619751,
      "learning_rate": 1.4558182882160428e-05,
      "loss": 0.2394,
      "step": 307300
    },
    {
      "epoch": 0.9517823471311844,
      "grad_norm": 60.459800720214844,
      "learning_rate": 1.4465295860644695e-05,
      "loss": 0.3581,
      "step": 307400
    },
    {
      "epoch": 0.9520919705362367,
      "grad_norm": 0.0017383621307089925,
      "learning_rate": 1.4372408839128966e-05,
      "loss": 0.2411,
      "step": 307500
    },
    {
      "epoch": 0.9524015939412892,
      "grad_norm": 0.047773461788892746,
      "learning_rate": 1.4279521817613235e-05,
      "loss": 0.2357,
      "step": 307600
    },
    {
      "epoch": 0.9527112173463417,
      "grad_norm": 12.427926063537598,
      "learning_rate": 1.4186634796097506e-05,
      "loss": 0.1151,
      "step": 307700
    },
    {
      "epoch": 0.953020840751394,
      "grad_norm": 0.003257057163864374,
      "learning_rate": 1.4093747774581775e-05,
      "loss": 0.1692,
      "step": 307800
    },
    {
      "epoch": 0.9533304641564465,
      "grad_norm": 0.007047352846711874,
      "learning_rate": 1.4000860753066045e-05,
      "loss": 0.2497,
      "step": 307900
    },
    {
      "epoch": 0.953640087561499,
      "grad_norm": 0.00037334486842155457,
      "learning_rate": 1.3907973731550314e-05,
      "loss": 0.1757,
      "step": 308000
    },
    {
      "epoch": 0.9539497109665513,
      "grad_norm": 0.055277880281209946,
      "learning_rate": 1.3815086710034583e-05,
      "loss": 0.1268,
      "step": 308100
    },
    {
      "epoch": 0.9542593343716038,
      "grad_norm": 0.0020853010937571526,
      "learning_rate": 1.3722199688518854e-05,
      "loss": 0.3013,
      "step": 308200
    },
    {
      "epoch": 0.9545689577766563,
      "grad_norm": 0.0007279864512383938,
      "learning_rate": 1.3629312667003123e-05,
      "loss": 0.1929,
      "step": 308300
    },
    {
      "epoch": 0.9548785811817087,
      "grad_norm": 0.0012267796555534005,
      "learning_rate": 1.3536425645487393e-05,
      "loss": 0.0596,
      "step": 308400
    },
    {
      "epoch": 0.9551882045867611,
      "grad_norm": 0.0027952471282333136,
      "learning_rate": 1.3443538623971662e-05,
      "loss": 0.2046,
      "step": 308500
    },
    {
      "epoch": 0.9554978279918136,
      "grad_norm": 0.0004179392126388848,
      "learning_rate": 1.3350651602455933e-05,
      "loss": 0.1679,
      "step": 308600
    },
    {
      "epoch": 0.955807451396866,
      "grad_norm": 11.138693809509277,
      "learning_rate": 1.32577645809402e-05,
      "loss": 0.1434,
      "step": 308700
    },
    {
      "epoch": 0.9561170748019184,
      "grad_norm": 0.00024830878828652203,
      "learning_rate": 1.316487755942447e-05,
      "loss": 0.2097,
      "step": 308800
    },
    {
      "epoch": 0.9564266982069709,
      "grad_norm": 0.00033926661126315594,
      "learning_rate": 1.307199053790874e-05,
      "loss": 0.1232,
      "step": 308900
    },
    {
      "epoch": 0.9567363216120233,
      "grad_norm": 0.0008710497058928013,
      "learning_rate": 1.297910351639301e-05,
      "loss": 0.1239,
      "step": 309000
    },
    {
      "epoch": 0.9570459450170757,
      "grad_norm": 5.38995361328125,
      "learning_rate": 1.2886216494877279e-05,
      "loss": 0.1625,
      "step": 309100
    },
    {
      "epoch": 0.9573555684221282,
      "grad_norm": 6.15518365520984e-05,
      "learning_rate": 1.279332947336155e-05,
      "loss": 0.2273,
      "step": 309200
    },
    {
      "epoch": 0.9576651918271806,
      "grad_norm": 0.0018244828097522259,
      "learning_rate": 1.2700442451845819e-05,
      "loss": 0.2969,
      "step": 309300
    },
    {
      "epoch": 0.957974815232233,
      "grad_norm": 0.01810990832746029,
      "learning_rate": 1.260755543033009e-05,
      "loss": 0.3134,
      "step": 309400
    },
    {
      "epoch": 0.9582844386372855,
      "grad_norm": 0.0010011870181187987,
      "learning_rate": 1.2514668408814358e-05,
      "loss": 0.0328,
      "step": 309500
    },
    {
      "epoch": 0.9585940620423379,
      "grad_norm": 0.06166373938322067,
      "learning_rate": 1.2421781387298627e-05,
      "loss": 0.1727,
      "step": 309600
    },
    {
      "epoch": 0.9589036854473904,
      "grad_norm": 0.012456014752388,
      "learning_rate": 1.2328894365782898e-05,
      "loss": 0.2016,
      "step": 309700
    },
    {
      "epoch": 0.9592133088524428,
      "grad_norm": 0.00088590441737324,
      "learning_rate": 1.2236007344267167e-05,
      "loss": 0.0587,
      "step": 309800
    },
    {
      "epoch": 0.9595229322574952,
      "grad_norm": 0.0005356983165256679,
      "learning_rate": 1.2143120322751437e-05,
      "loss": 0.276,
      "step": 309900
    },
    {
      "epoch": 0.9598325556625477,
      "grad_norm": 0.0017304025823250413,
      "learning_rate": 1.2050233301235705e-05,
      "loss": 0.1032,
      "step": 310000
    },
    {
      "epoch": 0.9601421790676001,
      "grad_norm": 0.002856305567547679,
      "learning_rate": 1.1957346279719975e-05,
      "loss": 0.1935,
      "step": 310100
    },
    {
      "epoch": 0.9604518024726525,
      "grad_norm": 0.0009258044883608818,
      "learning_rate": 1.1864459258204244e-05,
      "loss": 0.1189,
      "step": 310200
    },
    {
      "epoch": 0.960761425877705,
      "grad_norm": 0.3434295356273651,
      "learning_rate": 1.1771572236688515e-05,
      "loss": 0.1774,
      "step": 310300
    },
    {
      "epoch": 0.9610710492827573,
      "grad_norm": 0.0011385668767616153,
      "learning_rate": 1.1678685215172784e-05,
      "loss": 0.1408,
      "step": 310400
    },
    {
      "epoch": 0.9613806726878098,
      "grad_norm": 0.027910858392715454,
      "learning_rate": 1.1585798193657054e-05,
      "loss": 0.3172,
      "step": 310500
    },
    {
      "epoch": 0.9616902960928623,
      "grad_norm": 0.0013793848920613527,
      "learning_rate": 1.1492911172141323e-05,
      "loss": 0.1798,
      "step": 310600
    },
    {
      "epoch": 0.9619999194979146,
      "grad_norm": 0.005873037967830896,
      "learning_rate": 1.1400024150625594e-05,
      "loss": 0.1278,
      "step": 310700
    },
    {
      "epoch": 0.9623095429029671,
      "grad_norm": 0.0018888685153797269,
      "learning_rate": 1.1307137129109863e-05,
      "loss": 0.1007,
      "step": 310800
    },
    {
      "epoch": 0.9626191663080196,
      "grad_norm": 0.0005247119115665555,
      "learning_rate": 1.1214250107594133e-05,
      "loss": 0.23,
      "step": 310900
    },
    {
      "epoch": 0.9629287897130719,
      "grad_norm": 0.6004774570465088,
      "learning_rate": 1.1121363086078402e-05,
      "loss": 0.1094,
      "step": 311000
    },
    {
      "epoch": 0.9632384131181244,
      "grad_norm": 11.28844165802002,
      "learning_rate": 1.1028476064562671e-05,
      "loss": 0.1693,
      "step": 311100
    },
    {
      "epoch": 0.9635480365231769,
      "grad_norm": 0.00047998724039644003,
      "learning_rate": 1.0935589043046942e-05,
      "loss": 0.3384,
      "step": 311200
    },
    {
      "epoch": 0.9638576599282292,
      "grad_norm": 12.048720359802246,
      "learning_rate": 1.0842702021531209e-05,
      "loss": 0.1735,
      "step": 311300
    },
    {
      "epoch": 0.9641672833332817,
      "grad_norm": 0.0002225634380010888,
      "learning_rate": 1.074981500001548e-05,
      "loss": 0.1519,
      "step": 311400
    },
    {
      "epoch": 0.9644769067383342,
      "grad_norm": 0.0006552473641932011,
      "learning_rate": 1.0656927978499749e-05,
      "loss": 0.1766,
      "step": 311500
    },
    {
      "epoch": 0.9647865301433866,
      "grad_norm": 11.502176284790039,
      "learning_rate": 1.056404095698402e-05,
      "loss": 0.1796,
      "step": 311600
    },
    {
      "epoch": 0.965096153548439,
      "grad_norm": 0.0013639621902257204,
      "learning_rate": 1.0471153935468288e-05,
      "loss": 0.1901,
      "step": 311700
    },
    {
      "epoch": 0.9654057769534915,
      "grad_norm": 0.0008433405891992152,
      "learning_rate": 1.0378266913952559e-05,
      "loss": 0.1855,
      "step": 311800
    },
    {
      "epoch": 0.9657154003585439,
      "grad_norm": 0.00013840121391694993,
      "learning_rate": 1.0285379892436828e-05,
      "loss": 0.3192,
      "step": 311900
    },
    {
      "epoch": 0.9660250237635963,
      "grad_norm": 0.006679694168269634,
      "learning_rate": 1.0192492870921098e-05,
      "loss": 0.1628,
      "step": 312000
    },
    {
      "epoch": 0.9663346471686488,
      "grad_norm": 0.00037058768793940544,
      "learning_rate": 1.0099605849405367e-05,
      "loss": 0.2757,
      "step": 312100
    },
    {
      "epoch": 0.9666442705737012,
      "grad_norm": 0.8568403720855713,
      "learning_rate": 1.0006718827889638e-05,
      "loss": 0.1651,
      "step": 312200
    },
    {
      "epoch": 0.9669538939787536,
      "grad_norm": 0.0008938657119870186,
      "learning_rate": 9.913831806373907e-06,
      "loss": 0.1621,
      "step": 312300
    },
    {
      "epoch": 0.9672635173838061,
      "grad_norm": 0.0016851468244567513,
      "learning_rate": 9.820944784858177e-06,
      "loss": 0.2072,
      "step": 312400
    },
    {
      "epoch": 0.9675731407888585,
      "grad_norm": 0.009275618940591812,
      "learning_rate": 9.728057763342446e-06,
      "loss": 0.2339,
      "step": 312500
    },
    {
      "epoch": 0.967882764193911,
      "grad_norm": 0.0009414288215339184,
      "learning_rate": 9.635170741826714e-06,
      "loss": 0.3102,
      "step": 312600
    },
    {
      "epoch": 0.9681923875989634,
      "grad_norm": 0.0006678212666884065,
      "learning_rate": 9.542283720310984e-06,
      "loss": 0.2009,
      "step": 312700
    },
    {
      "epoch": 0.9685020110040158,
      "grad_norm": 0.0029866702388972044,
      "learning_rate": 9.449396698795253e-06,
      "loss": 0.1341,
      "step": 312800
    },
    {
      "epoch": 0.9688116344090683,
      "grad_norm": 0.0022315070964396,
      "learning_rate": 9.356509677279524e-06,
      "loss": 0.0681,
      "step": 312900
    },
    {
      "epoch": 0.9691212578141207,
      "grad_norm": 0.0018919731955975294,
      "learning_rate": 9.263622655763794e-06,
      "loss": 0.2155,
      "step": 313000
    },
    {
      "epoch": 0.9694308812191731,
      "grad_norm": 0.6038939952850342,
      "learning_rate": 9.170735634248063e-06,
      "loss": 0.3318,
      "step": 313100
    },
    {
      "epoch": 0.9697405046242256,
      "grad_norm": 1.1480835676193237,
      "learning_rate": 9.077848612732334e-06,
      "loss": 0.1777,
      "step": 313200
    },
    {
      "epoch": 0.970050128029278,
      "grad_norm": 0.0023262042086571455,
      "learning_rate": 8.984961591216601e-06,
      "loss": 0.3675,
      "step": 313300
    },
    {
      "epoch": 0.9703597514343304,
      "grad_norm": 6.476563430624083e-05,
      "learning_rate": 8.892074569700872e-06,
      "loss": 0.1469,
      "step": 313400
    },
    {
      "epoch": 0.9706693748393829,
      "grad_norm": 0.00013187133299652487,
      "learning_rate": 8.79918754818514e-06,
      "loss": 0.1864,
      "step": 313500
    },
    {
      "epoch": 0.9709789982444353,
      "grad_norm": 31.42364501953125,
      "learning_rate": 8.706300526669411e-06,
      "loss": 0.0657,
      "step": 313600
    },
    {
      "epoch": 0.9712886216494877,
      "grad_norm": 7.064524106681347e-05,
      "learning_rate": 8.61341350515368e-06,
      "loss": 0.0926,
      "step": 313700
    },
    {
      "epoch": 0.9715982450545402,
      "grad_norm": 0.011851348914206028,
      "learning_rate": 8.520526483637951e-06,
      "loss": 0.1581,
      "step": 313800
    },
    {
      "epoch": 0.9719078684595926,
      "grad_norm": 71.3407211303711,
      "learning_rate": 8.42763946212222e-06,
      "loss": 0.1975,
      "step": 313900
    },
    {
      "epoch": 0.972217491864645,
      "grad_norm": 37.434593200683594,
      "learning_rate": 8.334752440606489e-06,
      "loss": 0.1675,
      "step": 314000
    },
    {
      "epoch": 0.9725271152696975,
      "grad_norm": 0.0002530244819354266,
      "learning_rate": 8.24186541909076e-06,
      "loss": 0.1384,
      "step": 314100
    },
    {
      "epoch": 0.97283673867475,
      "grad_norm": 0.0033162657637149096,
      "learning_rate": 8.148978397575028e-06,
      "loss": 0.2105,
      "step": 314200
    },
    {
      "epoch": 0.9731463620798023,
      "grad_norm": 0.0010135291377082467,
      "learning_rate": 8.056091376059299e-06,
      "loss": 0.1791,
      "step": 314300
    },
    {
      "epoch": 0.9734559854848548,
      "grad_norm": 0.0008303561480715871,
      "learning_rate": 7.963204354543568e-06,
      "loss": 0.1646,
      "step": 314400
    },
    {
      "epoch": 0.9737656088899072,
      "grad_norm": 0.0005852209287695587,
      "learning_rate": 7.870317333027839e-06,
      "loss": 0.1606,
      "step": 314500
    },
    {
      "epoch": 0.9740752322949596,
      "grad_norm": 0.00010956975893350318,
      "learning_rate": 7.777430311512107e-06,
      "loss": 0.1491,
      "step": 314600
    },
    {
      "epoch": 0.9743848557000121,
      "grad_norm": 0.0001592397893546149,
      "learning_rate": 7.684543289996376e-06,
      "loss": 0.0646,
      "step": 314700
    },
    {
      "epoch": 0.9746944791050645,
      "grad_norm": 0.0016504275845363736,
      "learning_rate": 7.591656268480646e-06,
      "loss": 0.283,
      "step": 314800
    },
    {
      "epoch": 0.9750041025101169,
      "grad_norm": 0.0035247262567281723,
      "learning_rate": 7.498769246964915e-06,
      "loss": 0.1516,
      "step": 314900
    },
    {
      "epoch": 0.9753137259151694,
      "grad_norm": 0.2577720582485199,
      "learning_rate": 7.405882225449185e-06,
      "loss": 0.1531,
      "step": 315000
    },
    {
      "epoch": 0.9756233493202218,
      "grad_norm": 0.07508177310228348,
      "learning_rate": 7.312995203933455e-06,
      "loss": 0.0793,
      "step": 315100
    },
    {
      "epoch": 0.9759329727252742,
      "grad_norm": 0.00048546126345172524,
      "learning_rate": 7.220108182417724e-06,
      "loss": 0.1845,
      "step": 315200
    },
    {
      "epoch": 0.9762425961303267,
      "grad_norm": 0.00044908508425578475,
      "learning_rate": 7.127221160901994e-06,
      "loss": 0.2728,
      "step": 315300
    },
    {
      "epoch": 0.9765522195353791,
      "grad_norm": 0.6394317746162415,
      "learning_rate": 7.034334139386264e-06,
      "loss": 0.1192,
      "step": 315400
    },
    {
      "epoch": 0.9768618429404315,
      "grad_norm": 0.0013323433231562376,
      "learning_rate": 6.941447117870534e-06,
      "loss": 0.2592,
      "step": 315500
    },
    {
      "epoch": 0.977171466345484,
      "grad_norm": 0.0015588462119922042,
      "learning_rate": 6.8485600963548035e-06,
      "loss": 0.0993,
      "step": 315600
    },
    {
      "epoch": 0.9774810897505364,
      "grad_norm": 0.00028553526499308646,
      "learning_rate": 6.7556730748390724e-06,
      "loss": 0.0895,
      "step": 315700
    },
    {
      "epoch": 0.9777907131555889,
      "grad_norm": 0.0076601277105510235,
      "learning_rate": 6.662786053323342e-06,
      "loss": 0.1047,
      "step": 315800
    },
    {
      "epoch": 0.9781003365606413,
      "grad_norm": 0.00036039971746504307,
      "learning_rate": 6.569899031807612e-06,
      "loss": 0.145,
      "step": 315900
    },
    {
      "epoch": 0.9784099599656937,
      "grad_norm": 0.0026520865503698587,
      "learning_rate": 6.477012010291882e-06,
      "loss": 0.1889,
      "step": 316000
    },
    {
      "epoch": 0.9787195833707462,
      "grad_norm": 0.007044143509119749,
      "learning_rate": 6.3841249887761515e-06,
      "loss": 0.1333,
      "step": 316100
    },
    {
      "epoch": 0.9790292067757986,
      "grad_norm": 0.0003260826924815774,
      "learning_rate": 6.291237967260421e-06,
      "loss": 0.1864,
      "step": 316200
    },
    {
      "epoch": 0.979338830180851,
      "grad_norm": 0.0014216456329450011,
      "learning_rate": 6.198350945744689e-06,
      "loss": 0.258,
      "step": 316300
    },
    {
      "epoch": 0.9796484535859035,
      "grad_norm": 0.0005262906197458506,
      "learning_rate": 6.105463924228959e-06,
      "loss": 0.1431,
      "step": 316400
    },
    {
      "epoch": 0.9799580769909559,
      "grad_norm": 18.66737937927246,
      "learning_rate": 6.012576902713229e-06,
      "loss": 0.2667,
      "step": 316500
    },
    {
      "epoch": 0.9802677003960083,
      "grad_norm": 0.0003453083918429911,
      "learning_rate": 5.919689881197499e-06,
      "loss": 0.1593,
      "step": 316600
    },
    {
      "epoch": 0.9805773238010608,
      "grad_norm": 0.00040441390592604876,
      "learning_rate": 5.8268028596817685e-06,
      "loss": 0.1382,
      "step": 316700
    },
    {
      "epoch": 0.9808869472061132,
      "grad_norm": 0.0009099044254980981,
      "learning_rate": 5.733915838166038e-06,
      "loss": 0.1759,
      "step": 316800
    },
    {
      "epoch": 0.9811965706111656,
      "grad_norm": 0.00012032319500576705,
      "learning_rate": 5.641028816650308e-06,
      "loss": 0.1621,
      "step": 316900
    },
    {
      "epoch": 0.9815061940162181,
      "grad_norm": 0.000635038479231298,
      "learning_rate": 5.548141795134577e-06,
      "loss": 0.1255,
      "step": 317000
    },
    {
      "epoch": 0.9818158174212706,
      "grad_norm": 0.0016978720668703318,
      "learning_rate": 5.455254773618847e-06,
      "loss": 0.2334,
      "step": 317100
    },
    {
      "epoch": 0.9821254408263229,
      "grad_norm": 7.412024569930509e-05,
      "learning_rate": 5.3623677521031165e-06,
      "loss": 0.1577,
      "step": 317200
    },
    {
      "epoch": 0.9824350642313754,
      "grad_norm": 0.00030097365379333496,
      "learning_rate": 5.269480730587386e-06,
      "loss": 0.1756,
      "step": 317300
    },
    {
      "epoch": 0.9827446876364279,
      "grad_norm": 0.0002821385278366506,
      "learning_rate": 5.176593709071656e-06,
      "loss": 0.1488,
      "step": 317400
    },
    {
      "epoch": 0.9830543110414802,
      "grad_norm": 13.876126289367676,
      "learning_rate": 5.083706687555926e-06,
      "loss": 0.1379,
      "step": 317500
    },
    {
      "epoch": 0.9833639344465327,
      "grad_norm": 0.11135299503803253,
      "learning_rate": 4.990819666040196e-06,
      "loss": 0.1754,
      "step": 317600
    },
    {
      "epoch": 0.9836735578515852,
      "grad_norm": 0.0009337144438177347,
      "learning_rate": 4.8979326445244646e-06,
      "loss": 0.1842,
      "step": 317700
    },
    {
      "epoch": 0.9839831812566375,
      "grad_norm": 0.0011789478594437242,
      "learning_rate": 4.8050456230087335e-06,
      "loss": 0.1968,
      "step": 317800
    },
    {
      "epoch": 0.98429280466169,
      "grad_norm": 0.0016617116052657366,
      "learning_rate": 4.712158601493003e-06,
      "loss": 0.1398,
      "step": 317900
    },
    {
      "epoch": 0.9846024280667425,
      "grad_norm": 0.0012923567555844784,
      "learning_rate": 4.619271579977273e-06,
      "loss": 0.1484,
      "step": 318000
    },
    {
      "epoch": 0.9849120514717948,
      "grad_norm": 0.0011203368194401264,
      "learning_rate": 4.526384558461543e-06,
      "loss": 0.2645,
      "step": 318100
    },
    {
      "epoch": 0.9852216748768473,
      "grad_norm": 0.0018586769001558423,
      "learning_rate": 4.433497536945813e-06,
      "loss": 0.2993,
      "step": 318200
    },
    {
      "epoch": 0.9855312982818997,
      "grad_norm": 0.00013597302313428372,
      "learning_rate": 4.3406105154300815e-06,
      "loss": 0.0866,
      "step": 318300
    },
    {
      "epoch": 0.9858409216869521,
      "grad_norm": 17.97941780090332,
      "learning_rate": 4.247723493914351e-06,
      "loss": 0.2621,
      "step": 318400
    },
    {
      "epoch": 0.9861505450920046,
      "grad_norm": 0.0008917241939343512,
      "learning_rate": 4.154836472398621e-06,
      "loss": 0.0985,
      "step": 318500
    },
    {
      "epoch": 0.986460168497057,
      "grad_norm": 0.002813523169606924,
      "learning_rate": 4.061949450882891e-06,
      "loss": 0.0557,
      "step": 318600
    },
    {
      "epoch": 0.9867697919021094,
      "grad_norm": 0.00944299902766943,
      "learning_rate": 3.969062429367161e-06,
      "loss": 0.0644,
      "step": 318700
    },
    {
      "epoch": 0.9870794153071619,
      "grad_norm": 0.0008457516087219119,
      "learning_rate": 3.87617540785143e-06,
      "loss": 0.0988,
      "step": 318800
    },
    {
      "epoch": 0.9873890387122143,
      "grad_norm": 0.003696534549817443,
      "learning_rate": 3.7832883863356993e-06,
      "loss": 0.1285,
      "step": 318900
    },
    {
      "epoch": 0.9876986621172668,
      "grad_norm": 10.635157585144043,
      "learning_rate": 3.690401364819969e-06,
      "loss": 0.201,
      "step": 319000
    },
    {
      "epoch": 0.9880082855223192,
      "grad_norm": 0.0008968105539679527,
      "learning_rate": 3.5975143433042385e-06,
      "loss": 0.1788,
      "step": 319100
    },
    {
      "epoch": 0.9883179089273716,
      "grad_norm": 0.0008694928837940097,
      "learning_rate": 3.5046273217885082e-06,
      "loss": 0.0932,
      "step": 319200
    },
    {
      "epoch": 0.9886275323324241,
      "grad_norm": 0.00026286867796443403,
      "learning_rate": 3.411740300272778e-06,
      "loss": 0.1389,
      "step": 319300
    },
    {
      "epoch": 0.9889371557374765,
      "grad_norm": 0.0003537147422321141,
      "learning_rate": 3.318853278757048e-06,
      "loss": 0.0971,
      "step": 319400
    },
    {
      "epoch": 0.9892467791425289,
      "grad_norm": 0.0002343937667319551,
      "learning_rate": 3.225966257241317e-06,
      "loss": 0.2502,
      "step": 319500
    },
    {
      "epoch": 0.9895564025475814,
      "grad_norm": 0.00020826478430535644,
      "learning_rate": 3.133079235725587e-06,
      "loss": 0.2409,
      "step": 319600
    },
    {
      "epoch": 0.9898660259526338,
      "grad_norm": 0.0014594019157812,
      "learning_rate": 3.0401922142098563e-06,
      "loss": 0.2636,
      "step": 319700
    },
    {
      "epoch": 0.9901756493576862,
      "grad_norm": 0.001936734770424664,
      "learning_rate": 2.9473051926941256e-06,
      "loss": 0.1943,
      "step": 319800
    },
    {
      "epoch": 0.9904852727627387,
      "grad_norm": 0.0011437718058004975,
      "learning_rate": 2.8544181711783954e-06,
      "loss": 0.1641,
      "step": 319900
    },
    {
      "epoch": 0.9907948961677912,
      "grad_norm": 0.9831334948539734,
      "learning_rate": 2.761531149662665e-06,
      "loss": 0.0877,
      "step": 320000
    },
    {
      "epoch": 0.9911045195728435,
      "grad_norm": 0.00091608299408108,
      "learning_rate": 2.6686441281469345e-06,
      "loss": 0.3341,
      "step": 320100
    },
    {
      "epoch": 0.991414142977896,
      "grad_norm": 0.0014134094817563891,
      "learning_rate": 2.5757571066312043e-06,
      "loss": 0.1059,
      "step": 320200
    },
    {
      "epoch": 0.9917237663829485,
      "grad_norm": 0.0062663066200912,
      "learning_rate": 2.482870085115474e-06,
      "loss": 0.1058,
      "step": 320300
    },
    {
      "epoch": 0.9920333897880008,
      "grad_norm": 0.0006566463271155953,
      "learning_rate": 2.389983063599743e-06,
      "loss": 0.1608,
      "step": 320400
    },
    {
      "epoch": 0.9923430131930533,
      "grad_norm": 13.832718849182129,
      "learning_rate": 2.2970960420840132e-06,
      "loss": 0.102,
      "step": 320500
    },
    {
      "epoch": 0.9926526365981058,
      "grad_norm": 0.0033549745567142963,
      "learning_rate": 2.2042090205682826e-06,
      "loss": 0.2359,
      "step": 320600
    },
    {
      "epoch": 0.9929622600031581,
      "grad_norm": 0.00042381579987704754,
      "learning_rate": 2.1113219990525523e-06,
      "loss": 0.0968,
      "step": 320700
    },
    {
      "epoch": 0.9932718834082106,
      "grad_norm": 0.034424521028995514,
      "learning_rate": 2.0184349775368217e-06,
      "loss": 0.2076,
      "step": 320800
    },
    {
      "epoch": 0.9935815068132631,
      "grad_norm": 0.00030692314612679183,
      "learning_rate": 1.9255479560210915e-06,
      "loss": 0.278,
      "step": 320900
    },
    {
      "epoch": 0.9938911302183154,
      "grad_norm": 0.0047273943200707436,
      "learning_rate": 1.8326609345053608e-06,
      "loss": 0.1596,
      "step": 321000
    },
    {
      "epoch": 0.9942007536233679,
      "grad_norm": 0.0003908791986759752,
      "learning_rate": 1.7397739129896306e-06,
      "loss": 0.0863,
      "step": 321100
    },
    {
      "epoch": 0.9945103770284204,
      "grad_norm": 0.002351101953536272,
      "learning_rate": 1.6468868914739002e-06,
      "loss": 0.1758,
      "step": 321200
    },
    {
      "epoch": 0.9948200004334727,
      "grad_norm": 14.142684936523438,
      "learning_rate": 1.55399986995817e-06,
      "loss": 0.2235,
      "step": 321300
    },
    {
      "epoch": 0.9951296238385252,
      "grad_norm": 0.0003059903101529926,
      "learning_rate": 1.4611128484424393e-06,
      "loss": 0.2383,
      "step": 321400
    },
    {
      "epoch": 0.9954392472435777,
      "grad_norm": 0.0003216415934730321,
      "learning_rate": 1.3682258269267089e-06,
      "loss": 0.1044,
      "step": 321500
    },
    {
      "epoch": 0.99574887064863,
      "grad_norm": 0.0006582696223631501,
      "learning_rate": 1.2753388054109786e-06,
      "loss": 0.3372,
      "step": 321600
    },
    {
      "epoch": 0.9960584940536825,
      "grad_norm": 0.0004176005022600293,
      "learning_rate": 1.182451783895248e-06,
      "loss": 0.1602,
      "step": 321700
    },
    {
      "epoch": 0.996368117458735,
      "grad_norm": 0.0009754874045029283,
      "learning_rate": 1.0895647623795175e-06,
      "loss": 0.1307,
      "step": 321800
    },
    {
      "epoch": 0.9966777408637874,
      "grad_norm": 0.0010790072847157717,
      "learning_rate": 9.966777408637873e-07,
      "loss": 0.251,
      "step": 321900
    },
    {
      "epoch": 0.9969873642688398,
      "grad_norm": 28.31842803955078,
      "learning_rate": 9.037907193480569e-07,
      "loss": 0.0708,
      "step": 322000
    },
    {
      "epoch": 0.9972969876738923,
      "grad_norm": 0.035790227353572845,
      "learning_rate": 8.109036978323266e-07,
      "loss": 0.3017,
      "step": 322100
    },
    {
      "epoch": 0.9976066110789447,
      "grad_norm": 18.099821090698242,
      "learning_rate": 7.18016676316596e-07,
      "loss": 0.4375,
      "step": 322200
    },
    {
      "epoch": 0.9979162344839971,
      "grad_norm": 14.0594482421875,
      "learning_rate": 6.251296548008657e-07,
      "loss": 0.144,
      "step": 322300
    },
    {
      "epoch": 0.9982258578890495,
      "grad_norm": 0.00041902376688085496,
      "learning_rate": 5.322426332851352e-07,
      "loss": 0.1441,
      "step": 322400
    },
    {
      "epoch": 0.998535481294102,
      "grad_norm": 0.0027490933425724506,
      "learning_rate": 4.393556117694048e-07,
      "loss": 0.318,
      "step": 322500
    },
    {
      "epoch": 0.9988451046991544,
      "grad_norm": 0.00037900800816714764,
      "learning_rate": 3.4646859025367443e-07,
      "loss": 0.278,
      "step": 322600
    },
    {
      "epoch": 0.9991547281042068,
      "grad_norm": 56.50355529785156,
      "learning_rate": 2.53581568737944e-07,
      "loss": 0.1938,
      "step": 322700
    },
    {
      "epoch": 0.9994643515092593,
      "grad_norm": 0.07891198992729187,
      "learning_rate": 1.606945472222136e-07,
      "loss": 0.1635,
      "step": 322800
    },
    {
      "epoch": 0.9997739749143117,
      "grad_norm": 0.00044559352681972086,
      "learning_rate": 6.78075257064832e-08,
      "loss": 0.2192,
      "step": 322900
    }
  ],
  "logging_steps": 100,
  "max_steps": 322973,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.333439362159584e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
