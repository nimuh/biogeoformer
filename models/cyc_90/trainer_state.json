{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 306527,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00032623553553194336,
      "grad_norm": 3.740607976913452,
      "learning_rate": 0.0002999021293393404,
      "loss": 3.6349,
      "step": 100
    },
    {
      "epoch": 0.0006524710710638867,
      "grad_norm": 3.96622896194458,
      "learning_rate": 0.0002998042586786808,
      "loss": 3.6515,
      "step": 200
    },
    {
      "epoch": 0.00097870660659583,
      "grad_norm": 2.671549081802368,
      "learning_rate": 0.0002997063880180212,
      "loss": 3.6474,
      "step": 300
    },
    {
      "epoch": 0.0013049421421277734,
      "grad_norm": 3.298827648162842,
      "learning_rate": 0.00029960851735736163,
      "loss": 3.6177,
      "step": 400
    },
    {
      "epoch": 0.0016311776776597167,
      "grad_norm": 5.796653747558594,
      "learning_rate": 0.00029951064669670206,
      "loss": 3.535,
      "step": 500
    },
    {
      "epoch": 0.00195741321319166,
      "grad_norm": 4.845681190490723,
      "learning_rate": 0.0002994127760360425,
      "loss": 3.4369,
      "step": 600
    },
    {
      "epoch": 0.0022836487487236036,
      "grad_norm": 3.5472636222839355,
      "learning_rate": 0.0002993149053753829,
      "loss": 3.4346,
      "step": 700
    },
    {
      "epoch": 0.002609884284255547,
      "grad_norm": 3.5812599658966064,
      "learning_rate": 0.0002992170347147233,
      "loss": 3.4562,
      "step": 800
    },
    {
      "epoch": 0.00293611981978749,
      "grad_norm": 4.787569999694824,
      "learning_rate": 0.0002991191640540637,
      "loss": 3.3897,
      "step": 900
    },
    {
      "epoch": 0.0032623553553194334,
      "grad_norm": 3.905069589614868,
      "learning_rate": 0.00029902129339340414,
      "loss": 3.4311,
      "step": 1000
    },
    {
      "epoch": 0.003588590890851377,
      "grad_norm": 11.73417854309082,
      "learning_rate": 0.00029892342273274457,
      "loss": 3.2679,
      "step": 1100
    },
    {
      "epoch": 0.00391482642638332,
      "grad_norm": 5.533488750457764,
      "learning_rate": 0.000298825552072085,
      "loss": 3.1916,
      "step": 1200
    },
    {
      "epoch": 0.0042410619619152636,
      "grad_norm": 10.257280349731445,
      "learning_rate": 0.0002987276814114254,
      "loss": 3.1958,
      "step": 1300
    },
    {
      "epoch": 0.004567297497447207,
      "grad_norm": 7.679831504821777,
      "learning_rate": 0.0002986298107507658,
      "loss": 3.1418,
      "step": 1400
    },
    {
      "epoch": 0.00489353303297915,
      "grad_norm": 6.09012508392334,
      "learning_rate": 0.00029853194009010623,
      "loss": 3.0735,
      "step": 1500
    },
    {
      "epoch": 0.005219768568511094,
      "grad_norm": 7.308155059814453,
      "learning_rate": 0.00029843406942944666,
      "loss": 2.9427,
      "step": 1600
    },
    {
      "epoch": 0.005546004104043037,
      "grad_norm": 8.510470390319824,
      "learning_rate": 0.0002983361987687871,
      "loss": 2.8448,
      "step": 1700
    },
    {
      "epoch": 0.00587223963957498,
      "grad_norm": 7.998122692108154,
      "learning_rate": 0.0002982383281081275,
      "loss": 2.9185,
      "step": 1800
    },
    {
      "epoch": 0.006198475175106924,
      "grad_norm": 5.2041144371032715,
      "learning_rate": 0.0002981404574474679,
      "loss": 2.8689,
      "step": 1900
    },
    {
      "epoch": 0.006524710710638867,
      "grad_norm": 7.964056015014648,
      "learning_rate": 0.0002980425867868083,
      "loss": 2.7655,
      "step": 2000
    },
    {
      "epoch": 0.00685094624617081,
      "grad_norm": 12.991459846496582,
      "learning_rate": 0.00029794471612614874,
      "loss": 2.8417,
      "step": 2100
    },
    {
      "epoch": 0.007177181781702754,
      "grad_norm": 8.522321701049805,
      "learning_rate": 0.0002978468454654891,
      "loss": 2.9282,
      "step": 2200
    },
    {
      "epoch": 0.007503417317234697,
      "grad_norm": 13.795225143432617,
      "learning_rate": 0.00029774897480482954,
      "loss": 2.7527,
      "step": 2300
    },
    {
      "epoch": 0.00782965285276664,
      "grad_norm": 8.163244247436523,
      "learning_rate": 0.00029765110414416997,
      "loss": 2.7827,
      "step": 2400
    },
    {
      "epoch": 0.008155888388298584,
      "grad_norm": 6.666696071624756,
      "learning_rate": 0.0002975532334835104,
      "loss": 2.6722,
      "step": 2500
    },
    {
      "epoch": 0.008482123923830527,
      "grad_norm": 7.258597373962402,
      "learning_rate": 0.00029745536282285083,
      "loss": 2.6719,
      "step": 2600
    },
    {
      "epoch": 0.00880835945936247,
      "grad_norm": 6.845671653747559,
      "learning_rate": 0.00029735749216219126,
      "loss": 2.6991,
      "step": 2700
    },
    {
      "epoch": 0.009134594994894414,
      "grad_norm": 9.286894798278809,
      "learning_rate": 0.00029725962150153163,
      "loss": 2.5681,
      "step": 2800
    },
    {
      "epoch": 0.009460830530426357,
      "grad_norm": 9.027726173400879,
      "learning_rate": 0.00029716175084087206,
      "loss": 2.6431,
      "step": 2900
    },
    {
      "epoch": 0.0097870660659583,
      "grad_norm": 2.200766086578369,
      "learning_rate": 0.0002970638801802125,
      "loss": 2.4301,
      "step": 3000
    },
    {
      "epoch": 0.010113301601490245,
      "grad_norm": 10.597944259643555,
      "learning_rate": 0.0002969660095195529,
      "loss": 2.5093,
      "step": 3100
    },
    {
      "epoch": 0.010439537137022187,
      "grad_norm": 48.619632720947266,
      "learning_rate": 0.00029686813885889334,
      "loss": 2.5191,
      "step": 3200
    },
    {
      "epoch": 0.01076577267255413,
      "grad_norm": 15.156477928161621,
      "learning_rate": 0.0002967702681982337,
      "loss": 2.5416,
      "step": 3300
    },
    {
      "epoch": 0.011092008208086075,
      "grad_norm": 14.592612266540527,
      "learning_rate": 0.00029667239753757414,
      "loss": 2.2843,
      "step": 3400
    },
    {
      "epoch": 0.011418243743618018,
      "grad_norm": 14.94902515411377,
      "learning_rate": 0.00029657452687691457,
      "loss": 2.4508,
      "step": 3500
    },
    {
      "epoch": 0.01174447927914996,
      "grad_norm": 9.017059326171875,
      "learning_rate": 0.00029647665621625494,
      "loss": 2.3809,
      "step": 3600
    },
    {
      "epoch": 0.012070714814681903,
      "grad_norm": 8.638809204101562,
      "learning_rate": 0.00029637878555559537,
      "loss": 2.2542,
      "step": 3700
    },
    {
      "epoch": 0.012396950350213848,
      "grad_norm": 18.177249908447266,
      "learning_rate": 0.00029628091489493585,
      "loss": 2.2205,
      "step": 3800
    },
    {
      "epoch": 0.01272318588574579,
      "grad_norm": 13.023262023925781,
      "learning_rate": 0.00029618304423427623,
      "loss": 2.3277,
      "step": 3900
    },
    {
      "epoch": 0.013049421421277733,
      "grad_norm": 6.672982215881348,
      "learning_rate": 0.00029608517357361666,
      "loss": 2.3703,
      "step": 4000
    },
    {
      "epoch": 0.013375656956809678,
      "grad_norm": 11.301607131958008,
      "learning_rate": 0.0002959873029129571,
      "loss": 2.2607,
      "step": 4100
    },
    {
      "epoch": 0.01370189249234162,
      "grad_norm": 9.709244728088379,
      "learning_rate": 0.00029588943225229746,
      "loss": 2.3822,
      "step": 4200
    },
    {
      "epoch": 0.014028128027873564,
      "grad_norm": 7.524989604949951,
      "learning_rate": 0.0002957915615916379,
      "loss": 2.2551,
      "step": 4300
    },
    {
      "epoch": 0.014354363563405508,
      "grad_norm": 4.819920539855957,
      "learning_rate": 0.0002956936909309783,
      "loss": 2.1516,
      "step": 4400
    },
    {
      "epoch": 0.014680599098937451,
      "grad_norm": 26.269519805908203,
      "learning_rate": 0.00029559582027031874,
      "loss": 1.9556,
      "step": 4500
    },
    {
      "epoch": 0.015006834634469394,
      "grad_norm": 16.76308250427246,
      "learning_rate": 0.00029549794960965917,
      "loss": 2.1439,
      "step": 4600
    },
    {
      "epoch": 0.015333070170001338,
      "grad_norm": 8.212871551513672,
      "learning_rate": 0.0002954000789489996,
      "loss": 2.1748,
      "step": 4700
    },
    {
      "epoch": 0.01565930570553328,
      "grad_norm": 7.391061305999756,
      "learning_rate": 0.00029530220828833997,
      "loss": 2.0699,
      "step": 4800
    },
    {
      "epoch": 0.015985541241065226,
      "grad_norm": 21.63833236694336,
      "learning_rate": 0.0002952043376276804,
      "loss": 2.034,
      "step": 4900
    },
    {
      "epoch": 0.01631177677659717,
      "grad_norm": 9.782708168029785,
      "learning_rate": 0.00029510646696702083,
      "loss": 1.9463,
      "step": 5000
    },
    {
      "epoch": 0.01663801231212911,
      "grad_norm": 17.33791732788086,
      "learning_rate": 0.00029500859630636126,
      "loss": 2.1521,
      "step": 5100
    },
    {
      "epoch": 0.016964247847661054,
      "grad_norm": 8.611430168151855,
      "learning_rate": 0.0002949107256457017,
      "loss": 1.8914,
      "step": 5200
    },
    {
      "epoch": 0.017290483383192997,
      "grad_norm": 14.230169296264648,
      "learning_rate": 0.0002948128549850421,
      "loss": 1.9984,
      "step": 5300
    },
    {
      "epoch": 0.01761671891872494,
      "grad_norm": 9.532146453857422,
      "learning_rate": 0.0002947149843243825,
      "loss": 1.7349,
      "step": 5400
    },
    {
      "epoch": 0.017942954454256883,
      "grad_norm": 24.183761596679688,
      "learning_rate": 0.0002946171136637229,
      "loss": 1.9784,
      "step": 5500
    },
    {
      "epoch": 0.01826918998978883,
      "grad_norm": 6.56206750869751,
      "learning_rate": 0.0002945192430030633,
      "loss": 1.959,
      "step": 5600
    },
    {
      "epoch": 0.018595425525320772,
      "grad_norm": 14.036214828491211,
      "learning_rate": 0.0002944213723424037,
      "loss": 1.7847,
      "step": 5700
    },
    {
      "epoch": 0.018921661060852715,
      "grad_norm": 2.075068712234497,
      "learning_rate": 0.00029432350168174414,
      "loss": 1.8808,
      "step": 5800
    },
    {
      "epoch": 0.019247896596384657,
      "grad_norm": 14.17424488067627,
      "learning_rate": 0.00029422563102108457,
      "loss": 1.7397,
      "step": 5900
    },
    {
      "epoch": 0.0195741321319166,
      "grad_norm": 14.760605812072754,
      "learning_rate": 0.000294127760360425,
      "loss": 1.9049,
      "step": 6000
    },
    {
      "epoch": 0.019900367667448543,
      "grad_norm": 18.878379821777344,
      "learning_rate": 0.0002940298896997654,
      "loss": 1.6903,
      "step": 6100
    },
    {
      "epoch": 0.02022660320298049,
      "grad_norm": 17.196327209472656,
      "learning_rate": 0.0002939320190391058,
      "loss": 1.8935,
      "step": 6200
    },
    {
      "epoch": 0.020552838738512432,
      "grad_norm": 14.296989440917969,
      "learning_rate": 0.00029383414837844623,
      "loss": 1.6185,
      "step": 6300
    },
    {
      "epoch": 0.020879074274044375,
      "grad_norm": 28.906002044677734,
      "learning_rate": 0.00029373627771778666,
      "loss": 1.4792,
      "step": 6400
    },
    {
      "epoch": 0.021205309809576318,
      "grad_norm": 1.9703009128570557,
      "learning_rate": 0.0002936384070571271,
      "loss": 1.6686,
      "step": 6500
    },
    {
      "epoch": 0.02153154534510826,
      "grad_norm": 12.036762237548828,
      "learning_rate": 0.0002935405363964675,
      "loss": 1.7159,
      "step": 6600
    },
    {
      "epoch": 0.021857780880640203,
      "grad_norm": 42.32827377319336,
      "learning_rate": 0.00029344266573580794,
      "loss": 1.4455,
      "step": 6700
    },
    {
      "epoch": 0.02218401641617215,
      "grad_norm": 10.970335006713867,
      "learning_rate": 0.0002933447950751483,
      "loss": 1.5489,
      "step": 6800
    },
    {
      "epoch": 0.022510251951704092,
      "grad_norm": 19.00070571899414,
      "learning_rate": 0.00029324692441448874,
      "loss": 1.5494,
      "step": 6900
    },
    {
      "epoch": 0.022836487487236035,
      "grad_norm": 10.164484024047852,
      "learning_rate": 0.00029314905375382917,
      "loss": 1.853,
      "step": 7000
    },
    {
      "epoch": 0.023162723022767978,
      "grad_norm": 12.120329856872559,
      "learning_rate": 0.0002930511830931696,
      "loss": 1.519,
      "step": 7100
    },
    {
      "epoch": 0.02348895855829992,
      "grad_norm": 12.72906494140625,
      "learning_rate": 0.00029295331243251,
      "loss": 1.7392,
      "step": 7200
    },
    {
      "epoch": 0.023815194093831864,
      "grad_norm": 10.558574676513672,
      "learning_rate": 0.00029285544177185045,
      "loss": 1.6727,
      "step": 7300
    },
    {
      "epoch": 0.024141429629363807,
      "grad_norm": 15.793978691101074,
      "learning_rate": 0.0002927575711111908,
      "loss": 1.858,
      "step": 7400
    },
    {
      "epoch": 0.024467665164895753,
      "grad_norm": 14.845444679260254,
      "learning_rate": 0.00029265970045053125,
      "loss": 1.6465,
      "step": 7500
    },
    {
      "epoch": 0.024793900700427696,
      "grad_norm": 19.132694244384766,
      "learning_rate": 0.0002925618297898717,
      "loss": 1.7311,
      "step": 7600
    },
    {
      "epoch": 0.02512013623595964,
      "grad_norm": 0.28846484422683716,
      "learning_rate": 0.00029246395912921206,
      "loss": 1.5154,
      "step": 7700
    },
    {
      "epoch": 0.02544637177149158,
      "grad_norm": 0.668289840221405,
      "learning_rate": 0.0002923660884685525,
      "loss": 1.5209,
      "step": 7800
    },
    {
      "epoch": 0.025772607307023524,
      "grad_norm": 9.002595901489258,
      "learning_rate": 0.0002922682178078929,
      "loss": 1.585,
      "step": 7900
    },
    {
      "epoch": 0.026098842842555467,
      "grad_norm": 6.577146530151367,
      "learning_rate": 0.00029217034714723334,
      "loss": 1.4831,
      "step": 8000
    },
    {
      "epoch": 0.026425078378087413,
      "grad_norm": 20.09491539001465,
      "learning_rate": 0.00029207247648657377,
      "loss": 1.6252,
      "step": 8100
    },
    {
      "epoch": 0.026751313913619356,
      "grad_norm": 1.9483534097671509,
      "learning_rate": 0.00029197460582591414,
      "loss": 1.5633,
      "step": 8200
    },
    {
      "epoch": 0.0270775494491513,
      "grad_norm": 20.898334503173828,
      "learning_rate": 0.00029187673516525457,
      "loss": 1.6094,
      "step": 8300
    },
    {
      "epoch": 0.02740378498468324,
      "grad_norm": 9.518394470214844,
      "learning_rate": 0.000291778864504595,
      "loss": 1.7078,
      "step": 8400
    },
    {
      "epoch": 0.027730020520215184,
      "grad_norm": 8.197165489196777,
      "learning_rate": 0.0002916809938439354,
      "loss": 1.5212,
      "step": 8500
    },
    {
      "epoch": 0.028056256055747127,
      "grad_norm": 28.95721435546875,
      "learning_rate": 0.00029158312318327585,
      "loss": 1.5512,
      "step": 8600
    },
    {
      "epoch": 0.02838249159127907,
      "grad_norm": 19.831682205200195,
      "learning_rate": 0.0002914852525226163,
      "loss": 1.4874,
      "step": 8700
    },
    {
      "epoch": 0.028708727126811016,
      "grad_norm": 12.920631408691406,
      "learning_rate": 0.00029138738186195665,
      "loss": 1.526,
      "step": 8800
    },
    {
      "epoch": 0.02903496266234296,
      "grad_norm": 26.659912109375,
      "learning_rate": 0.0002912895112012971,
      "loss": 1.7593,
      "step": 8900
    },
    {
      "epoch": 0.029361198197874902,
      "grad_norm": 8.316696166992188,
      "learning_rate": 0.0002911916405406375,
      "loss": 1.3653,
      "step": 9000
    },
    {
      "epoch": 0.029687433733406845,
      "grad_norm": 17.08783721923828,
      "learning_rate": 0.00029109376987997794,
      "loss": 1.4558,
      "step": 9100
    },
    {
      "epoch": 0.030013669268938788,
      "grad_norm": 41.963741302490234,
      "learning_rate": 0.00029099589921931837,
      "loss": 1.4813,
      "step": 9200
    },
    {
      "epoch": 0.03033990480447073,
      "grad_norm": 33.76863479614258,
      "learning_rate": 0.0002908980285586588,
      "loss": 1.595,
      "step": 9300
    },
    {
      "epoch": 0.030666140340002677,
      "grad_norm": 21.62820053100586,
      "learning_rate": 0.00029080015789799917,
      "loss": 1.5999,
      "step": 9400
    },
    {
      "epoch": 0.03099237587553462,
      "grad_norm": 20.936370849609375,
      "learning_rate": 0.0002907022872373396,
      "loss": 1.5814,
      "step": 9500
    },
    {
      "epoch": 0.03131861141106656,
      "grad_norm": 15.731407165527344,
      "learning_rate": 0.00029060441657668,
      "loss": 1.4973,
      "step": 9600
    },
    {
      "epoch": 0.031644846946598505,
      "grad_norm": 16.985227584838867,
      "learning_rate": 0.0002905065459160204,
      "loss": 1.5745,
      "step": 9700
    },
    {
      "epoch": 0.03197108248213045,
      "grad_norm": 9.422852516174316,
      "learning_rate": 0.0002904086752553608,
      "loss": 1.3303,
      "step": 9800
    },
    {
      "epoch": 0.03229731801766239,
      "grad_norm": 8.909661293029785,
      "learning_rate": 0.00029031080459470125,
      "loss": 1.4569,
      "step": 9900
    },
    {
      "epoch": 0.03262355355319434,
      "grad_norm": 18.39917755126953,
      "learning_rate": 0.0002902129339340417,
      "loss": 1.5629,
      "step": 10000
    },
    {
      "epoch": 0.032949789088726276,
      "grad_norm": 8.795003890991211,
      "learning_rate": 0.0002901150632733821,
      "loss": 1.3722,
      "step": 10100
    },
    {
      "epoch": 0.03327602462425822,
      "grad_norm": 18.641820907592773,
      "learning_rate": 0.0002900171926127225,
      "loss": 1.4539,
      "step": 10200
    },
    {
      "epoch": 0.03360226015979016,
      "grad_norm": 11.559331893920898,
      "learning_rate": 0.0002899193219520629,
      "loss": 1.6188,
      "step": 10300
    },
    {
      "epoch": 0.03392849569532211,
      "grad_norm": 40.22816848754883,
      "learning_rate": 0.00028982145129140334,
      "loss": 1.1215,
      "step": 10400
    },
    {
      "epoch": 0.034254731230854055,
      "grad_norm": 3.893585443496704,
      "learning_rate": 0.00028972358063074377,
      "loss": 1.4597,
      "step": 10500
    },
    {
      "epoch": 0.034580966766385994,
      "grad_norm": 21.82729148864746,
      "learning_rate": 0.0002896257099700842,
      "loss": 1.414,
      "step": 10600
    },
    {
      "epoch": 0.03490720230191794,
      "grad_norm": 11.542142868041992,
      "learning_rate": 0.0002895278393094246,
      "loss": 1.645,
      "step": 10700
    },
    {
      "epoch": 0.03523343783744988,
      "grad_norm": 0.05034603551030159,
      "learning_rate": 0.000289429968648765,
      "loss": 1.1865,
      "step": 10800
    },
    {
      "epoch": 0.035559673372981826,
      "grad_norm": 9.259086608886719,
      "learning_rate": 0.0002893320979881054,
      "loss": 1.4792,
      "step": 10900
    },
    {
      "epoch": 0.035885908908513765,
      "grad_norm": 33.544071197509766,
      "learning_rate": 0.00028923422732744585,
      "loss": 1.5404,
      "step": 11000
    },
    {
      "epoch": 0.03621214444404571,
      "grad_norm": 35.48256301879883,
      "learning_rate": 0.0002891363566667862,
      "loss": 1.4971,
      "step": 11100
    },
    {
      "epoch": 0.03653837997957766,
      "grad_norm": 6.048948287963867,
      "learning_rate": 0.00028903848600612665,
      "loss": 1.4151,
      "step": 11200
    },
    {
      "epoch": 0.0368646155151096,
      "grad_norm": 21.746925354003906,
      "learning_rate": 0.00028894061534546713,
      "loss": 1.1073,
      "step": 11300
    },
    {
      "epoch": 0.037190851050641544,
      "grad_norm": 6.0632429122924805,
      "learning_rate": 0.0002888427446848075,
      "loss": 1.2683,
      "step": 11400
    },
    {
      "epoch": 0.03751708658617348,
      "grad_norm": 86.25950622558594,
      "learning_rate": 0.00028874487402414794,
      "loss": 1.2166,
      "step": 11500
    },
    {
      "epoch": 0.03784332212170543,
      "grad_norm": 11.696721076965332,
      "learning_rate": 0.00028864700336348836,
      "loss": 1.4962,
      "step": 11600
    },
    {
      "epoch": 0.038169557657237375,
      "grad_norm": 2.3238091468811035,
      "learning_rate": 0.00028854913270282874,
      "loss": 1.3587,
      "step": 11700
    },
    {
      "epoch": 0.038495793192769315,
      "grad_norm": 18.493099212646484,
      "learning_rate": 0.00028845126204216917,
      "loss": 1.5938,
      "step": 11800
    },
    {
      "epoch": 0.03882202872830126,
      "grad_norm": 24.457958221435547,
      "learning_rate": 0.0002883533913815096,
      "loss": 1.2781,
      "step": 11900
    },
    {
      "epoch": 0.0391482642638332,
      "grad_norm": 48.03207015991211,
      "learning_rate": 0.00028825552072085,
      "loss": 1.0175,
      "step": 12000
    },
    {
      "epoch": 0.03947449979936515,
      "grad_norm": 6.651885509490967,
      "learning_rate": 0.00028815765006019045,
      "loss": 1.0202,
      "step": 12100
    },
    {
      "epoch": 0.039800735334897086,
      "grad_norm": 12.055047988891602,
      "learning_rate": 0.0002880597793995308,
      "loss": 1.2426,
      "step": 12200
    },
    {
      "epoch": 0.04012697087042903,
      "grad_norm": 38.46310043334961,
      "learning_rate": 0.00028796190873887125,
      "loss": 1.6367,
      "step": 12300
    },
    {
      "epoch": 0.04045320640596098,
      "grad_norm": 93.03995513916016,
      "learning_rate": 0.0002878640380782117,
      "loss": 1.3197,
      "step": 12400
    },
    {
      "epoch": 0.04077944194149292,
      "grad_norm": 63.53346252441406,
      "learning_rate": 0.0002877661674175521,
      "loss": 1.346,
      "step": 12500
    },
    {
      "epoch": 0.041105677477024864,
      "grad_norm": 11.252411842346191,
      "learning_rate": 0.00028766829675689254,
      "loss": 1.3969,
      "step": 12600
    },
    {
      "epoch": 0.041431913012556804,
      "grad_norm": 29.1763858795166,
      "learning_rate": 0.00028757042609623296,
      "loss": 1.4656,
      "step": 12700
    },
    {
      "epoch": 0.04175814854808875,
      "grad_norm": 19.57012939453125,
      "learning_rate": 0.00028747255543557334,
      "loss": 1.5584,
      "step": 12800
    },
    {
      "epoch": 0.04208438408362069,
      "grad_norm": 33.93960952758789,
      "learning_rate": 0.00028737468477491376,
      "loss": 1.4442,
      "step": 12900
    },
    {
      "epoch": 0.042410619619152636,
      "grad_norm": 6.5122971534729,
      "learning_rate": 0.0002872768141142542,
      "loss": 1.4044,
      "step": 13000
    },
    {
      "epoch": 0.04273685515468458,
      "grad_norm": 13.214008331298828,
      "learning_rate": 0.00028717894345359457,
      "loss": 1.4358,
      "step": 13100
    },
    {
      "epoch": 0.04306309069021652,
      "grad_norm": 2.332191228866577,
      "learning_rate": 0.000287081072792935,
      "loss": 1.0799,
      "step": 13200
    },
    {
      "epoch": 0.04338932622574847,
      "grad_norm": 1.6591358184814453,
      "learning_rate": 0.0002869832021322754,
      "loss": 1.2353,
      "step": 13300
    },
    {
      "epoch": 0.04371556176128041,
      "grad_norm": 22.227231979370117,
      "learning_rate": 0.00028688533147161585,
      "loss": 1.3382,
      "step": 13400
    },
    {
      "epoch": 0.04404179729681235,
      "grad_norm": 18.872299194335938,
      "learning_rate": 0.0002867874608109563,
      "loss": 1.2191,
      "step": 13500
    },
    {
      "epoch": 0.0443680328323443,
      "grad_norm": 26.617042541503906,
      "learning_rate": 0.0002866895901502967,
      "loss": 1.3511,
      "step": 13600
    },
    {
      "epoch": 0.04469426836787624,
      "grad_norm": 15.808451652526855,
      "learning_rate": 0.0002865917194896371,
      "loss": 1.4436,
      "step": 13700
    },
    {
      "epoch": 0.045020503903408185,
      "grad_norm": 12.450569152832031,
      "learning_rate": 0.0002864938488289775,
      "loss": 1.5015,
      "step": 13800
    },
    {
      "epoch": 0.045346739438940124,
      "grad_norm": 21.6400089263916,
      "learning_rate": 0.00028639597816831794,
      "loss": 1.3235,
      "step": 13900
    },
    {
      "epoch": 0.04567297497447207,
      "grad_norm": 11.696040153503418,
      "learning_rate": 0.00028629810750765836,
      "loss": 1.2894,
      "step": 14000
    },
    {
      "epoch": 0.04599921051000401,
      "grad_norm": 29.07622718811035,
      "learning_rate": 0.0002862002368469988,
      "loss": 1.3118,
      "step": 14100
    },
    {
      "epoch": 0.046325446045535956,
      "grad_norm": 10.279265403747559,
      "learning_rate": 0.0002861023661863392,
      "loss": 1.3641,
      "step": 14200
    },
    {
      "epoch": 0.0466516815810679,
      "grad_norm": 0.8727884888648987,
      "learning_rate": 0.0002860044955256796,
      "loss": 1.289,
      "step": 14300
    },
    {
      "epoch": 0.04697791711659984,
      "grad_norm": 26.86601448059082,
      "learning_rate": 0.00028590662486502,
      "loss": 1.3447,
      "step": 14400
    },
    {
      "epoch": 0.04730415265213179,
      "grad_norm": 0.4751681983470917,
      "learning_rate": 0.00028580875420436045,
      "loss": 1.2942,
      "step": 14500
    },
    {
      "epoch": 0.04763038818766373,
      "grad_norm": 20.994956970214844,
      "learning_rate": 0.0002857108835437009,
      "loss": 1.3465,
      "step": 14600
    },
    {
      "epoch": 0.047956623723195674,
      "grad_norm": 2.2297139167785645,
      "learning_rate": 0.0002856130128830413,
      "loss": 1.2358,
      "step": 14700
    },
    {
      "epoch": 0.04828285925872761,
      "grad_norm": 0.18375124037265778,
      "learning_rate": 0.0002855151422223817,
      "loss": 1.4497,
      "step": 14800
    },
    {
      "epoch": 0.04860909479425956,
      "grad_norm": 16.34557342529297,
      "learning_rate": 0.0002854172715617221,
      "loss": 1.2744,
      "step": 14900
    },
    {
      "epoch": 0.048935330329791506,
      "grad_norm": 12.876463890075684,
      "learning_rate": 0.00028531940090106253,
      "loss": 1.1721,
      "step": 15000
    },
    {
      "epoch": 0.049261565865323445,
      "grad_norm": 33.422237396240234,
      "learning_rate": 0.0002852215302404029,
      "loss": 1.4685,
      "step": 15100
    },
    {
      "epoch": 0.04958780140085539,
      "grad_norm": 16.88471221923828,
      "learning_rate": 0.00028512365957974334,
      "loss": 1.0964,
      "step": 15200
    },
    {
      "epoch": 0.04991403693638733,
      "grad_norm": 13.89758014678955,
      "learning_rate": 0.00028502578891908376,
      "loss": 1.1385,
      "step": 15300
    },
    {
      "epoch": 0.05024027247191928,
      "grad_norm": 28.481801986694336,
      "learning_rate": 0.0002849279182584242,
      "loss": 1.1524,
      "step": 15400
    },
    {
      "epoch": 0.050566508007451216,
      "grad_norm": 8.010098457336426,
      "learning_rate": 0.0002848300475977646,
      "loss": 1.1563,
      "step": 15500
    },
    {
      "epoch": 0.05089274354298316,
      "grad_norm": 22.380048751831055,
      "learning_rate": 0.00028473217693710505,
      "loss": 1.3164,
      "step": 15600
    },
    {
      "epoch": 0.05121897907851511,
      "grad_norm": 13.915796279907227,
      "learning_rate": 0.0002846343062764454,
      "loss": 0.893,
      "step": 15700
    },
    {
      "epoch": 0.05154521461404705,
      "grad_norm": 37.198265075683594,
      "learning_rate": 0.00028453643561578585,
      "loss": 1.4691,
      "step": 15800
    },
    {
      "epoch": 0.051871450149578995,
      "grad_norm": 8.91595458984375,
      "learning_rate": 0.0002844385649551263,
      "loss": 1.4532,
      "step": 15900
    },
    {
      "epoch": 0.052197685685110934,
      "grad_norm": 0.20194700360298157,
      "learning_rate": 0.0002843406942944667,
      "loss": 1.3529,
      "step": 16000
    },
    {
      "epoch": 0.05252392122064288,
      "grad_norm": 5.36097526550293,
      "learning_rate": 0.00028424282363380713,
      "loss": 1.3557,
      "step": 16100
    },
    {
      "epoch": 0.052850156756174826,
      "grad_norm": 25.11280059814453,
      "learning_rate": 0.00028414495297314756,
      "loss": 1.0833,
      "step": 16200
    },
    {
      "epoch": 0.053176392291706766,
      "grad_norm": 0.7982428669929504,
      "learning_rate": 0.00028404708231248793,
      "loss": 1.1577,
      "step": 16300
    },
    {
      "epoch": 0.05350262782723871,
      "grad_norm": 0.9203855395317078,
      "learning_rate": 0.00028394921165182836,
      "loss": 1.1555,
      "step": 16400
    },
    {
      "epoch": 0.05382886336277065,
      "grad_norm": 15.74006175994873,
      "learning_rate": 0.0002838513409911688,
      "loss": 1.3878,
      "step": 16500
    },
    {
      "epoch": 0.0541550988983026,
      "grad_norm": 23.421154022216797,
      "learning_rate": 0.0002837534703305092,
      "loss": 1.377,
      "step": 16600
    },
    {
      "epoch": 0.05448133443383454,
      "grad_norm": 26.33360481262207,
      "learning_rate": 0.00028365559966984965,
      "loss": 1.0539,
      "step": 16700
    },
    {
      "epoch": 0.05480756996936648,
      "grad_norm": 82.02877807617188,
      "learning_rate": 0.00028355772900919,
      "loss": 1.1664,
      "step": 16800
    },
    {
      "epoch": 0.05513380550489843,
      "grad_norm": 5.645324230194092,
      "learning_rate": 0.00028345985834853045,
      "loss": 1.4317,
      "step": 16900
    },
    {
      "epoch": 0.05546004104043037,
      "grad_norm": 35.36859893798828,
      "learning_rate": 0.0002833619876878709,
      "loss": 1.2302,
      "step": 17000
    },
    {
      "epoch": 0.055786276575962315,
      "grad_norm": 18.819643020629883,
      "learning_rate": 0.00028326411702721125,
      "loss": 1.2332,
      "step": 17100
    },
    {
      "epoch": 0.056112512111494255,
      "grad_norm": 21.172513961791992,
      "learning_rate": 0.0002831662463665517,
      "loss": 1.3117,
      "step": 17200
    },
    {
      "epoch": 0.0564387476470262,
      "grad_norm": 8.634232521057129,
      "learning_rate": 0.0002830683757058921,
      "loss": 1.1541,
      "step": 17300
    },
    {
      "epoch": 0.05676498318255814,
      "grad_norm": 37.138362884521484,
      "learning_rate": 0.00028297050504523253,
      "loss": 0.9701,
      "step": 17400
    },
    {
      "epoch": 0.057091218718090087,
      "grad_norm": 17.009042739868164,
      "learning_rate": 0.00028287263438457296,
      "loss": 1.2342,
      "step": 17500
    },
    {
      "epoch": 0.05741745425362203,
      "grad_norm": 0.5462512969970703,
      "learning_rate": 0.0002827747637239134,
      "loss": 0.9633,
      "step": 17600
    },
    {
      "epoch": 0.05774368978915397,
      "grad_norm": 3.6213057041168213,
      "learning_rate": 0.00028267689306325376,
      "loss": 0.8178,
      "step": 17700
    },
    {
      "epoch": 0.05806992532468592,
      "grad_norm": 57.650630950927734,
      "learning_rate": 0.0002825790224025942,
      "loss": 1.2123,
      "step": 17800
    },
    {
      "epoch": 0.05839616086021786,
      "grad_norm": 0.49959301948547363,
      "learning_rate": 0.0002824811517419346,
      "loss": 1.4361,
      "step": 17900
    },
    {
      "epoch": 0.058722396395749804,
      "grad_norm": 49.54001235961914,
      "learning_rate": 0.00028238328108127505,
      "loss": 1.3231,
      "step": 18000
    },
    {
      "epoch": 0.05904863193128174,
      "grad_norm": 27.423812866210938,
      "learning_rate": 0.0002822854104206155,
      "loss": 1.2746,
      "step": 18100
    },
    {
      "epoch": 0.05937486746681369,
      "grad_norm": 43.896705627441406,
      "learning_rate": 0.0002821875397599559,
      "loss": 1.0629,
      "step": 18200
    },
    {
      "epoch": 0.059701103002345636,
      "grad_norm": 22.379383087158203,
      "learning_rate": 0.0002820896690992963,
      "loss": 1.0438,
      "step": 18300
    },
    {
      "epoch": 0.060027338537877575,
      "grad_norm": 27.373733520507812,
      "learning_rate": 0.0002819917984386367,
      "loss": 1.0658,
      "step": 18400
    },
    {
      "epoch": 0.06035357407340952,
      "grad_norm": 0.3320990204811096,
      "learning_rate": 0.00028189392777797713,
      "loss": 1.0465,
      "step": 18500
    },
    {
      "epoch": 0.06067980960894146,
      "grad_norm": 28.615583419799805,
      "learning_rate": 0.0002817960571173175,
      "loss": 1.1441,
      "step": 18600
    },
    {
      "epoch": 0.06100604514447341,
      "grad_norm": 10.195792198181152,
      "learning_rate": 0.00028169818645665793,
      "loss": 1.3528,
      "step": 18700
    },
    {
      "epoch": 0.061332280680005354,
      "grad_norm": 67.68102264404297,
      "learning_rate": 0.00028160031579599836,
      "loss": 1.1531,
      "step": 18800
    },
    {
      "epoch": 0.06165851621553729,
      "grad_norm": 0.3807260990142822,
      "learning_rate": 0.0002815024451353388,
      "loss": 1.1254,
      "step": 18900
    },
    {
      "epoch": 0.06198475175106924,
      "grad_norm": 21.88309669494629,
      "learning_rate": 0.0002814045744746792,
      "loss": 1.2246,
      "step": 19000
    },
    {
      "epoch": 0.06231098728660118,
      "grad_norm": 15.452656745910645,
      "learning_rate": 0.0002813067038140196,
      "loss": 1.0748,
      "step": 19100
    },
    {
      "epoch": 0.06263722282213312,
      "grad_norm": 31.634429931640625,
      "learning_rate": 0.00028120883315336,
      "loss": 1.1349,
      "step": 19200
    },
    {
      "epoch": 0.06296345835766506,
      "grad_norm": 0.2220660299062729,
      "learning_rate": 0.00028111096249270045,
      "loss": 0.9922,
      "step": 19300
    },
    {
      "epoch": 0.06328969389319701,
      "grad_norm": 0.7088963389396667,
      "learning_rate": 0.0002810130918320409,
      "loss": 1.1045,
      "step": 19400
    },
    {
      "epoch": 0.06361592942872896,
      "grad_norm": 30.498016357421875,
      "learning_rate": 0.0002809152211713813,
      "loss": 0.883,
      "step": 19500
    },
    {
      "epoch": 0.0639421649642609,
      "grad_norm": 13.445731163024902,
      "learning_rate": 0.00028081735051072173,
      "loss": 0.8861,
      "step": 19600
    },
    {
      "epoch": 0.06426840049979284,
      "grad_norm": 45.09925079345703,
      "learning_rate": 0.0002807194798500621,
      "loss": 1.2733,
      "step": 19700
    },
    {
      "epoch": 0.06459463603532478,
      "grad_norm": 1.2389276027679443,
      "learning_rate": 0.00028062160918940253,
      "loss": 0.9853,
      "step": 19800
    },
    {
      "epoch": 0.06492087157085673,
      "grad_norm": 0.2459385097026825,
      "learning_rate": 0.00028052373852874296,
      "loss": 1.1588,
      "step": 19900
    },
    {
      "epoch": 0.06524710710638867,
      "grad_norm": 48.21909713745117,
      "learning_rate": 0.0002804258678680834,
      "loss": 0.8631,
      "step": 20000
    },
    {
      "epoch": 0.06557334264192062,
      "grad_norm": 0.42907729744911194,
      "learning_rate": 0.0002803279972074238,
      "loss": 0.7937,
      "step": 20100
    },
    {
      "epoch": 0.06589957817745255,
      "grad_norm": 33.61023712158203,
      "learning_rate": 0.00028023012654676424,
      "loss": 1.2389,
      "step": 20200
    },
    {
      "epoch": 0.0662258137129845,
      "grad_norm": 40.7024040222168,
      "learning_rate": 0.0002801322558861046,
      "loss": 1.2466,
      "step": 20300
    },
    {
      "epoch": 0.06655204924851645,
      "grad_norm": 27.107210159301758,
      "learning_rate": 0.00028003438522544504,
      "loss": 1.1244,
      "step": 20400
    },
    {
      "epoch": 0.06687828478404839,
      "grad_norm": 3.7577202320098877,
      "learning_rate": 0.00027993651456478547,
      "loss": 1.122,
      "step": 20500
    },
    {
      "epoch": 0.06720452031958032,
      "grad_norm": 4.608267784118652,
      "learning_rate": 0.00027983864390412585,
      "loss": 1.175,
      "step": 20600
    },
    {
      "epoch": 0.06753075585511227,
      "grad_norm": 21.76932716369629,
      "learning_rate": 0.0002797407732434663,
      "loss": 1.1286,
      "step": 20700
    },
    {
      "epoch": 0.06785699139064422,
      "grad_norm": 20.561908721923828,
      "learning_rate": 0.0002796429025828067,
      "loss": 1.1826,
      "step": 20800
    },
    {
      "epoch": 0.06818322692617616,
      "grad_norm": 4.830076694488525,
      "learning_rate": 0.00027954503192214713,
      "loss": 1.1596,
      "step": 20900
    },
    {
      "epoch": 0.06850946246170811,
      "grad_norm": 24.754255294799805,
      "learning_rate": 0.00027944716126148756,
      "loss": 0.9883,
      "step": 21000
    },
    {
      "epoch": 0.06883569799724004,
      "grad_norm": 3.544297456741333,
      "learning_rate": 0.00027934929060082793,
      "loss": 1.2619,
      "step": 21100
    },
    {
      "epoch": 0.06916193353277199,
      "grad_norm": 0.24251367151737213,
      "learning_rate": 0.00027925141994016836,
      "loss": 1.0731,
      "step": 21200
    },
    {
      "epoch": 0.06948816906830393,
      "grad_norm": 0.01324144471436739,
      "learning_rate": 0.0002791535492795088,
      "loss": 0.892,
      "step": 21300
    },
    {
      "epoch": 0.06981440460383588,
      "grad_norm": 41.454978942871094,
      "learning_rate": 0.0002790556786188492,
      "loss": 1.358,
      "step": 21400
    },
    {
      "epoch": 0.07014064013936783,
      "grad_norm": 0.28227850794792175,
      "learning_rate": 0.00027895780795818964,
      "loss": 1.1063,
      "step": 21500
    },
    {
      "epoch": 0.07046687567489976,
      "grad_norm": 0.9008479714393616,
      "learning_rate": 0.00027885993729753007,
      "loss": 1.0056,
      "step": 21600
    },
    {
      "epoch": 0.0707931112104317,
      "grad_norm": 6.181144714355469,
      "learning_rate": 0.00027876206663687045,
      "loss": 1.2114,
      "step": 21700
    },
    {
      "epoch": 0.07111934674596365,
      "grad_norm": 19.022424697875977,
      "learning_rate": 0.0002786641959762109,
      "loss": 1.2014,
      "step": 21800
    },
    {
      "epoch": 0.0714455822814956,
      "grad_norm": 0.718664824962616,
      "learning_rate": 0.0002785663253155513,
      "loss": 1.0429,
      "step": 21900
    },
    {
      "epoch": 0.07177181781702753,
      "grad_norm": 6.798110485076904,
      "learning_rate": 0.00027846845465489173,
      "loss": 1.315,
      "step": 22000
    },
    {
      "epoch": 0.07209805335255948,
      "grad_norm": 67.55598449707031,
      "learning_rate": 0.00027837058399423216,
      "loss": 1.0788,
      "step": 22100
    },
    {
      "epoch": 0.07242428888809142,
      "grad_norm": 0.09703099727630615,
      "learning_rate": 0.0002782727133335726,
      "loss": 1.0859,
      "step": 22200
    },
    {
      "epoch": 0.07275052442362337,
      "grad_norm": 51.63994598388672,
      "learning_rate": 0.00027817484267291296,
      "loss": 1.0,
      "step": 22300
    },
    {
      "epoch": 0.07307675995915532,
      "grad_norm": 0.9139014482498169,
      "learning_rate": 0.0002780769720122534,
      "loss": 1.5159,
      "step": 22400
    },
    {
      "epoch": 0.07340299549468725,
      "grad_norm": 1.7006953954696655,
      "learning_rate": 0.0002779791013515938,
      "loss": 1.1063,
      "step": 22500
    },
    {
      "epoch": 0.0737292310302192,
      "grad_norm": 51.40180587768555,
      "learning_rate": 0.0002778812306909342,
      "loss": 1.1887,
      "step": 22600
    },
    {
      "epoch": 0.07405546656575114,
      "grad_norm": 27.57472801208496,
      "learning_rate": 0.0002777833600302746,
      "loss": 1.045,
      "step": 22700
    },
    {
      "epoch": 0.07438170210128309,
      "grad_norm": 29.55084800720215,
      "learning_rate": 0.00027768548936961504,
      "loss": 1.0465,
      "step": 22800
    },
    {
      "epoch": 0.07470793763681503,
      "grad_norm": 1.0754873752593994,
      "learning_rate": 0.00027758761870895547,
      "loss": 1.048,
      "step": 22900
    },
    {
      "epoch": 0.07503417317234697,
      "grad_norm": 2.9808497428894043,
      "learning_rate": 0.0002774897480482959,
      "loss": 1.0278,
      "step": 23000
    },
    {
      "epoch": 0.07536040870787891,
      "grad_norm": 6.558430194854736,
      "learning_rate": 0.0002773918773876363,
      "loss": 1.0959,
      "step": 23100
    },
    {
      "epoch": 0.07568664424341086,
      "grad_norm": 0.2769302427768707,
      "learning_rate": 0.0002772940067269767,
      "loss": 0.8305,
      "step": 23200
    },
    {
      "epoch": 0.0760128797789428,
      "grad_norm": 0.14344704151153564,
      "learning_rate": 0.00027719613606631713,
      "loss": 1.1338,
      "step": 23300
    },
    {
      "epoch": 0.07633911531447475,
      "grad_norm": 29.209545135498047,
      "learning_rate": 0.00027709826540565756,
      "loss": 0.9191,
      "step": 23400
    },
    {
      "epoch": 0.07666535085000668,
      "grad_norm": 0.04148480296134949,
      "learning_rate": 0.000277000394744998,
      "loss": 0.8176,
      "step": 23500
    },
    {
      "epoch": 0.07699158638553863,
      "grad_norm": 29.003725051879883,
      "learning_rate": 0.0002769025240843384,
      "loss": 1.0766,
      "step": 23600
    },
    {
      "epoch": 0.07731782192107058,
      "grad_norm": 50.204986572265625,
      "learning_rate": 0.0002768046534236788,
      "loss": 1.1265,
      "step": 23700
    },
    {
      "epoch": 0.07764405745660252,
      "grad_norm": 10.238513946533203,
      "learning_rate": 0.0002767067827630192,
      "loss": 1.3722,
      "step": 23800
    },
    {
      "epoch": 0.07797029299213445,
      "grad_norm": 22.462608337402344,
      "learning_rate": 0.00027660891210235964,
      "loss": 0.894,
      "step": 23900
    },
    {
      "epoch": 0.0782965285276664,
      "grad_norm": 15.029033660888672,
      "learning_rate": 0.00027651104144170007,
      "loss": 1.1886,
      "step": 24000
    },
    {
      "epoch": 0.07862276406319835,
      "grad_norm": 1.202513575553894,
      "learning_rate": 0.0002764131707810405,
      "loss": 1.0393,
      "step": 24100
    },
    {
      "epoch": 0.0789489995987303,
      "grad_norm": 14.507795333862305,
      "learning_rate": 0.0002763153001203809,
      "loss": 1.007,
      "step": 24200
    },
    {
      "epoch": 0.07927523513426224,
      "grad_norm": 3.0511155128479004,
      "learning_rate": 0.0002762174294597213,
      "loss": 1.1159,
      "step": 24300
    },
    {
      "epoch": 0.07960147066979417,
      "grad_norm": 45.052490234375,
      "learning_rate": 0.00027611955879906173,
      "loss": 0.9012,
      "step": 24400
    },
    {
      "epoch": 0.07992770620532612,
      "grad_norm": 30.578611373901367,
      "learning_rate": 0.00027602168813840216,
      "loss": 1.2048,
      "step": 24500
    },
    {
      "epoch": 0.08025394174085806,
      "grad_norm": 14.227283477783203,
      "learning_rate": 0.00027592381747774253,
      "loss": 1.1506,
      "step": 24600
    },
    {
      "epoch": 0.08058017727639001,
      "grad_norm": 2.28771710395813,
      "learning_rate": 0.00027582594681708296,
      "loss": 1.0092,
      "step": 24700
    },
    {
      "epoch": 0.08090641281192196,
      "grad_norm": 2.5681488513946533,
      "learning_rate": 0.0002757280761564234,
      "loss": 1.1245,
      "step": 24800
    },
    {
      "epoch": 0.08123264834745389,
      "grad_norm": 0.12482041120529175,
      "learning_rate": 0.0002756302054957638,
      "loss": 0.8949,
      "step": 24900
    },
    {
      "epoch": 0.08155888388298584,
      "grad_norm": 0.6883585453033447,
      "learning_rate": 0.00027553233483510424,
      "loss": 0.8006,
      "step": 25000
    },
    {
      "epoch": 0.08188511941851778,
      "grad_norm": 24.951692581176758,
      "learning_rate": 0.00027543446417444467,
      "loss": 0.954,
      "step": 25100
    },
    {
      "epoch": 0.08221135495404973,
      "grad_norm": 6.567860126495361,
      "learning_rate": 0.00027533659351378504,
      "loss": 1.0533,
      "step": 25200
    },
    {
      "epoch": 0.08253759048958167,
      "grad_norm": 2.037264585494995,
      "learning_rate": 0.00027523872285312547,
      "loss": 0.9501,
      "step": 25300
    },
    {
      "epoch": 0.08286382602511361,
      "grad_norm": 26.891117095947266,
      "learning_rate": 0.0002751408521924659,
      "loss": 1.0811,
      "step": 25400
    },
    {
      "epoch": 0.08319006156064555,
      "grad_norm": 36.99190902709961,
      "learning_rate": 0.0002750429815318063,
      "loss": 0.8732,
      "step": 25500
    },
    {
      "epoch": 0.0835162970961775,
      "grad_norm": 53.15895080566406,
      "learning_rate": 0.00027494511087114675,
      "loss": 0.9737,
      "step": 25600
    },
    {
      "epoch": 0.08384253263170945,
      "grad_norm": 0.003307789098471403,
      "learning_rate": 0.00027484724021048713,
      "loss": 0.9911,
      "step": 25700
    },
    {
      "epoch": 0.08416876816724138,
      "grad_norm": 0.046538226306438446,
      "learning_rate": 0.00027474936954982756,
      "loss": 0.8494,
      "step": 25800
    },
    {
      "epoch": 0.08449500370277332,
      "grad_norm": 36.04267883300781,
      "learning_rate": 0.000274651498889168,
      "loss": 0.9887,
      "step": 25900
    },
    {
      "epoch": 0.08482123923830527,
      "grad_norm": 0.15577512979507446,
      "learning_rate": 0.00027455362822850836,
      "loss": 1.1662,
      "step": 26000
    },
    {
      "epoch": 0.08514747477383722,
      "grad_norm": 42.24699401855469,
      "learning_rate": 0.0002744557575678488,
      "loss": 0.9461,
      "step": 26100
    },
    {
      "epoch": 0.08547371030936916,
      "grad_norm": 0.12476270645856857,
      "learning_rate": 0.0002743578869071892,
      "loss": 0.8916,
      "step": 26200
    },
    {
      "epoch": 0.0857999458449011,
      "grad_norm": 29.8809871673584,
      "learning_rate": 0.00027426001624652964,
      "loss": 1.1398,
      "step": 26300
    },
    {
      "epoch": 0.08612618138043304,
      "grad_norm": 37.43113327026367,
      "learning_rate": 0.00027416214558587007,
      "loss": 1.1352,
      "step": 26400
    },
    {
      "epoch": 0.08645241691596499,
      "grad_norm": 10.049208641052246,
      "learning_rate": 0.0002740642749252105,
      "loss": 0.7398,
      "step": 26500
    },
    {
      "epoch": 0.08677865245149693,
      "grad_norm": 2.586272954940796,
      "learning_rate": 0.00027396640426455087,
      "loss": 1.1828,
      "step": 26600
    },
    {
      "epoch": 0.08710488798702888,
      "grad_norm": 61.06357955932617,
      "learning_rate": 0.0002738685336038913,
      "loss": 1.1266,
      "step": 26700
    },
    {
      "epoch": 0.08743112352256081,
      "grad_norm": 12.23857593536377,
      "learning_rate": 0.0002737706629432317,
      "loss": 1.0507,
      "step": 26800
    },
    {
      "epoch": 0.08775735905809276,
      "grad_norm": 0.6094986200332642,
      "learning_rate": 0.00027367279228257215,
      "loss": 0.9418,
      "step": 26900
    },
    {
      "epoch": 0.0880835945936247,
      "grad_norm": 2.472295045852661,
      "learning_rate": 0.0002735749216219126,
      "loss": 0.9982,
      "step": 27000
    },
    {
      "epoch": 0.08840983012915665,
      "grad_norm": 9.250852584838867,
      "learning_rate": 0.000273477050961253,
      "loss": 0.9489,
      "step": 27100
    },
    {
      "epoch": 0.0887360656646886,
      "grad_norm": 95.54989624023438,
      "learning_rate": 0.0002733791803005934,
      "loss": 1.2151,
      "step": 27200
    },
    {
      "epoch": 0.08906230120022053,
      "grad_norm": 0.4509666860103607,
      "learning_rate": 0.0002732813096399338,
      "loss": 0.8045,
      "step": 27300
    },
    {
      "epoch": 0.08938853673575248,
      "grad_norm": 0.5726351737976074,
      "learning_rate": 0.00027318343897927424,
      "loss": 0.8214,
      "step": 27400
    },
    {
      "epoch": 0.08971477227128442,
      "grad_norm": 21.90838623046875,
      "learning_rate": 0.00027308556831861467,
      "loss": 0.9662,
      "step": 27500
    },
    {
      "epoch": 0.09004100780681637,
      "grad_norm": 0.7599408030509949,
      "learning_rate": 0.0002729876976579551,
      "loss": 0.8995,
      "step": 27600
    },
    {
      "epoch": 0.0903672433423483,
      "grad_norm": 16.69175148010254,
      "learning_rate": 0.00027288982699729547,
      "loss": 1.0941,
      "step": 27700
    },
    {
      "epoch": 0.09069347887788025,
      "grad_norm": 18.49388313293457,
      "learning_rate": 0.0002727919563366359,
      "loss": 1.1829,
      "step": 27800
    },
    {
      "epoch": 0.0910197144134122,
      "grad_norm": 11.368559837341309,
      "learning_rate": 0.0002726940856759763,
      "loss": 1.1861,
      "step": 27900
    },
    {
      "epoch": 0.09134594994894414,
      "grad_norm": 0.01764008402824402,
      "learning_rate": 0.0002725962150153167,
      "loss": 0.8787,
      "step": 28000
    },
    {
      "epoch": 0.09167218548447609,
      "grad_norm": 1.9764471054077148,
      "learning_rate": 0.0002724983443546571,
      "loss": 0.9928,
      "step": 28100
    },
    {
      "epoch": 0.09199842102000802,
      "grad_norm": 4.833498001098633,
      "learning_rate": 0.00027240047369399755,
      "loss": 1.0666,
      "step": 28200
    },
    {
      "epoch": 0.09232465655553997,
      "grad_norm": 0.015096534974873066,
      "learning_rate": 0.000272302603033338,
      "loss": 0.8798,
      "step": 28300
    },
    {
      "epoch": 0.09265089209107191,
      "grad_norm": 0.7760637998580933,
      "learning_rate": 0.0002722047323726784,
      "loss": 1.0289,
      "step": 28400
    },
    {
      "epoch": 0.09297712762660386,
      "grad_norm": 0.06532804667949677,
      "learning_rate": 0.00027210686171201884,
      "loss": 0.9834,
      "step": 28500
    },
    {
      "epoch": 0.0933033631621358,
      "grad_norm": 12.684985160827637,
      "learning_rate": 0.0002720089910513592,
      "loss": 1.1265,
      "step": 28600
    },
    {
      "epoch": 0.09362959869766774,
      "grad_norm": 0.2482980638742447,
      "learning_rate": 0.00027191112039069964,
      "loss": 0.7754,
      "step": 28700
    },
    {
      "epoch": 0.09395583423319968,
      "grad_norm": 0.01879304088652134,
      "learning_rate": 0.00027181324973004007,
      "loss": 1.3842,
      "step": 28800
    },
    {
      "epoch": 0.09428206976873163,
      "grad_norm": 0.3419126868247986,
      "learning_rate": 0.0002717153790693805,
      "loss": 0.9918,
      "step": 28900
    },
    {
      "epoch": 0.09460830530426358,
      "grad_norm": 0.47520220279693604,
      "learning_rate": 0.0002716175084087209,
      "loss": 0.8531,
      "step": 29000
    },
    {
      "epoch": 0.09493454083979551,
      "grad_norm": 0.41824546456336975,
      "learning_rate": 0.00027151963774806135,
      "loss": 0.9024,
      "step": 29100
    },
    {
      "epoch": 0.09526077637532746,
      "grad_norm": 0.2583673298358917,
      "learning_rate": 0.0002714217670874017,
      "loss": 0.855,
      "step": 29200
    },
    {
      "epoch": 0.0955870119108594,
      "grad_norm": 50.6487922668457,
      "learning_rate": 0.00027132389642674215,
      "loss": 0.782,
      "step": 29300
    },
    {
      "epoch": 0.09591324744639135,
      "grad_norm": 173.21127319335938,
      "learning_rate": 0.0002712260257660826,
      "loss": 1.0434,
      "step": 29400
    },
    {
      "epoch": 0.0962394829819233,
      "grad_norm": 15.911858558654785,
      "learning_rate": 0.000271128155105423,
      "loss": 0.9888,
      "step": 29500
    },
    {
      "epoch": 0.09656571851745523,
      "grad_norm": 1.6791220903396606,
      "learning_rate": 0.00027103028444476344,
      "loss": 0.7853,
      "step": 29600
    },
    {
      "epoch": 0.09689195405298717,
      "grad_norm": 34.197147369384766,
      "learning_rate": 0.0002709324137841038,
      "loss": 1.0358,
      "step": 29700
    },
    {
      "epoch": 0.09721818958851912,
      "grad_norm": 35.369564056396484,
      "learning_rate": 0.00027083454312344424,
      "loss": 0.8493,
      "step": 29800
    },
    {
      "epoch": 0.09754442512405107,
      "grad_norm": 0.5220837593078613,
      "learning_rate": 0.00027073667246278467,
      "loss": 0.8728,
      "step": 29900
    },
    {
      "epoch": 0.09787066065958301,
      "grad_norm": 23.709177017211914,
      "learning_rate": 0.00027063880180212504,
      "loss": 0.9758,
      "step": 30000
    },
    {
      "epoch": 0.09819689619511494,
      "grad_norm": 42.38838195800781,
      "learning_rate": 0.00027054093114146547,
      "loss": 0.7508,
      "step": 30100
    },
    {
      "epoch": 0.09852313173064689,
      "grad_norm": 0.17458191514015198,
      "learning_rate": 0.0002704430604808059,
      "loss": 1.0085,
      "step": 30200
    },
    {
      "epoch": 0.09884936726617884,
      "grad_norm": 0.08133427053689957,
      "learning_rate": 0.0002703451898201463,
      "loss": 0.9376,
      "step": 30300
    },
    {
      "epoch": 0.09917560280171078,
      "grad_norm": 40.17919158935547,
      "learning_rate": 0.00027024731915948675,
      "loss": 1.057,
      "step": 30400
    },
    {
      "epoch": 0.09950183833724273,
      "grad_norm": 0.008564271032810211,
      "learning_rate": 0.0002701494484988272,
      "loss": 0.9792,
      "step": 30500
    },
    {
      "epoch": 0.09982807387277466,
      "grad_norm": 14.749785423278809,
      "learning_rate": 0.00027005157783816755,
      "loss": 1.0385,
      "step": 30600
    },
    {
      "epoch": 0.10015430940830661,
      "grad_norm": 0.2280372977256775,
      "learning_rate": 0.000269953707177508,
      "loss": 0.8869,
      "step": 30700
    },
    {
      "epoch": 0.10048054494383855,
      "grad_norm": 13.744959831237793,
      "learning_rate": 0.0002698558365168484,
      "loss": 0.8657,
      "step": 30800
    },
    {
      "epoch": 0.1008067804793705,
      "grad_norm": 55.917686462402344,
      "learning_rate": 0.00026975796585618884,
      "loss": 0.8452,
      "step": 30900
    },
    {
      "epoch": 0.10113301601490243,
      "grad_norm": 0.22863458096981049,
      "learning_rate": 0.00026966009519552926,
      "loss": 1.2576,
      "step": 31000
    },
    {
      "epoch": 0.10145925155043438,
      "grad_norm": 17.496383666992188,
      "learning_rate": 0.0002695622245348697,
      "loss": 0.9512,
      "step": 31100
    },
    {
      "epoch": 0.10178548708596633,
      "grad_norm": 17.01030731201172,
      "learning_rate": 0.00026946435387421007,
      "loss": 1.0054,
      "step": 31200
    },
    {
      "epoch": 0.10211172262149827,
      "grad_norm": 0.2190493494272232,
      "learning_rate": 0.0002693664832135505,
      "loss": 0.9012,
      "step": 31300
    },
    {
      "epoch": 0.10243795815703022,
      "grad_norm": 29.90163230895996,
      "learning_rate": 0.0002692686125528909,
      "loss": 1.0704,
      "step": 31400
    },
    {
      "epoch": 0.10276419369256215,
      "grad_norm": 0.43932044506073,
      "learning_rate": 0.0002691707418922313,
      "loss": 0.9218,
      "step": 31500
    },
    {
      "epoch": 0.1030904292280941,
      "grad_norm": 0.8827868700027466,
      "learning_rate": 0.0002690728712315718,
      "loss": 1.1328,
      "step": 31600
    },
    {
      "epoch": 0.10341666476362604,
      "grad_norm": 35.78361129760742,
      "learning_rate": 0.0002689750005709122,
      "loss": 0.874,
      "step": 31700
    },
    {
      "epoch": 0.10374290029915799,
      "grad_norm": 38.440338134765625,
      "learning_rate": 0.0002688771299102526,
      "loss": 0.9371,
      "step": 31800
    },
    {
      "epoch": 0.10406913583468994,
      "grad_norm": 0.01126179564744234,
      "learning_rate": 0.000268779259249593,
      "loss": 0.7788,
      "step": 31900
    },
    {
      "epoch": 0.10439537137022187,
      "grad_norm": 17.35552215576172,
      "learning_rate": 0.0002686813885889334,
      "loss": 1.0132,
      "step": 32000
    },
    {
      "epoch": 0.10472160690575381,
      "grad_norm": 5.258221626281738,
      "learning_rate": 0.0002685835179282738,
      "loss": 0.9128,
      "step": 32100
    },
    {
      "epoch": 0.10504784244128576,
      "grad_norm": 0.028323575854301453,
      "learning_rate": 0.00026848564726761424,
      "loss": 1.0487,
      "step": 32200
    },
    {
      "epoch": 0.1053740779768177,
      "grad_norm": 25.910480499267578,
      "learning_rate": 0.00026838777660695467,
      "loss": 0.8625,
      "step": 32300
    },
    {
      "epoch": 0.10570031351234965,
      "grad_norm": 0.004908696282655001,
      "learning_rate": 0.0002682899059462951,
      "loss": 0.7313,
      "step": 32400
    },
    {
      "epoch": 0.10602654904788159,
      "grad_norm": 2.808746814727783,
      "learning_rate": 0.0002681920352856355,
      "loss": 0.9857,
      "step": 32500
    },
    {
      "epoch": 0.10635278458341353,
      "grad_norm": 0.012463758699595928,
      "learning_rate": 0.0002680941646249759,
      "loss": 0.8747,
      "step": 32600
    },
    {
      "epoch": 0.10667902011894548,
      "grad_norm": 22.659809112548828,
      "learning_rate": 0.0002679962939643163,
      "loss": 0.9476,
      "step": 32700
    },
    {
      "epoch": 0.10700525565447742,
      "grad_norm": 92.6870346069336,
      "learning_rate": 0.00026789842330365675,
      "loss": 1.0666,
      "step": 32800
    },
    {
      "epoch": 0.10733149119000936,
      "grad_norm": 0.02594088949263096,
      "learning_rate": 0.0002678005526429972,
      "loss": 0.8725,
      "step": 32900
    },
    {
      "epoch": 0.1076577267255413,
      "grad_norm": 44.49202346801758,
      "learning_rate": 0.0002677026819823376,
      "loss": 0.7176,
      "step": 33000
    },
    {
      "epoch": 0.10798396226107325,
      "grad_norm": 43.862144470214844,
      "learning_rate": 0.00026760481132167803,
      "loss": 0.7667,
      "step": 33100
    },
    {
      "epoch": 0.1083101977966052,
      "grad_norm": 38.588706970214844,
      "learning_rate": 0.0002675069406610184,
      "loss": 0.9306,
      "step": 33200
    },
    {
      "epoch": 0.10863643333213714,
      "grad_norm": 56.592350006103516,
      "learning_rate": 0.00026740907000035884,
      "loss": 0.9792,
      "step": 33300
    },
    {
      "epoch": 0.10896266886766907,
      "grad_norm": 0.09851950407028198,
      "learning_rate": 0.00026731119933969926,
      "loss": 0.9873,
      "step": 33400
    },
    {
      "epoch": 0.10928890440320102,
      "grad_norm": 3.264112710952759,
      "learning_rate": 0.00026721332867903964,
      "loss": 0.7803,
      "step": 33500
    },
    {
      "epoch": 0.10961513993873297,
      "grad_norm": 0.07079252600669861,
      "learning_rate": 0.00026711545801838007,
      "loss": 0.9597,
      "step": 33600
    },
    {
      "epoch": 0.10994137547426491,
      "grad_norm": 0.191428080201149,
      "learning_rate": 0.0002670175873577205,
      "loss": 0.9155,
      "step": 33700
    },
    {
      "epoch": 0.11026761100979686,
      "grad_norm": 50.342987060546875,
      "learning_rate": 0.0002669197166970609,
      "loss": 0.7093,
      "step": 33800
    },
    {
      "epoch": 0.11059384654532879,
      "grad_norm": 0.13759452104568481,
      "learning_rate": 0.00026682184603640135,
      "loss": 0.8801,
      "step": 33900
    },
    {
      "epoch": 0.11092008208086074,
      "grad_norm": 2.6693756580352783,
      "learning_rate": 0.0002667239753757417,
      "loss": 0.7888,
      "step": 34000
    },
    {
      "epoch": 0.11124631761639268,
      "grad_norm": 7.9739990234375,
      "learning_rate": 0.00026662610471508215,
      "loss": 0.9049,
      "step": 34100
    },
    {
      "epoch": 0.11157255315192463,
      "grad_norm": 0.2745260000228882,
      "learning_rate": 0.0002665282340544226,
      "loss": 0.9285,
      "step": 34200
    },
    {
      "epoch": 0.11189878868745658,
      "grad_norm": 38.20699691772461,
      "learning_rate": 0.000266430363393763,
      "loss": 0.9952,
      "step": 34300
    },
    {
      "epoch": 0.11222502422298851,
      "grad_norm": 13.496749877929688,
      "learning_rate": 0.00026633249273310343,
      "loss": 1.1529,
      "step": 34400
    },
    {
      "epoch": 0.11255125975852046,
      "grad_norm": 0.0010594982886686921,
      "learning_rate": 0.00026623462207244386,
      "loss": 0.8605,
      "step": 34500
    },
    {
      "epoch": 0.1128774952940524,
      "grad_norm": 0.09945035725831985,
      "learning_rate": 0.00026613675141178424,
      "loss": 0.8467,
      "step": 34600
    },
    {
      "epoch": 0.11320373082958435,
      "grad_norm": 17.902862548828125,
      "learning_rate": 0.00026603888075112466,
      "loss": 0.7356,
      "step": 34700
    },
    {
      "epoch": 0.11352996636511628,
      "grad_norm": 9.83482551574707,
      "learning_rate": 0.0002659410100904651,
      "loss": 0.8103,
      "step": 34800
    },
    {
      "epoch": 0.11385620190064823,
      "grad_norm": 0.8135719299316406,
      "learning_rate": 0.0002658431394298055,
      "loss": 1.0738,
      "step": 34900
    },
    {
      "epoch": 0.11418243743618017,
      "grad_norm": 29.064096450805664,
      "learning_rate": 0.00026574526876914595,
      "loss": 0.8158,
      "step": 35000
    },
    {
      "epoch": 0.11450867297171212,
      "grad_norm": 6.216747760772705,
      "learning_rate": 0.0002656473981084864,
      "loss": 0.9891,
      "step": 35100
    },
    {
      "epoch": 0.11483490850724407,
      "grad_norm": 0.39347073435783386,
      "learning_rate": 0.00026554952744782675,
      "loss": 0.8144,
      "step": 35200
    },
    {
      "epoch": 0.115161144042776,
      "grad_norm": 0.9450457096099854,
      "learning_rate": 0.0002654516567871672,
      "loss": 0.7428,
      "step": 35300
    },
    {
      "epoch": 0.11548737957830794,
      "grad_norm": 52.840911865234375,
      "learning_rate": 0.0002653537861265076,
      "loss": 0.8392,
      "step": 35400
    },
    {
      "epoch": 0.11581361511383989,
      "grad_norm": 42.433109283447266,
      "learning_rate": 0.000265255915465848,
      "loss": 1.0559,
      "step": 35500
    },
    {
      "epoch": 0.11613985064937184,
      "grad_norm": 21.44585418701172,
      "learning_rate": 0.0002651580448051884,
      "loss": 0.9324,
      "step": 35600
    },
    {
      "epoch": 0.11646608618490378,
      "grad_norm": 1.268885850906372,
      "learning_rate": 0.00026506017414452883,
      "loss": 0.9491,
      "step": 35700
    },
    {
      "epoch": 0.11679232172043572,
      "grad_norm": 26.813108444213867,
      "learning_rate": 0.00026496230348386926,
      "loss": 0.8422,
      "step": 35800
    },
    {
      "epoch": 0.11711855725596766,
      "grad_norm": 3.6151235103607178,
      "learning_rate": 0.0002648644328232097,
      "loss": 0.8698,
      "step": 35900
    },
    {
      "epoch": 0.11744479279149961,
      "grad_norm": 42.73711013793945,
      "learning_rate": 0.0002647665621625501,
      "loss": 1.0355,
      "step": 36000
    },
    {
      "epoch": 0.11777102832703155,
      "grad_norm": 0.024338267743587494,
      "learning_rate": 0.0002646686915018905,
      "loss": 0.8124,
      "step": 36100
    },
    {
      "epoch": 0.11809726386256349,
      "grad_norm": 0.0016975298058241606,
      "learning_rate": 0.0002645708208412309,
      "loss": 0.905,
      "step": 36200
    },
    {
      "epoch": 0.11842349939809543,
      "grad_norm": 0.6885080933570862,
      "learning_rate": 0.00026447295018057135,
      "loss": 0.7219,
      "step": 36300
    },
    {
      "epoch": 0.11874973493362738,
      "grad_norm": 0.43999162316322327,
      "learning_rate": 0.0002643750795199118,
      "loss": 1.0298,
      "step": 36400
    },
    {
      "epoch": 0.11907597046915933,
      "grad_norm": 0.14654219150543213,
      "learning_rate": 0.0002642772088592522,
      "loss": 0.8699,
      "step": 36500
    },
    {
      "epoch": 0.11940220600469127,
      "grad_norm": 0.002939131809398532,
      "learning_rate": 0.0002641793381985926,
      "loss": 0.8022,
      "step": 36600
    },
    {
      "epoch": 0.1197284415402232,
      "grad_norm": 0.13248026371002197,
      "learning_rate": 0.000264081467537933,
      "loss": 0.8591,
      "step": 36700
    },
    {
      "epoch": 0.12005467707575515,
      "grad_norm": 122.09381103515625,
      "learning_rate": 0.00026398359687727343,
      "loss": 0.763,
      "step": 36800
    },
    {
      "epoch": 0.1203809126112871,
      "grad_norm": 0.7461978197097778,
      "learning_rate": 0.00026388572621661386,
      "loss": 0.9735,
      "step": 36900
    },
    {
      "epoch": 0.12070714814681904,
      "grad_norm": 0.003439568215981126,
      "learning_rate": 0.0002637878555559543,
      "loss": 0.7888,
      "step": 37000
    },
    {
      "epoch": 0.12103338368235099,
      "grad_norm": 12.475830078125,
      "learning_rate": 0.0002636899848952947,
      "loss": 0.8063,
      "step": 37100
    },
    {
      "epoch": 0.12135961921788292,
      "grad_norm": 2.3779823780059814,
      "learning_rate": 0.0002635921142346351,
      "loss": 0.5207,
      "step": 37200
    },
    {
      "epoch": 0.12168585475341487,
      "grad_norm": 38.6775016784668,
      "learning_rate": 0.0002634942435739755,
      "loss": 0.909,
      "step": 37300
    },
    {
      "epoch": 0.12201209028894681,
      "grad_norm": 37.46018600463867,
      "learning_rate": 0.00026339637291331595,
      "loss": 0.7076,
      "step": 37400
    },
    {
      "epoch": 0.12233832582447876,
      "grad_norm": 0.6548798680305481,
      "learning_rate": 0.0002632985022526563,
      "loss": 0.7587,
      "step": 37500
    },
    {
      "epoch": 0.12266456136001071,
      "grad_norm": 0.03568272665143013,
      "learning_rate": 0.00026320063159199675,
      "loss": 0.9709,
      "step": 37600
    },
    {
      "epoch": 0.12299079689554264,
      "grad_norm": 1.4337544441223145,
      "learning_rate": 0.0002631027609313372,
      "loss": 0.8844,
      "step": 37700
    },
    {
      "epoch": 0.12331703243107459,
      "grad_norm": 9.331894874572754,
      "learning_rate": 0.0002630048902706776,
      "loss": 0.8766,
      "step": 37800
    },
    {
      "epoch": 0.12364326796660653,
      "grad_norm": 1.8125940561294556,
      "learning_rate": 0.00026290701961001803,
      "loss": 0.8038,
      "step": 37900
    },
    {
      "epoch": 0.12396950350213848,
      "grad_norm": 0.5270215272903442,
      "learning_rate": 0.00026280914894935846,
      "loss": 0.8076,
      "step": 38000
    },
    {
      "epoch": 0.12429573903767041,
      "grad_norm": 0.05480613559484482,
      "learning_rate": 0.00026271127828869883,
      "loss": 0.9629,
      "step": 38100
    },
    {
      "epoch": 0.12462197457320236,
      "grad_norm": 0.0022390291560441256,
      "learning_rate": 0.00026261340762803926,
      "loss": 0.7319,
      "step": 38200
    },
    {
      "epoch": 0.1249482101087343,
      "grad_norm": 0.2956473231315613,
      "learning_rate": 0.0002625155369673797,
      "loss": 0.7832,
      "step": 38300
    },
    {
      "epoch": 0.12527444564426624,
      "grad_norm": 24.69525146484375,
      "learning_rate": 0.0002624176663067201,
      "loss": 0.9922,
      "step": 38400
    },
    {
      "epoch": 0.1256006811797982,
      "grad_norm": 0.0029368684627115726,
      "learning_rate": 0.00026231979564606055,
      "loss": 0.9188,
      "step": 38500
    },
    {
      "epoch": 0.12592691671533013,
      "grad_norm": 4.8425517082214355,
      "learning_rate": 0.0002622219249854009,
      "loss": 0.8193,
      "step": 38600
    },
    {
      "epoch": 0.1262531522508621,
      "grad_norm": 87.04828643798828,
      "learning_rate": 0.00026212405432474135,
      "loss": 0.9654,
      "step": 38700
    },
    {
      "epoch": 0.12657938778639402,
      "grad_norm": 76.08880615234375,
      "learning_rate": 0.0002620261836640818,
      "loss": 0.624,
      "step": 38800
    },
    {
      "epoch": 0.12690562332192595,
      "grad_norm": 0.8497915267944336,
      "learning_rate": 0.00026192831300342215,
      "loss": 0.7755,
      "step": 38900
    },
    {
      "epoch": 0.1272318588574579,
      "grad_norm": 47.195465087890625,
      "learning_rate": 0.0002618304423427626,
      "loss": 0.6354,
      "step": 39000
    },
    {
      "epoch": 0.12755809439298985,
      "grad_norm": 27.281949996948242,
      "learning_rate": 0.00026173257168210306,
      "loss": 0.9599,
      "step": 39100
    },
    {
      "epoch": 0.1278843299285218,
      "grad_norm": 1.457236409187317,
      "learning_rate": 0.00026163470102144343,
      "loss": 0.8395,
      "step": 39200
    },
    {
      "epoch": 0.12821056546405374,
      "grad_norm": 0.35004186630249023,
      "learning_rate": 0.00026153683036078386,
      "loss": 0.7162,
      "step": 39300
    },
    {
      "epoch": 0.12853680099958567,
      "grad_norm": 3.3416125774383545,
      "learning_rate": 0.0002614389597001243,
      "loss": 0.8122,
      "step": 39400
    },
    {
      "epoch": 0.12886303653511763,
      "grad_norm": 0.011894099414348602,
      "learning_rate": 0.00026134108903946466,
      "loss": 0.9599,
      "step": 39500
    },
    {
      "epoch": 0.12918927207064956,
      "grad_norm": 0.0016331912484019995,
      "learning_rate": 0.0002612432183788051,
      "loss": 0.8882,
      "step": 39600
    },
    {
      "epoch": 0.12951550760618152,
      "grad_norm": 20.67342185974121,
      "learning_rate": 0.0002611453477181455,
      "loss": 0.6823,
      "step": 39700
    },
    {
      "epoch": 0.12984174314171346,
      "grad_norm": 53.37139129638672,
      "learning_rate": 0.00026104747705748595,
      "loss": 0.7153,
      "step": 39800
    },
    {
      "epoch": 0.1301679786772454,
      "grad_norm": 5.548326015472412,
      "learning_rate": 0.0002609496063968264,
      "loss": 0.8204,
      "step": 39900
    },
    {
      "epoch": 0.13049421421277735,
      "grad_norm": 0.030702903866767883,
      "learning_rate": 0.0002608517357361668,
      "loss": 0.6553,
      "step": 40000
    },
    {
      "epoch": 0.13082044974830928,
      "grad_norm": 92.86363983154297,
      "learning_rate": 0.0002607538650755072,
      "loss": 0.7121,
      "step": 40100
    },
    {
      "epoch": 0.13114668528384124,
      "grad_norm": 74.867431640625,
      "learning_rate": 0.0002606559944148476,
      "loss": 0.9456,
      "step": 40200
    },
    {
      "epoch": 0.13147292081937317,
      "grad_norm": 32.30998229980469,
      "learning_rate": 0.00026055812375418803,
      "loss": 1.0597,
      "step": 40300
    },
    {
      "epoch": 0.1317991563549051,
      "grad_norm": 0.005946785677224398,
      "learning_rate": 0.00026046025309352846,
      "loss": 0.587,
      "step": 40400
    },
    {
      "epoch": 0.13212539189043707,
      "grad_norm": 0.13859757781028748,
      "learning_rate": 0.0002603623824328689,
      "loss": 0.7228,
      "step": 40500
    },
    {
      "epoch": 0.132451627425969,
      "grad_norm": 0.004090628586709499,
      "learning_rate": 0.00026026451177220926,
      "loss": 0.6743,
      "step": 40600
    },
    {
      "epoch": 0.13277786296150093,
      "grad_norm": 37.76267623901367,
      "learning_rate": 0.0002601666411115497,
      "loss": 0.8182,
      "step": 40700
    },
    {
      "epoch": 0.1331040984970329,
      "grad_norm": 0.23969204723834991,
      "learning_rate": 0.0002600687704508901,
      "loss": 0.7263,
      "step": 40800
    },
    {
      "epoch": 0.13343033403256482,
      "grad_norm": 35.82154083251953,
      "learning_rate": 0.0002599708997902305,
      "loss": 0.6384,
      "step": 40900
    },
    {
      "epoch": 0.13375656956809678,
      "grad_norm": 0.006119224708527327,
      "learning_rate": 0.0002598730291295709,
      "loss": 0.9718,
      "step": 41000
    },
    {
      "epoch": 0.13408280510362872,
      "grad_norm": 18.835294723510742,
      "learning_rate": 0.00025977515846891135,
      "loss": 1.0389,
      "step": 41100
    },
    {
      "epoch": 0.13440904063916065,
      "grad_norm": 5.085014820098877,
      "learning_rate": 0.0002596772878082518,
      "loss": 1.0391,
      "step": 41200
    },
    {
      "epoch": 0.1347352761746926,
      "grad_norm": 0.029212892055511475,
      "learning_rate": 0.0002595794171475922,
      "loss": 1.0348,
      "step": 41300
    },
    {
      "epoch": 0.13506151171022454,
      "grad_norm": 30.158405303955078,
      "learning_rate": 0.00025948154648693263,
      "loss": 0.8171,
      "step": 41400
    },
    {
      "epoch": 0.1353877472457565,
      "grad_norm": 0.014864050783216953,
      "learning_rate": 0.000259383675826273,
      "loss": 1.108,
      "step": 41500
    },
    {
      "epoch": 0.13571398278128843,
      "grad_norm": 14.654562950134277,
      "learning_rate": 0.00025928580516561343,
      "loss": 0.7422,
      "step": 41600
    },
    {
      "epoch": 0.13604021831682037,
      "grad_norm": 0.04862260818481445,
      "learning_rate": 0.00025918793450495386,
      "loss": 0.8126,
      "step": 41700
    },
    {
      "epoch": 0.13636645385235233,
      "grad_norm": 0.03503160923719406,
      "learning_rate": 0.0002590900638442943,
      "loss": 0.8383,
      "step": 41800
    },
    {
      "epoch": 0.13669268938788426,
      "grad_norm": 0.14384394884109497,
      "learning_rate": 0.0002589921931836347,
      "loss": 0.6968,
      "step": 41900
    },
    {
      "epoch": 0.13701892492341622,
      "grad_norm": 51.87143325805664,
      "learning_rate": 0.00025889432252297514,
      "loss": 0.5101,
      "step": 42000
    },
    {
      "epoch": 0.13734516045894815,
      "grad_norm": 0.0775257796049118,
      "learning_rate": 0.0002587964518623155,
      "loss": 0.8894,
      "step": 42100
    },
    {
      "epoch": 0.13767139599448008,
      "grad_norm": 3.1306276321411133,
      "learning_rate": 0.00025869858120165594,
      "loss": 0.7042,
      "step": 42200
    },
    {
      "epoch": 0.13799763153001204,
      "grad_norm": 26.85575294494629,
      "learning_rate": 0.00025860071054099637,
      "loss": 0.532,
      "step": 42300
    },
    {
      "epoch": 0.13832386706554398,
      "grad_norm": 0.011831405572593212,
      "learning_rate": 0.0002585028398803368,
      "loss": 1.0608,
      "step": 42400
    },
    {
      "epoch": 0.13865010260107594,
      "grad_norm": 0.2090371549129486,
      "learning_rate": 0.00025840496921967723,
      "loss": 0.8396,
      "step": 42500
    },
    {
      "epoch": 0.13897633813660787,
      "grad_norm": 24.84945297241211,
      "learning_rate": 0.00025830709855901766,
      "loss": 0.7898,
      "step": 42600
    },
    {
      "epoch": 0.1393025736721398,
      "grad_norm": 0.22553221881389618,
      "learning_rate": 0.00025820922789835803,
      "loss": 0.5313,
      "step": 42700
    },
    {
      "epoch": 0.13962880920767176,
      "grad_norm": 75.73651885986328,
      "learning_rate": 0.00025811135723769846,
      "loss": 0.8493,
      "step": 42800
    },
    {
      "epoch": 0.1399550447432037,
      "grad_norm": 23.06736183166504,
      "learning_rate": 0.00025801348657703883,
      "loss": 0.8082,
      "step": 42900
    },
    {
      "epoch": 0.14028128027873565,
      "grad_norm": 6.113914966583252,
      "learning_rate": 0.00025791561591637926,
      "loss": 0.8334,
      "step": 43000
    },
    {
      "epoch": 0.1406075158142676,
      "grad_norm": 50.749515533447266,
      "learning_rate": 0.0002578177452557197,
      "loss": 0.7584,
      "step": 43100
    },
    {
      "epoch": 0.14093375134979952,
      "grad_norm": 0.027956653386354446,
      "learning_rate": 0.0002577198745950601,
      "loss": 0.9197,
      "step": 43200
    },
    {
      "epoch": 0.14125998688533148,
      "grad_norm": 5.259092330932617,
      "learning_rate": 0.00025762200393440054,
      "loss": 0.874,
      "step": 43300
    },
    {
      "epoch": 0.1415862224208634,
      "grad_norm": 0.29243460297584534,
      "learning_rate": 0.00025752413327374097,
      "loss": 0.9342,
      "step": 43400
    },
    {
      "epoch": 0.14191245795639537,
      "grad_norm": 0.39997321367263794,
      "learning_rate": 0.00025742626261308134,
      "loss": 0.6931,
      "step": 43500
    },
    {
      "epoch": 0.1422386934919273,
      "grad_norm": 78.09674835205078,
      "learning_rate": 0.00025732839195242177,
      "loss": 0.8033,
      "step": 43600
    },
    {
      "epoch": 0.14256492902745924,
      "grad_norm": 0.06796245276927948,
      "learning_rate": 0.0002572305212917622,
      "loss": 0.6506,
      "step": 43700
    },
    {
      "epoch": 0.1428911645629912,
      "grad_norm": 0.05122348293662071,
      "learning_rate": 0.00025713265063110263,
      "loss": 0.8964,
      "step": 43800
    },
    {
      "epoch": 0.14321740009852313,
      "grad_norm": 0.0033649494871497154,
      "learning_rate": 0.00025703477997044306,
      "loss": 1.0546,
      "step": 43900
    },
    {
      "epoch": 0.14354363563405506,
      "grad_norm": 0.07825159281492233,
      "learning_rate": 0.0002569369093097835,
      "loss": 0.8465,
      "step": 44000
    },
    {
      "epoch": 0.14386987116958702,
      "grad_norm": 0.022496003657579422,
      "learning_rate": 0.00025683903864912386,
      "loss": 0.7043,
      "step": 44100
    },
    {
      "epoch": 0.14419610670511895,
      "grad_norm": 75.0525131225586,
      "learning_rate": 0.0002567411679884643,
      "loss": 0.6795,
      "step": 44200
    },
    {
      "epoch": 0.14452234224065091,
      "grad_norm": 38.901493072509766,
      "learning_rate": 0.0002566432973278047,
      "loss": 0.8831,
      "step": 44300
    },
    {
      "epoch": 0.14484857777618285,
      "grad_norm": 0.0029256639536470175,
      "learning_rate": 0.00025654542666714514,
      "loss": 0.6701,
      "step": 44400
    },
    {
      "epoch": 0.14517481331171478,
      "grad_norm": 0.03077578730881214,
      "learning_rate": 0.00025644755600648557,
      "loss": 0.6902,
      "step": 44500
    },
    {
      "epoch": 0.14550104884724674,
      "grad_norm": 7.357793807983398,
      "learning_rate": 0.000256349685345826,
      "loss": 1.014,
      "step": 44600
    },
    {
      "epoch": 0.14582728438277867,
      "grad_norm": 62.656394958496094,
      "learning_rate": 0.00025625181468516637,
      "loss": 0.6055,
      "step": 44700
    },
    {
      "epoch": 0.14615351991831063,
      "grad_norm": 0.27602750062942505,
      "learning_rate": 0.0002561539440245068,
      "loss": 0.855,
      "step": 44800
    },
    {
      "epoch": 0.14647975545384256,
      "grad_norm": 0.2013423591852188,
      "learning_rate": 0.00025605607336384717,
      "loss": 0.8048,
      "step": 44900
    },
    {
      "epoch": 0.1468059909893745,
      "grad_norm": 12.198552131652832,
      "learning_rate": 0.0002559582027031876,
      "loss": 0.5849,
      "step": 45000
    },
    {
      "epoch": 0.14713222652490646,
      "grad_norm": 0.0004248353070579469,
      "learning_rate": 0.00025586033204252803,
      "loss": 0.9578,
      "step": 45100
    },
    {
      "epoch": 0.1474584620604384,
      "grad_norm": 0.0585952028632164,
      "learning_rate": 0.00025576246138186846,
      "loss": 0.6572,
      "step": 45200
    },
    {
      "epoch": 0.14778469759597035,
      "grad_norm": 1.0445767641067505,
      "learning_rate": 0.0002556645907212089,
      "loss": 0.7361,
      "step": 45300
    },
    {
      "epoch": 0.14811093313150228,
      "grad_norm": 5.875891208648682,
      "learning_rate": 0.0002555667200605493,
      "loss": 0.6383,
      "step": 45400
    },
    {
      "epoch": 0.1484371686670342,
      "grad_norm": 0.4892365634441376,
      "learning_rate": 0.0002554688493998897,
      "loss": 0.9134,
      "step": 45500
    },
    {
      "epoch": 0.14876340420256617,
      "grad_norm": 0.002298385603353381,
      "learning_rate": 0.0002553709787392301,
      "loss": 0.814,
      "step": 45600
    },
    {
      "epoch": 0.1490896397380981,
      "grad_norm": 23.128154754638672,
      "learning_rate": 0.00025527310807857054,
      "loss": 0.8697,
      "step": 45700
    },
    {
      "epoch": 0.14941587527363007,
      "grad_norm": 12.821650505065918,
      "learning_rate": 0.00025517523741791097,
      "loss": 0.7767,
      "step": 45800
    },
    {
      "epoch": 0.149742110809162,
      "grad_norm": 0.008516739122569561,
      "learning_rate": 0.0002550773667572514,
      "loss": 0.7594,
      "step": 45900
    },
    {
      "epoch": 0.15006834634469393,
      "grad_norm": 2.8236093521118164,
      "learning_rate": 0.0002549794960965918,
      "loss": 0.8408,
      "step": 46000
    },
    {
      "epoch": 0.1503945818802259,
      "grad_norm": 1.676429033279419,
      "learning_rate": 0.0002548816254359322,
      "loss": 0.8629,
      "step": 46100
    },
    {
      "epoch": 0.15072081741575782,
      "grad_norm": 0.9674299955368042,
      "learning_rate": 0.0002547837547752726,
      "loss": 0.8785,
      "step": 46200
    },
    {
      "epoch": 0.15104705295128978,
      "grad_norm": 0.800984263420105,
      "learning_rate": 0.00025468588411461305,
      "loss": 0.799,
      "step": 46300
    },
    {
      "epoch": 0.15137328848682172,
      "grad_norm": 2.547609329223633,
      "learning_rate": 0.00025458801345395343,
      "loss": 0.6754,
      "step": 46400
    },
    {
      "epoch": 0.15169952402235365,
      "grad_norm": 30.113800048828125,
      "learning_rate": 0.00025449014279329386,
      "loss": 0.5717,
      "step": 46500
    },
    {
      "epoch": 0.1520257595578856,
      "grad_norm": 0.14309357106685638,
      "learning_rate": 0.00025439227213263434,
      "loss": 0.7829,
      "step": 46600
    },
    {
      "epoch": 0.15235199509341754,
      "grad_norm": 1.4449816942214966,
      "learning_rate": 0.0002542944014719747,
      "loss": 0.7768,
      "step": 46700
    },
    {
      "epoch": 0.1526782306289495,
      "grad_norm": 64.48041534423828,
      "learning_rate": 0.00025419653081131514,
      "loss": 0.8577,
      "step": 46800
    },
    {
      "epoch": 0.15300446616448143,
      "grad_norm": 30.37393569946289,
      "learning_rate": 0.00025409866015065557,
      "loss": 0.7521,
      "step": 46900
    },
    {
      "epoch": 0.15333070170001337,
      "grad_norm": 2.963258981704712,
      "learning_rate": 0.00025400078948999594,
      "loss": 0.7715,
      "step": 47000
    },
    {
      "epoch": 0.15365693723554533,
      "grad_norm": 11.825263023376465,
      "learning_rate": 0.00025390291882933637,
      "loss": 0.5771,
      "step": 47100
    },
    {
      "epoch": 0.15398317277107726,
      "grad_norm": 56.3387451171875,
      "learning_rate": 0.0002538050481686768,
      "loss": 0.9024,
      "step": 47200
    },
    {
      "epoch": 0.15430940830660922,
      "grad_norm": 0.15679264068603516,
      "learning_rate": 0.0002537071775080172,
      "loss": 1.085,
      "step": 47300
    },
    {
      "epoch": 0.15463564384214115,
      "grad_norm": 20.484838485717773,
      "learning_rate": 0.00025360930684735765,
      "loss": 0.4945,
      "step": 47400
    },
    {
      "epoch": 0.15496187937767308,
      "grad_norm": 30.641714096069336,
      "learning_rate": 0.000253511436186698,
      "loss": 0.8033,
      "step": 47500
    },
    {
      "epoch": 0.15528811491320504,
      "grad_norm": 0.03411846235394478,
      "learning_rate": 0.00025341356552603845,
      "loss": 0.6544,
      "step": 47600
    },
    {
      "epoch": 0.15561435044873698,
      "grad_norm": 0.013077200390398502,
      "learning_rate": 0.0002533156948653789,
      "loss": 0.7522,
      "step": 47700
    },
    {
      "epoch": 0.1559405859842689,
      "grad_norm": 4.75823974609375,
      "learning_rate": 0.0002532178242047193,
      "loss": 0.6606,
      "step": 47800
    },
    {
      "epoch": 0.15626682151980087,
      "grad_norm": 1.0089069604873657,
      "learning_rate": 0.00025311995354405974,
      "loss": 0.7265,
      "step": 47900
    },
    {
      "epoch": 0.1565930570553328,
      "grad_norm": 0.014851709827780724,
      "learning_rate": 0.00025302208288340017,
      "loss": 0.786,
      "step": 48000
    },
    {
      "epoch": 0.15691929259086476,
      "grad_norm": 67.9179916381836,
      "learning_rate": 0.00025292421222274054,
      "loss": 0.7958,
      "step": 48100
    },
    {
      "epoch": 0.1572455281263967,
      "grad_norm": 0.10516874492168427,
      "learning_rate": 0.00025282634156208097,
      "loss": 0.8885,
      "step": 48200
    },
    {
      "epoch": 0.15757176366192863,
      "grad_norm": 10.838719367980957,
      "learning_rate": 0.0002527284709014214,
      "loss": 0.7705,
      "step": 48300
    },
    {
      "epoch": 0.1578979991974606,
      "grad_norm": 21.68465232849121,
      "learning_rate": 0.00025263060024076177,
      "loss": 1.0285,
      "step": 48400
    },
    {
      "epoch": 0.15822423473299252,
      "grad_norm": 0.5274643898010254,
      "learning_rate": 0.0002525327295801022,
      "loss": 0.9381,
      "step": 48500
    },
    {
      "epoch": 0.15855047026852448,
      "grad_norm": 0.6866881251335144,
      "learning_rate": 0.0002524348589194426,
      "loss": 0.935,
      "step": 48600
    },
    {
      "epoch": 0.1588767058040564,
      "grad_norm": 63.262969970703125,
      "learning_rate": 0.00025233698825878305,
      "loss": 0.9498,
      "step": 48700
    },
    {
      "epoch": 0.15920294133958834,
      "grad_norm": 0.0787758007645607,
      "learning_rate": 0.0002522391175981235,
      "loss": 0.6239,
      "step": 48800
    },
    {
      "epoch": 0.1595291768751203,
      "grad_norm": 0.6596024632453918,
      "learning_rate": 0.0002521412469374639,
      "loss": 0.6443,
      "step": 48900
    },
    {
      "epoch": 0.15985541241065224,
      "grad_norm": 37.8380012512207,
      "learning_rate": 0.0002520433762768043,
      "loss": 0.8328,
      "step": 49000
    },
    {
      "epoch": 0.1601816479461842,
      "grad_norm": 68.84972381591797,
      "learning_rate": 0.0002519455056161447,
      "loss": 0.6082,
      "step": 49100
    },
    {
      "epoch": 0.16050788348171613,
      "grad_norm": 0.014784651808440685,
      "learning_rate": 0.00025184763495548514,
      "loss": 0.569,
      "step": 49200
    },
    {
      "epoch": 0.16083411901724806,
      "grad_norm": 5.180136203765869,
      "learning_rate": 0.00025174976429482557,
      "loss": 0.83,
      "step": 49300
    },
    {
      "epoch": 0.16116035455278002,
      "grad_norm": 0.3304334878921509,
      "learning_rate": 0.000251651893634166,
      "loss": 0.8443,
      "step": 49400
    },
    {
      "epoch": 0.16148659008831195,
      "grad_norm": 0.005056136287748814,
      "learning_rate": 0.00025155402297350637,
      "loss": 0.8532,
      "step": 49500
    },
    {
      "epoch": 0.16181282562384391,
      "grad_norm": 17.418432235717773,
      "learning_rate": 0.0002514561523128468,
      "loss": 0.63,
      "step": 49600
    },
    {
      "epoch": 0.16213906115937585,
      "grad_norm": 38.09611511230469,
      "learning_rate": 0.0002513582816521872,
      "loss": 0.8006,
      "step": 49700
    },
    {
      "epoch": 0.16246529669490778,
      "grad_norm": 0.20729894936084747,
      "learning_rate": 0.00025126041099152765,
      "loss": 0.7233,
      "step": 49800
    },
    {
      "epoch": 0.16279153223043974,
      "grad_norm": 0.03859696909785271,
      "learning_rate": 0.0002511625403308681,
      "loss": 0.578,
      "step": 49900
    },
    {
      "epoch": 0.16311776776597167,
      "grad_norm": 30.423072814941406,
      "learning_rate": 0.0002510646696702085,
      "loss": 0.7919,
      "step": 50000
    },
    {
      "epoch": 0.16344400330150363,
      "grad_norm": 0.01071759033948183,
      "learning_rate": 0.0002509667990095489,
      "loss": 0.8442,
      "step": 50100
    },
    {
      "epoch": 0.16377023883703556,
      "grad_norm": 2.4496827125549316,
      "learning_rate": 0.0002508689283488893,
      "loss": 0.8103,
      "step": 50200
    },
    {
      "epoch": 0.1640964743725675,
      "grad_norm": 90.11176300048828,
      "learning_rate": 0.00025077105768822974,
      "loss": 0.7238,
      "step": 50300
    },
    {
      "epoch": 0.16442270990809946,
      "grad_norm": 0.0020329656545072794,
      "learning_rate": 0.0002506731870275701,
      "loss": 0.6067,
      "step": 50400
    },
    {
      "epoch": 0.1647489454436314,
      "grad_norm": 0.03499223291873932,
      "learning_rate": 0.00025057531636691054,
      "loss": 0.7622,
      "step": 50500
    },
    {
      "epoch": 0.16507518097916335,
      "grad_norm": 35.924861907958984,
      "learning_rate": 0.00025047744570625097,
      "loss": 0.7826,
      "step": 50600
    },
    {
      "epoch": 0.16540141651469528,
      "grad_norm": 0.10688982158899307,
      "learning_rate": 0.0002503795750455914,
      "loss": 0.6154,
      "step": 50700
    },
    {
      "epoch": 0.16572765205022721,
      "grad_norm": 51.88743209838867,
      "learning_rate": 0.0002502817043849318,
      "loss": 0.6632,
      "step": 50800
    },
    {
      "epoch": 0.16605388758575917,
      "grad_norm": 0.03880837559700012,
      "learning_rate": 0.00025018383372427225,
      "loss": 0.5767,
      "step": 50900
    },
    {
      "epoch": 0.1663801231212911,
      "grad_norm": 24.90479278564453,
      "learning_rate": 0.0002500859630636126,
      "loss": 0.8804,
      "step": 51000
    },
    {
      "epoch": 0.16670635865682304,
      "grad_norm": 108.83545684814453,
      "learning_rate": 0.00024998809240295305,
      "loss": 0.7475,
      "step": 51100
    },
    {
      "epoch": 0.167032594192355,
      "grad_norm": 33.1158447265625,
      "learning_rate": 0.0002498902217422935,
      "loss": 0.6436,
      "step": 51200
    },
    {
      "epoch": 0.16735882972788693,
      "grad_norm": 0.0006166860111989081,
      "learning_rate": 0.0002497923510816339,
      "loss": 0.8703,
      "step": 51300
    },
    {
      "epoch": 0.1676850652634189,
      "grad_norm": 1.518578290939331,
      "learning_rate": 0.00024969448042097434,
      "loss": 0.5385,
      "step": 51400
    },
    {
      "epoch": 0.16801130079895082,
      "grad_norm": 0.602270245552063,
      "learning_rate": 0.0002495966097603147,
      "loss": 0.5368,
      "step": 51500
    },
    {
      "epoch": 0.16833753633448276,
      "grad_norm": 1.6131291389465332,
      "learning_rate": 0.00024949873909965514,
      "loss": 0.5117,
      "step": 51600
    },
    {
      "epoch": 0.16866377187001472,
      "grad_norm": 5.678609848022461,
      "learning_rate": 0.00024940086843899557,
      "loss": 0.731,
      "step": 51700
    },
    {
      "epoch": 0.16899000740554665,
      "grad_norm": 23.888219833374023,
      "learning_rate": 0.000249302997778336,
      "loss": 0.6789,
      "step": 51800
    },
    {
      "epoch": 0.1693162429410786,
      "grad_norm": 0.0086422860622406,
      "learning_rate": 0.0002492051271176764,
      "loss": 0.9352,
      "step": 51900
    },
    {
      "epoch": 0.16964247847661054,
      "grad_norm": 0.1382128894329071,
      "learning_rate": 0.00024910725645701685,
      "loss": 0.6659,
      "step": 52000
    },
    {
      "epoch": 0.16996871401214247,
      "grad_norm": 0.0554855577647686,
      "learning_rate": 0.0002490093857963572,
      "loss": 0.6532,
      "step": 52100
    },
    {
      "epoch": 0.17029494954767443,
      "grad_norm": 29.535057067871094,
      "learning_rate": 0.00024891151513569765,
      "loss": 0.6627,
      "step": 52200
    },
    {
      "epoch": 0.17062118508320637,
      "grad_norm": 66.66389465332031,
      "learning_rate": 0.0002488136444750381,
      "loss": 0.5911,
      "step": 52300
    },
    {
      "epoch": 0.17094742061873833,
      "grad_norm": 0.19365961849689484,
      "learning_rate": 0.00024871577381437845,
      "loss": 0.8071,
      "step": 52400
    },
    {
      "epoch": 0.17127365615427026,
      "grad_norm": 0.39181700348854065,
      "learning_rate": 0.0002486179031537189,
      "loss": 0.9766,
      "step": 52500
    },
    {
      "epoch": 0.1715998916898022,
      "grad_norm": 30.96392059326172,
      "learning_rate": 0.0002485200324930593,
      "loss": 0.5283,
      "step": 52600
    },
    {
      "epoch": 0.17192612722533415,
      "grad_norm": 0.000456184265203774,
      "learning_rate": 0.00024842216183239974,
      "loss": 0.8743,
      "step": 52700
    },
    {
      "epoch": 0.17225236276086608,
      "grad_norm": 20.11043930053711,
      "learning_rate": 0.00024832429117174016,
      "loss": 0.7434,
      "step": 52800
    },
    {
      "epoch": 0.17257859829639804,
      "grad_norm": 0.14285516738891602,
      "learning_rate": 0.0002482264205110806,
      "loss": 0.7819,
      "step": 52900
    },
    {
      "epoch": 0.17290483383192998,
      "grad_norm": 50.461883544921875,
      "learning_rate": 0.00024812854985042097,
      "loss": 0.8445,
      "step": 53000
    },
    {
      "epoch": 0.1732310693674619,
      "grad_norm": 0.2128445953130722,
      "learning_rate": 0.0002480306791897614,
      "loss": 0.9614,
      "step": 53100
    },
    {
      "epoch": 0.17355730490299387,
      "grad_norm": 0.004686526954174042,
      "learning_rate": 0.0002479328085291018,
      "loss": 0.6264,
      "step": 53200
    },
    {
      "epoch": 0.1738835404385258,
      "grad_norm": 4.4336161613464355,
      "learning_rate": 0.00024783493786844225,
      "loss": 0.6039,
      "step": 53300
    },
    {
      "epoch": 0.17420977597405776,
      "grad_norm": 35.31013488769531,
      "learning_rate": 0.0002477370672077827,
      "loss": 0.828,
      "step": 53400
    },
    {
      "epoch": 0.1745360115095897,
      "grad_norm": 1.5253808498382568,
      "learning_rate": 0.0002476391965471231,
      "loss": 0.7248,
      "step": 53500
    },
    {
      "epoch": 0.17486224704512163,
      "grad_norm": 45.97465515136719,
      "learning_rate": 0.0002475413258864635,
      "loss": 0.8087,
      "step": 53600
    },
    {
      "epoch": 0.1751884825806536,
      "grad_norm": 26.48416519165039,
      "learning_rate": 0.0002474434552258039,
      "loss": 0.9003,
      "step": 53700
    },
    {
      "epoch": 0.17551471811618552,
      "grad_norm": 0.01588524878025055,
      "learning_rate": 0.0002473455845651443,
      "loss": 0.7657,
      "step": 53800
    },
    {
      "epoch": 0.17584095365171748,
      "grad_norm": 0.004018418490886688,
      "learning_rate": 0.0002472477139044847,
      "loss": 0.8324,
      "step": 53900
    },
    {
      "epoch": 0.1761671891872494,
      "grad_norm": 0.0008187048370018601,
      "learning_rate": 0.00024714984324382514,
      "loss": 0.6162,
      "step": 54000
    },
    {
      "epoch": 0.17649342472278134,
      "grad_norm": 84.70710754394531,
      "learning_rate": 0.00024705197258316556,
      "loss": 0.6624,
      "step": 54100
    },
    {
      "epoch": 0.1768196602583133,
      "grad_norm": 19.225975036621094,
      "learning_rate": 0.000246954101922506,
      "loss": 0.9926,
      "step": 54200
    },
    {
      "epoch": 0.17714589579384524,
      "grad_norm": 0.40060824155807495,
      "learning_rate": 0.0002468562312618464,
      "loss": 0.6111,
      "step": 54300
    },
    {
      "epoch": 0.1774721313293772,
      "grad_norm": 16.484174728393555,
      "learning_rate": 0.0002467583606011868,
      "loss": 0.5693,
      "step": 54400
    },
    {
      "epoch": 0.17779836686490913,
      "grad_norm": 0.04466799646615982,
      "learning_rate": 0.0002466604899405272,
      "loss": 0.9188,
      "step": 54500
    },
    {
      "epoch": 0.17812460240044106,
      "grad_norm": 11.612789154052734,
      "learning_rate": 0.00024656261927986765,
      "loss": 0.6747,
      "step": 54600
    },
    {
      "epoch": 0.17845083793597302,
      "grad_norm": 0.1911955624818802,
      "learning_rate": 0.0002464647486192081,
      "loss": 0.6446,
      "step": 54700
    },
    {
      "epoch": 0.17877707347150495,
      "grad_norm": 0.091948002576828,
      "learning_rate": 0.0002463668779585485,
      "loss": 0.6749,
      "step": 54800
    },
    {
      "epoch": 0.1791033090070369,
      "grad_norm": 2.775101661682129,
      "learning_rate": 0.00024626900729788893,
      "loss": 0.4577,
      "step": 54900
    },
    {
      "epoch": 0.17942954454256885,
      "grad_norm": 0.0006114530842751265,
      "learning_rate": 0.0002461711366372293,
      "loss": 0.5964,
      "step": 55000
    },
    {
      "epoch": 0.17975578007810078,
      "grad_norm": 9.843504905700684,
      "learning_rate": 0.00024607326597656974,
      "loss": 0.7023,
      "step": 55100
    },
    {
      "epoch": 0.18008201561363274,
      "grad_norm": 0.0018784510903060436,
      "learning_rate": 0.00024597539531591016,
      "loss": 0.5277,
      "step": 55200
    },
    {
      "epoch": 0.18040825114916467,
      "grad_norm": 62.28358840942383,
      "learning_rate": 0.0002458775246552506,
      "loss": 0.7025,
      "step": 55300
    },
    {
      "epoch": 0.1807344866846966,
      "grad_norm": 0.08656982332468033,
      "learning_rate": 0.000245779653994591,
      "loss": 0.6492,
      "step": 55400
    },
    {
      "epoch": 0.18106072222022856,
      "grad_norm": 0.0006421782891266048,
      "learning_rate": 0.00024568178333393145,
      "loss": 0.7059,
      "step": 55500
    },
    {
      "epoch": 0.1813869577557605,
      "grad_norm": 64.1048812866211,
      "learning_rate": 0.0002455839126732718,
      "loss": 0.7493,
      "step": 55600
    },
    {
      "epoch": 0.18171319329129246,
      "grad_norm": 6.433559417724609,
      "learning_rate": 0.00024548604201261225,
      "loss": 0.6359,
      "step": 55700
    },
    {
      "epoch": 0.1820394288268244,
      "grad_norm": 0.03827037662267685,
      "learning_rate": 0.0002453881713519526,
      "loss": 0.7682,
      "step": 55800
    },
    {
      "epoch": 0.18236566436235632,
      "grad_norm": 0.06983133405447006,
      "learning_rate": 0.00024529030069129305,
      "loss": 0.8222,
      "step": 55900
    },
    {
      "epoch": 0.18269189989788828,
      "grad_norm": 19.88631248474121,
      "learning_rate": 0.0002451924300306335,
      "loss": 0.5846,
      "step": 56000
    },
    {
      "epoch": 0.18301813543342021,
      "grad_norm": 0.42312005162239075,
      "learning_rate": 0.0002450945593699739,
      "loss": 0.6399,
      "step": 56100
    },
    {
      "epoch": 0.18334437096895218,
      "grad_norm": 1.2861440181732178,
      "learning_rate": 0.00024499668870931433,
      "loss": 0.6505,
      "step": 56200
    },
    {
      "epoch": 0.1836706065044841,
      "grad_norm": 0.0005755466991104186,
      "learning_rate": 0.00024489881804865476,
      "loss": 0.637,
      "step": 56300
    },
    {
      "epoch": 0.18399684204001604,
      "grad_norm": 50.203556060791016,
      "learning_rate": 0.00024480094738799514,
      "loss": 0.5171,
      "step": 56400
    },
    {
      "epoch": 0.184323077575548,
      "grad_norm": 64.254150390625,
      "learning_rate": 0.00024470307672733556,
      "loss": 0.697,
      "step": 56500
    },
    {
      "epoch": 0.18464931311107993,
      "grad_norm": 0.0341501384973526,
      "learning_rate": 0.000244605206066676,
      "loss": 0.3596,
      "step": 56600
    },
    {
      "epoch": 0.1849755486466119,
      "grad_norm": 18.641948699951172,
      "learning_rate": 0.0002445073354060164,
      "loss": 0.8879,
      "step": 56700
    },
    {
      "epoch": 0.18530178418214382,
      "grad_norm": 56.37897491455078,
      "learning_rate": 0.00024440946474535685,
      "loss": 0.6505,
      "step": 56800
    },
    {
      "epoch": 0.18562801971767576,
      "grad_norm": 0.09246095269918442,
      "learning_rate": 0.0002443115940846973,
      "loss": 0.7066,
      "step": 56900
    },
    {
      "epoch": 0.18595425525320772,
      "grad_norm": 0.1925441324710846,
      "learning_rate": 0.00024421372342403765,
      "loss": 0.658,
      "step": 57000
    },
    {
      "epoch": 0.18628049078873965,
      "grad_norm": 0.017697472125291824,
      "learning_rate": 0.00024411585276337808,
      "loss": 0.7031,
      "step": 57100
    },
    {
      "epoch": 0.1866067263242716,
      "grad_norm": 0.039910688996315,
      "learning_rate": 0.0002440179821027185,
      "loss": 1.0737,
      "step": 57200
    },
    {
      "epoch": 0.18693296185980354,
      "grad_norm": 44.44887924194336,
      "learning_rate": 0.0002439201114420589,
      "loss": 0.7195,
      "step": 57300
    },
    {
      "epoch": 0.18725919739533547,
      "grad_norm": 0.009428244084119797,
      "learning_rate": 0.00024382224078139933,
      "loss": 0.4511,
      "step": 57400
    },
    {
      "epoch": 0.18758543293086744,
      "grad_norm": 23.231891632080078,
      "learning_rate": 0.00024372437012073976,
      "loss": 0.729,
      "step": 57500
    },
    {
      "epoch": 0.18791166846639937,
      "grad_norm": 0.007342408876866102,
      "learning_rate": 0.00024362649946008016,
      "loss": 0.7933,
      "step": 57600
    },
    {
      "epoch": 0.18823790400193133,
      "grad_norm": 1.9662688970565796,
      "learning_rate": 0.0002435286287994206,
      "loss": 0.8035,
      "step": 57700
    },
    {
      "epoch": 0.18856413953746326,
      "grad_norm": 46.4290885925293,
      "learning_rate": 0.00024343075813876102,
      "loss": 0.8362,
      "step": 57800
    },
    {
      "epoch": 0.1888903750729952,
      "grad_norm": 0.026490332558751106,
      "learning_rate": 0.00024333288747810142,
      "loss": 0.4867,
      "step": 57900
    },
    {
      "epoch": 0.18921661060852715,
      "grad_norm": 18.22743034362793,
      "learning_rate": 0.00024323501681744185,
      "loss": 0.7077,
      "step": 58000
    },
    {
      "epoch": 0.18954284614405909,
      "grad_norm": 25.687211990356445,
      "learning_rate": 0.00024313714615678225,
      "loss": 0.638,
      "step": 58100
    },
    {
      "epoch": 0.18986908167959102,
      "grad_norm": 0.004331562202423811,
      "learning_rate": 0.00024303927549612267,
      "loss": 0.7713,
      "step": 58200
    },
    {
      "epoch": 0.19019531721512298,
      "grad_norm": 0.039648931473493576,
      "learning_rate": 0.0002429414048354631,
      "loss": 0.4135,
      "step": 58300
    },
    {
      "epoch": 0.1905215527506549,
      "grad_norm": 0.0031834514811635017,
      "learning_rate": 0.00024284353417480348,
      "loss": 0.6894,
      "step": 58400
    },
    {
      "epoch": 0.19084778828618687,
      "grad_norm": 0.04858541861176491,
      "learning_rate": 0.0002427456635141439,
      "loss": 0.742,
      "step": 58500
    },
    {
      "epoch": 0.1911740238217188,
      "grad_norm": 35.56901550292969,
      "learning_rate": 0.00024264779285348436,
      "loss": 0.7197,
      "step": 58600
    },
    {
      "epoch": 0.19150025935725074,
      "grad_norm": 0.010240561328828335,
      "learning_rate": 0.00024254992219282473,
      "loss": 0.6298,
      "step": 58700
    },
    {
      "epoch": 0.1918264948927827,
      "grad_norm": 16.323654174804688,
      "learning_rate": 0.00024245205153216516,
      "loss": 0.8411,
      "step": 58800
    },
    {
      "epoch": 0.19215273042831463,
      "grad_norm": 46.5020751953125,
      "learning_rate": 0.0002423541808715056,
      "loss": 0.8271,
      "step": 58900
    },
    {
      "epoch": 0.1924789659638466,
      "grad_norm": 0.005544507410377264,
      "learning_rate": 0.000242256310210846,
      "loss": 0.5621,
      "step": 59000
    },
    {
      "epoch": 0.19280520149937852,
      "grad_norm": 32.851905822753906,
      "learning_rate": 0.00024215843955018642,
      "loss": 0.5944,
      "step": 59100
    },
    {
      "epoch": 0.19313143703491045,
      "grad_norm": 0.0010897471802309155,
      "learning_rate": 0.00024206056888952685,
      "loss": 0.5609,
      "step": 59200
    },
    {
      "epoch": 0.1934576725704424,
      "grad_norm": 0.7207399606704712,
      "learning_rate": 0.00024196269822886725,
      "loss": 0.5561,
      "step": 59300
    },
    {
      "epoch": 0.19378390810597435,
      "grad_norm": 0.00041668812627904117,
      "learning_rate": 0.00024186482756820767,
      "loss": 0.6029,
      "step": 59400
    },
    {
      "epoch": 0.1941101436415063,
      "grad_norm": 0.041231341660022736,
      "learning_rate": 0.0002417669569075481,
      "loss": 0.5564,
      "step": 59500
    },
    {
      "epoch": 0.19443637917703824,
      "grad_norm": 5.821399211883545,
      "learning_rate": 0.0002416690862468885,
      "loss": 0.9016,
      "step": 59600
    },
    {
      "epoch": 0.19476261471257017,
      "grad_norm": 0.0013255482772365212,
      "learning_rate": 0.00024157121558622893,
      "loss": 0.7204,
      "step": 59700
    },
    {
      "epoch": 0.19508885024810213,
      "grad_norm": 0.0010350766824558377,
      "learning_rate": 0.00024147334492556936,
      "loss": 0.6795,
      "step": 59800
    },
    {
      "epoch": 0.19541508578363406,
      "grad_norm": 46.973907470703125,
      "learning_rate": 0.00024137547426490976,
      "loss": 0.6514,
      "step": 59900
    },
    {
      "epoch": 0.19574132131916602,
      "grad_norm": 0.19428177177906036,
      "learning_rate": 0.0002412776036042502,
      "loss": 0.6742,
      "step": 60000
    },
    {
      "epoch": 0.19606755685469796,
      "grad_norm": 90.77509307861328,
      "learning_rate": 0.00024117973294359062,
      "loss": 0.8005,
      "step": 60100
    },
    {
      "epoch": 0.1963937923902299,
      "grad_norm": 0.03805287927389145,
      "learning_rate": 0.00024108186228293102,
      "loss": 0.5795,
      "step": 60200
    },
    {
      "epoch": 0.19672002792576185,
      "grad_norm": 58.081214904785156,
      "learning_rate": 0.00024098399162227144,
      "loss": 0.5181,
      "step": 60300
    },
    {
      "epoch": 0.19704626346129378,
      "grad_norm": 0.11093442887067795,
      "learning_rate": 0.00024088612096161182,
      "loss": 0.666,
      "step": 60400
    },
    {
      "epoch": 0.19737249899682574,
      "grad_norm": 54.8184700012207,
      "learning_rate": 0.00024078825030095225,
      "loss": 0.54,
      "step": 60500
    },
    {
      "epoch": 0.19769873453235767,
      "grad_norm": 137.7963104248047,
      "learning_rate": 0.00024069037964029267,
      "loss": 0.7899,
      "step": 60600
    },
    {
      "epoch": 0.1980249700678896,
      "grad_norm": 0.00869455561041832,
      "learning_rate": 0.00024059250897963307,
      "loss": 0.78,
      "step": 60700
    },
    {
      "epoch": 0.19835120560342157,
      "grad_norm": 24.68142318725586,
      "learning_rate": 0.0002404946383189735,
      "loss": 1.0426,
      "step": 60800
    },
    {
      "epoch": 0.1986774411389535,
      "grad_norm": 14.502745628356934,
      "learning_rate": 0.00024039676765831393,
      "loss": 0.7632,
      "step": 60900
    },
    {
      "epoch": 0.19900367667448546,
      "grad_norm": 0.5976234674453735,
      "learning_rate": 0.00024029889699765433,
      "loss": 0.6745,
      "step": 61000
    },
    {
      "epoch": 0.1993299122100174,
      "grad_norm": 0.36299455165863037,
      "learning_rate": 0.00024020102633699476,
      "loss": 0.5635,
      "step": 61100
    },
    {
      "epoch": 0.19965614774554932,
      "grad_norm": 24.401052474975586,
      "learning_rate": 0.0002401031556763352,
      "loss": 0.5644,
      "step": 61200
    },
    {
      "epoch": 0.19998238328108128,
      "grad_norm": 0.658366322517395,
      "learning_rate": 0.0002400052850156756,
      "loss": 0.8482,
      "step": 61300
    },
    {
      "epoch": 0.20030861881661322,
      "grad_norm": 0.2837980389595032,
      "learning_rate": 0.00023990741435501602,
      "loss": 0.8605,
      "step": 61400
    },
    {
      "epoch": 0.20063485435214518,
      "grad_norm": 0.8432654738426208,
      "learning_rate": 0.00023980954369435644,
      "loss": 0.4671,
      "step": 61500
    },
    {
      "epoch": 0.2009610898876771,
      "grad_norm": 15.391473770141602,
      "learning_rate": 0.00023971167303369684,
      "loss": 0.7005,
      "step": 61600
    },
    {
      "epoch": 0.20128732542320904,
      "grad_norm": 0.22842076420783997,
      "learning_rate": 0.00023961380237303727,
      "loss": 0.7972,
      "step": 61700
    },
    {
      "epoch": 0.201613560958741,
      "grad_norm": 0.008410696871578693,
      "learning_rate": 0.0002395159317123777,
      "loss": 0.5946,
      "step": 61800
    },
    {
      "epoch": 0.20193979649427293,
      "grad_norm": 34.1750373840332,
      "learning_rate": 0.0002394180610517181,
      "loss": 0.7675,
      "step": 61900
    },
    {
      "epoch": 0.20226603202980487,
      "grad_norm": 0.004770937841385603,
      "learning_rate": 0.00023932019039105853,
      "loss": 0.4459,
      "step": 62000
    },
    {
      "epoch": 0.20259226756533683,
      "grad_norm": 29.688337326049805,
      "learning_rate": 0.00023922231973039896,
      "loss": 0.4051,
      "step": 62100
    },
    {
      "epoch": 0.20291850310086876,
      "grad_norm": 0.0020820433273911476,
      "learning_rate": 0.00023912444906973933,
      "loss": 0.7206,
      "step": 62200
    },
    {
      "epoch": 0.20324473863640072,
      "grad_norm": 0.1286206990480423,
      "learning_rate": 0.00023902657840907976,
      "loss": 0.7204,
      "step": 62300
    },
    {
      "epoch": 0.20357097417193265,
      "grad_norm": 19.440641403198242,
      "learning_rate": 0.00023892870774842016,
      "loss": 0.4888,
      "step": 62400
    },
    {
      "epoch": 0.20389720970746458,
      "grad_norm": 2.5752758979797363,
      "learning_rate": 0.0002388308370877606,
      "loss": 0.6873,
      "step": 62500
    },
    {
      "epoch": 0.20422344524299654,
      "grad_norm": 0.019698260352015495,
      "learning_rate": 0.00023873296642710102,
      "loss": 0.4701,
      "step": 62600
    },
    {
      "epoch": 0.20454968077852848,
      "grad_norm": 55.19300842285156,
      "learning_rate": 0.00023863509576644142,
      "loss": 0.5313,
      "step": 62700
    },
    {
      "epoch": 0.20487591631406044,
      "grad_norm": 62.61351013183594,
      "learning_rate": 0.00023853722510578184,
      "loss": 0.7049,
      "step": 62800
    },
    {
      "epoch": 0.20520215184959237,
      "grad_norm": 26.69808006286621,
      "learning_rate": 0.00023843935444512227,
      "loss": 0.499,
      "step": 62900
    },
    {
      "epoch": 0.2055283873851243,
      "grad_norm": 67.17281341552734,
      "learning_rate": 0.00023834148378446267,
      "loss": 0.5249,
      "step": 63000
    },
    {
      "epoch": 0.20585462292065626,
      "grad_norm": 0.4259870648384094,
      "learning_rate": 0.0002382436131238031,
      "loss": 0.5244,
      "step": 63100
    },
    {
      "epoch": 0.2061808584561882,
      "grad_norm": 0.01368043478578329,
      "learning_rate": 0.00023814574246314353,
      "loss": 0.6836,
      "step": 63200
    },
    {
      "epoch": 0.20650709399172015,
      "grad_norm": 12.337038040161133,
      "learning_rate": 0.00023804787180248393,
      "loss": 0.7349,
      "step": 63300
    },
    {
      "epoch": 0.20683332952725209,
      "grad_norm": 0.34748730063438416,
      "learning_rate": 0.00023795000114182436,
      "loss": 0.588,
      "step": 63400
    },
    {
      "epoch": 0.20715956506278402,
      "grad_norm": 26.49667739868164,
      "learning_rate": 0.00023785213048116479,
      "loss": 0.7148,
      "step": 63500
    },
    {
      "epoch": 0.20748580059831598,
      "grad_norm": 7.444369839504361e-05,
      "learning_rate": 0.00023775425982050519,
      "loss": 0.8126,
      "step": 63600
    },
    {
      "epoch": 0.2078120361338479,
      "grad_norm": 0.0031948720570653677,
      "learning_rate": 0.00023765638915984561,
      "loss": 0.5926,
      "step": 63700
    },
    {
      "epoch": 0.20813827166937987,
      "grad_norm": 0.04429652914404869,
      "learning_rate": 0.00023755851849918604,
      "loss": 0.5249,
      "step": 63800
    },
    {
      "epoch": 0.2084645072049118,
      "grad_norm": 0.06623438745737076,
      "learning_rate": 0.00023746064783852644,
      "loss": 0.5621,
      "step": 63900
    },
    {
      "epoch": 0.20879074274044374,
      "grad_norm": 0.004287688992917538,
      "learning_rate": 0.00023736277717786687,
      "loss": 0.4932,
      "step": 64000
    },
    {
      "epoch": 0.2091169782759757,
      "grad_norm": 0.007224659435451031,
      "learning_rate": 0.0002372649065172073,
      "loss": 0.5182,
      "step": 64100
    },
    {
      "epoch": 0.20944321381150763,
      "grad_norm": 0.011548702605068684,
      "learning_rate": 0.00023716703585654767,
      "loss": 0.6994,
      "step": 64200
    },
    {
      "epoch": 0.2097694493470396,
      "grad_norm": 0.017804637551307678,
      "learning_rate": 0.0002370691651958881,
      "loss": 0.6446,
      "step": 64300
    },
    {
      "epoch": 0.21009568488257152,
      "grad_norm": 2.096611261367798,
      "learning_rate": 0.00023697129453522853,
      "loss": 0.5565,
      "step": 64400
    },
    {
      "epoch": 0.21042192041810345,
      "grad_norm": 0.7056823372840881,
      "learning_rate": 0.00023687342387456893,
      "loss": 0.544,
      "step": 64500
    },
    {
      "epoch": 0.2107481559536354,
      "grad_norm": 20.83892059326172,
      "learning_rate": 0.00023677555321390936,
      "loss": 0.5561,
      "step": 64600
    },
    {
      "epoch": 0.21107439148916735,
      "grad_norm": 4.246652603149414,
      "learning_rate": 0.00023667768255324976,
      "loss": 0.7452,
      "step": 64700
    },
    {
      "epoch": 0.2114006270246993,
      "grad_norm": 0.001870257081463933,
      "learning_rate": 0.00023657981189259019,
      "loss": 0.5091,
      "step": 64800
    },
    {
      "epoch": 0.21172686256023124,
      "grad_norm": 0.004944159183651209,
      "learning_rate": 0.0002364819412319306,
      "loss": 0.7911,
      "step": 64900
    },
    {
      "epoch": 0.21205309809576317,
      "grad_norm": 3.838306427001953,
      "learning_rate": 0.00023638407057127101,
      "loss": 0.8001,
      "step": 65000
    },
    {
      "epoch": 0.21237933363129513,
      "grad_norm": 23.989160537719727,
      "learning_rate": 0.00023628619991061144,
      "loss": 0.5328,
      "step": 65100
    },
    {
      "epoch": 0.21270556916682706,
      "grad_norm": 0.08120360225439072,
      "learning_rate": 0.00023618832924995187,
      "loss": 0.41,
      "step": 65200
    },
    {
      "epoch": 0.213031804702359,
      "grad_norm": 40.1209716796875,
      "learning_rate": 0.00023609045858929227,
      "loss": 0.7839,
      "step": 65300
    },
    {
      "epoch": 0.21335804023789096,
      "grad_norm": 0.03412441536784172,
      "learning_rate": 0.0002359925879286327,
      "loss": 0.7812,
      "step": 65400
    },
    {
      "epoch": 0.2136842757734229,
      "grad_norm": 0.4463437795639038,
      "learning_rate": 0.00023589471726797313,
      "loss": 0.8737,
      "step": 65500
    },
    {
      "epoch": 0.21401051130895485,
      "grad_norm": 1.6592692136764526,
      "learning_rate": 0.00023579684660731353,
      "loss": 0.4917,
      "step": 65600
    },
    {
      "epoch": 0.21433674684448678,
      "grad_norm": 0.04417560622096062,
      "learning_rate": 0.00023569897594665396,
      "loss": 0.6777,
      "step": 65700
    },
    {
      "epoch": 0.2146629823800187,
      "grad_norm": 0.048132844269275665,
      "learning_rate": 0.00023560110528599438,
      "loss": 0.5742,
      "step": 65800
    },
    {
      "epoch": 0.21498921791555067,
      "grad_norm": 1.937349557876587,
      "learning_rate": 0.00023550323462533476,
      "loss": 0.6311,
      "step": 65900
    },
    {
      "epoch": 0.2153154534510826,
      "grad_norm": 1.6800713539123535,
      "learning_rate": 0.00023540536396467518,
      "loss": 0.7839,
      "step": 66000
    },
    {
      "epoch": 0.21564168898661457,
      "grad_norm": 65.709228515625,
      "learning_rate": 0.00023530749330401564,
      "loss": 0.5473,
      "step": 66100
    },
    {
      "epoch": 0.2159679245221465,
      "grad_norm": 0.0013344883918762207,
      "learning_rate": 0.000235209622643356,
      "loss": 0.8281,
      "step": 66200
    },
    {
      "epoch": 0.21629416005767843,
      "grad_norm": 0.022879868745803833,
      "learning_rate": 0.00023511175198269644,
      "loss": 0.7192,
      "step": 66300
    },
    {
      "epoch": 0.2166203955932104,
      "grad_norm": 33.93933868408203,
      "learning_rate": 0.00023501388132203687,
      "loss": 0.6746,
      "step": 66400
    },
    {
      "epoch": 0.21694663112874232,
      "grad_norm": 0.010010935366153717,
      "learning_rate": 0.00023491601066137727,
      "loss": 0.8197,
      "step": 66500
    },
    {
      "epoch": 0.21727286666427428,
      "grad_norm": 0.5835222601890564,
      "learning_rate": 0.0002348181400007177,
      "loss": 0.6827,
      "step": 66600
    },
    {
      "epoch": 0.21759910219980622,
      "grad_norm": 0.08022863417863846,
      "learning_rate": 0.0002347202693400581,
      "loss": 0.7215,
      "step": 66700
    },
    {
      "epoch": 0.21792533773533815,
      "grad_norm": 0.0020756670273840427,
      "learning_rate": 0.00023462239867939853,
      "loss": 0.7847,
      "step": 66800
    },
    {
      "epoch": 0.2182515732708701,
      "grad_norm": 0.08484912663698196,
      "learning_rate": 0.00023452452801873895,
      "loss": 0.7681,
      "step": 66900
    },
    {
      "epoch": 0.21857780880640204,
      "grad_norm": 0.02903040312230587,
      "learning_rate": 0.00023442665735807936,
      "loss": 0.7026,
      "step": 67000
    },
    {
      "epoch": 0.218904044341934,
      "grad_norm": 0.009386898949742317,
      "learning_rate": 0.00023432878669741978,
      "loss": 0.6761,
      "step": 67100
    },
    {
      "epoch": 0.21923027987746593,
      "grad_norm": 1.200405478477478,
      "learning_rate": 0.0002342309160367602,
      "loss": 0.6937,
      "step": 67200
    },
    {
      "epoch": 0.21955651541299787,
      "grad_norm": 12.774128913879395,
      "learning_rate": 0.0002341330453761006,
      "loss": 0.623,
      "step": 67300
    },
    {
      "epoch": 0.21988275094852983,
      "grad_norm": 0.0007372902473434806,
      "learning_rate": 0.00023403517471544104,
      "loss": 0.6342,
      "step": 67400
    },
    {
      "epoch": 0.22020898648406176,
      "grad_norm": 0.15519654750823975,
      "learning_rate": 0.00023393730405478147,
      "loss": 0.8371,
      "step": 67500
    },
    {
      "epoch": 0.22053522201959372,
      "grad_norm": 0.527989387512207,
      "learning_rate": 0.00023383943339412187,
      "loss": 0.522,
      "step": 67600
    },
    {
      "epoch": 0.22086145755512565,
      "grad_norm": 0.00047605240251868963,
      "learning_rate": 0.0002337415627334623,
      "loss": 0.6146,
      "step": 67700
    },
    {
      "epoch": 0.22118769309065758,
      "grad_norm": 0.02637333795428276,
      "learning_rate": 0.00023364369207280272,
      "loss": 0.7176,
      "step": 67800
    },
    {
      "epoch": 0.22151392862618954,
      "grad_norm": 2.71433687210083,
      "learning_rate": 0.0002335458214121431,
      "loss": 0.7037,
      "step": 67900
    },
    {
      "epoch": 0.22184016416172148,
      "grad_norm": 18.21495819091797,
      "learning_rate": 0.00023344795075148353,
      "loss": 0.6834,
      "step": 68000
    },
    {
      "epoch": 0.22216639969725344,
      "grad_norm": 0.03873078152537346,
      "learning_rate": 0.00023335008009082395,
      "loss": 0.4705,
      "step": 68100
    },
    {
      "epoch": 0.22249263523278537,
      "grad_norm": 41.41792678833008,
      "learning_rate": 0.00023325220943016435,
      "loss": 0.5539,
      "step": 68200
    },
    {
      "epoch": 0.2228188707683173,
      "grad_norm": 0.2500118315219879,
      "learning_rate": 0.00023315433876950478,
      "loss": 0.4242,
      "step": 68300
    },
    {
      "epoch": 0.22314510630384926,
      "grad_norm": 26.412620544433594,
      "learning_rate": 0.0002330564681088452,
      "loss": 0.9166,
      "step": 68400
    },
    {
      "epoch": 0.2234713418393812,
      "grad_norm": 63.798118591308594,
      "learning_rate": 0.0002329585974481856,
      "loss": 0.6573,
      "step": 68500
    },
    {
      "epoch": 0.22379757737491315,
      "grad_norm": 0.015986956655979156,
      "learning_rate": 0.00023286072678752604,
      "loss": 0.681,
      "step": 68600
    },
    {
      "epoch": 0.22412381291044509,
      "grad_norm": 0.0025880641769617796,
      "learning_rate": 0.00023276285612686647,
      "loss": 0.6512,
      "step": 68700
    },
    {
      "epoch": 0.22445004844597702,
      "grad_norm": 67.61273956298828,
      "learning_rate": 0.00023266498546620687,
      "loss": 0.5098,
      "step": 68800
    },
    {
      "epoch": 0.22477628398150898,
      "grad_norm": 1.4941092729568481,
      "learning_rate": 0.0002325671148055473,
      "loss": 0.5129,
      "step": 68900
    },
    {
      "epoch": 0.2251025195170409,
      "grad_norm": 0.012930184602737427,
      "learning_rate": 0.0002324692441448877,
      "loss": 0.8591,
      "step": 69000
    },
    {
      "epoch": 0.22542875505257284,
      "grad_norm": 1.247987985610962,
      "learning_rate": 0.00023237137348422812,
      "loss": 0.5947,
      "step": 69100
    },
    {
      "epoch": 0.2257549905881048,
      "grad_norm": 23.028127670288086,
      "learning_rate": 0.00023227350282356855,
      "loss": 0.5793,
      "step": 69200
    },
    {
      "epoch": 0.22608122612363674,
      "grad_norm": 3.202188014984131,
      "learning_rate": 0.00023217563216290895,
      "loss": 0.4438,
      "step": 69300
    },
    {
      "epoch": 0.2264074616591687,
      "grad_norm": 0.007207653485238552,
      "learning_rate": 0.00023207776150224938,
      "loss": 0.6384,
      "step": 69400
    },
    {
      "epoch": 0.22673369719470063,
      "grad_norm": 0.007256270386278629,
      "learning_rate": 0.0002319798908415898,
      "loss": 0.7373,
      "step": 69500
    },
    {
      "epoch": 0.22705993273023256,
      "grad_norm": 0.693688154220581,
      "learning_rate": 0.00023188202018093018,
      "loss": 0.6773,
      "step": 69600
    },
    {
      "epoch": 0.22738616826576452,
      "grad_norm": 0.3190873861312866,
      "learning_rate": 0.0002317841495202706,
      "loss": 0.5227,
      "step": 69700
    },
    {
      "epoch": 0.22771240380129645,
      "grad_norm": 0.0003608274273574352,
      "learning_rate": 0.00023168627885961104,
      "loss": 0.52,
      "step": 69800
    },
    {
      "epoch": 0.2280386393368284,
      "grad_norm": 0.04027827829122543,
      "learning_rate": 0.00023158840819895144,
      "loss": 0.8245,
      "step": 69900
    },
    {
      "epoch": 0.22836487487236035,
      "grad_norm": 49.27814865112305,
      "learning_rate": 0.00023149053753829187,
      "loss": 0.5633,
      "step": 70000
    },
    {
      "epoch": 0.22869111040789228,
      "grad_norm": 20.188264846801758,
      "learning_rate": 0.0002313926668776323,
      "loss": 0.4645,
      "step": 70100
    },
    {
      "epoch": 0.22901734594342424,
      "grad_norm": 0.007010472472757101,
      "learning_rate": 0.0002312947962169727,
      "loss": 0.4754,
      "step": 70200
    },
    {
      "epoch": 0.22934358147895617,
      "grad_norm": 0.01940600574016571,
      "learning_rate": 0.00023119692555631312,
      "loss": 0.5191,
      "step": 70300
    },
    {
      "epoch": 0.22966981701448813,
      "grad_norm": 0.0035452682059258223,
      "learning_rate": 0.00023109905489565355,
      "loss": 0.5921,
      "step": 70400
    },
    {
      "epoch": 0.22999605255002006,
      "grad_norm": 9.443143844604492,
      "learning_rate": 0.00023100118423499395,
      "loss": 0.5611,
      "step": 70500
    },
    {
      "epoch": 0.230322288085552,
      "grad_norm": 45.216888427734375,
      "learning_rate": 0.00023090331357433438,
      "loss": 0.7986,
      "step": 70600
    },
    {
      "epoch": 0.23064852362108396,
      "grad_norm": 0.002816920867189765,
      "learning_rate": 0.0002308054429136748,
      "loss": 0.7421,
      "step": 70700
    },
    {
      "epoch": 0.2309747591566159,
      "grad_norm": 65.88861083984375,
      "learning_rate": 0.0002307075722530152,
      "loss": 0.7127,
      "step": 70800
    },
    {
      "epoch": 0.23130099469214785,
      "grad_norm": 0.06900187581777573,
      "learning_rate": 0.00023060970159235564,
      "loss": 0.5615,
      "step": 70900
    },
    {
      "epoch": 0.23162723022767978,
      "grad_norm": 0.9803552031517029,
      "learning_rate": 0.00023051183093169607,
      "loss": 0.5164,
      "step": 71000
    },
    {
      "epoch": 0.2319534657632117,
      "grad_norm": 0.5330260992050171,
      "learning_rate": 0.00023041396027103647,
      "loss": 0.5601,
      "step": 71100
    },
    {
      "epoch": 0.23227970129874367,
      "grad_norm": 0.0032787590753287077,
      "learning_rate": 0.0002303160896103769,
      "loss": 0.6308,
      "step": 71200
    },
    {
      "epoch": 0.2326059368342756,
      "grad_norm": 52.50080108642578,
      "learning_rate": 0.0002302182189497173,
      "loss": 0.5995,
      "step": 71300
    },
    {
      "epoch": 0.23293217236980757,
      "grad_norm": 0.00895413663238287,
      "learning_rate": 0.00023012034828905772,
      "loss": 0.5858,
      "step": 71400
    },
    {
      "epoch": 0.2332584079053395,
      "grad_norm": 0.002279982902109623,
      "learning_rate": 0.00023002247762839815,
      "loss": 0.6613,
      "step": 71500
    },
    {
      "epoch": 0.23358464344087143,
      "grad_norm": 0.002313036937266588,
      "learning_rate": 0.00022992460696773852,
      "loss": 0.4859,
      "step": 71600
    },
    {
      "epoch": 0.2339108789764034,
      "grad_norm": 0.002182316966354847,
      "learning_rate": 0.00022982673630707895,
      "loss": 0.4981,
      "step": 71700
    },
    {
      "epoch": 0.23423711451193532,
      "grad_norm": 0.03890552744269371,
      "learning_rate": 0.00022972886564641938,
      "loss": 0.474,
      "step": 71800
    },
    {
      "epoch": 0.23456335004746728,
      "grad_norm": 0.01874970830976963,
      "learning_rate": 0.00022963099498575978,
      "loss": 0.5845,
      "step": 71900
    },
    {
      "epoch": 0.23488958558299922,
      "grad_norm": 43.999420166015625,
      "learning_rate": 0.0002295331243251002,
      "loss": 0.8192,
      "step": 72000
    },
    {
      "epoch": 0.23521582111853115,
      "grad_norm": 0.0032198941335082054,
      "learning_rate": 0.00022943525366444064,
      "loss": 0.5114,
      "step": 72100
    },
    {
      "epoch": 0.2355420566540631,
      "grad_norm": 0.004886697046458721,
      "learning_rate": 0.00022933738300378104,
      "loss": 0.3937,
      "step": 72200
    },
    {
      "epoch": 0.23586829218959504,
      "grad_norm": 54.184410095214844,
      "learning_rate": 0.00022923951234312147,
      "loss": 0.6125,
      "step": 72300
    },
    {
      "epoch": 0.23619452772512697,
      "grad_norm": 0.00012482907914090902,
      "learning_rate": 0.0002291416416824619,
      "loss": 0.5743,
      "step": 72400
    },
    {
      "epoch": 0.23652076326065893,
      "grad_norm": 0.000785443524364382,
      "learning_rate": 0.0002290437710218023,
      "loss": 0.5068,
      "step": 72500
    },
    {
      "epoch": 0.23684699879619087,
      "grad_norm": 0.05988205969333649,
      "learning_rate": 0.00022894590036114272,
      "loss": 0.5287,
      "step": 72600
    },
    {
      "epoch": 0.23717323433172283,
      "grad_norm": 12.037578582763672,
      "learning_rate": 0.00022884802970048315,
      "loss": 0.758,
      "step": 72700
    },
    {
      "epoch": 0.23749946986725476,
      "grad_norm": 1.8830307722091675,
      "learning_rate": 0.00022875015903982355,
      "loss": 0.6692,
      "step": 72800
    },
    {
      "epoch": 0.2378257054027867,
      "grad_norm": 0.4323185980319977,
      "learning_rate": 0.00022865228837916398,
      "loss": 0.4472,
      "step": 72900
    },
    {
      "epoch": 0.23815194093831865,
      "grad_norm": 28.651609420776367,
      "learning_rate": 0.0002285544177185044,
      "loss": 0.6454,
      "step": 73000
    },
    {
      "epoch": 0.23847817647385058,
      "grad_norm": 105.12715911865234,
      "learning_rate": 0.0002284565470578448,
      "loss": 0.7622,
      "step": 73100
    },
    {
      "epoch": 0.23880441200938254,
      "grad_norm": 0.1264185756444931,
      "learning_rate": 0.00022835867639718524,
      "loss": 0.4639,
      "step": 73200
    },
    {
      "epoch": 0.23913064754491448,
      "grad_norm": 0.01450581755489111,
      "learning_rate": 0.0002282608057365256,
      "loss": 0.4876,
      "step": 73300
    },
    {
      "epoch": 0.2394568830804464,
      "grad_norm": 111.07317352294922,
      "learning_rate": 0.00022816293507586604,
      "loss": 0.5202,
      "step": 73400
    },
    {
      "epoch": 0.23978311861597837,
      "grad_norm": 0.0021776652429252863,
      "learning_rate": 0.00022806506441520646,
      "loss": 0.6493,
      "step": 73500
    },
    {
      "epoch": 0.2401093541515103,
      "grad_norm": 0.09385145455598831,
      "learning_rate": 0.00022796719375454687,
      "loss": 0.5379,
      "step": 73600
    },
    {
      "epoch": 0.24043558968704226,
      "grad_norm": 0.5734156966209412,
      "learning_rate": 0.0002278693230938873,
      "loss": 0.6602,
      "step": 73700
    },
    {
      "epoch": 0.2407618252225742,
      "grad_norm": 2.4157943725585938,
      "learning_rate": 0.00022777145243322772,
      "loss": 0.4474,
      "step": 73800
    },
    {
      "epoch": 0.24108806075810613,
      "grad_norm": 0.1896766573190689,
      "learning_rate": 0.00022767358177256812,
      "loss": 0.3801,
      "step": 73900
    },
    {
      "epoch": 0.2414142962936381,
      "grad_norm": 67.6352767944336,
      "learning_rate": 0.00022757571111190855,
      "loss": 0.5928,
      "step": 74000
    },
    {
      "epoch": 0.24174053182917002,
      "grad_norm": 0.008965229615569115,
      "learning_rate": 0.00022747784045124898,
      "loss": 0.4861,
      "step": 74100
    },
    {
      "epoch": 0.24206676736470198,
      "grad_norm": 0.00018267548875883222,
      "learning_rate": 0.00022737996979058938,
      "loss": 0.5462,
      "step": 74200
    },
    {
      "epoch": 0.2423930029002339,
      "grad_norm": 0.000537756597623229,
      "learning_rate": 0.0002272820991299298,
      "loss": 0.5047,
      "step": 74300
    },
    {
      "epoch": 0.24271923843576584,
      "grad_norm": 102.7114028930664,
      "learning_rate": 0.00022718422846927023,
      "loss": 0.6002,
      "step": 74400
    },
    {
      "epoch": 0.2430454739712978,
      "grad_norm": 0.27911484241485596,
      "learning_rate": 0.00022708635780861064,
      "loss": 0.7613,
      "step": 74500
    },
    {
      "epoch": 0.24337170950682974,
      "grad_norm": 0.0002368858113186434,
      "learning_rate": 0.00022698848714795106,
      "loss": 0.4653,
      "step": 74600
    },
    {
      "epoch": 0.2436979450423617,
      "grad_norm": 0.4514423906803131,
      "learning_rate": 0.0002268906164872915,
      "loss": 0.3976,
      "step": 74700
    },
    {
      "epoch": 0.24402418057789363,
      "grad_norm": 0.001718630432151258,
      "learning_rate": 0.0002267927458266319,
      "loss": 0.3772,
      "step": 74800
    },
    {
      "epoch": 0.24435041611342556,
      "grad_norm": 0.15131856501102448,
      "learning_rate": 0.00022669487516597232,
      "loss": 0.5701,
      "step": 74900
    },
    {
      "epoch": 0.24467665164895752,
      "grad_norm": 18.126386642456055,
      "learning_rate": 0.00022659700450531275,
      "loss": 0.6287,
      "step": 75000
    },
    {
      "epoch": 0.24500288718448945,
      "grad_norm": 0.0015825469745323062,
      "learning_rate": 0.00022649913384465315,
      "loss": 0.736,
      "step": 75100
    },
    {
      "epoch": 0.24532912272002141,
      "grad_norm": 0.001158307772129774,
      "learning_rate": 0.00022640126318399358,
      "loss": 0.4866,
      "step": 75200
    },
    {
      "epoch": 0.24565535825555335,
      "grad_norm": 0.0003940500027965754,
      "learning_rate": 0.000226303392523334,
      "loss": 0.9592,
      "step": 75300
    },
    {
      "epoch": 0.24598159379108528,
      "grad_norm": 1.3299556970596313,
      "learning_rate": 0.00022620552186267438,
      "loss": 0.5524,
      "step": 75400
    },
    {
      "epoch": 0.24630782932661724,
      "grad_norm": 25.953266143798828,
      "learning_rate": 0.0002261076512020148,
      "loss": 0.7987,
      "step": 75500
    },
    {
      "epoch": 0.24663406486214917,
      "grad_norm": 0.008376507088541985,
      "learning_rate": 0.0002260097805413552,
      "loss": 0.63,
      "step": 75600
    },
    {
      "epoch": 0.2469603003976811,
      "grad_norm": 0.0003274621849413961,
      "learning_rate": 0.00022591190988069563,
      "loss": 0.8557,
      "step": 75700
    },
    {
      "epoch": 0.24728653593321306,
      "grad_norm": 13.32177448272705,
      "learning_rate": 0.00022581403922003606,
      "loss": 0.6485,
      "step": 75800
    },
    {
      "epoch": 0.247612771468745,
      "grad_norm": 0.010931271128356457,
      "learning_rate": 0.00022571616855937646,
      "loss": 0.7459,
      "step": 75900
    },
    {
      "epoch": 0.24793900700427696,
      "grad_norm": 0.0010623635025694966,
      "learning_rate": 0.0002256182978987169,
      "loss": 0.5778,
      "step": 76000
    },
    {
      "epoch": 0.2482652425398089,
      "grad_norm": 0.671759307384491,
      "learning_rate": 0.00022552042723805732,
      "loss": 0.7495,
      "step": 76100
    },
    {
      "epoch": 0.24859147807534082,
      "grad_norm": 0.006357186008244753,
      "learning_rate": 0.00022542255657739772,
      "loss": 0.6174,
      "step": 76200
    },
    {
      "epoch": 0.24891771361087278,
      "grad_norm": 0.011377622373402119,
      "learning_rate": 0.00022532468591673815,
      "loss": 0.7374,
      "step": 76300
    },
    {
      "epoch": 0.24924394914640471,
      "grad_norm": 0.1495504379272461,
      "learning_rate": 0.00022522681525607858,
      "loss": 0.6954,
      "step": 76400
    },
    {
      "epoch": 0.24957018468193667,
      "grad_norm": 0.06884853541851044,
      "learning_rate": 0.00022512894459541898,
      "loss": 0.5693,
      "step": 76500
    },
    {
      "epoch": 0.2498964202174686,
      "grad_norm": 0.0008141621947288513,
      "learning_rate": 0.0002250310739347594,
      "loss": 0.6523,
      "step": 76600
    },
    {
      "epoch": 0.25022265575300057,
      "grad_norm": 0.0007993518374860287,
      "learning_rate": 0.00022493320327409983,
      "loss": 0.4861,
      "step": 76700
    },
    {
      "epoch": 0.25054889128853247,
      "grad_norm": 0.2902641296386719,
      "learning_rate": 0.00022483533261344023,
      "loss": 0.7661,
      "step": 76800
    },
    {
      "epoch": 0.25087512682406443,
      "grad_norm": 42.758399963378906,
      "learning_rate": 0.00022473746195278066,
      "loss": 0.5341,
      "step": 76900
    },
    {
      "epoch": 0.2512013623595964,
      "grad_norm": 40.9202995300293,
      "learning_rate": 0.0002246395912921211,
      "loss": 0.6464,
      "step": 77000
    },
    {
      "epoch": 0.25152759789512835,
      "grad_norm": 4.875598430633545,
      "learning_rate": 0.00022454172063146146,
      "loss": 0.6835,
      "step": 77100
    },
    {
      "epoch": 0.25185383343066026,
      "grad_norm": 0.027849500998854637,
      "learning_rate": 0.0002244438499708019,
      "loss": 0.5559,
      "step": 77200
    },
    {
      "epoch": 0.2521800689661922,
      "grad_norm": 0.002994013950228691,
      "learning_rate": 0.00022434597931014232,
      "loss": 0.3429,
      "step": 77300
    },
    {
      "epoch": 0.2525063045017242,
      "grad_norm": 0.008947133086621761,
      "learning_rate": 0.00022424810864948272,
      "loss": 0.6125,
      "step": 77400
    },
    {
      "epoch": 0.2528325400372561,
      "grad_norm": 0.12336605787277222,
      "learning_rate": 0.00022415023798882315,
      "loss": 0.4298,
      "step": 77500
    },
    {
      "epoch": 0.25315877557278804,
      "grad_norm": 99.8949966430664,
      "learning_rate": 0.00022405236732816358,
      "loss": 0.7207,
      "step": 77600
    },
    {
      "epoch": 0.25348501110832,
      "grad_norm": 37.27149963378906,
      "learning_rate": 0.00022395449666750398,
      "loss": 0.4517,
      "step": 77700
    },
    {
      "epoch": 0.2538112466438519,
      "grad_norm": 0.02000713162124157,
      "learning_rate": 0.0002238566260068444,
      "loss": 0.4816,
      "step": 77800
    },
    {
      "epoch": 0.25413748217938387,
      "grad_norm": 0.0037929476238787174,
      "learning_rate": 0.0002237587553461848,
      "loss": 0.6958,
      "step": 77900
    },
    {
      "epoch": 0.2544637177149158,
      "grad_norm": 0.00038106017746031284,
      "learning_rate": 0.00022366088468552523,
      "loss": 0.5767,
      "step": 78000
    },
    {
      "epoch": 0.25478995325044773,
      "grad_norm": 0.0056988950818777084,
      "learning_rate": 0.00022356301402486566,
      "loss": 0.5576,
      "step": 78100
    },
    {
      "epoch": 0.2551161887859797,
      "grad_norm": 0.2012098878622055,
      "learning_rate": 0.00022346514336420606,
      "loss": 0.5587,
      "step": 78200
    },
    {
      "epoch": 0.25544242432151165,
      "grad_norm": 3.9332592487335205,
      "learning_rate": 0.0002233672727035465,
      "loss": 0.6966,
      "step": 78300
    },
    {
      "epoch": 0.2557686598570436,
      "grad_norm": 10.253141403198242,
      "learning_rate": 0.00022326940204288692,
      "loss": 0.5799,
      "step": 78400
    },
    {
      "epoch": 0.2560948953925755,
      "grad_norm": 0.004868195857852697,
      "learning_rate": 0.00022317153138222732,
      "loss": 0.6027,
      "step": 78500
    },
    {
      "epoch": 0.2564211309281075,
      "grad_norm": 0.012267091311514378,
      "learning_rate": 0.00022307366072156775,
      "loss": 0.6426,
      "step": 78600
    },
    {
      "epoch": 0.25674736646363944,
      "grad_norm": 0.1801680028438568,
      "learning_rate": 0.00022297579006090817,
      "loss": 0.6006,
      "step": 78700
    },
    {
      "epoch": 0.25707360199917134,
      "grad_norm": 0.005023072008043528,
      "learning_rate": 0.00022287791940024857,
      "loss": 0.623,
      "step": 78800
    },
    {
      "epoch": 0.2573998375347033,
      "grad_norm": 0.00495856162160635,
      "learning_rate": 0.000222780048739589,
      "loss": 0.603,
      "step": 78900
    },
    {
      "epoch": 0.25772607307023526,
      "grad_norm": 0.004667543340474367,
      "learning_rate": 0.00022268217807892943,
      "loss": 0.7656,
      "step": 79000
    },
    {
      "epoch": 0.25805230860576717,
      "grad_norm": 0.010420435108244419,
      "learning_rate": 0.0002225843074182698,
      "loss": 0.4492,
      "step": 79100
    },
    {
      "epoch": 0.2583785441412991,
      "grad_norm": 26.926589965820312,
      "learning_rate": 0.00022248643675761023,
      "loss": 0.4859,
      "step": 79200
    },
    {
      "epoch": 0.2587047796768311,
      "grad_norm": 30.679058074951172,
      "learning_rate": 0.00022238856609695066,
      "loss": 0.6832,
      "step": 79300
    },
    {
      "epoch": 0.25903101521236305,
      "grad_norm": 0.00998186506330967,
      "learning_rate": 0.00022229069543629106,
      "loss": 0.6519,
      "step": 79400
    },
    {
      "epoch": 0.25935725074789495,
      "grad_norm": 4.51926851272583,
      "learning_rate": 0.0002221928247756315,
      "loss": 0.6943,
      "step": 79500
    },
    {
      "epoch": 0.2596834862834269,
      "grad_norm": 31.29145050048828,
      "learning_rate": 0.00022209495411497192,
      "loss": 0.6363,
      "step": 79600
    },
    {
      "epoch": 0.26000972181895887,
      "grad_norm": 0.013850594870746136,
      "learning_rate": 0.00022199708345431232,
      "loss": 0.5977,
      "step": 79700
    },
    {
      "epoch": 0.2603359573544908,
      "grad_norm": 44.681922912597656,
      "learning_rate": 0.00022189921279365275,
      "loss": 0.4576,
      "step": 79800
    },
    {
      "epoch": 0.26066219289002274,
      "grad_norm": 1.4322924613952637,
      "learning_rate": 0.00022180134213299315,
      "loss": 0.8533,
      "step": 79900
    },
    {
      "epoch": 0.2609884284255547,
      "grad_norm": 0.00481458893045783,
      "learning_rate": 0.00022170347147233357,
      "loss": 0.2684,
      "step": 80000
    },
    {
      "epoch": 0.2613146639610866,
      "grad_norm": 0.5251895785331726,
      "learning_rate": 0.000221605600811674,
      "loss": 0.5913,
      "step": 80100
    },
    {
      "epoch": 0.26164089949661856,
      "grad_norm": 0.009502406232059002,
      "learning_rate": 0.0002215077301510144,
      "loss": 0.418,
      "step": 80200
    },
    {
      "epoch": 0.2619671350321505,
      "grad_norm": 61.687557220458984,
      "learning_rate": 0.00022140985949035483,
      "loss": 0.5818,
      "step": 80300
    },
    {
      "epoch": 0.2622933705676825,
      "grad_norm": 1.0579447746276855,
      "learning_rate": 0.00022131198882969526,
      "loss": 0.4258,
      "step": 80400
    },
    {
      "epoch": 0.2626196061032144,
      "grad_norm": 0.006841886788606644,
      "learning_rate": 0.00022121411816903566,
      "loss": 0.6851,
      "step": 80500
    },
    {
      "epoch": 0.26294584163874635,
      "grad_norm": 126.65370178222656,
      "learning_rate": 0.0002211162475083761,
      "loss": 0.781,
      "step": 80600
    },
    {
      "epoch": 0.2632720771742783,
      "grad_norm": 0.7914193868637085,
      "learning_rate": 0.00022101837684771652,
      "loss": 0.4108,
      "step": 80700
    },
    {
      "epoch": 0.2635983127098102,
      "grad_norm": 0.010896171443164349,
      "learning_rate": 0.0002209205061870569,
      "loss": 0.4543,
      "step": 80800
    },
    {
      "epoch": 0.26392454824534217,
      "grad_norm": 21.16316795349121,
      "learning_rate": 0.00022082263552639732,
      "loss": 0.6048,
      "step": 80900
    },
    {
      "epoch": 0.26425078378087413,
      "grad_norm": 46.12242126464844,
      "learning_rate": 0.00022072476486573774,
      "loss": 0.6142,
      "step": 81000
    },
    {
      "epoch": 0.26457701931640604,
      "grad_norm": 0.01229643914848566,
      "learning_rate": 0.00022062689420507815,
      "loss": 0.6351,
      "step": 81100
    },
    {
      "epoch": 0.264903254851938,
      "grad_norm": 104.2309799194336,
      "learning_rate": 0.00022052902354441857,
      "loss": 0.8297,
      "step": 81200
    },
    {
      "epoch": 0.26522949038746996,
      "grad_norm": 0.0047960891388356686,
      "learning_rate": 0.000220431152883759,
      "loss": 0.5841,
      "step": 81300
    },
    {
      "epoch": 0.26555572592300186,
      "grad_norm": 0.01584116742014885,
      "learning_rate": 0.0002203332822230994,
      "loss": 0.4386,
      "step": 81400
    },
    {
      "epoch": 0.2658819614585338,
      "grad_norm": 123.78469848632812,
      "learning_rate": 0.00022023541156243983,
      "loss": 0.7483,
      "step": 81500
    },
    {
      "epoch": 0.2662081969940658,
      "grad_norm": 12.916997909545898,
      "learning_rate": 0.00022013754090178026,
      "loss": 0.568,
      "step": 81600
    },
    {
      "epoch": 0.26653443252959774,
      "grad_norm": 13.619732856750488,
      "learning_rate": 0.00022003967024112066,
      "loss": 0.4949,
      "step": 81700
    },
    {
      "epoch": 0.26686066806512965,
      "grad_norm": 0.001074557309038937,
      "learning_rate": 0.0002199417995804611,
      "loss": 0.4321,
      "step": 81800
    },
    {
      "epoch": 0.2671869036006616,
      "grad_norm": 0.0004726103215944022,
      "learning_rate": 0.00021984392891980151,
      "loss": 0.6876,
      "step": 81900
    },
    {
      "epoch": 0.26751313913619357,
      "grad_norm": 0.0007266673492267728,
      "learning_rate": 0.00021974605825914192,
      "loss": 0.3452,
      "step": 82000
    },
    {
      "epoch": 0.26783937467172547,
      "grad_norm": 13.755758285522461,
      "learning_rate": 0.00021964818759848234,
      "loss": 0.5882,
      "step": 82100
    },
    {
      "epoch": 0.26816561020725743,
      "grad_norm": 0.0010455738520249724,
      "learning_rate": 0.00021955031693782274,
      "loss": 0.5505,
      "step": 82200
    },
    {
      "epoch": 0.2684918457427894,
      "grad_norm": 0.0005882586701773107,
      "learning_rate": 0.00021945244627716317,
      "loss": 0.4573,
      "step": 82300
    },
    {
      "epoch": 0.2688180812783213,
      "grad_norm": 0.5923104286193848,
      "learning_rate": 0.0002193545756165036,
      "loss": 0.4095,
      "step": 82400
    },
    {
      "epoch": 0.26914431681385326,
      "grad_norm": 0.009733212180435658,
      "learning_rate": 0.00021925670495584397,
      "loss": 0.6364,
      "step": 82500
    },
    {
      "epoch": 0.2694705523493852,
      "grad_norm": 0.0712113231420517,
      "learning_rate": 0.00021915883429518443,
      "loss": 0.7639,
      "step": 82600
    },
    {
      "epoch": 0.2697967878849172,
      "grad_norm": 14.785534858703613,
      "learning_rate": 0.00021906096363452486,
      "loss": 0.3478,
      "step": 82700
    },
    {
      "epoch": 0.2701230234204491,
      "grad_norm": 28.0367488861084,
      "learning_rate": 0.00021896309297386523,
      "loss": 0.4474,
      "step": 82800
    },
    {
      "epoch": 0.27044925895598104,
      "grad_norm": 0.34431734681129456,
      "learning_rate": 0.00021886522231320566,
      "loss": 0.397,
      "step": 82900
    },
    {
      "epoch": 0.270775494491513,
      "grad_norm": 0.015491420403122902,
      "learning_rate": 0.00021876735165254609,
      "loss": 0.3915,
      "step": 83000
    },
    {
      "epoch": 0.2711017300270449,
      "grad_norm": 2.5592851638793945,
      "learning_rate": 0.0002186694809918865,
      "loss": 0.7575,
      "step": 83100
    },
    {
      "epoch": 0.27142796556257687,
      "grad_norm": 2.212559461593628,
      "learning_rate": 0.00021857161033122691,
      "loss": 0.5444,
      "step": 83200
    },
    {
      "epoch": 0.2717542010981088,
      "grad_norm": 0.0028943342622369528,
      "learning_rate": 0.00021847373967056734,
      "loss": 0.4935,
      "step": 83300
    },
    {
      "epoch": 0.27208043663364073,
      "grad_norm": 0.005968363489955664,
      "learning_rate": 0.00021837586900990774,
      "loss": 0.6332,
      "step": 83400
    },
    {
      "epoch": 0.2724066721691727,
      "grad_norm": 22.876750946044922,
      "learning_rate": 0.00021827799834924817,
      "loss": 0.8189,
      "step": 83500
    },
    {
      "epoch": 0.27273290770470465,
      "grad_norm": 0.023041216656565666,
      "learning_rate": 0.0002181801276885886,
      "loss": 0.7037,
      "step": 83600
    },
    {
      "epoch": 0.2730591432402366,
      "grad_norm": 0.005179408472031355,
      "learning_rate": 0.000218082257027929,
      "loss": 0.4762,
      "step": 83700
    },
    {
      "epoch": 0.2733853787757685,
      "grad_norm": 21.60761070251465,
      "learning_rate": 0.00021798438636726943,
      "loss": 0.5095,
      "step": 83800
    },
    {
      "epoch": 0.2737116143113005,
      "grad_norm": 0.24068811535835266,
      "learning_rate": 0.00021788651570660986,
      "loss": 0.3933,
      "step": 83900
    },
    {
      "epoch": 0.27403784984683244,
      "grad_norm": 2.6195430755615234,
      "learning_rate": 0.00021778864504595026,
      "loss": 0.5067,
      "step": 84000
    },
    {
      "epoch": 0.27436408538236434,
      "grad_norm": 0.006058721803128719,
      "learning_rate": 0.00021769077438529068,
      "loss": 0.4475,
      "step": 84100
    },
    {
      "epoch": 0.2746903209178963,
      "grad_norm": 0.3395514190196991,
      "learning_rate": 0.00021759290372463109,
      "loss": 0.4489,
      "step": 84200
    },
    {
      "epoch": 0.27501655645342826,
      "grad_norm": 12.92460823059082,
      "learning_rate": 0.00021749503306397151,
      "loss": 0.5919,
      "step": 84300
    },
    {
      "epoch": 0.27534279198896017,
      "grad_norm": 0.003119188826531172,
      "learning_rate": 0.00021739716240331194,
      "loss": 0.6275,
      "step": 84400
    },
    {
      "epoch": 0.2756690275244921,
      "grad_norm": 0.005920297000557184,
      "learning_rate": 0.00021729929174265232,
      "loss": 0.4263,
      "step": 84500
    },
    {
      "epoch": 0.2759952630600241,
      "grad_norm": 6.052402019500732,
      "learning_rate": 0.00021720142108199274,
      "loss": 0.5954,
      "step": 84600
    },
    {
      "epoch": 0.276321498595556,
      "grad_norm": 69.15003967285156,
      "learning_rate": 0.00021710355042133317,
      "loss": 0.4552,
      "step": 84700
    },
    {
      "epoch": 0.27664773413108795,
      "grad_norm": 25.362045288085938,
      "learning_rate": 0.00021700567976067357,
      "loss": 0.467,
      "step": 84800
    },
    {
      "epoch": 0.2769739696666199,
      "grad_norm": 115.68389892578125,
      "learning_rate": 0.000216907809100014,
      "loss": 0.5098,
      "step": 84900
    },
    {
      "epoch": 0.2773002052021519,
      "grad_norm": 0.04706691578030586,
      "learning_rate": 0.00021680993843935443,
      "loss": 0.678,
      "step": 85000
    },
    {
      "epoch": 0.2776264407376838,
      "grad_norm": 0.0028639412485063076,
      "learning_rate": 0.00021671206777869483,
      "loss": 0.4973,
      "step": 85100
    },
    {
      "epoch": 0.27795267627321574,
      "grad_norm": 0.014420084655284882,
      "learning_rate": 0.00021661419711803526,
      "loss": 0.5716,
      "step": 85200
    },
    {
      "epoch": 0.2782789118087477,
      "grad_norm": 0.00039490661583840847,
      "learning_rate": 0.00021651632645737568,
      "loss": 0.5603,
      "step": 85300
    },
    {
      "epoch": 0.2786051473442796,
      "grad_norm": 0.08330079168081284,
      "learning_rate": 0.00021641845579671609,
      "loss": 0.3946,
      "step": 85400
    },
    {
      "epoch": 0.27893138287981156,
      "grad_norm": 0.00766270374879241,
      "learning_rate": 0.0002163205851360565,
      "loss": 0.6823,
      "step": 85500
    },
    {
      "epoch": 0.2792576184153435,
      "grad_norm": 0.7655968070030212,
      "learning_rate": 0.00021622271447539694,
      "loss": 0.5663,
      "step": 85600
    },
    {
      "epoch": 0.2795838539508754,
      "grad_norm": 0.017467470839619637,
      "learning_rate": 0.00021612484381473734,
      "loss": 0.6801,
      "step": 85700
    },
    {
      "epoch": 0.2799100894864074,
      "grad_norm": 0.03691854700446129,
      "learning_rate": 0.00021602697315407777,
      "loss": 0.6146,
      "step": 85800
    },
    {
      "epoch": 0.28023632502193935,
      "grad_norm": 0.0017786804819479585,
      "learning_rate": 0.0002159291024934182,
      "loss": 0.4625,
      "step": 85900
    },
    {
      "epoch": 0.2805625605574713,
      "grad_norm": 1.2141181230545044,
      "learning_rate": 0.0002158312318327586,
      "loss": 0.7214,
      "step": 86000
    },
    {
      "epoch": 0.2808887960930032,
      "grad_norm": 2.2725143432617188,
      "learning_rate": 0.00021573336117209903,
      "loss": 0.483,
      "step": 86100
    },
    {
      "epoch": 0.2812150316285352,
      "grad_norm": 0.00392531743273139,
      "learning_rate": 0.00021563549051143945,
      "loss": 0.5584,
      "step": 86200
    },
    {
      "epoch": 0.28154126716406713,
      "grad_norm": 68.1041488647461,
      "learning_rate": 0.00021553761985077983,
      "loss": 0.7059,
      "step": 86300
    },
    {
      "epoch": 0.28186750269959904,
      "grad_norm": 0.00012172977585578337,
      "learning_rate": 0.00021543974919012028,
      "loss": 0.4258,
      "step": 86400
    },
    {
      "epoch": 0.282193738235131,
      "grad_norm": 31.276865005493164,
      "learning_rate": 0.00021534187852946066,
      "loss": 0.5104,
      "step": 86500
    },
    {
      "epoch": 0.28251997377066296,
      "grad_norm": 0.059558939188718796,
      "learning_rate": 0.00021524400786880108,
      "loss": 0.4338,
      "step": 86600
    },
    {
      "epoch": 0.28284620930619486,
      "grad_norm": 43.38835906982422,
      "learning_rate": 0.0002151461372081415,
      "loss": 0.735,
      "step": 86700
    },
    {
      "epoch": 0.2831724448417268,
      "grad_norm": 0.03302757069468498,
      "learning_rate": 0.0002150482665474819,
      "loss": 0.9659,
      "step": 86800
    },
    {
      "epoch": 0.2834986803772588,
      "grad_norm": 59.5176887512207,
      "learning_rate": 0.00021495039588682234,
      "loss": 0.6629,
      "step": 86900
    },
    {
      "epoch": 0.28382491591279074,
      "grad_norm": 17.425071716308594,
      "learning_rate": 0.00021485252522616277,
      "loss": 0.7499,
      "step": 87000
    },
    {
      "epoch": 0.28415115144832265,
      "grad_norm": 0.0004374075506348163,
      "learning_rate": 0.00021475465456550317,
      "loss": 0.4941,
      "step": 87100
    },
    {
      "epoch": 0.2844773869838546,
      "grad_norm": 0.0006146603845991194,
      "learning_rate": 0.0002146567839048436,
      "loss": 0.6303,
      "step": 87200
    },
    {
      "epoch": 0.28480362251938657,
      "grad_norm": 0.12776543200016022,
      "learning_rate": 0.00021455891324418403,
      "loss": 0.4911,
      "step": 87300
    },
    {
      "epoch": 0.2851298580549185,
      "grad_norm": 0.0007571199676021934,
      "learning_rate": 0.00021446104258352443,
      "loss": 0.7369,
      "step": 87400
    },
    {
      "epoch": 0.28545609359045043,
      "grad_norm": 0.712263286113739,
      "learning_rate": 0.00021436317192286485,
      "loss": 0.4111,
      "step": 87500
    },
    {
      "epoch": 0.2857823291259824,
      "grad_norm": 12.549736976623535,
      "learning_rate": 0.00021426530126220528,
      "loss": 0.577,
      "step": 87600
    },
    {
      "epoch": 0.2861085646615143,
      "grad_norm": 0.0023054033517837524,
      "learning_rate": 0.00021416743060154568,
      "loss": 0.4974,
      "step": 87700
    },
    {
      "epoch": 0.28643480019704626,
      "grad_norm": 0.007727344986051321,
      "learning_rate": 0.0002140695599408861,
      "loss": 0.4121,
      "step": 87800
    },
    {
      "epoch": 0.2867610357325782,
      "grad_norm": 0.0015385457081720233,
      "learning_rate": 0.00021397168928022654,
      "loss": 0.5825,
      "step": 87900
    },
    {
      "epoch": 0.2870872712681101,
      "grad_norm": 37.458831787109375,
      "learning_rate": 0.00021387381861956694,
      "loss": 0.5721,
      "step": 88000
    },
    {
      "epoch": 0.2874135068036421,
      "grad_norm": 1.4783622026443481,
      "learning_rate": 0.00021377594795890737,
      "loss": 0.4385,
      "step": 88100
    },
    {
      "epoch": 0.28773974233917404,
      "grad_norm": 1.177119255065918,
      "learning_rate": 0.0002136780772982478,
      "loss": 0.3683,
      "step": 88200
    },
    {
      "epoch": 0.288065977874706,
      "grad_norm": 0.24007469415664673,
      "learning_rate": 0.00021358020663758817,
      "loss": 0.5087,
      "step": 88300
    },
    {
      "epoch": 0.2883922134102379,
      "grad_norm": 0.0006448093336075544,
      "learning_rate": 0.0002134823359769286,
      "loss": 0.5526,
      "step": 88400
    },
    {
      "epoch": 0.28871844894576987,
      "grad_norm": 0.0031406038906425238,
      "learning_rate": 0.00021338446531626903,
      "loss": 0.5221,
      "step": 88500
    },
    {
      "epoch": 0.28904468448130183,
      "grad_norm": 53.845149993896484,
      "learning_rate": 0.00021328659465560943,
      "loss": 0.7424,
      "step": 88600
    },
    {
      "epoch": 0.28937092001683373,
      "grad_norm": 15.510712623596191,
      "learning_rate": 0.00021318872399494985,
      "loss": 0.6165,
      "step": 88700
    },
    {
      "epoch": 0.2896971555523657,
      "grad_norm": 0.0014748915564268827,
      "learning_rate": 0.00021309085333429025,
      "loss": 0.4569,
      "step": 88800
    },
    {
      "epoch": 0.29002339108789765,
      "grad_norm": 0.01615358516573906,
      "learning_rate": 0.00021299298267363068,
      "loss": 0.6328,
      "step": 88900
    },
    {
      "epoch": 0.29034962662342956,
      "grad_norm": 0.6015630960464478,
      "learning_rate": 0.0002128951120129711,
      "loss": 0.339,
      "step": 89000
    },
    {
      "epoch": 0.2906758621589615,
      "grad_norm": 55.65556716918945,
      "learning_rate": 0.0002127972413523115,
      "loss": 0.4828,
      "step": 89100
    },
    {
      "epoch": 0.2910020976944935,
      "grad_norm": 0.0026383090298622847,
      "learning_rate": 0.00021269937069165194,
      "loss": 0.635,
      "step": 89200
    },
    {
      "epoch": 0.29132833323002544,
      "grad_norm": 0.0006160290213301778,
      "learning_rate": 0.00021260150003099237,
      "loss": 0.6505,
      "step": 89300
    },
    {
      "epoch": 0.29165456876555734,
      "grad_norm": 0.007846093736588955,
      "learning_rate": 0.00021250362937033277,
      "loss": 0.4915,
      "step": 89400
    },
    {
      "epoch": 0.2919808043010893,
      "grad_norm": 0.00025344634195789695,
      "learning_rate": 0.0002124057587096732,
      "loss": 0.5008,
      "step": 89500
    },
    {
      "epoch": 0.29230703983662126,
      "grad_norm": 58.80747985839844,
      "learning_rate": 0.00021230788804901362,
      "loss": 0.5889,
      "step": 89600
    },
    {
      "epoch": 0.29263327537215317,
      "grad_norm": 25.624448776245117,
      "learning_rate": 0.00021221001738835402,
      "loss": 0.4981,
      "step": 89700
    },
    {
      "epoch": 0.29295951090768513,
      "grad_norm": 36.137794494628906,
      "learning_rate": 0.00021211214672769445,
      "loss": 0.509,
      "step": 89800
    },
    {
      "epoch": 0.2932857464432171,
      "grad_norm": 0.004980921745300293,
      "learning_rate": 0.00021201427606703488,
      "loss": 0.4221,
      "step": 89900
    },
    {
      "epoch": 0.293611981978749,
      "grad_norm": 1.6892327070236206,
      "learning_rate": 0.00021191640540637525,
      "loss": 0.518,
      "step": 90000
    },
    {
      "epoch": 0.29393821751428095,
      "grad_norm": 38.691871643066406,
      "learning_rate": 0.0002118185347457157,
      "loss": 0.5736,
      "step": 90100
    },
    {
      "epoch": 0.2942644530498129,
      "grad_norm": 0.004991529043763876,
      "learning_rate": 0.00021172066408505614,
      "loss": 0.9127,
      "step": 90200
    },
    {
      "epoch": 0.2945906885853449,
      "grad_norm": 0.00749127846211195,
      "learning_rate": 0.0002116227934243965,
      "loss": 0.5305,
      "step": 90300
    },
    {
      "epoch": 0.2949169241208768,
      "grad_norm": 4.160675525665283,
      "learning_rate": 0.00021152492276373694,
      "loss": 0.6594,
      "step": 90400
    },
    {
      "epoch": 0.29524315965640874,
      "grad_norm": 3.8066906929016113,
      "learning_rate": 0.00021142705210307737,
      "loss": 0.4538,
      "step": 90500
    },
    {
      "epoch": 0.2955693951919407,
      "grad_norm": 0.003106097225099802,
      "learning_rate": 0.00021132918144241777,
      "loss": 0.6261,
      "step": 90600
    },
    {
      "epoch": 0.2958956307274726,
      "grad_norm": 40.58960723876953,
      "learning_rate": 0.0002112313107817582,
      "loss": 0.5814,
      "step": 90700
    },
    {
      "epoch": 0.29622186626300456,
      "grad_norm": 0.010371433570981026,
      "learning_rate": 0.0002111334401210986,
      "loss": 0.4898,
      "step": 90800
    },
    {
      "epoch": 0.2965481017985365,
      "grad_norm": 1.316329836845398,
      "learning_rate": 0.00021103556946043902,
      "loss": 0.6806,
      "step": 90900
    },
    {
      "epoch": 0.2968743373340684,
      "grad_norm": 0.006543587893247604,
      "learning_rate": 0.00021093769879977945,
      "loss": 0.6567,
      "step": 91000
    },
    {
      "epoch": 0.2972005728696004,
      "grad_norm": 0.007486341986805201,
      "learning_rate": 0.00021083982813911985,
      "loss": 0.7659,
      "step": 91100
    },
    {
      "epoch": 0.29752680840513235,
      "grad_norm": 0.0592711977660656,
      "learning_rate": 0.00021074195747846028,
      "loss": 0.4676,
      "step": 91200
    },
    {
      "epoch": 0.2978530439406643,
      "grad_norm": 0.004962317179888487,
      "learning_rate": 0.0002106440868178007,
      "loss": 0.7507,
      "step": 91300
    },
    {
      "epoch": 0.2981792794761962,
      "grad_norm": 0.008193855173885822,
      "learning_rate": 0.0002105462161571411,
      "loss": 0.4346,
      "step": 91400
    },
    {
      "epoch": 0.2985055150117282,
      "grad_norm": 45.08957290649414,
      "learning_rate": 0.00021044834549648154,
      "loss": 0.4284,
      "step": 91500
    },
    {
      "epoch": 0.29883175054726013,
      "grad_norm": 0.02599317766726017,
      "learning_rate": 0.00021035047483582196,
      "loss": 0.6335,
      "step": 91600
    },
    {
      "epoch": 0.29915798608279204,
      "grad_norm": 0.05594391003251076,
      "learning_rate": 0.00021025260417516237,
      "loss": 0.3832,
      "step": 91700
    },
    {
      "epoch": 0.299484221618324,
      "grad_norm": 10.22304630279541,
      "learning_rate": 0.0002101547335145028,
      "loss": 0.4877,
      "step": 91800
    },
    {
      "epoch": 0.29981045715385596,
      "grad_norm": 44.71488571166992,
      "learning_rate": 0.00021005686285384322,
      "loss": 0.4641,
      "step": 91900
    },
    {
      "epoch": 0.30013669268938786,
      "grad_norm": 0.0009628619882278144,
      "learning_rate": 0.0002099589921931836,
      "loss": 0.5989,
      "step": 92000
    },
    {
      "epoch": 0.3004629282249198,
      "grad_norm": 178.75299072265625,
      "learning_rate": 0.00020986112153252402,
      "loss": 0.6435,
      "step": 92100
    },
    {
      "epoch": 0.3007891637604518,
      "grad_norm": 0.13384762406349182,
      "learning_rate": 0.00020976325087186445,
      "loss": 0.2636,
      "step": 92200
    },
    {
      "epoch": 0.3011153992959837,
      "grad_norm": 0.10356976091861725,
      "learning_rate": 0.00020966538021120485,
      "loss": 0.5709,
      "step": 92300
    },
    {
      "epoch": 0.30144163483151565,
      "grad_norm": 20.78355598449707,
      "learning_rate": 0.00020956750955054528,
      "loss": 0.4318,
      "step": 92400
    },
    {
      "epoch": 0.3017678703670476,
      "grad_norm": 0.30278491973876953,
      "learning_rate": 0.0002094696388898857,
      "loss": 0.7252,
      "step": 92500
    },
    {
      "epoch": 0.30209410590257957,
      "grad_norm": 0.00036422486300580204,
      "learning_rate": 0.0002093717682292261,
      "loss": 0.5932,
      "step": 92600
    },
    {
      "epoch": 0.3024203414381115,
      "grad_norm": 60.5390625,
      "learning_rate": 0.00020927389756856654,
      "loss": 0.5317,
      "step": 92700
    },
    {
      "epoch": 0.30274657697364343,
      "grad_norm": 0.0005659385933540761,
      "learning_rate": 0.00020917602690790696,
      "loss": 0.2588,
      "step": 92800
    },
    {
      "epoch": 0.3030728125091754,
      "grad_norm": 40.25726318359375,
      "learning_rate": 0.00020907815624724737,
      "loss": 0.5024,
      "step": 92900
    },
    {
      "epoch": 0.3033990480447073,
      "grad_norm": 2.9489049911499023,
      "learning_rate": 0.0002089802855865878,
      "loss": 0.5754,
      "step": 93000
    },
    {
      "epoch": 0.30372528358023926,
      "grad_norm": 0.013109181076288223,
      "learning_rate": 0.0002088824149259282,
      "loss": 0.2524,
      "step": 93100
    },
    {
      "epoch": 0.3040515191157712,
      "grad_norm": 0.0036913766525685787,
      "learning_rate": 0.00020878454426526862,
      "loss": 0.5478,
      "step": 93200
    },
    {
      "epoch": 0.3043777546513031,
      "grad_norm": 0.012700100429356098,
      "learning_rate": 0.00020868667360460905,
      "loss": 0.3825,
      "step": 93300
    },
    {
      "epoch": 0.3047039901868351,
      "grad_norm": 0.01318830531090498,
      "learning_rate": 0.00020858880294394945,
      "loss": 0.4857,
      "step": 93400
    },
    {
      "epoch": 0.30503022572236704,
      "grad_norm": 0.0017584870802238584,
      "learning_rate": 0.00020849093228328988,
      "loss": 0.5859,
      "step": 93500
    },
    {
      "epoch": 0.305356461257899,
      "grad_norm": 0.029968177899718285,
      "learning_rate": 0.0002083930616226303,
      "loss": 0.5916,
      "step": 93600
    },
    {
      "epoch": 0.3056826967934309,
      "grad_norm": 0.00027881874120794237,
      "learning_rate": 0.00020829519096197068,
      "loss": 0.3094,
      "step": 93700
    },
    {
      "epoch": 0.30600893232896287,
      "grad_norm": 0.009337003342807293,
      "learning_rate": 0.0002081973203013111,
      "loss": 0.3423,
      "step": 93800
    },
    {
      "epoch": 0.30633516786449483,
      "grad_norm": 0.0033829982858151197,
      "learning_rate": 0.00020809944964065156,
      "loss": 0.4537,
      "step": 93900
    },
    {
      "epoch": 0.30666140340002673,
      "grad_norm": 0.0003290025924798101,
      "learning_rate": 0.00020800157897999194,
      "loss": 0.4787,
      "step": 94000
    },
    {
      "epoch": 0.3069876389355587,
      "grad_norm": 34.28590393066406,
      "learning_rate": 0.00020790370831933236,
      "loss": 0.5187,
      "step": 94100
    },
    {
      "epoch": 0.30731387447109065,
      "grad_norm": 4.8005676944740117e-05,
      "learning_rate": 0.0002078058376586728,
      "loss": 0.5022,
      "step": 94200
    },
    {
      "epoch": 0.30764011000662256,
      "grad_norm": 0.11307456344366074,
      "learning_rate": 0.0002077079669980132,
      "loss": 0.5698,
      "step": 94300
    },
    {
      "epoch": 0.3079663455421545,
      "grad_norm": 0.005731240380555391,
      "learning_rate": 0.00020761009633735362,
      "loss": 0.3088,
      "step": 94400
    },
    {
      "epoch": 0.3082925810776865,
      "grad_norm": 0.0009983722120523453,
      "learning_rate": 0.00020751222567669405,
      "loss": 0.4137,
      "step": 94500
    },
    {
      "epoch": 0.30861881661321844,
      "grad_norm": 0.02688801847398281,
      "learning_rate": 0.00020741435501603445,
      "loss": 0.5061,
      "step": 94600
    },
    {
      "epoch": 0.30894505214875034,
      "grad_norm": 3.0503907203674316,
      "learning_rate": 0.00020731648435537488,
      "loss": 0.5652,
      "step": 94700
    },
    {
      "epoch": 0.3092712876842823,
      "grad_norm": 0.14324288070201874,
      "learning_rate": 0.0002072186136947153,
      "loss": 0.4609,
      "step": 94800
    },
    {
      "epoch": 0.30959752321981426,
      "grad_norm": 0.0007982816896401346,
      "learning_rate": 0.0002071207430340557,
      "loss": 0.5144,
      "step": 94900
    },
    {
      "epoch": 0.30992375875534617,
      "grad_norm": 1.4156734943389893,
      "learning_rate": 0.00020702287237339613,
      "loss": 0.717,
      "step": 95000
    },
    {
      "epoch": 0.31024999429087813,
      "grad_norm": 0.0013569684233516455,
      "learning_rate": 0.00020692500171273656,
      "loss": 0.4848,
      "step": 95100
    },
    {
      "epoch": 0.3105762298264101,
      "grad_norm": 0.31456902623176575,
      "learning_rate": 0.00020682713105207696,
      "loss": 0.505,
      "step": 95200
    },
    {
      "epoch": 0.310902465361942,
      "grad_norm": 0.0021368153393268585,
      "learning_rate": 0.0002067292603914174,
      "loss": 0.4494,
      "step": 95300
    },
    {
      "epoch": 0.31122870089747395,
      "grad_norm": 60.39364242553711,
      "learning_rate": 0.0002066313897307578,
      "loss": 0.6204,
      "step": 95400
    },
    {
      "epoch": 0.3115549364330059,
      "grad_norm": 0.03686853125691414,
      "learning_rate": 0.00020653351907009822,
      "loss": 0.6263,
      "step": 95500
    },
    {
      "epoch": 0.3118811719685378,
      "grad_norm": 30.062135696411133,
      "learning_rate": 0.00020643564840943865,
      "loss": 0.5604,
      "step": 95600
    },
    {
      "epoch": 0.3122074075040698,
      "grad_norm": 0.04775049537420273,
      "learning_rate": 0.00020633777774877902,
      "loss": 0.4703,
      "step": 95700
    },
    {
      "epoch": 0.31253364303960174,
      "grad_norm": 0.0030804062262177467,
      "learning_rate": 0.00020623990708811945,
      "loss": 0.4536,
      "step": 95800
    },
    {
      "epoch": 0.3128598785751337,
      "grad_norm": 44.56017303466797,
      "learning_rate": 0.00020614203642745988,
      "loss": 0.3713,
      "step": 95900
    },
    {
      "epoch": 0.3131861141106656,
      "grad_norm": 47.69194412231445,
      "learning_rate": 0.00020604416576680028,
      "loss": 0.6012,
      "step": 96000
    },
    {
      "epoch": 0.31351234964619756,
      "grad_norm": 0.1408591866493225,
      "learning_rate": 0.0002059462951061407,
      "loss": 0.4669,
      "step": 96100
    },
    {
      "epoch": 0.3138385851817295,
      "grad_norm": 0.004283413290977478,
      "learning_rate": 0.00020584842444548113,
      "loss": 0.3707,
      "step": 96200
    },
    {
      "epoch": 0.31416482071726143,
      "grad_norm": 0.006324994843453169,
      "learning_rate": 0.00020575055378482153,
      "loss": 0.4459,
      "step": 96300
    },
    {
      "epoch": 0.3144910562527934,
      "grad_norm": 0.00043418302084319293,
      "learning_rate": 0.00020565268312416196,
      "loss": 0.4059,
      "step": 96400
    },
    {
      "epoch": 0.31481729178832535,
      "grad_norm": 4.740523815155029,
      "learning_rate": 0.0002055548124635024,
      "loss": 0.3048,
      "step": 96500
    },
    {
      "epoch": 0.31514352732385725,
      "grad_norm": 0.013726132921874523,
      "learning_rate": 0.0002054569418028428,
      "loss": 0.4526,
      "step": 96600
    },
    {
      "epoch": 0.3154697628593892,
      "grad_norm": 0.0002630889939609915,
      "learning_rate": 0.00020535907114218322,
      "loss": 0.7078,
      "step": 96700
    },
    {
      "epoch": 0.3157959983949212,
      "grad_norm": 0.003621076699346304,
      "learning_rate": 0.00020526120048152365,
      "loss": 0.4305,
      "step": 96800
    },
    {
      "epoch": 0.31612223393045313,
      "grad_norm": 32.69223403930664,
      "learning_rate": 0.00020516332982086405,
      "loss": 0.5864,
      "step": 96900
    },
    {
      "epoch": 0.31644846946598504,
      "grad_norm": 0.008540194481611252,
      "learning_rate": 0.00020506545916020448,
      "loss": 0.4025,
      "step": 97000
    },
    {
      "epoch": 0.316774705001517,
      "grad_norm": 0.00022261547564994544,
      "learning_rate": 0.0002049675884995449,
      "loss": 0.6366,
      "step": 97100
    },
    {
      "epoch": 0.31710094053704896,
      "grad_norm": 34.32152557373047,
      "learning_rate": 0.0002048697178388853,
      "loss": 0.5416,
      "step": 97200
    },
    {
      "epoch": 0.31742717607258086,
      "grad_norm": 0.003104708855971694,
      "learning_rate": 0.00020477184717822573,
      "loss": 0.2353,
      "step": 97300
    },
    {
      "epoch": 0.3177534116081128,
      "grad_norm": 70.57317352294922,
      "learning_rate": 0.0002046739765175661,
      "loss": 0.4369,
      "step": 97400
    },
    {
      "epoch": 0.3180796471436448,
      "grad_norm": 0.5460072755813599,
      "learning_rate": 0.00020457610585690653,
      "loss": 0.4903,
      "step": 97500
    },
    {
      "epoch": 0.3184058826791767,
      "grad_norm": 0.00036489299964159727,
      "learning_rate": 0.000204478235196247,
      "loss": 0.2764,
      "step": 97600
    },
    {
      "epoch": 0.31873211821470865,
      "grad_norm": 0.0005739622283726931,
      "learning_rate": 0.00020438036453558736,
      "loss": 0.6442,
      "step": 97700
    },
    {
      "epoch": 0.3190583537502406,
      "grad_norm": 0.002390815643593669,
      "learning_rate": 0.0002042824938749278,
      "loss": 0.3642,
      "step": 97800
    },
    {
      "epoch": 0.31938458928577257,
      "grad_norm": 11.988677978515625,
      "learning_rate": 0.00020418462321426822,
      "loss": 0.4308,
      "step": 97900
    },
    {
      "epoch": 0.3197108248213045,
      "grad_norm": 0.04030076786875725,
      "learning_rate": 0.00020408675255360862,
      "loss": 0.8842,
      "step": 98000
    },
    {
      "epoch": 0.32003706035683643,
      "grad_norm": 51.89982223510742,
      "learning_rate": 0.00020398888189294905,
      "loss": 0.5272,
      "step": 98100
    },
    {
      "epoch": 0.3203632958923684,
      "grad_norm": 0.010392294265329838,
      "learning_rate": 0.00020389101123228948,
      "loss": 0.421,
      "step": 98200
    },
    {
      "epoch": 0.3206895314279003,
      "grad_norm": 0.000990621862001717,
      "learning_rate": 0.00020379314057162988,
      "loss": 0.5031,
      "step": 98300
    },
    {
      "epoch": 0.32101576696343226,
      "grad_norm": 0.0081191286444664,
      "learning_rate": 0.0002036952699109703,
      "loss": 0.6519,
      "step": 98400
    },
    {
      "epoch": 0.3213420024989642,
      "grad_norm": 0.0017886891728267074,
      "learning_rate": 0.00020359739925031073,
      "loss": 0.2765,
      "step": 98500
    },
    {
      "epoch": 0.3216682380344961,
      "grad_norm": 0.010469331406056881,
      "learning_rate": 0.00020349952858965113,
      "loss": 0.6568,
      "step": 98600
    },
    {
      "epoch": 0.3219944735700281,
      "grad_norm": 0.016784407198429108,
      "learning_rate": 0.00020340165792899156,
      "loss": 0.4314,
      "step": 98700
    },
    {
      "epoch": 0.32232070910556004,
      "grad_norm": 0.0006732149631716311,
      "learning_rate": 0.000203303787268332,
      "loss": 0.6245,
      "step": 98800
    },
    {
      "epoch": 0.32264694464109195,
      "grad_norm": 0.37543487548828125,
      "learning_rate": 0.0002032059166076724,
      "loss": 0.381,
      "step": 98900
    },
    {
      "epoch": 0.3229731801766239,
      "grad_norm": 13.767951011657715,
      "learning_rate": 0.00020310804594701282,
      "loss": 0.4406,
      "step": 99000
    },
    {
      "epoch": 0.32329941571215587,
      "grad_norm": 0.00011467105650808662,
      "learning_rate": 0.00020301017528635325,
      "loss": 0.2555,
      "step": 99100
    },
    {
      "epoch": 0.32362565124768783,
      "grad_norm": 0.24896427989006042,
      "learning_rate": 0.00020291230462569365,
      "loss": 0.4798,
      "step": 99200
    },
    {
      "epoch": 0.32395188678321973,
      "grad_norm": 0.02509262226521969,
      "learning_rate": 0.00020281443396503407,
      "loss": 0.5496,
      "step": 99300
    },
    {
      "epoch": 0.3242781223187517,
      "grad_norm": 0.34553346037864685,
      "learning_rate": 0.0002027165633043745,
      "loss": 0.6055,
      "step": 99400
    },
    {
      "epoch": 0.32460435785428365,
      "grad_norm": 53.31846618652344,
      "learning_rate": 0.00020261869264371488,
      "loss": 0.5054,
      "step": 99500
    },
    {
      "epoch": 0.32493059338981556,
      "grad_norm": 7.2641520500183105,
      "learning_rate": 0.0002025208219830553,
      "loss": 0.4495,
      "step": 99600
    },
    {
      "epoch": 0.3252568289253475,
      "grad_norm": 66.86226654052734,
      "learning_rate": 0.0002024229513223957,
      "loss": 0.5299,
      "step": 99700
    },
    {
      "epoch": 0.3255830644608795,
      "grad_norm": 0.22631709277629852,
      "learning_rate": 0.00020232508066173613,
      "loss": 0.4764,
      "step": 99800
    },
    {
      "epoch": 0.3259092999964114,
      "grad_norm": 0.003455741098150611,
      "learning_rate": 0.00020222721000107656,
      "loss": 0.4934,
      "step": 99900
    },
    {
      "epoch": 0.32623553553194334,
      "grad_norm": 22.88315200805664,
      "learning_rate": 0.00020212933934041696,
      "loss": 0.3815,
      "step": 100000
    },
    {
      "epoch": 0.3265617710674753,
      "grad_norm": 0.2825477719306946,
      "learning_rate": 0.0002020314686797574,
      "loss": 0.3233,
      "step": 100100
    },
    {
      "epoch": 0.32688800660300726,
      "grad_norm": 0.021772462874650955,
      "learning_rate": 0.00020193359801909782,
      "loss": 0.5187,
      "step": 100200
    },
    {
      "epoch": 0.32721424213853917,
      "grad_norm": 0.0005251573747955263,
      "learning_rate": 0.00020183572735843822,
      "loss": 0.4799,
      "step": 100300
    },
    {
      "epoch": 0.32754047767407113,
      "grad_norm": 0.0005271707777865231,
      "learning_rate": 0.00020173785669777865,
      "loss": 0.5065,
      "step": 100400
    },
    {
      "epoch": 0.3278667132096031,
      "grad_norm": 0.0003971014521084726,
      "learning_rate": 0.00020163998603711907,
      "loss": 0.4852,
      "step": 100500
    },
    {
      "epoch": 0.328192948745135,
      "grad_norm": 0.003546324325725436,
      "learning_rate": 0.00020154211537645947,
      "loss": 0.4765,
      "step": 100600
    },
    {
      "epoch": 0.32851918428066695,
      "grad_norm": 47.435665130615234,
      "learning_rate": 0.0002014442447157999,
      "loss": 0.5456,
      "step": 100700
    },
    {
      "epoch": 0.3288454198161989,
      "grad_norm": 1.0394953489303589,
      "learning_rate": 0.00020134637405514033,
      "loss": 0.4354,
      "step": 100800
    },
    {
      "epoch": 0.3291716553517308,
      "grad_norm": 57.99398422241211,
      "learning_rate": 0.00020124850339448073,
      "loss": 0.5442,
      "step": 100900
    },
    {
      "epoch": 0.3294978908872628,
      "grad_norm": 0.0007133746985346079,
      "learning_rate": 0.00020115063273382116,
      "loss": 0.4163,
      "step": 101000
    },
    {
      "epoch": 0.32982412642279474,
      "grad_norm": 0.9253498315811157,
      "learning_rate": 0.0002010527620731616,
      "loss": 0.3007,
      "step": 101100
    },
    {
      "epoch": 0.3301503619583267,
      "grad_norm": 30.868982315063477,
      "learning_rate": 0.00020095489141250196,
      "loss": 0.5545,
      "step": 101200
    },
    {
      "epoch": 0.3304765974938586,
      "grad_norm": 0.3790401816368103,
      "learning_rate": 0.0002008570207518424,
      "loss": 0.8199,
      "step": 101300
    },
    {
      "epoch": 0.33080283302939056,
      "grad_norm": 0.38588955998420715,
      "learning_rate": 0.00020075915009118284,
      "loss": 0.3891,
      "step": 101400
    },
    {
      "epoch": 0.3311290685649225,
      "grad_norm": 76.42843627929688,
      "learning_rate": 0.00020066127943052322,
      "loss": 0.4186,
      "step": 101500
    },
    {
      "epoch": 0.33145530410045443,
      "grad_norm": 0.001075788284651935,
      "learning_rate": 0.00020056340876986364,
      "loss": 0.507,
      "step": 101600
    },
    {
      "epoch": 0.3317815396359864,
      "grad_norm": 0.0002172713284380734,
      "learning_rate": 0.00020046553810920405,
      "loss": 0.6197,
      "step": 101700
    },
    {
      "epoch": 0.33210777517151835,
      "grad_norm": 64.9477310180664,
      "learning_rate": 0.00020036766744854447,
      "loss": 0.3278,
      "step": 101800
    },
    {
      "epoch": 0.33243401070705025,
      "grad_norm": 43.22230911254883,
      "learning_rate": 0.0002002697967878849,
      "loss": 0.4429,
      "step": 101900
    },
    {
      "epoch": 0.3327602462425822,
      "grad_norm": 0.2713027596473694,
      "learning_rate": 0.0002001719261272253,
      "loss": 0.6041,
      "step": 102000
    },
    {
      "epoch": 0.3330864817781142,
      "grad_norm": 0.04507537931203842,
      "learning_rate": 0.00020007405546656573,
      "loss": 0.3931,
      "step": 102100
    },
    {
      "epoch": 0.3334127173136461,
      "grad_norm": 0.02580161765217781,
      "learning_rate": 0.00019997618480590616,
      "loss": 0.5975,
      "step": 102200
    },
    {
      "epoch": 0.33373895284917804,
      "grad_norm": 0.005872155074030161,
      "learning_rate": 0.00019987831414524656,
      "loss": 0.7961,
      "step": 102300
    },
    {
      "epoch": 0.33406518838471,
      "grad_norm": 0.02219386026263237,
      "learning_rate": 0.000199780443484587,
      "loss": 0.4415,
      "step": 102400
    },
    {
      "epoch": 0.33439142392024196,
      "grad_norm": 0.006578941363841295,
      "learning_rate": 0.00019968257282392741,
      "loss": 0.5845,
      "step": 102500
    },
    {
      "epoch": 0.33471765945577386,
      "grad_norm": 0.3613545000553131,
      "learning_rate": 0.00019958470216326782,
      "loss": 0.3759,
      "step": 102600
    },
    {
      "epoch": 0.3350438949913058,
      "grad_norm": 64.37922668457031,
      "learning_rate": 0.00019948683150260824,
      "loss": 0.531,
      "step": 102700
    },
    {
      "epoch": 0.3353701305268378,
      "grad_norm": 0.004988836590200663,
      "learning_rate": 0.00019938896084194867,
      "loss": 0.4576,
      "step": 102800
    },
    {
      "epoch": 0.3356963660623697,
      "grad_norm": 2.8578896522521973,
      "learning_rate": 0.00019929109018128907,
      "loss": 0.4406,
      "step": 102900
    },
    {
      "epoch": 0.33602260159790165,
      "grad_norm": 18.628110885620117,
      "learning_rate": 0.0001991932195206295,
      "loss": 0.4437,
      "step": 103000
    },
    {
      "epoch": 0.3363488371334336,
      "grad_norm": 0.0007847557426430285,
      "learning_rate": 0.00019909534885996993,
      "loss": 0.6514,
      "step": 103100
    },
    {
      "epoch": 0.3366750726689655,
      "grad_norm": 0.00993230752646923,
      "learning_rate": 0.0001989974781993103,
      "loss": 0.4322,
      "step": 103200
    },
    {
      "epoch": 0.3370013082044975,
      "grad_norm": 6.174275040393695e-05,
      "learning_rate": 0.00019889960753865073,
      "loss": 0.2975,
      "step": 103300
    },
    {
      "epoch": 0.33732754374002943,
      "grad_norm": 10.179518699645996,
      "learning_rate": 0.00019880173687799116,
      "loss": 0.4371,
      "step": 103400
    },
    {
      "epoch": 0.3376537792755614,
      "grad_norm": 0.00017127321916632354,
      "learning_rate": 0.00019870386621733156,
      "loss": 0.3559,
      "step": 103500
    },
    {
      "epoch": 0.3379800148110933,
      "grad_norm": 11.294644355773926,
      "learning_rate": 0.00019860599555667199,
      "loss": 0.6452,
      "step": 103600
    },
    {
      "epoch": 0.33830625034662526,
      "grad_norm": 0.15438678860664368,
      "learning_rate": 0.00019850812489601241,
      "loss": 0.3463,
      "step": 103700
    },
    {
      "epoch": 0.3386324858821572,
      "grad_norm": 0.0013624465791508555,
      "learning_rate": 0.00019841025423535281,
      "loss": 0.6191,
      "step": 103800
    },
    {
      "epoch": 0.3389587214176891,
      "grad_norm": 0.005960078909993172,
      "learning_rate": 0.00019831238357469324,
      "loss": 0.4281,
      "step": 103900
    },
    {
      "epoch": 0.3392849569532211,
      "grad_norm": 0.007560177240520716,
      "learning_rate": 0.00019821451291403364,
      "loss": 0.3706,
      "step": 104000
    },
    {
      "epoch": 0.33961119248875304,
      "grad_norm": 4.730044841766357,
      "learning_rate": 0.00019811664225337407,
      "loss": 0.4167,
      "step": 104100
    },
    {
      "epoch": 0.33993742802428495,
      "grad_norm": 39.99937057495117,
      "learning_rate": 0.0001980187715927145,
      "loss": 0.1586,
      "step": 104200
    },
    {
      "epoch": 0.3402636635598169,
      "grad_norm": 0.11478623002767563,
      "learning_rate": 0.0001979209009320549,
      "loss": 0.712,
      "step": 104300
    },
    {
      "epoch": 0.34058989909534887,
      "grad_norm": 0.17938072979450226,
      "learning_rate": 0.00019782303027139533,
      "loss": 0.5443,
      "step": 104400
    },
    {
      "epoch": 0.34091613463088083,
      "grad_norm": 11.909965515136719,
      "learning_rate": 0.00019772515961073576,
      "loss": 0.4785,
      "step": 104500
    },
    {
      "epoch": 0.34124237016641273,
      "grad_norm": 0.0035558235831558704,
      "learning_rate": 0.00019762728895007616,
      "loss": 0.8343,
      "step": 104600
    },
    {
      "epoch": 0.3415686057019447,
      "grad_norm": 26.2082576751709,
      "learning_rate": 0.00019752941828941658,
      "loss": 0.2924,
      "step": 104700
    },
    {
      "epoch": 0.34189484123747665,
      "grad_norm": 64.4749526977539,
      "learning_rate": 0.000197431547628757,
      "loss": 0.5271,
      "step": 104800
    },
    {
      "epoch": 0.34222107677300856,
      "grad_norm": 0.0013708823826164007,
      "learning_rate": 0.00019733367696809739,
      "loss": 0.5986,
      "step": 104900
    },
    {
      "epoch": 0.3425473123085405,
      "grad_norm": 0.0008649016381241381,
      "learning_rate": 0.00019723580630743781,
      "loss": 0.4979,
      "step": 105000
    },
    {
      "epoch": 0.3428735478440725,
      "grad_norm": 0.6368421912193298,
      "learning_rate": 0.00019713793564677824,
      "loss": 0.5067,
      "step": 105100
    },
    {
      "epoch": 0.3431997833796044,
      "grad_norm": 0.17033779621124268,
      "learning_rate": 0.00019704006498611864,
      "loss": 0.4164,
      "step": 105200
    },
    {
      "epoch": 0.34352601891513634,
      "grad_norm": 0.0042961640283465385,
      "learning_rate": 0.00019694219432545907,
      "loss": 0.6322,
      "step": 105300
    },
    {
      "epoch": 0.3438522544506683,
      "grad_norm": 11.56332778930664,
      "learning_rate": 0.0001968443236647995,
      "loss": 0.3231,
      "step": 105400
    },
    {
      "epoch": 0.34417848998620026,
      "grad_norm": 0.2958590090274811,
      "learning_rate": 0.0001967464530041399,
      "loss": 0.3887,
      "step": 105500
    },
    {
      "epoch": 0.34450472552173217,
      "grad_norm": 52.534423828125,
      "learning_rate": 0.00019664858234348033,
      "loss": 0.4364,
      "step": 105600
    },
    {
      "epoch": 0.34483096105726413,
      "grad_norm": 0.007030182518064976,
      "learning_rate": 0.00019655071168282076,
      "loss": 0.456,
      "step": 105700
    },
    {
      "epoch": 0.3451571965927961,
      "grad_norm": 38.56854248046875,
      "learning_rate": 0.00019645284102216116,
      "loss": 0.3539,
      "step": 105800
    },
    {
      "epoch": 0.345483432128328,
      "grad_norm": 0.0018048231722787023,
      "learning_rate": 0.00019635497036150158,
      "loss": 0.3879,
      "step": 105900
    },
    {
      "epoch": 0.34580966766385995,
      "grad_norm": 0.35567396879196167,
      "learning_rate": 0.000196257099700842,
      "loss": 0.5494,
      "step": 106000
    },
    {
      "epoch": 0.3461359031993919,
      "grad_norm": 0.0006928596994839609,
      "learning_rate": 0.0001961592290401824,
      "loss": 0.4168,
      "step": 106100
    },
    {
      "epoch": 0.3464621387349238,
      "grad_norm": 0.00709049915894866,
      "learning_rate": 0.00019606135837952284,
      "loss": 0.3368,
      "step": 106200
    },
    {
      "epoch": 0.3467883742704558,
      "grad_norm": 0.7356056571006775,
      "learning_rate": 0.00019596348771886324,
      "loss": 0.5787,
      "step": 106300
    },
    {
      "epoch": 0.34711460980598774,
      "grad_norm": 0.014207729138433933,
      "learning_rate": 0.00019586561705820367,
      "loss": 0.7104,
      "step": 106400
    },
    {
      "epoch": 0.34744084534151964,
      "grad_norm": 0.0029644800815731287,
      "learning_rate": 0.0001957677463975441,
      "loss": 0.4412,
      "step": 106500
    },
    {
      "epoch": 0.3477670808770516,
      "grad_norm": 27.142932891845703,
      "learning_rate": 0.0001956698757368845,
      "loss": 0.5021,
      "step": 106600
    },
    {
      "epoch": 0.34809331641258356,
      "grad_norm": 0.06578510254621506,
      "learning_rate": 0.00019557200507622493,
      "loss": 0.3647,
      "step": 106700
    },
    {
      "epoch": 0.3484195519481155,
      "grad_norm": 0.00255828769877553,
      "learning_rate": 0.00019547413441556535,
      "loss": 0.3545,
      "step": 106800
    },
    {
      "epoch": 0.34874578748364743,
      "grad_norm": 0.005341530777513981,
      "learning_rate": 0.00019537626375490573,
      "loss": 0.2948,
      "step": 106900
    },
    {
      "epoch": 0.3490720230191794,
      "grad_norm": 2.144263505935669,
      "learning_rate": 0.00019527839309424616,
      "loss": 0.3952,
      "step": 107000
    },
    {
      "epoch": 0.34939825855471135,
      "grad_norm": 38.67481994628906,
      "learning_rate": 0.00019518052243358658,
      "loss": 0.5327,
      "step": 107100
    },
    {
      "epoch": 0.34972449409024325,
      "grad_norm": 56.41238784790039,
      "learning_rate": 0.00019508265177292698,
      "loss": 0.3531,
      "step": 107200
    },
    {
      "epoch": 0.3500507296257752,
      "grad_norm": 7.418620225507766e-05,
      "learning_rate": 0.0001949847811122674,
      "loss": 0.3365,
      "step": 107300
    },
    {
      "epoch": 0.3503769651613072,
      "grad_norm": 11.93083381652832,
      "learning_rate": 0.00019488691045160784,
      "loss": 0.4195,
      "step": 107400
    },
    {
      "epoch": 0.3507032006968391,
      "grad_norm": 0.0026921434327960014,
      "learning_rate": 0.00019478903979094824,
      "loss": 0.6435,
      "step": 107500
    },
    {
      "epoch": 0.35102943623237104,
      "grad_norm": 0.00031121663050726056,
      "learning_rate": 0.00019469116913028867,
      "loss": 0.223,
      "step": 107600
    },
    {
      "epoch": 0.351355671767903,
      "grad_norm": 1.8476283550262451,
      "learning_rate": 0.0001945932984696291,
      "loss": 0.6059,
      "step": 107700
    },
    {
      "epoch": 0.35168190730343496,
      "grad_norm": 2.247462511062622,
      "learning_rate": 0.0001944954278089695,
      "loss": 0.5145,
      "step": 107800
    },
    {
      "epoch": 0.35200814283896686,
      "grad_norm": 0.02392369508743286,
      "learning_rate": 0.00019439755714830993,
      "loss": 0.4303,
      "step": 107900
    },
    {
      "epoch": 0.3523343783744988,
      "grad_norm": 0.06720145046710968,
      "learning_rate": 0.00019429968648765035,
      "loss": 0.4537,
      "step": 108000
    },
    {
      "epoch": 0.3526606139100308,
      "grad_norm": 0.05826667696237564,
      "learning_rate": 0.00019420181582699075,
      "loss": 0.4188,
      "step": 108100
    },
    {
      "epoch": 0.3529868494455627,
      "grad_norm": 0.0036647284869104624,
      "learning_rate": 0.00019410394516633118,
      "loss": 0.7791,
      "step": 108200
    },
    {
      "epoch": 0.35331308498109465,
      "grad_norm": 0.0038902340456843376,
      "learning_rate": 0.00019400607450567158,
      "loss": 0.4633,
      "step": 108300
    },
    {
      "epoch": 0.3536393205166266,
      "grad_norm": 0.1381538212299347,
      "learning_rate": 0.000193908203845012,
      "loss": 0.488,
      "step": 108400
    },
    {
      "epoch": 0.3539655560521585,
      "grad_norm": 3.8274803161621094,
      "learning_rate": 0.00019381033318435244,
      "loss": 0.6525,
      "step": 108500
    },
    {
      "epoch": 0.3542917915876905,
      "grad_norm": 0.0004838532186113298,
      "learning_rate": 0.0001937124625236928,
      "loss": 0.6169,
      "step": 108600
    },
    {
      "epoch": 0.35461802712322243,
      "grad_norm": 0.04659786447882652,
      "learning_rate": 0.00019361459186303324,
      "loss": 0.4232,
      "step": 108700
    },
    {
      "epoch": 0.3549442626587544,
      "grad_norm": 0.24653872847557068,
      "learning_rate": 0.00019351672120237367,
      "loss": 0.4357,
      "step": 108800
    },
    {
      "epoch": 0.3552704981942863,
      "grad_norm": 40.924373626708984,
      "learning_rate": 0.00019341885054171407,
      "loss": 0.5046,
      "step": 108900
    },
    {
      "epoch": 0.35559673372981826,
      "grad_norm": 0.0028434903360903263,
      "learning_rate": 0.0001933209798810545,
      "loss": 0.4646,
      "step": 109000
    },
    {
      "epoch": 0.3559229692653502,
      "grad_norm": 0.0006710781017318368,
      "learning_rate": 0.00019322310922039492,
      "loss": 0.2893,
      "step": 109100
    },
    {
      "epoch": 0.3562492048008821,
      "grad_norm": 1.232337474822998,
      "learning_rate": 0.00019312523855973533,
      "loss": 0.528,
      "step": 109200
    },
    {
      "epoch": 0.3565754403364141,
      "grad_norm": 0.0021301673259586096,
      "learning_rate": 0.00019302736789907575,
      "loss": 0.2857,
      "step": 109300
    },
    {
      "epoch": 0.35690167587194604,
      "grad_norm": 2.9931821823120117,
      "learning_rate": 0.00019292949723841618,
      "loss": 0.2974,
      "step": 109400
    },
    {
      "epoch": 0.35722791140747795,
      "grad_norm": 33.671085357666016,
      "learning_rate": 0.00019283162657775658,
      "loss": 0.5173,
      "step": 109500
    },
    {
      "epoch": 0.3575541469430099,
      "grad_norm": 3.8912246227264404,
      "learning_rate": 0.000192733755917097,
      "loss": 0.4235,
      "step": 109600
    },
    {
      "epoch": 0.35788038247854187,
      "grad_norm": 0.001756168669089675,
      "learning_rate": 0.00019263588525643744,
      "loss": 0.5184,
      "step": 109700
    },
    {
      "epoch": 0.3582066180140738,
      "grad_norm": 0.002373393625020981,
      "learning_rate": 0.00019253801459577784,
      "loss": 0.5422,
      "step": 109800
    },
    {
      "epoch": 0.35853285354960573,
      "grad_norm": 44.472900390625,
      "learning_rate": 0.00019244014393511827,
      "loss": 0.3818,
      "step": 109900
    },
    {
      "epoch": 0.3588590890851377,
      "grad_norm": 0.014089137315750122,
      "learning_rate": 0.0001923422732744587,
      "loss": 0.4905,
      "step": 110000
    },
    {
      "epoch": 0.35918532462066965,
      "grad_norm": 0.0014924744609743357,
      "learning_rate": 0.0001922444026137991,
      "loss": 0.4258,
      "step": 110100
    },
    {
      "epoch": 0.35951156015620156,
      "grad_norm": 23.157960891723633,
      "learning_rate": 0.00019214653195313952,
      "loss": 0.3513,
      "step": 110200
    },
    {
      "epoch": 0.3598377956917335,
      "grad_norm": 71.1266860961914,
      "learning_rate": 0.00019204866129247995,
      "loss": 0.5492,
      "step": 110300
    },
    {
      "epoch": 0.3601640312272655,
      "grad_norm": 36.20513916015625,
      "learning_rate": 0.00019195079063182035,
      "loss": 0.2679,
      "step": 110400
    },
    {
      "epoch": 0.3604902667627974,
      "grad_norm": 0.018482079729437828,
      "learning_rate": 0.00019185291997116078,
      "loss": 0.454,
      "step": 110500
    },
    {
      "epoch": 0.36081650229832934,
      "grad_norm": 0.0046487171202898026,
      "learning_rate": 0.00019175504931050115,
      "loss": 0.3098,
      "step": 110600
    },
    {
      "epoch": 0.3611427378338613,
      "grad_norm": 0.2917936146259308,
      "learning_rate": 0.00019165717864984158,
      "loss": 0.4482,
      "step": 110700
    },
    {
      "epoch": 0.3614689733693932,
      "grad_norm": 35.64859390258789,
      "learning_rate": 0.000191559307989182,
      "loss": 0.4405,
      "step": 110800
    },
    {
      "epoch": 0.36179520890492517,
      "grad_norm": 3.730175018310547,
      "learning_rate": 0.0001914614373285224,
      "loss": 0.2953,
      "step": 110900
    },
    {
      "epoch": 0.36212144444045713,
      "grad_norm": 0.0021434961818158627,
      "learning_rate": 0.00019136356666786284,
      "loss": 0.676,
      "step": 111000
    },
    {
      "epoch": 0.3624476799759891,
      "grad_norm": 0.008289439603686333,
      "learning_rate": 0.00019126569600720327,
      "loss": 0.3848,
      "step": 111100
    },
    {
      "epoch": 0.362773915511521,
      "grad_norm": 0.17234008014202118,
      "learning_rate": 0.00019116782534654367,
      "loss": 0.4825,
      "step": 111200
    },
    {
      "epoch": 0.36310015104705295,
      "grad_norm": 14.377017974853516,
      "learning_rate": 0.0001910699546858841,
      "loss": 0.6002,
      "step": 111300
    },
    {
      "epoch": 0.3634263865825849,
      "grad_norm": 0.0444909930229187,
      "learning_rate": 0.00019097208402522452,
      "loss": 0.4862,
      "step": 111400
    },
    {
      "epoch": 0.3637526221181168,
      "grad_norm": 0.0010115080513060093,
      "learning_rate": 0.00019087421336456492,
      "loss": 0.5884,
      "step": 111500
    },
    {
      "epoch": 0.3640788576536488,
      "grad_norm": 0.0036622625775635242,
      "learning_rate": 0.00019077634270390535,
      "loss": 0.6048,
      "step": 111600
    },
    {
      "epoch": 0.36440509318918074,
      "grad_norm": 0.002116783522069454,
      "learning_rate": 0.00019067847204324578,
      "loss": 0.5033,
      "step": 111700
    },
    {
      "epoch": 0.36473132872471264,
      "grad_norm": 0.000940164492931217,
      "learning_rate": 0.00019058060138258618,
      "loss": 0.3679,
      "step": 111800
    },
    {
      "epoch": 0.3650575642602446,
      "grad_norm": 14.04959487915039,
      "learning_rate": 0.0001904827307219266,
      "loss": 0.4473,
      "step": 111900
    },
    {
      "epoch": 0.36538379979577656,
      "grad_norm": 0.733710527420044,
      "learning_rate": 0.00019038486006126704,
      "loss": 0.4064,
      "step": 112000
    },
    {
      "epoch": 0.3657100353313085,
      "grad_norm": 18.516040802001953,
      "learning_rate": 0.00019028698940060744,
      "loss": 0.4284,
      "step": 112100
    },
    {
      "epoch": 0.36603627086684043,
      "grad_norm": 0.024239107966423035,
      "learning_rate": 0.00019018911873994786,
      "loss": 0.6099,
      "step": 112200
    },
    {
      "epoch": 0.3663625064023724,
      "grad_norm": 0.029041150584816933,
      "learning_rate": 0.0001900912480792883,
      "loss": 0.4632,
      "step": 112300
    },
    {
      "epoch": 0.36668874193790435,
      "grad_norm": 0.6708757281303406,
      "learning_rate": 0.00018999337741862867,
      "loss": 0.5486,
      "step": 112400
    },
    {
      "epoch": 0.36701497747343625,
      "grad_norm": 0.03097923845052719,
      "learning_rate": 0.0001898955067579691,
      "loss": 0.5469,
      "step": 112500
    },
    {
      "epoch": 0.3673412130089682,
      "grad_norm": 0.024279480800032616,
      "learning_rate": 0.0001897976360973095,
      "loss": 0.6184,
      "step": 112600
    },
    {
      "epoch": 0.3676674485445002,
      "grad_norm": 0.5184285640716553,
      "learning_rate": 0.00018969976543664992,
      "loss": 0.5119,
      "step": 112700
    },
    {
      "epoch": 0.3679936840800321,
      "grad_norm": 0.00567221362143755,
      "learning_rate": 0.00018960189477599035,
      "loss": 0.4128,
      "step": 112800
    },
    {
      "epoch": 0.36831991961556404,
      "grad_norm": 0.02525697648525238,
      "learning_rate": 0.00018950402411533075,
      "loss": 0.3152,
      "step": 112900
    },
    {
      "epoch": 0.368646155151096,
      "grad_norm": 0.027421772480010986,
      "learning_rate": 0.00018940615345467118,
      "loss": 0.3714,
      "step": 113000
    },
    {
      "epoch": 0.3689723906866279,
      "grad_norm": 0.06452623754739761,
      "learning_rate": 0.0001893082827940116,
      "loss": 0.3613,
      "step": 113100
    },
    {
      "epoch": 0.36929862622215986,
      "grad_norm": 0.004793410189449787,
      "learning_rate": 0.000189210412133352,
      "loss": 0.3875,
      "step": 113200
    },
    {
      "epoch": 0.3696248617576918,
      "grad_norm": 0.0015238664345815778,
      "learning_rate": 0.00018911254147269244,
      "loss": 0.4409,
      "step": 113300
    },
    {
      "epoch": 0.3699510972932238,
      "grad_norm": 0.1412806659936905,
      "learning_rate": 0.00018901467081203286,
      "loss": 0.4225,
      "step": 113400
    },
    {
      "epoch": 0.3702773328287557,
      "grad_norm": 0.000972216424997896,
      "learning_rate": 0.00018891680015137327,
      "loss": 0.5394,
      "step": 113500
    },
    {
      "epoch": 0.37060356836428765,
      "grad_norm": 0.021110769361257553,
      "learning_rate": 0.0001888189294907137,
      "loss": 0.3813,
      "step": 113600
    },
    {
      "epoch": 0.3709298038998196,
      "grad_norm": 0.482968270778656,
      "learning_rate": 0.00018872105883005412,
      "loss": 0.5117,
      "step": 113700
    },
    {
      "epoch": 0.3712560394353515,
      "grad_norm": 32.120811462402344,
      "learning_rate": 0.00018862318816939452,
      "loss": 0.5847,
      "step": 113800
    },
    {
      "epoch": 0.3715822749708835,
      "grad_norm": 0.0012526323553174734,
      "learning_rate": 0.00018852531750873495,
      "loss": 0.3975,
      "step": 113900
    },
    {
      "epoch": 0.37190851050641544,
      "grad_norm": 14.192032814025879,
      "learning_rate": 0.00018842744684807538,
      "loss": 0.3894,
      "step": 114000
    },
    {
      "epoch": 0.37223474604194734,
      "grad_norm": 0.7412333488464355,
      "learning_rate": 0.00018832957618741578,
      "loss": 0.6104,
      "step": 114100
    },
    {
      "epoch": 0.3725609815774793,
      "grad_norm": 0.0011099260300397873,
      "learning_rate": 0.0001882317055267562,
      "loss": 0.3424,
      "step": 114200
    },
    {
      "epoch": 0.37288721711301126,
      "grad_norm": 0.0029782732017338276,
      "learning_rate": 0.00018813383486609663,
      "loss": 0.5784,
      "step": 114300
    },
    {
      "epoch": 0.3732134526485432,
      "grad_norm": 0.10462744534015656,
      "learning_rate": 0.000188035964205437,
      "loss": 0.4046,
      "step": 114400
    },
    {
      "epoch": 0.3735396881840751,
      "grad_norm": 0.00423252210021019,
      "learning_rate": 0.00018793809354477744,
      "loss": 0.2004,
      "step": 114500
    },
    {
      "epoch": 0.3738659237196071,
      "grad_norm": 0.5216207504272461,
      "learning_rate": 0.00018784022288411786,
      "loss": 0.5372,
      "step": 114600
    },
    {
      "epoch": 0.37419215925513905,
      "grad_norm": 0.27775344252586365,
      "learning_rate": 0.00018774235222345826,
      "loss": 0.4098,
      "step": 114700
    },
    {
      "epoch": 0.37451839479067095,
      "grad_norm": 0.2213008999824524,
      "learning_rate": 0.0001876444815627987,
      "loss": 0.4353,
      "step": 114800
    },
    {
      "epoch": 0.3748446303262029,
      "grad_norm": 0.000739976589102298,
      "learning_rate": 0.0001875466109021391,
      "loss": 0.5453,
      "step": 114900
    },
    {
      "epoch": 0.37517086586173487,
      "grad_norm": 0.030299270525574684,
      "learning_rate": 0.00018744874024147952,
      "loss": 0.4783,
      "step": 115000
    },
    {
      "epoch": 0.3754971013972668,
      "grad_norm": 0.0003618017944972962,
      "learning_rate": 0.00018735086958081995,
      "loss": 0.4628,
      "step": 115100
    },
    {
      "epoch": 0.37582333693279873,
      "grad_norm": 7.391867256956175e-05,
      "learning_rate": 0.00018725299892016035,
      "loss": 0.2173,
      "step": 115200
    },
    {
      "epoch": 0.3761495724683307,
      "grad_norm": 0.01034502312541008,
      "learning_rate": 0.00018715512825950078,
      "loss": 0.4525,
      "step": 115300
    },
    {
      "epoch": 0.37647580800386266,
      "grad_norm": 11.721835136413574,
      "learning_rate": 0.0001870572575988412,
      "loss": 0.4992,
      "step": 115400
    },
    {
      "epoch": 0.37680204353939456,
      "grad_norm": 0.003731236094608903,
      "learning_rate": 0.0001869593869381816,
      "loss": 0.4283,
      "step": 115500
    },
    {
      "epoch": 0.3771282790749265,
      "grad_norm": 0.03434034064412117,
      "learning_rate": 0.00018686151627752203,
      "loss": 0.4099,
      "step": 115600
    },
    {
      "epoch": 0.3774545146104585,
      "grad_norm": 11.647294044494629,
      "learning_rate": 0.00018676364561686246,
      "loss": 0.5042,
      "step": 115700
    },
    {
      "epoch": 0.3777807501459904,
      "grad_norm": 0.00790348183363676,
      "learning_rate": 0.00018666577495620286,
      "loss": 0.4196,
      "step": 115800
    },
    {
      "epoch": 0.37810698568152235,
      "grad_norm": 0.000125840277178213,
      "learning_rate": 0.0001865679042955433,
      "loss": 0.4123,
      "step": 115900
    },
    {
      "epoch": 0.3784332212170543,
      "grad_norm": 1.6981281042099,
      "learning_rate": 0.00018647003363488372,
      "loss": 0.1376,
      "step": 116000
    },
    {
      "epoch": 0.3787594567525862,
      "grad_norm": 75.50053405761719,
      "learning_rate": 0.0001863721629742241,
      "loss": 0.427,
      "step": 116100
    },
    {
      "epoch": 0.37908569228811817,
      "grad_norm": 4.0446248054504395,
      "learning_rate": 0.00018627429231356452,
      "loss": 0.5238,
      "step": 116200
    },
    {
      "epoch": 0.37941192782365013,
      "grad_norm": 18.760351181030273,
      "learning_rate": 0.00018617642165290495,
      "loss": 0.3595,
      "step": 116300
    },
    {
      "epoch": 0.37973816335918203,
      "grad_norm": 0.0030029478948563337,
      "learning_rate": 0.00018607855099224535,
      "loss": 0.2837,
      "step": 116400
    },
    {
      "epoch": 0.380064398894714,
      "grad_norm": 3.273756504058838,
      "learning_rate": 0.00018598068033158578,
      "loss": 0.5045,
      "step": 116500
    },
    {
      "epoch": 0.38039063443024596,
      "grad_norm": 0.004233695100992918,
      "learning_rate": 0.0001858828096709262,
      "loss": 0.605,
      "step": 116600
    },
    {
      "epoch": 0.3807168699657779,
      "grad_norm": 5.29241151525639e-05,
      "learning_rate": 0.0001857849390102666,
      "loss": 0.4643,
      "step": 116700
    },
    {
      "epoch": 0.3810431055013098,
      "grad_norm": 0.0006012695375829935,
      "learning_rate": 0.00018568706834960703,
      "loss": 0.4068,
      "step": 116800
    },
    {
      "epoch": 0.3813693410368418,
      "grad_norm": 0.0023526735603809357,
      "learning_rate": 0.00018558919768894746,
      "loss": 0.4189,
      "step": 116900
    },
    {
      "epoch": 0.38169557657237374,
      "grad_norm": 0.13084614276885986,
      "learning_rate": 0.00018549132702828786,
      "loss": 0.3406,
      "step": 117000
    },
    {
      "epoch": 0.38202181210790565,
      "grad_norm": 0.0002685348445083946,
      "learning_rate": 0.0001853934563676283,
      "loss": 0.5803,
      "step": 117100
    },
    {
      "epoch": 0.3823480476434376,
      "grad_norm": 29.715669631958008,
      "learning_rate": 0.0001852955857069687,
      "loss": 0.6374,
      "step": 117200
    },
    {
      "epoch": 0.38267428317896957,
      "grad_norm": 9.96651554107666,
      "learning_rate": 0.00018519771504630912,
      "loss": 0.5,
      "step": 117300
    },
    {
      "epoch": 0.38300051871450147,
      "grad_norm": 0.0004343984182924032,
      "learning_rate": 0.00018509984438564955,
      "loss": 0.3805,
      "step": 117400
    },
    {
      "epoch": 0.38332675425003343,
      "grad_norm": 7.1157989501953125,
      "learning_rate": 0.00018500197372498995,
      "loss": 0.4508,
      "step": 117500
    },
    {
      "epoch": 0.3836529897855654,
      "grad_norm": 69.4090347290039,
      "learning_rate": 0.00018490410306433038,
      "loss": 0.3287,
      "step": 117600
    },
    {
      "epoch": 0.38397922532109735,
      "grad_norm": 28.48560333251953,
      "learning_rate": 0.0001848062324036708,
      "loss": 0.5507,
      "step": 117700
    },
    {
      "epoch": 0.38430546085662926,
      "grad_norm": 0.7020975351333618,
      "learning_rate": 0.00018470836174301118,
      "loss": 0.3784,
      "step": 117800
    },
    {
      "epoch": 0.3846316963921612,
      "grad_norm": 0.021600136533379555,
      "learning_rate": 0.00018461049108235163,
      "loss": 0.391,
      "step": 117900
    },
    {
      "epoch": 0.3849579319276932,
      "grad_norm": 0.25532013177871704,
      "learning_rate": 0.00018451262042169206,
      "loss": 0.5662,
      "step": 118000
    },
    {
      "epoch": 0.3852841674632251,
      "grad_norm": 27.0089168548584,
      "learning_rate": 0.00018441474976103243,
      "loss": 0.5811,
      "step": 118100
    },
    {
      "epoch": 0.38561040299875704,
      "grad_norm": 0.0039107524789869785,
      "learning_rate": 0.00018431687910037286,
      "loss": 0.4203,
      "step": 118200
    },
    {
      "epoch": 0.385936638534289,
      "grad_norm": 0.01932203210890293,
      "learning_rate": 0.0001842190084397133,
      "loss": 0.2781,
      "step": 118300
    },
    {
      "epoch": 0.3862628740698209,
      "grad_norm": 0.0013049902627244592,
      "learning_rate": 0.0001841211377790537,
      "loss": 0.3932,
      "step": 118400
    },
    {
      "epoch": 0.38658910960535287,
      "grad_norm": 0.00021683552768081427,
      "learning_rate": 0.00018402326711839412,
      "loss": 0.4502,
      "step": 118500
    },
    {
      "epoch": 0.3869153451408848,
      "grad_norm": 0.08037576824426651,
      "learning_rate": 0.00018392539645773455,
      "loss": 0.425,
      "step": 118600
    },
    {
      "epoch": 0.3872415806764168,
      "grad_norm": 0.00032011978328227997,
      "learning_rate": 0.00018382752579707495,
      "loss": 0.2768,
      "step": 118700
    },
    {
      "epoch": 0.3875678162119487,
      "grad_norm": 12.053471565246582,
      "learning_rate": 0.00018372965513641538,
      "loss": 0.5054,
      "step": 118800
    },
    {
      "epoch": 0.38789405174748065,
      "grad_norm": 0.09980026632547379,
      "learning_rate": 0.0001836317844757558,
      "loss": 0.4722,
      "step": 118900
    },
    {
      "epoch": 0.3882202872830126,
      "grad_norm": 1.7062335014343262,
      "learning_rate": 0.0001835339138150962,
      "loss": 0.5747,
      "step": 119000
    },
    {
      "epoch": 0.3885465228185445,
      "grad_norm": 0.043370746076107025,
      "learning_rate": 0.00018343604315443663,
      "loss": 0.3175,
      "step": 119100
    },
    {
      "epoch": 0.3888727583540765,
      "grad_norm": 6.112620758358389e-05,
      "learning_rate": 0.00018333817249377703,
      "loss": 0.3577,
      "step": 119200
    },
    {
      "epoch": 0.38919899388960844,
      "grad_norm": 0.017276659607887268,
      "learning_rate": 0.00018324030183311746,
      "loss": 0.2861,
      "step": 119300
    },
    {
      "epoch": 0.38952522942514034,
      "grad_norm": 24.666818618774414,
      "learning_rate": 0.0001831424311724579,
      "loss": 0.6319,
      "step": 119400
    },
    {
      "epoch": 0.3898514649606723,
      "grad_norm": 5.559762001037598,
      "learning_rate": 0.0001830445605117983,
      "loss": 0.4774,
      "step": 119500
    },
    {
      "epoch": 0.39017770049620426,
      "grad_norm": 25.131633758544922,
      "learning_rate": 0.00018294668985113872,
      "loss": 0.8544,
      "step": 119600
    },
    {
      "epoch": 0.39050393603173617,
      "grad_norm": 0.003837496042251587,
      "learning_rate": 0.00018284881919047914,
      "loss": 0.2291,
      "step": 119700
    },
    {
      "epoch": 0.3908301715672681,
      "grad_norm": 19.837472915649414,
      "learning_rate": 0.00018275094852981952,
      "loss": 0.5597,
      "step": 119800
    },
    {
      "epoch": 0.3911564071028001,
      "grad_norm": 0.0011888407170772552,
      "learning_rate": 0.00018265307786915995,
      "loss": 0.4112,
      "step": 119900
    },
    {
      "epoch": 0.39148264263833205,
      "grad_norm": 0.0010954331373795867,
      "learning_rate": 0.00018255520720850037,
      "loss": 0.4219,
      "step": 120000
    },
    {
      "epoch": 0.39180887817386395,
      "grad_norm": 0.009786836802959442,
      "learning_rate": 0.00018245733654784078,
      "loss": 0.5318,
      "step": 120100
    },
    {
      "epoch": 0.3921351137093959,
      "grad_norm": 0.0009754869970493019,
      "learning_rate": 0.0001823594658871812,
      "loss": 0.6026,
      "step": 120200
    },
    {
      "epoch": 0.39246134924492787,
      "grad_norm": 0.004640663042664528,
      "learning_rate": 0.00018226159522652163,
      "loss": 0.4764,
      "step": 120300
    },
    {
      "epoch": 0.3927875847804598,
      "grad_norm": 73.62229919433594,
      "learning_rate": 0.00018216372456586203,
      "loss": 0.5733,
      "step": 120400
    },
    {
      "epoch": 0.39311382031599174,
      "grad_norm": 0.3294646143913269,
      "learning_rate": 0.00018206585390520246,
      "loss": 0.3569,
      "step": 120500
    },
    {
      "epoch": 0.3934400558515237,
      "grad_norm": 6.905062675476074,
      "learning_rate": 0.0001819679832445429,
      "loss": 0.3683,
      "step": 120600
    },
    {
      "epoch": 0.3937662913870556,
      "grad_norm": 0.0012105393689125776,
      "learning_rate": 0.0001818701125838833,
      "loss": 0.3081,
      "step": 120700
    },
    {
      "epoch": 0.39409252692258756,
      "grad_norm": 0.00027700155624188483,
      "learning_rate": 0.00018177224192322372,
      "loss": 0.4338,
      "step": 120800
    },
    {
      "epoch": 0.3944187624581195,
      "grad_norm": 0.0009985036449506879,
      "learning_rate": 0.00018167437126256414,
      "loss": 0.3298,
      "step": 120900
    },
    {
      "epoch": 0.3947449979936515,
      "grad_norm": 25.552122116088867,
      "learning_rate": 0.00018157650060190455,
      "loss": 0.3446,
      "step": 121000
    },
    {
      "epoch": 0.3950712335291834,
      "grad_norm": 93.66537475585938,
      "learning_rate": 0.00018147862994124497,
      "loss": 0.5307,
      "step": 121100
    },
    {
      "epoch": 0.39539746906471535,
      "grad_norm": 0.0002687387459445745,
      "learning_rate": 0.0001813807592805854,
      "loss": 0.5391,
      "step": 121200
    },
    {
      "epoch": 0.3957237046002473,
      "grad_norm": 21.07516098022461,
      "learning_rate": 0.0001812828886199258,
      "loss": 0.3995,
      "step": 121300
    },
    {
      "epoch": 0.3960499401357792,
      "grad_norm": 0.007633285131305456,
      "learning_rate": 0.00018118501795926623,
      "loss": 0.3781,
      "step": 121400
    },
    {
      "epoch": 0.39637617567131117,
      "grad_norm": 67.13333892822266,
      "learning_rate": 0.0001810871472986066,
      "loss": 0.5417,
      "step": 121500
    },
    {
      "epoch": 0.39670241120684313,
      "grad_norm": 0.013876260258257389,
      "learning_rate": 0.00018098927663794706,
      "loss": 0.3859,
      "step": 121600
    },
    {
      "epoch": 0.39702864674237504,
      "grad_norm": 1.212465524673462,
      "learning_rate": 0.00018089140597728749,
      "loss": 0.5277,
      "step": 121700
    },
    {
      "epoch": 0.397354882277907,
      "grad_norm": 0.2334272414445877,
      "learning_rate": 0.00018079353531662786,
      "loss": 0.4528,
      "step": 121800
    },
    {
      "epoch": 0.39768111781343896,
      "grad_norm": 11.114500999450684,
      "learning_rate": 0.0001806956646559683,
      "loss": 0.494,
      "step": 121900
    },
    {
      "epoch": 0.3980073533489709,
      "grad_norm": 0.015479094348847866,
      "learning_rate": 0.00018059779399530872,
      "loss": 0.3349,
      "step": 122000
    },
    {
      "epoch": 0.3983335888845028,
      "grad_norm": 0.02915806882083416,
      "learning_rate": 0.00018049992333464912,
      "loss": 0.541,
      "step": 122100
    },
    {
      "epoch": 0.3986598244200348,
      "grad_norm": 0.00034321378916502,
      "learning_rate": 0.00018040205267398954,
      "loss": 0.495,
      "step": 122200
    },
    {
      "epoch": 0.39898605995556674,
      "grad_norm": 10.047924995422363,
      "learning_rate": 0.00018030418201332997,
      "loss": 0.4988,
      "step": 122300
    },
    {
      "epoch": 0.39931229549109865,
      "grad_norm": 69.14990997314453,
      "learning_rate": 0.00018020631135267037,
      "loss": 0.3325,
      "step": 122400
    },
    {
      "epoch": 0.3996385310266306,
      "grad_norm": 0.0004071294388268143,
      "learning_rate": 0.0001801084406920108,
      "loss": 0.4082,
      "step": 122500
    },
    {
      "epoch": 0.39996476656216257,
      "grad_norm": 0.000600766041316092,
      "learning_rate": 0.00018001057003135123,
      "loss": 0.275,
      "step": 122600
    },
    {
      "epoch": 0.40029100209769447,
      "grad_norm": 52.08720779418945,
      "learning_rate": 0.00017991269937069163,
      "loss": 0.2218,
      "step": 122700
    },
    {
      "epoch": 0.40061723763322643,
      "grad_norm": 0.0004521127848420292,
      "learning_rate": 0.00017981482871003206,
      "loss": 0.4412,
      "step": 122800
    },
    {
      "epoch": 0.4009434731687584,
      "grad_norm": 0.0004501781368162483,
      "learning_rate": 0.00017971695804937249,
      "loss": 0.4964,
      "step": 122900
    },
    {
      "epoch": 0.40126970870429035,
      "grad_norm": 134.4260711669922,
      "learning_rate": 0.0001796190873887129,
      "loss": 0.4485,
      "step": 123000
    },
    {
      "epoch": 0.40159594423982226,
      "grad_norm": 3.0101072788238525,
      "learning_rate": 0.00017952121672805331,
      "loss": 0.4164,
      "step": 123100
    },
    {
      "epoch": 0.4019221797753542,
      "grad_norm": 12.8165283203125,
      "learning_rate": 0.00017942334606739374,
      "loss": 0.5559,
      "step": 123200
    },
    {
      "epoch": 0.4022484153108862,
      "grad_norm": 0.0017584911547601223,
      "learning_rate": 0.00017932547540673414,
      "loss": 0.4465,
      "step": 123300
    },
    {
      "epoch": 0.4025746508464181,
      "grad_norm": 46.540340423583984,
      "learning_rate": 0.00017922760474607457,
      "loss": 0.4496,
      "step": 123400
    },
    {
      "epoch": 0.40290088638195004,
      "grad_norm": 0.0038340857718139887,
      "learning_rate": 0.000179129734085415,
      "loss": 0.3803,
      "step": 123500
    },
    {
      "epoch": 0.403227121917482,
      "grad_norm": 0.004333798307925463,
      "learning_rate": 0.00017903186342475537,
      "loss": 0.2562,
      "step": 123600
    },
    {
      "epoch": 0.4035533574530139,
      "grad_norm": 0.00018132885452359915,
      "learning_rate": 0.0001789339927640958,
      "loss": 0.7324,
      "step": 123700
    },
    {
      "epoch": 0.40387959298854587,
      "grad_norm": 0.000981525401584804,
      "learning_rate": 0.0001788361221034362,
      "loss": 0.3516,
      "step": 123800
    },
    {
      "epoch": 0.4042058285240778,
      "grad_norm": 51.521324157714844,
      "learning_rate": 0.00017873825144277663,
      "loss": 0.4841,
      "step": 123900
    },
    {
      "epoch": 0.40453206405960973,
      "grad_norm": 35.69257354736328,
      "learning_rate": 0.00017864038078211706,
      "loss": 0.5202,
      "step": 124000
    },
    {
      "epoch": 0.4048582995951417,
      "grad_norm": 0.0001374031708110124,
      "learning_rate": 0.00017854251012145746,
      "loss": 0.4244,
      "step": 124100
    },
    {
      "epoch": 0.40518453513067365,
      "grad_norm": 0.0006649488350376487,
      "learning_rate": 0.00017844463946079789,
      "loss": 0.4864,
      "step": 124200
    },
    {
      "epoch": 0.4055107706662056,
      "grad_norm": 26.123470306396484,
      "learning_rate": 0.00017834676880013831,
      "loss": 0.3522,
      "step": 124300
    },
    {
      "epoch": 0.4058370062017375,
      "grad_norm": 0.009249737486243248,
      "learning_rate": 0.00017824889813947871,
      "loss": 0.3923,
      "step": 124400
    },
    {
      "epoch": 0.4061632417372695,
      "grad_norm": 43.05966567993164,
      "learning_rate": 0.00017815102747881914,
      "loss": 0.5204,
      "step": 124500
    },
    {
      "epoch": 0.40648947727280144,
      "grad_norm": 0.0013591214083135128,
      "learning_rate": 0.00017805315681815957,
      "loss": 0.5325,
      "step": 124600
    },
    {
      "epoch": 0.40681571280833334,
      "grad_norm": 6.426627078326419e-05,
      "learning_rate": 0.00017795528615749997,
      "loss": 0.5061,
      "step": 124700
    },
    {
      "epoch": 0.4071419483438653,
      "grad_norm": 43.544368743896484,
      "learning_rate": 0.0001778574154968404,
      "loss": 0.6107,
      "step": 124800
    },
    {
      "epoch": 0.40746818387939726,
      "grad_norm": 0.0018439509440213442,
      "learning_rate": 0.00017775954483618083,
      "loss": 0.3947,
      "step": 124900
    },
    {
      "epoch": 0.40779441941492917,
      "grad_norm": 2.8688268661499023,
      "learning_rate": 0.00017766167417552123,
      "loss": 0.3666,
      "step": 125000
    },
    {
      "epoch": 0.4081206549504611,
      "grad_norm": 246.7342529296875,
      "learning_rate": 0.00017756380351486166,
      "loss": 0.5202,
      "step": 125100
    },
    {
      "epoch": 0.4084468904859931,
      "grad_norm": 0.07930063456296921,
      "learning_rate": 0.00017746593285420208,
      "loss": 0.626,
      "step": 125200
    },
    {
      "epoch": 0.40877312602152505,
      "grad_norm": 0.0037938968744128942,
      "learning_rate": 0.00017736806219354246,
      "loss": 0.5082,
      "step": 125300
    },
    {
      "epoch": 0.40909936155705695,
      "grad_norm": 0.010836657136678696,
      "learning_rate": 0.0001772701915328829,
      "loss": 0.4122,
      "step": 125400
    },
    {
      "epoch": 0.4094255970925889,
      "grad_norm": 0.010478588752448559,
      "learning_rate": 0.00017717232087222334,
      "loss": 0.444,
      "step": 125500
    },
    {
      "epoch": 0.40975183262812087,
      "grad_norm": 0.02340051904320717,
      "learning_rate": 0.00017707445021156371,
      "loss": 0.4034,
      "step": 125600
    },
    {
      "epoch": 0.4100780681636528,
      "grad_norm": 0.001548675587400794,
      "learning_rate": 0.00017697657955090414,
      "loss": 0.4847,
      "step": 125700
    },
    {
      "epoch": 0.41040430369918474,
      "grad_norm": 0.00876689050346613,
      "learning_rate": 0.00017687870889024454,
      "loss": 0.4715,
      "step": 125800
    },
    {
      "epoch": 0.4107305392347167,
      "grad_norm": 0.610133945941925,
      "learning_rate": 0.00017678083822958497,
      "loss": 0.6144,
      "step": 125900
    },
    {
      "epoch": 0.4110567747702486,
      "grad_norm": 0.004932202864438295,
      "learning_rate": 0.0001766829675689254,
      "loss": 0.2867,
      "step": 126000
    },
    {
      "epoch": 0.41138301030578056,
      "grad_norm": 0.0005309682455845177,
      "learning_rate": 0.0001765850969082658,
      "loss": 0.3312,
      "step": 126100
    },
    {
      "epoch": 0.4117092458413125,
      "grad_norm": 0.0012273736065253615,
      "learning_rate": 0.00017648722624760623,
      "loss": 0.4885,
      "step": 126200
    },
    {
      "epoch": 0.4120354813768445,
      "grad_norm": 0.071548692882061,
      "learning_rate": 0.00017638935558694666,
      "loss": 0.2973,
      "step": 126300
    },
    {
      "epoch": 0.4123617169123764,
      "grad_norm": 1.3489363193511963,
      "learning_rate": 0.00017629148492628706,
      "loss": 0.3589,
      "step": 126400
    },
    {
      "epoch": 0.41268795244790835,
      "grad_norm": 40.3797721862793,
      "learning_rate": 0.00017619361426562748,
      "loss": 0.4617,
      "step": 126500
    },
    {
      "epoch": 0.4130141879834403,
      "grad_norm": 0.0007214572397060692,
      "learning_rate": 0.0001760957436049679,
      "loss": 0.4475,
      "step": 126600
    },
    {
      "epoch": 0.4133404235189722,
      "grad_norm": 0.002181197749450803,
      "learning_rate": 0.0001759978729443083,
      "loss": 0.3861,
      "step": 126700
    },
    {
      "epoch": 0.41366665905450417,
      "grad_norm": 0.0005917425733059645,
      "learning_rate": 0.00017590000228364874,
      "loss": 0.2858,
      "step": 126800
    },
    {
      "epoch": 0.41399289459003613,
      "grad_norm": 0.11102110147476196,
      "learning_rate": 0.00017580213162298917,
      "loss": 0.543,
      "step": 126900
    },
    {
      "epoch": 0.41431913012556804,
      "grad_norm": 0.0012881712755188346,
      "learning_rate": 0.00017570426096232957,
      "loss": 0.3574,
      "step": 127000
    },
    {
      "epoch": 0.4146453656611,
      "grad_norm": 0.012015401385724545,
      "learning_rate": 0.00017560639030167,
      "loss": 0.5757,
      "step": 127100
    },
    {
      "epoch": 0.41497160119663196,
      "grad_norm": 0.005165336187928915,
      "learning_rate": 0.00017550851964101043,
      "loss": 0.3961,
      "step": 127200
    },
    {
      "epoch": 0.41529783673216386,
      "grad_norm": 0.005829678848385811,
      "learning_rate": 0.0001754106489803508,
      "loss": 0.453,
      "step": 127300
    },
    {
      "epoch": 0.4156240722676958,
      "grad_norm": 0.6411382555961609,
      "learning_rate": 0.00017531277831969123,
      "loss": 0.441,
      "step": 127400
    },
    {
      "epoch": 0.4159503078032278,
      "grad_norm": 9.628067016601562,
      "learning_rate": 0.00017521490765903165,
      "loss": 0.4847,
      "step": 127500
    },
    {
      "epoch": 0.41627654333875974,
      "grad_norm": 0.017127223312854767,
      "learning_rate": 0.00017511703699837206,
      "loss": 0.3254,
      "step": 127600
    },
    {
      "epoch": 0.41660277887429165,
      "grad_norm": 0.055515460669994354,
      "learning_rate": 0.00017501916633771248,
      "loss": 0.4212,
      "step": 127700
    },
    {
      "epoch": 0.4169290144098236,
      "grad_norm": 26.85065269470215,
      "learning_rate": 0.0001749212956770529,
      "loss": 0.5045,
      "step": 127800
    },
    {
      "epoch": 0.41725524994535557,
      "grad_norm": 0.0021890460047870874,
      "learning_rate": 0.0001748234250163933,
      "loss": 0.2775,
      "step": 127900
    },
    {
      "epoch": 0.41758148548088747,
      "grad_norm": 52.28758239746094,
      "learning_rate": 0.00017472555435573374,
      "loss": 0.3206,
      "step": 128000
    },
    {
      "epoch": 0.41790772101641943,
      "grad_norm": 0.02246977761387825,
      "learning_rate": 0.00017462768369507414,
      "loss": 0.3048,
      "step": 128100
    },
    {
      "epoch": 0.4182339565519514,
      "grad_norm": 32.17558670043945,
      "learning_rate": 0.00017452981303441457,
      "loss": 0.5348,
      "step": 128200
    },
    {
      "epoch": 0.4185601920874833,
      "grad_norm": 0.00043040356831625104,
      "learning_rate": 0.000174431942373755,
      "loss": 0.6514,
      "step": 128300
    },
    {
      "epoch": 0.41888642762301526,
      "grad_norm": 0.2665649354457855,
      "learning_rate": 0.0001743340717130954,
      "loss": 0.479,
      "step": 128400
    },
    {
      "epoch": 0.4192126631585472,
      "grad_norm": 4.715967952506617e-05,
      "learning_rate": 0.00017423620105243583,
      "loss": 0.4778,
      "step": 128500
    },
    {
      "epoch": 0.4195388986940792,
      "grad_norm": 37.62053298950195,
      "learning_rate": 0.00017413833039177625,
      "loss": 0.2697,
      "step": 128600
    },
    {
      "epoch": 0.4198651342296111,
      "grad_norm": 0.0010604955023154616,
      "learning_rate": 0.00017404045973111665,
      "loss": 0.4261,
      "step": 128700
    },
    {
      "epoch": 0.42019136976514304,
      "grad_norm": 13.579972267150879,
      "learning_rate": 0.00017394258907045708,
      "loss": 0.3909,
      "step": 128800
    },
    {
      "epoch": 0.420517605300675,
      "grad_norm": 0.00639986339956522,
      "learning_rate": 0.0001738447184097975,
      "loss": 0.3139,
      "step": 128900
    },
    {
      "epoch": 0.4208438408362069,
      "grad_norm": 0.000328483380144462,
      "learning_rate": 0.00017374684774913788,
      "loss": 0.2872,
      "step": 129000
    },
    {
      "epoch": 0.42117007637173887,
      "grad_norm": 0.002997687319293618,
      "learning_rate": 0.0001736489770884783,
      "loss": 0.3564,
      "step": 129100
    },
    {
      "epoch": 0.4214963119072708,
      "grad_norm": 0.276875764131546,
      "learning_rate": 0.00017355110642781877,
      "loss": 0.3788,
      "step": 129200
    },
    {
      "epoch": 0.42182254744280273,
      "grad_norm": 0.11804624646902084,
      "learning_rate": 0.00017345323576715914,
      "loss": 0.4297,
      "step": 129300
    },
    {
      "epoch": 0.4221487829783347,
      "grad_norm": 0.014690608717501163,
      "learning_rate": 0.00017335536510649957,
      "loss": 0.3031,
      "step": 129400
    },
    {
      "epoch": 0.42247501851386665,
      "grad_norm": 38.040496826171875,
      "learning_rate": 0.00017325749444584,
      "loss": 0.496,
      "step": 129500
    },
    {
      "epoch": 0.4228012540493986,
      "grad_norm": 0.00031605135882273316,
      "learning_rate": 0.0001731596237851804,
      "loss": 0.23,
      "step": 129600
    },
    {
      "epoch": 0.4231274895849305,
      "grad_norm": 0.0005200842279009521,
      "learning_rate": 0.00017306175312452082,
      "loss": 0.2224,
      "step": 129700
    },
    {
      "epoch": 0.4234537251204625,
      "grad_norm": 0.0020036939531564713,
      "learning_rate": 0.00017296388246386125,
      "loss": 0.1965,
      "step": 129800
    },
    {
      "epoch": 0.42377996065599444,
      "grad_norm": 0.0008988323388621211,
      "learning_rate": 0.00017286601180320165,
      "loss": 0.4617,
      "step": 129900
    },
    {
      "epoch": 0.42410619619152634,
      "grad_norm": 0.0021361398976296186,
      "learning_rate": 0.00017276814114254208,
      "loss": 0.6274,
      "step": 130000
    },
    {
      "epoch": 0.4244324317270583,
      "grad_norm": 1.3894543647766113,
      "learning_rate": 0.00017267027048188248,
      "loss": 0.2941,
      "step": 130100
    },
    {
      "epoch": 0.42475866726259026,
      "grad_norm": 34.40394973754883,
      "learning_rate": 0.0001725723998212229,
      "loss": 0.5024,
      "step": 130200
    },
    {
      "epoch": 0.42508490279812217,
      "grad_norm": 16.499359130859375,
      "learning_rate": 0.00017247452916056334,
      "loss": 0.3469,
      "step": 130300
    },
    {
      "epoch": 0.4254111383336541,
      "grad_norm": 0.006916327401995659,
      "learning_rate": 0.00017237665849990374,
      "loss": 0.2284,
      "step": 130400
    },
    {
      "epoch": 0.4257373738691861,
      "grad_norm": 0.09842241555452347,
      "learning_rate": 0.00017227878783924417,
      "loss": 0.4597,
      "step": 130500
    },
    {
      "epoch": 0.426063609404718,
      "grad_norm": 0.002778989030048251,
      "learning_rate": 0.0001721809171785846,
      "loss": 0.3742,
      "step": 130600
    },
    {
      "epoch": 0.42638984494024995,
      "grad_norm": 0.010619613341987133,
      "learning_rate": 0.000172083046517925,
      "loss": 0.5017,
      "step": 130700
    },
    {
      "epoch": 0.4267160804757819,
      "grad_norm": 0.0011050779139623046,
      "learning_rate": 0.00017198517585726542,
      "loss": 0.4268,
      "step": 130800
    },
    {
      "epoch": 0.42704231601131387,
      "grad_norm": 0.22679953277111053,
      "learning_rate": 0.00017188730519660585,
      "loss": 0.4801,
      "step": 130900
    },
    {
      "epoch": 0.4273685515468458,
      "grad_norm": 0.009157170541584492,
      "learning_rate": 0.00017178943453594622,
      "loss": 0.2253,
      "step": 131000
    },
    {
      "epoch": 0.42769478708237774,
      "grad_norm": 11.758556365966797,
      "learning_rate": 0.00017169156387528665,
      "loss": 0.5585,
      "step": 131100
    },
    {
      "epoch": 0.4280210226179097,
      "grad_norm": 0.0014949707547202706,
      "learning_rate": 0.00017159369321462708,
      "loss": 0.2866,
      "step": 131200
    },
    {
      "epoch": 0.4283472581534416,
      "grad_norm": 0.0006321390392258763,
      "learning_rate": 0.00017149582255396748,
      "loss": 0.3958,
      "step": 131300
    },
    {
      "epoch": 0.42867349368897356,
      "grad_norm": 0.01728028617799282,
      "learning_rate": 0.0001713979518933079,
      "loss": 0.4417,
      "step": 131400
    },
    {
      "epoch": 0.4289997292245055,
      "grad_norm": 0.0011399323120713234,
      "learning_rate": 0.00017130008123264834,
      "loss": 0.2879,
      "step": 131500
    },
    {
      "epoch": 0.4293259647600374,
      "grad_norm": 0.003586955601349473,
      "learning_rate": 0.00017120221057198874,
      "loss": 0.3853,
      "step": 131600
    },
    {
      "epoch": 0.4296522002955694,
      "grad_norm": 0.0921081155538559,
      "learning_rate": 0.00017110433991132917,
      "loss": 0.3385,
      "step": 131700
    },
    {
      "epoch": 0.42997843583110135,
      "grad_norm": 12.654778480529785,
      "learning_rate": 0.0001710064692506696,
      "loss": 0.4837,
      "step": 131800
    },
    {
      "epoch": 0.4303046713666333,
      "grad_norm": 0.0011412324383854866,
      "learning_rate": 0.00017090859859001,
      "loss": 0.2723,
      "step": 131900
    },
    {
      "epoch": 0.4306309069021652,
      "grad_norm": 0.0005304324440658092,
      "learning_rate": 0.00017081072792935042,
      "loss": 0.5711,
      "step": 132000
    },
    {
      "epoch": 0.43095714243769717,
      "grad_norm": 0.0021045436151325703,
      "learning_rate": 0.00017071285726869085,
      "loss": 0.4645,
      "step": 132100
    },
    {
      "epoch": 0.43128337797322913,
      "grad_norm": 0.00034716964000836015,
      "learning_rate": 0.00017061498660803125,
      "loss": 0.3223,
      "step": 132200
    },
    {
      "epoch": 0.43160961350876104,
      "grad_norm": 0.007833505980670452,
      "learning_rate": 0.00017051711594737168,
      "loss": 0.2991,
      "step": 132300
    },
    {
      "epoch": 0.431935849044293,
      "grad_norm": 0.0006830692291259766,
      "learning_rate": 0.00017041924528671208,
      "loss": 0.3405,
      "step": 132400
    },
    {
      "epoch": 0.43226208457982496,
      "grad_norm": 0.004460094030946493,
      "learning_rate": 0.0001703213746260525,
      "loss": 0.3909,
      "step": 132500
    },
    {
      "epoch": 0.43258832011535686,
      "grad_norm": 0.005908605642616749,
      "learning_rate": 0.00017022350396539294,
      "loss": 0.3155,
      "step": 132600
    },
    {
      "epoch": 0.4329145556508888,
      "grad_norm": 0.00813428033143282,
      "learning_rate": 0.0001701256333047333,
      "loss": 0.6828,
      "step": 132700
    },
    {
      "epoch": 0.4332407911864208,
      "grad_norm": 0.00014171672228258103,
      "learning_rate": 0.00017002776264407374,
      "loss": 0.3064,
      "step": 132800
    },
    {
      "epoch": 0.43356702672195274,
      "grad_norm": 1.4416849613189697,
      "learning_rate": 0.0001699298919834142,
      "loss": 0.4404,
      "step": 132900
    },
    {
      "epoch": 0.43389326225748465,
      "grad_norm": 0.0018083518370985985,
      "learning_rate": 0.00016983202132275457,
      "loss": 0.1673,
      "step": 133000
    },
    {
      "epoch": 0.4342194977930166,
      "grad_norm": 0.05078829452395439,
      "learning_rate": 0.000169734150662095,
      "loss": 0.3368,
      "step": 133100
    },
    {
      "epoch": 0.43454573332854857,
      "grad_norm": 0.0013024825602769852,
      "learning_rate": 0.00016963628000143542,
      "loss": 0.4925,
      "step": 133200
    },
    {
      "epoch": 0.43487196886408047,
      "grad_norm": 0.2904333174228668,
      "learning_rate": 0.00016953840934077582,
      "loss": 0.4258,
      "step": 133300
    },
    {
      "epoch": 0.43519820439961243,
      "grad_norm": 8.815321922302246,
      "learning_rate": 0.00016944053868011625,
      "loss": 0.4087,
      "step": 133400
    },
    {
      "epoch": 0.4355244399351444,
      "grad_norm": 0.04750322178006172,
      "learning_rate": 0.00016934266801945668,
      "loss": 0.384,
      "step": 133500
    },
    {
      "epoch": 0.4358506754706763,
      "grad_norm": 2.884305953979492,
      "learning_rate": 0.00016924479735879708,
      "loss": 0.4842,
      "step": 133600
    },
    {
      "epoch": 0.43617691100620826,
      "grad_norm": 14.632200241088867,
      "learning_rate": 0.0001691469266981375,
      "loss": 0.2988,
      "step": 133700
    },
    {
      "epoch": 0.4365031465417402,
      "grad_norm": 30.436296463012695,
      "learning_rate": 0.00016904905603747794,
      "loss": 0.458,
      "step": 133800
    },
    {
      "epoch": 0.4368293820772721,
      "grad_norm": 0.010109923779964447,
      "learning_rate": 0.00016895118537681834,
      "loss": 0.2676,
      "step": 133900
    },
    {
      "epoch": 0.4371556176128041,
      "grad_norm": 1.06951904296875,
      "learning_rate": 0.00016885331471615876,
      "loss": 0.4088,
      "step": 134000
    },
    {
      "epoch": 0.43748185314833604,
      "grad_norm": 107.46272277832031,
      "learning_rate": 0.0001687554440554992,
      "loss": 0.3657,
      "step": 134100
    },
    {
      "epoch": 0.437808088683868,
      "grad_norm": 0.00046325914445333183,
      "learning_rate": 0.0001686575733948396,
      "loss": 0.2443,
      "step": 134200
    },
    {
      "epoch": 0.4381343242193999,
      "grad_norm": 0.002522572409361601,
      "learning_rate": 0.00016855970273418002,
      "loss": 0.4459,
      "step": 134300
    },
    {
      "epoch": 0.43846055975493187,
      "grad_norm": 3.155470132827759,
      "learning_rate": 0.00016846183207352045,
      "loss": 0.4233,
      "step": 134400
    },
    {
      "epoch": 0.4387867952904638,
      "grad_norm": 0.010475062765181065,
      "learning_rate": 0.00016836396141286085,
      "loss": 0.4357,
      "step": 134500
    },
    {
      "epoch": 0.43911303082599573,
      "grad_norm": 0.8838623762130737,
      "learning_rate": 0.00016826609075220128,
      "loss": 0.3379,
      "step": 134600
    },
    {
      "epoch": 0.4394392663615277,
      "grad_norm": 0.0008078829851001501,
      "learning_rate": 0.00016816822009154165,
      "loss": 0.4527,
      "step": 134700
    },
    {
      "epoch": 0.43976550189705965,
      "grad_norm": 0.003494078991934657,
      "learning_rate": 0.00016807034943088208,
      "loss": 0.4505,
      "step": 134800
    },
    {
      "epoch": 0.44009173743259156,
      "grad_norm": 36.08433151245117,
      "learning_rate": 0.0001679724787702225,
      "loss": 0.5398,
      "step": 134900
    },
    {
      "epoch": 0.4404179729681235,
      "grad_norm": 34.2829475402832,
      "learning_rate": 0.0001678746081095629,
      "loss": 0.5287,
      "step": 135000
    },
    {
      "epoch": 0.4407442085036555,
      "grad_norm": 62.07378005981445,
      "learning_rate": 0.00016777673744890334,
      "loss": 0.4618,
      "step": 135100
    },
    {
      "epoch": 0.44107044403918744,
      "grad_norm": 41.484130859375,
      "learning_rate": 0.00016767886678824376,
      "loss": 0.5312,
      "step": 135200
    },
    {
      "epoch": 0.44139667957471934,
      "grad_norm": 111.07237243652344,
      "learning_rate": 0.00016758099612758416,
      "loss": 0.3164,
      "step": 135300
    },
    {
      "epoch": 0.4417229151102513,
      "grad_norm": 0.002608555369079113,
      "learning_rate": 0.0001674831254669246,
      "loss": 0.2503,
      "step": 135400
    },
    {
      "epoch": 0.44204915064578326,
      "grad_norm": 0.00795060582458973,
      "learning_rate": 0.00016738525480626502,
      "loss": 0.5177,
      "step": 135500
    },
    {
      "epoch": 0.44237538618131517,
      "grad_norm": 0.3150801956653595,
      "learning_rate": 0.00016728738414560542,
      "loss": 0.3514,
      "step": 135600
    },
    {
      "epoch": 0.4427016217168471,
      "grad_norm": 0.001980229513719678,
      "learning_rate": 0.00016718951348494585,
      "loss": 0.3863,
      "step": 135700
    },
    {
      "epoch": 0.4430278572523791,
      "grad_norm": 0.0005502094863913953,
      "learning_rate": 0.00016709164282428628,
      "loss": 0.3653,
      "step": 135800
    },
    {
      "epoch": 0.443354092787911,
      "grad_norm": 0.09172764420509338,
      "learning_rate": 0.00016699377216362668,
      "loss": 0.5335,
      "step": 135900
    },
    {
      "epoch": 0.44368032832344295,
      "grad_norm": 0.7758982181549072,
      "learning_rate": 0.0001668959015029671,
      "loss": 0.2339,
      "step": 136000
    },
    {
      "epoch": 0.4440065638589749,
      "grad_norm": 14.367793083190918,
      "learning_rate": 0.00016679803084230753,
      "loss": 0.2618,
      "step": 136100
    },
    {
      "epoch": 0.44433279939450687,
      "grad_norm": 16.536457061767578,
      "learning_rate": 0.00016670016018164793,
      "loss": 0.5463,
      "step": 136200
    },
    {
      "epoch": 0.4446590349300388,
      "grad_norm": 52.626731872558594,
      "learning_rate": 0.00016660228952098836,
      "loss": 0.2812,
      "step": 136300
    },
    {
      "epoch": 0.44498527046557074,
      "grad_norm": 28.676515579223633,
      "learning_rate": 0.0001665044188603288,
      "loss": 0.4096,
      "step": 136400
    },
    {
      "epoch": 0.4453115060011027,
      "grad_norm": 0.0005993638187646866,
      "learning_rate": 0.00016640654819966916,
      "loss": 0.5076,
      "step": 136500
    },
    {
      "epoch": 0.4456377415366346,
      "grad_norm": 0.17330215871334076,
      "learning_rate": 0.0001663086775390096,
      "loss": 0.2625,
      "step": 136600
    },
    {
      "epoch": 0.44596397707216656,
      "grad_norm": 0.037257902324199677,
      "learning_rate": 0.00016621080687835,
      "loss": 0.5152,
      "step": 136700
    },
    {
      "epoch": 0.4462902126076985,
      "grad_norm": 43.58877944946289,
      "learning_rate": 0.00016611293621769042,
      "loss": 0.5084,
      "step": 136800
    },
    {
      "epoch": 0.4466164481432304,
      "grad_norm": 0.0011622804449871182,
      "learning_rate": 0.00016601506555703085,
      "loss": 0.4404,
      "step": 136900
    },
    {
      "epoch": 0.4469426836787624,
      "grad_norm": 0.0008933161152526736,
      "learning_rate": 0.00016591719489637125,
      "loss": 0.33,
      "step": 137000
    },
    {
      "epoch": 0.44726891921429435,
      "grad_norm": 0.0005909425090067089,
      "learning_rate": 0.00016581932423571168,
      "loss": 0.4446,
      "step": 137100
    },
    {
      "epoch": 0.4475951547498263,
      "grad_norm": 0.14536435902118683,
      "learning_rate": 0.0001657214535750521,
      "loss": 0.3661,
      "step": 137200
    },
    {
      "epoch": 0.4479213902853582,
      "grad_norm": 0.00025678795645944774,
      "learning_rate": 0.0001656235829143925,
      "loss": 0.3412,
      "step": 137300
    },
    {
      "epoch": 0.44824762582089017,
      "grad_norm": 0.16649986803531647,
      "learning_rate": 0.00016552571225373293,
      "loss": 0.3981,
      "step": 137400
    },
    {
      "epoch": 0.44857386135642213,
      "grad_norm": 0.0022939699701964855,
      "learning_rate": 0.00016542784159307336,
      "loss": 0.6369,
      "step": 137500
    },
    {
      "epoch": 0.44890009689195404,
      "grad_norm": 0.0008845075499266386,
      "learning_rate": 0.00016532997093241376,
      "loss": 0.4073,
      "step": 137600
    },
    {
      "epoch": 0.449226332427486,
      "grad_norm": 0.017726115882396698,
      "learning_rate": 0.0001652321002717542,
      "loss": 0.2759,
      "step": 137700
    },
    {
      "epoch": 0.44955256796301796,
      "grad_norm": 0.0002147447521565482,
      "learning_rate": 0.00016513422961109462,
      "loss": 0.4537,
      "step": 137800
    },
    {
      "epoch": 0.44987880349854986,
      "grad_norm": 1.9620294570922852,
      "learning_rate": 0.00016503635895043502,
      "loss": 0.4241,
      "step": 137900
    },
    {
      "epoch": 0.4502050390340818,
      "grad_norm": 27.0848388671875,
      "learning_rate": 0.00016493848828977545,
      "loss": 0.2673,
      "step": 138000
    },
    {
      "epoch": 0.4505312745696138,
      "grad_norm": 0.0004970175796188414,
      "learning_rate": 0.00016484061762911587,
      "loss": 0.3884,
      "step": 138100
    },
    {
      "epoch": 0.4508575101051457,
      "grad_norm": 14.215494155883789,
      "learning_rate": 0.00016474274696845628,
      "loss": 0.4346,
      "step": 138200
    },
    {
      "epoch": 0.45118374564067765,
      "grad_norm": 0.34379929304122925,
      "learning_rate": 0.0001646448763077967,
      "loss": 0.4792,
      "step": 138300
    },
    {
      "epoch": 0.4515099811762096,
      "grad_norm": 0.0012591556878760457,
      "learning_rate": 0.00016454700564713713,
      "loss": 0.4326,
      "step": 138400
    },
    {
      "epoch": 0.45183621671174157,
      "grad_norm": 0.00011232485121581703,
      "learning_rate": 0.0001644491349864775,
      "loss": 0.2339,
      "step": 138500
    },
    {
      "epoch": 0.45216245224727347,
      "grad_norm": 0.0005081266281194985,
      "learning_rate": 0.00016435126432581793,
      "loss": 0.4049,
      "step": 138600
    },
    {
      "epoch": 0.45248868778280543,
      "grad_norm": 25.651325225830078,
      "learning_rate": 0.00016425339366515836,
      "loss": 0.3026,
      "step": 138700
    },
    {
      "epoch": 0.4528149233183374,
      "grad_norm": 0.030120238661766052,
      "learning_rate": 0.00016415552300449876,
      "loss": 0.4033,
      "step": 138800
    },
    {
      "epoch": 0.4531411588538693,
      "grad_norm": 65.34574890136719,
      "learning_rate": 0.0001640576523438392,
      "loss": 0.2449,
      "step": 138900
    },
    {
      "epoch": 0.45346739438940126,
      "grad_norm": 0.0015776981599628925,
      "learning_rate": 0.0001639597816831796,
      "loss": 0.473,
      "step": 139000
    },
    {
      "epoch": 0.4537936299249332,
      "grad_norm": 0.005462561268359423,
      "learning_rate": 0.00016386191102252002,
      "loss": 0.3346,
      "step": 139100
    },
    {
      "epoch": 0.4541198654604651,
      "grad_norm": 2.763805627822876,
      "learning_rate": 0.00016376404036186045,
      "loss": 0.3159,
      "step": 139200
    },
    {
      "epoch": 0.4544461009959971,
      "grad_norm": 0.37041786313056946,
      "learning_rate": 0.00016366616970120085,
      "loss": 0.3942,
      "step": 139300
    },
    {
      "epoch": 0.45477233653152904,
      "grad_norm": 0.0001922138180816546,
      "learning_rate": 0.00016356829904054127,
      "loss": 0.6379,
      "step": 139400
    },
    {
      "epoch": 0.455098572067061,
      "grad_norm": 44.036197662353516,
      "learning_rate": 0.0001634704283798817,
      "loss": 0.3031,
      "step": 139500
    },
    {
      "epoch": 0.4554248076025929,
      "grad_norm": 0.1959654539823532,
      "learning_rate": 0.0001633725577192221,
      "loss": 0.4517,
      "step": 139600
    },
    {
      "epoch": 0.45575104313812487,
      "grad_norm": 0.0006923917680978775,
      "learning_rate": 0.00016327468705856253,
      "loss": 0.577,
      "step": 139700
    },
    {
      "epoch": 0.4560772786736568,
      "grad_norm": 48.99980545043945,
      "learning_rate": 0.00016317681639790296,
      "loss": 0.2886,
      "step": 139800
    },
    {
      "epoch": 0.45640351420918873,
      "grad_norm": 0.0034788944758474827,
      "learning_rate": 0.00016307894573724336,
      "loss": 0.3464,
      "step": 139900
    },
    {
      "epoch": 0.4567297497447207,
      "grad_norm": 0.0745861753821373,
      "learning_rate": 0.0001629810750765838,
      "loss": 0.3922,
      "step": 140000
    },
    {
      "epoch": 0.45705598528025265,
      "grad_norm": 0.007224796339869499,
      "learning_rate": 0.00016288320441592422,
      "loss": 0.2929,
      "step": 140100
    },
    {
      "epoch": 0.45738222081578456,
      "grad_norm": 1.0123238563537598,
      "learning_rate": 0.0001627853337552646,
      "loss": 0.5354,
      "step": 140200
    },
    {
      "epoch": 0.4577084563513165,
      "grad_norm": 0.0009190115961246192,
      "learning_rate": 0.00016268746309460502,
      "loss": 0.3502,
      "step": 140300
    },
    {
      "epoch": 0.4580346918868485,
      "grad_norm": 8.356532096862793,
      "learning_rate": 0.00016258959243394547,
      "loss": 0.2243,
      "step": 140400
    },
    {
      "epoch": 0.45836092742238044,
      "grad_norm": 0.003827219596132636,
      "learning_rate": 0.00016249172177328585,
      "loss": 0.2467,
      "step": 140500
    },
    {
      "epoch": 0.45868716295791234,
      "grad_norm": 0.0012992876581847668,
      "learning_rate": 0.00016239385111262627,
      "loss": 0.4777,
      "step": 140600
    },
    {
      "epoch": 0.4590133984934443,
      "grad_norm": 72.6592788696289,
      "learning_rate": 0.0001622959804519667,
      "loss": 0.3995,
      "step": 140700
    },
    {
      "epoch": 0.45933963402897626,
      "grad_norm": 0.0005862943362444639,
      "learning_rate": 0.0001621981097913071,
      "loss": 0.2563,
      "step": 140800
    },
    {
      "epoch": 0.45966586956450817,
      "grad_norm": 17.441282272338867,
      "learning_rate": 0.00016210023913064753,
      "loss": 0.2842,
      "step": 140900
    },
    {
      "epoch": 0.4599921051000401,
      "grad_norm": 33.93928527832031,
      "learning_rate": 0.00016200236846998796,
      "loss": 0.2878,
      "step": 141000
    },
    {
      "epoch": 0.4603183406355721,
      "grad_norm": 0.00036439558607526124,
      "learning_rate": 0.00016190449780932836,
      "loss": 0.2932,
      "step": 141100
    },
    {
      "epoch": 0.460644576171104,
      "grad_norm": 0.009810126386582851,
      "learning_rate": 0.0001618066271486688,
      "loss": 0.4698,
      "step": 141200
    },
    {
      "epoch": 0.46097081170663595,
      "grad_norm": 0.0018569550011307001,
      "learning_rate": 0.0001617087564880092,
      "loss": 0.4055,
      "step": 141300
    },
    {
      "epoch": 0.4612970472421679,
      "grad_norm": 0.003188523231074214,
      "learning_rate": 0.00016161088582734962,
      "loss": 0.3108,
      "step": 141400
    },
    {
      "epoch": 0.4616232827776998,
      "grad_norm": 4.867766857147217,
      "learning_rate": 0.00016151301516669004,
      "loss": 0.4564,
      "step": 141500
    },
    {
      "epoch": 0.4619495183132318,
      "grad_norm": 0.11440563946962357,
      "learning_rate": 0.00016141514450603044,
      "loss": 0.4781,
      "step": 141600
    },
    {
      "epoch": 0.46227575384876374,
      "grad_norm": 0.21846231818199158,
      "learning_rate": 0.00016131727384537087,
      "loss": 0.3782,
      "step": 141700
    },
    {
      "epoch": 0.4626019893842957,
      "grad_norm": 0.0014842641539871693,
      "learning_rate": 0.0001612194031847113,
      "loss": 0.3974,
      "step": 141800
    },
    {
      "epoch": 0.4629282249198276,
      "grad_norm": 0.0025592318270355463,
      "learning_rate": 0.0001611215325240517,
      "loss": 0.3303,
      "step": 141900
    },
    {
      "epoch": 0.46325446045535956,
      "grad_norm": 0.0009974214481189847,
      "learning_rate": 0.00016102366186339213,
      "loss": 0.3707,
      "step": 142000
    },
    {
      "epoch": 0.4635806959908915,
      "grad_norm": 0.00479794992133975,
      "learning_rate": 0.00016092579120273256,
      "loss": 0.1841,
      "step": 142100
    },
    {
      "epoch": 0.4639069315264234,
      "grad_norm": 20.307178497314453,
      "learning_rate": 0.00016082792054207293,
      "loss": 0.5306,
      "step": 142200
    },
    {
      "epoch": 0.4642331670619554,
      "grad_norm": 0.00016860909818205982,
      "learning_rate": 0.00016073004988141336,
      "loss": 0.4009,
      "step": 142300
    },
    {
      "epoch": 0.46455940259748735,
      "grad_norm": 0.015065127983689308,
      "learning_rate": 0.0001606321792207538,
      "loss": 0.4253,
      "step": 142400
    },
    {
      "epoch": 0.46488563813301925,
      "grad_norm": 0.5787288546562195,
      "learning_rate": 0.0001605343085600942,
      "loss": 0.3799,
      "step": 142500
    },
    {
      "epoch": 0.4652118736685512,
      "grad_norm": 0.30100584030151367,
      "learning_rate": 0.00016043643789943462,
      "loss": 0.4337,
      "step": 142600
    },
    {
      "epoch": 0.4655381092040832,
      "grad_norm": 0.00027627404779195786,
      "learning_rate": 0.00016033856723877504,
      "loss": 0.4869,
      "step": 142700
    },
    {
      "epoch": 0.46586434473961513,
      "grad_norm": 0.0008994677336886525,
      "learning_rate": 0.00016024069657811544,
      "loss": 0.5338,
      "step": 142800
    },
    {
      "epoch": 0.46619058027514704,
      "grad_norm": 0.000570730131585151,
      "learning_rate": 0.00016014282591745587,
      "loss": 0.5583,
      "step": 142900
    },
    {
      "epoch": 0.466516815810679,
      "grad_norm": 0.0001463753287680447,
      "learning_rate": 0.0001600449552567963,
      "loss": 0.3094,
      "step": 143000
    },
    {
      "epoch": 0.46684305134621096,
      "grad_norm": 95.8818130493164,
      "learning_rate": 0.0001599470845961367,
      "loss": 0.313,
      "step": 143100
    },
    {
      "epoch": 0.46716928688174286,
      "grad_norm": 0.00042204748024232686,
      "learning_rate": 0.00015984921393547713,
      "loss": 0.3617,
      "step": 143200
    },
    {
      "epoch": 0.4674955224172748,
      "grad_norm": 0.0012793419882655144,
      "learning_rate": 0.00015975134327481753,
      "loss": 0.3026,
      "step": 143300
    },
    {
      "epoch": 0.4678217579528068,
      "grad_norm": 0.003797754645347595,
      "learning_rate": 0.00015965347261415796,
      "loss": 0.4924,
      "step": 143400
    },
    {
      "epoch": 0.4681479934883387,
      "grad_norm": 39.13474655151367,
      "learning_rate": 0.00015955560195349839,
      "loss": 0.3996,
      "step": 143500
    },
    {
      "epoch": 0.46847422902387065,
      "grad_norm": 75.17142486572266,
      "learning_rate": 0.00015945773129283879,
      "loss": 0.4948,
      "step": 143600
    },
    {
      "epoch": 0.4688004645594026,
      "grad_norm": 0.01110087614506483,
      "learning_rate": 0.00015935986063217921,
      "loss": 0.4354,
      "step": 143700
    },
    {
      "epoch": 0.46912670009493457,
      "grad_norm": 18.3555965423584,
      "learning_rate": 0.00015926198997151964,
      "loss": 0.3337,
      "step": 143800
    },
    {
      "epoch": 0.4694529356304665,
      "grad_norm": 0.0023767403326928616,
      "learning_rate": 0.00015916411931086002,
      "loss": 0.3235,
      "step": 143900
    },
    {
      "epoch": 0.46977917116599843,
      "grad_norm": 0.35763201117515564,
      "learning_rate": 0.00015906624865020044,
      "loss": 0.31,
      "step": 144000
    },
    {
      "epoch": 0.4701054067015304,
      "grad_norm": 0.0011388070415705442,
      "learning_rate": 0.00015896837798954087,
      "loss": 0.5043,
      "step": 144100
    },
    {
      "epoch": 0.4704316422370623,
      "grad_norm": 1.009813666343689,
      "learning_rate": 0.00015887050732888127,
      "loss": 0.3496,
      "step": 144200
    },
    {
      "epoch": 0.47075787777259426,
      "grad_norm": 0.06021301820874214,
      "learning_rate": 0.0001587726366682217,
      "loss": 0.7152,
      "step": 144300
    },
    {
      "epoch": 0.4710841133081262,
      "grad_norm": 0.0021681159269064665,
      "learning_rate": 0.00015867476600756213,
      "loss": 0.4183,
      "step": 144400
    },
    {
      "epoch": 0.4714103488436581,
      "grad_norm": 113.65419006347656,
      "learning_rate": 0.00015857689534690253,
      "loss": 0.3084,
      "step": 144500
    },
    {
      "epoch": 0.4717365843791901,
      "grad_norm": 0.00012826263264287263,
      "learning_rate": 0.00015847902468624296,
      "loss": 0.4187,
      "step": 144600
    },
    {
      "epoch": 0.47206281991472204,
      "grad_norm": 20.319416046142578,
      "learning_rate": 0.00015838115402558338,
      "loss": 0.2939,
      "step": 144700
    },
    {
      "epoch": 0.47238905545025395,
      "grad_norm": 9.764513969421387,
      "learning_rate": 0.00015828328336492379,
      "loss": 0.2039,
      "step": 144800
    },
    {
      "epoch": 0.4727152909857859,
      "grad_norm": 0.001521817408502102,
      "learning_rate": 0.00015818541270426421,
      "loss": 0.5501,
      "step": 144900
    },
    {
      "epoch": 0.47304152652131787,
      "grad_norm": 0.017655646428465843,
      "learning_rate": 0.00015808754204360464,
      "loss": 0.3214,
      "step": 145000
    },
    {
      "epoch": 0.47336776205684983,
      "grad_norm": 0.006112837698310614,
      "learning_rate": 0.00015798967138294504,
      "loss": 0.5066,
      "step": 145100
    },
    {
      "epoch": 0.47369399759238173,
      "grad_norm": 0.0005072050844319165,
      "learning_rate": 0.00015789180072228547,
      "loss": 0.3059,
      "step": 145200
    },
    {
      "epoch": 0.4740202331279137,
      "grad_norm": 0.00580041017383337,
      "learning_rate": 0.0001577939300616259,
      "loss": 0.1996,
      "step": 145300
    },
    {
      "epoch": 0.47434646866344565,
      "grad_norm": 0.008984391577541828,
      "learning_rate": 0.0001576960594009663,
      "loss": 0.4458,
      "step": 145400
    },
    {
      "epoch": 0.47467270419897756,
      "grad_norm": 3.2523441314697266,
      "learning_rate": 0.00015759818874030673,
      "loss": 0.4023,
      "step": 145500
    },
    {
      "epoch": 0.4749989397345095,
      "grad_norm": 0.0004962665843777359,
      "learning_rate": 0.00015750031807964713,
      "loss": 0.3859,
      "step": 145600
    },
    {
      "epoch": 0.4753251752700415,
      "grad_norm": 0.0014115424128249288,
      "learning_rate": 0.00015740244741898756,
      "loss": 0.5508,
      "step": 145700
    },
    {
      "epoch": 0.4756514108055734,
      "grad_norm": 0.0027557755820453167,
      "learning_rate": 0.00015730457675832798,
      "loss": 0.3811,
      "step": 145800
    },
    {
      "epoch": 0.47597764634110534,
      "grad_norm": 3.2784953117370605,
      "learning_rate": 0.00015720670609766836,
      "loss": 0.2985,
      "step": 145900
    },
    {
      "epoch": 0.4763038818766373,
      "grad_norm": 0.1319790631532669,
      "learning_rate": 0.00015710883543700879,
      "loss": 0.1448,
      "step": 146000
    },
    {
      "epoch": 0.47663011741216926,
      "grad_norm": 13.331210136413574,
      "learning_rate": 0.0001570109647763492,
      "loss": 0.6395,
      "step": 146100
    },
    {
      "epoch": 0.47695635294770117,
      "grad_norm": 49.06583786010742,
      "learning_rate": 0.00015691309411568961,
      "loss": 0.3102,
      "step": 146200
    },
    {
      "epoch": 0.47728258848323313,
      "grad_norm": 0.004613977856934071,
      "learning_rate": 0.00015681522345503004,
      "loss": 0.3229,
      "step": 146300
    },
    {
      "epoch": 0.4776088240187651,
      "grad_norm": 0.7436861395835876,
      "learning_rate": 0.00015671735279437047,
      "loss": 0.5199,
      "step": 146400
    },
    {
      "epoch": 0.477935059554297,
      "grad_norm": 0.00031212568865157664,
      "learning_rate": 0.00015661948213371087,
      "loss": 0.3649,
      "step": 146500
    },
    {
      "epoch": 0.47826129508982895,
      "grad_norm": 0.2117350846529007,
      "learning_rate": 0.0001565216114730513,
      "loss": 0.2769,
      "step": 146600
    },
    {
      "epoch": 0.4785875306253609,
      "grad_norm": 95.05084228515625,
      "learning_rate": 0.00015642374081239173,
      "loss": 0.5726,
      "step": 146700
    },
    {
      "epoch": 0.4789137661608928,
      "grad_norm": 0.0009529793169349432,
      "learning_rate": 0.00015632587015173213,
      "loss": 0.3224,
      "step": 146800
    },
    {
      "epoch": 0.4792400016964248,
      "grad_norm": 8.676252365112305,
      "learning_rate": 0.00015622799949107256,
      "loss": 0.3111,
      "step": 146900
    },
    {
      "epoch": 0.47956623723195674,
      "grad_norm": 0.01270705834031105,
      "learning_rate": 0.00015613012883041298,
      "loss": 0.4192,
      "step": 147000
    },
    {
      "epoch": 0.4798924727674887,
      "grad_norm": 0.0020042939577251673,
      "learning_rate": 0.00015603225816975338,
      "loss": 0.2574,
      "step": 147100
    },
    {
      "epoch": 0.4802187083030206,
      "grad_norm": 0.007605019025504589,
      "learning_rate": 0.0001559343875090938,
      "loss": 0.4519,
      "step": 147200
    },
    {
      "epoch": 0.48054494383855256,
      "grad_norm": 0.0015551506076008081,
      "learning_rate": 0.00015583651684843424,
      "loss": 0.2987,
      "step": 147300
    },
    {
      "epoch": 0.4808711793740845,
      "grad_norm": 0.0012433403171598911,
      "learning_rate": 0.00015573864618777464,
      "loss": 0.6481,
      "step": 147400
    },
    {
      "epoch": 0.4811974149096164,
      "grad_norm": 8.878755761543289e-05,
      "learning_rate": 0.00015564077552711507,
      "loss": 0.2054,
      "step": 147500
    },
    {
      "epoch": 0.4815236504451484,
      "grad_norm": 16.01812171936035,
      "learning_rate": 0.00015554290486645544,
      "loss": 0.4982,
      "step": 147600
    },
    {
      "epoch": 0.48184988598068035,
      "grad_norm": 3.1196510791778564,
      "learning_rate": 0.00015544503420579587,
      "loss": 0.2596,
      "step": 147700
    },
    {
      "epoch": 0.48217612151621225,
      "grad_norm": 0.018535101786255836,
      "learning_rate": 0.0001553471635451363,
      "loss": 0.4087,
      "step": 147800
    },
    {
      "epoch": 0.4825023570517442,
      "grad_norm": 1.0667030811309814,
      "learning_rate": 0.0001552492928844767,
      "loss": 0.4404,
      "step": 147900
    },
    {
      "epoch": 0.4828285925872762,
      "grad_norm": 0.0831330418586731,
      "learning_rate": 0.00015515142222381713,
      "loss": 0.4204,
      "step": 148000
    },
    {
      "epoch": 0.4831548281228081,
      "grad_norm": 0.0012200095225125551,
      "learning_rate": 0.00015505355156315755,
      "loss": 0.4653,
      "step": 148100
    },
    {
      "epoch": 0.48348106365834004,
      "grad_norm": 0.018387719988822937,
      "learning_rate": 0.00015495568090249796,
      "loss": 0.2843,
      "step": 148200
    },
    {
      "epoch": 0.483807299193872,
      "grad_norm": 0.001087484648451209,
      "learning_rate": 0.00015485781024183838,
      "loss": 0.4782,
      "step": 148300
    },
    {
      "epoch": 0.48413353472940396,
      "grad_norm": 6.982009654166177e-05,
      "learning_rate": 0.0001547599395811788,
      "loss": 0.3695,
      "step": 148400
    },
    {
      "epoch": 0.48445977026493586,
      "grad_norm": 0.0009461874142289162,
      "learning_rate": 0.0001546620689205192,
      "loss": 0.2237,
      "step": 148500
    },
    {
      "epoch": 0.4847860058004678,
      "grad_norm": 0.0014215705450624228,
      "learning_rate": 0.00015456419825985964,
      "loss": 0.3081,
      "step": 148600
    },
    {
      "epoch": 0.4851122413359998,
      "grad_norm": 0.002430662279948592,
      "learning_rate": 0.00015446632759920007,
      "loss": 0.4816,
      "step": 148700
    },
    {
      "epoch": 0.4854384768715317,
      "grad_norm": 0.0027422383427619934,
      "learning_rate": 0.00015436845693854047,
      "loss": 0.1887,
      "step": 148800
    },
    {
      "epoch": 0.48576471240706365,
      "grad_norm": 0.006888675503432751,
      "learning_rate": 0.0001542705862778809,
      "loss": 0.2134,
      "step": 148900
    },
    {
      "epoch": 0.4860909479425956,
      "grad_norm": 27.224220275878906,
      "learning_rate": 0.00015417271561722132,
      "loss": 0.394,
      "step": 149000
    },
    {
      "epoch": 0.4864171834781275,
      "grad_norm": 0.002205051016062498,
      "learning_rate": 0.00015407484495656173,
      "loss": 0.5316,
      "step": 149100
    },
    {
      "epoch": 0.4867434190136595,
      "grad_norm": 11.836227416992188,
      "learning_rate": 0.00015397697429590215,
      "loss": 0.5251,
      "step": 149200
    },
    {
      "epoch": 0.48706965454919143,
      "grad_norm": 16.99049186706543,
      "learning_rate": 0.00015387910363524258,
      "loss": 0.585,
      "step": 149300
    },
    {
      "epoch": 0.4873958900847234,
      "grad_norm": 7.381491661071777,
      "learning_rate": 0.00015378123297458298,
      "loss": 0.3853,
      "step": 149400
    },
    {
      "epoch": 0.4877221256202553,
      "grad_norm": 2.330609083175659,
      "learning_rate": 0.0001536833623139234,
      "loss": 0.3373,
      "step": 149500
    },
    {
      "epoch": 0.48804836115578726,
      "grad_norm": 0.00016496292664669454,
      "learning_rate": 0.00015358549165326384,
      "loss": 0.2906,
      "step": 149600
    },
    {
      "epoch": 0.4883745966913192,
      "grad_norm": 28.782617568969727,
      "learning_rate": 0.0001534876209926042,
      "loss": 0.453,
      "step": 149700
    },
    {
      "epoch": 0.4887008322268511,
      "grad_norm": 4.8765788960736245e-05,
      "learning_rate": 0.00015338975033194464,
      "loss": 0.2502,
      "step": 149800
    },
    {
      "epoch": 0.4890270677623831,
      "grad_norm": 0.015114451758563519,
      "learning_rate": 0.00015329187967128504,
      "loss": 0.3345,
      "step": 149900
    },
    {
      "epoch": 0.48935330329791504,
      "grad_norm": 22.466665267944336,
      "learning_rate": 0.00015319400901062547,
      "loss": 0.4394,
      "step": 150000
    },
    {
      "epoch": 0.48967953883344695,
      "grad_norm": 0.0009926927741616964,
      "learning_rate": 0.0001530961383499659,
      "loss": 0.3151,
      "step": 150100
    },
    {
      "epoch": 0.4900057743689789,
      "grad_norm": 0.0058195884339511395,
      "learning_rate": 0.0001529982676893063,
      "loss": 0.5873,
      "step": 150200
    },
    {
      "epoch": 0.49033200990451087,
      "grad_norm": 0.006223119795322418,
      "learning_rate": 0.00015290039702864672,
      "loss": 0.3831,
      "step": 150300
    },
    {
      "epoch": 0.49065824544004283,
      "grad_norm": 0.20475049316883087,
      "learning_rate": 0.00015280252636798715,
      "loss": 0.2948,
      "step": 150400
    },
    {
      "epoch": 0.49098448097557473,
      "grad_norm": 0.009171347133815289,
      "learning_rate": 0.00015270465570732755,
      "loss": 0.1997,
      "step": 150500
    },
    {
      "epoch": 0.4913107165111067,
      "grad_norm": 0.0018129562959074974,
      "learning_rate": 0.00015260678504666798,
      "loss": 0.2072,
      "step": 150600
    },
    {
      "epoch": 0.49163695204663865,
      "grad_norm": 89.61516571044922,
      "learning_rate": 0.0001525089143860084,
      "loss": 0.3857,
      "step": 150700
    },
    {
      "epoch": 0.49196318758217056,
      "grad_norm": 0.014820655807852745,
      "learning_rate": 0.0001524110437253488,
      "loss": 0.3769,
      "step": 150800
    },
    {
      "epoch": 0.4922894231177025,
      "grad_norm": 40.40397644042969,
      "learning_rate": 0.00015231317306468924,
      "loss": 0.4857,
      "step": 150900
    },
    {
      "epoch": 0.4926156586532345,
      "grad_norm": 0.004574467893689871,
      "learning_rate": 0.00015221530240402967,
      "loss": 0.3879,
      "step": 151000
    },
    {
      "epoch": 0.4929418941887664,
      "grad_norm": 0.0022805093321949244,
      "learning_rate": 0.00015211743174337007,
      "loss": 0.3641,
      "step": 151100
    },
    {
      "epoch": 0.49326812972429834,
      "grad_norm": 0.05199994519352913,
      "learning_rate": 0.0001520195610827105,
      "loss": 0.392,
      "step": 151200
    },
    {
      "epoch": 0.4935943652598303,
      "grad_norm": 102.70703887939453,
      "learning_rate": 0.00015192169042205092,
      "loss": 0.5007,
      "step": 151300
    },
    {
      "epoch": 0.4939206007953622,
      "grad_norm": 0.006623428780585527,
      "learning_rate": 0.0001518238197613913,
      "loss": 0.4201,
      "step": 151400
    },
    {
      "epoch": 0.49424683633089417,
      "grad_norm": 0.7427207827568054,
      "learning_rate": 0.00015172594910073172,
      "loss": 0.4479,
      "step": 151500
    },
    {
      "epoch": 0.49457307186642613,
      "grad_norm": 0.0015368913300335407,
      "learning_rate": 0.00015162807844007215,
      "loss": 0.426,
      "step": 151600
    },
    {
      "epoch": 0.4948993074019581,
      "grad_norm": 29.907241821289062,
      "learning_rate": 0.00015153020777941255,
      "loss": 0.3419,
      "step": 151700
    },
    {
      "epoch": 0.49522554293749,
      "grad_norm": 27.629179000854492,
      "learning_rate": 0.00015143233711875298,
      "loss": 0.4921,
      "step": 151800
    },
    {
      "epoch": 0.49555177847302195,
      "grad_norm": 0.0002468173624947667,
      "learning_rate": 0.0001513344664580934,
      "loss": 0.4372,
      "step": 151900
    },
    {
      "epoch": 0.4958780140085539,
      "grad_norm": 61.24134063720703,
      "learning_rate": 0.0001512365957974338,
      "loss": 0.6507,
      "step": 152000
    },
    {
      "epoch": 0.4962042495440858,
      "grad_norm": 0.001390888704918325,
      "learning_rate": 0.00015113872513677424,
      "loss": 0.4248,
      "step": 152100
    },
    {
      "epoch": 0.4965304850796178,
      "grad_norm": 30.796443939208984,
      "learning_rate": 0.00015104085447611464,
      "loss": 0.3594,
      "step": 152200
    },
    {
      "epoch": 0.49685672061514974,
      "grad_norm": 5.962974548339844,
      "learning_rate": 0.00015094298381545507,
      "loss": 0.3496,
      "step": 152300
    },
    {
      "epoch": 0.49718295615068164,
      "grad_norm": 0.011824662797152996,
      "learning_rate": 0.0001508451131547955,
      "loss": 0.599,
      "step": 152400
    },
    {
      "epoch": 0.4975091916862136,
      "grad_norm": 0.012647443450987339,
      "learning_rate": 0.0001507472424941359,
      "loss": 0.4356,
      "step": 152500
    },
    {
      "epoch": 0.49783542722174556,
      "grad_norm": 8.404675463680178e-05,
      "learning_rate": 0.00015064937183347632,
      "loss": 0.2593,
      "step": 152600
    },
    {
      "epoch": 0.4981616627572775,
      "grad_norm": 0.21267114579677582,
      "learning_rate": 0.00015055150117281675,
      "loss": 0.359,
      "step": 152700
    },
    {
      "epoch": 0.49848789829280943,
      "grad_norm": 0.07793522626161575,
      "learning_rate": 0.00015045363051215715,
      "loss": 0.1074,
      "step": 152800
    },
    {
      "epoch": 0.4988141338283414,
      "grad_norm": 0.00046340253902599216,
      "learning_rate": 0.00015035575985149758,
      "loss": 0.3057,
      "step": 152900
    },
    {
      "epoch": 0.49914036936387335,
      "grad_norm": 0.0006107307272031903,
      "learning_rate": 0.000150257889190838,
      "loss": 0.3282,
      "step": 153000
    },
    {
      "epoch": 0.49946660489940525,
      "grad_norm": 0.0003539597964845598,
      "learning_rate": 0.00015016001853017838,
      "loss": 0.4053,
      "step": 153100
    },
    {
      "epoch": 0.4997928404349372,
      "grad_norm": 0.01685977354645729,
      "learning_rate": 0.00015006214786951884,
      "loss": 0.3926,
      "step": 153200
    },
    {
      "epoch": 0.5001190759704691,
      "grad_norm": 22.63051414489746,
      "learning_rate": 0.00014996427720885924,
      "loss": 0.2807,
      "step": 153300
    },
    {
      "epoch": 0.5004453115060011,
      "grad_norm": 0.8755621910095215,
      "learning_rate": 0.00014986640654819966,
      "loss": 0.5326,
      "step": 153400
    },
    {
      "epoch": 0.500771547041533,
      "grad_norm": 10.436057090759277,
      "learning_rate": 0.00014976853588754007,
      "loss": 0.2208,
      "step": 153500
    },
    {
      "epoch": 0.5010977825770649,
      "grad_norm": 0.4948633015155792,
      "learning_rate": 0.0001496706652268805,
      "loss": 0.25,
      "step": 153600
    },
    {
      "epoch": 0.501424018112597,
      "grad_norm": 9.961232717614621e-05,
      "learning_rate": 0.0001495727945662209,
      "loss": 0.4116,
      "step": 153700
    },
    {
      "epoch": 0.5017502536481289,
      "grad_norm": 47.029327392578125,
      "learning_rate": 0.00014947492390556132,
      "loss": 0.2734,
      "step": 153800
    },
    {
      "epoch": 0.5020764891836608,
      "grad_norm": 46.05852127075195,
      "learning_rate": 0.00014937705324490175,
      "loss": 0.4399,
      "step": 153900
    },
    {
      "epoch": 0.5024027247191928,
      "grad_norm": 0.0047686039470136166,
      "learning_rate": 0.00014927918258424215,
      "loss": 0.1259,
      "step": 154000
    },
    {
      "epoch": 0.5027289602547247,
      "grad_norm": 0.0014110031770542264,
      "learning_rate": 0.00014918131192358258,
      "loss": 0.2844,
      "step": 154100
    },
    {
      "epoch": 0.5030551957902567,
      "grad_norm": 0.0013581658713519573,
      "learning_rate": 0.00014908344126292298,
      "loss": 0.2037,
      "step": 154200
    },
    {
      "epoch": 0.5033814313257886,
      "grad_norm": 0.006097038742154837,
      "learning_rate": 0.0001489855706022634,
      "loss": 0.3092,
      "step": 154300
    },
    {
      "epoch": 0.5037076668613205,
      "grad_norm": 0.0007344463374465704,
      "learning_rate": 0.00014888769994160384,
      "loss": 0.4342,
      "step": 154400
    },
    {
      "epoch": 0.5040339023968525,
      "grad_norm": 0.0004865118826273829,
      "learning_rate": 0.00014878982928094424,
      "loss": 0.4943,
      "step": 154500
    },
    {
      "epoch": 0.5043601379323844,
      "grad_norm": 67.25373840332031,
      "learning_rate": 0.00014869195862028466,
      "loss": 0.3353,
      "step": 154600
    },
    {
      "epoch": 0.5046863734679163,
      "grad_norm": 0.0222498569637537,
      "learning_rate": 0.00014859408795962506,
      "loss": 0.2421,
      "step": 154700
    },
    {
      "epoch": 0.5050126090034484,
      "grad_norm": 0.08037357777357101,
      "learning_rate": 0.0001484962172989655,
      "loss": 0.5726,
      "step": 154800
    },
    {
      "epoch": 0.5053388445389803,
      "grad_norm": 0.03608395904302597,
      "learning_rate": 0.00014839834663830592,
      "loss": 0.388,
      "step": 154900
    },
    {
      "epoch": 0.5056650800745122,
      "grad_norm": 0.004664053209125996,
      "learning_rate": 0.00014830047597764632,
      "loss": 0.3998,
      "step": 155000
    },
    {
      "epoch": 0.5059913156100442,
      "grad_norm": 0.0019218453671783209,
      "learning_rate": 0.00014820260531698675,
      "loss": 0.1398,
      "step": 155100
    },
    {
      "epoch": 0.5063175511455761,
      "grad_norm": 0.00849764421582222,
      "learning_rate": 0.00014810473465632715,
      "loss": 0.2793,
      "step": 155200
    },
    {
      "epoch": 0.506643786681108,
      "grad_norm": 0.005140356253832579,
      "learning_rate": 0.00014800686399566758,
      "loss": 0.472,
      "step": 155300
    },
    {
      "epoch": 0.50697002221664,
      "grad_norm": 0.0035448467824608088,
      "learning_rate": 0.000147908993335008,
      "loss": 0.2956,
      "step": 155400
    },
    {
      "epoch": 0.5072962577521719,
      "grad_norm": 36.5052490234375,
      "learning_rate": 0.0001478111226743484,
      "loss": 0.4813,
      "step": 155500
    },
    {
      "epoch": 0.5076224932877038,
      "grad_norm": 141.2854766845703,
      "learning_rate": 0.00014771325201368883,
      "loss": 0.21,
      "step": 155600
    },
    {
      "epoch": 0.5079487288232358,
      "grad_norm": 0.0019449640531092882,
      "learning_rate": 0.00014761538135302926,
      "loss": 0.4132,
      "step": 155700
    },
    {
      "epoch": 0.5082749643587677,
      "grad_norm": 0.009038918651640415,
      "learning_rate": 0.00014751751069236966,
      "loss": 0.4314,
      "step": 155800
    },
    {
      "epoch": 0.5086011998942996,
      "grad_norm": 17.65716552734375,
      "learning_rate": 0.0001474196400317101,
      "loss": 0.2779,
      "step": 155900
    },
    {
      "epoch": 0.5089274354298317,
      "grad_norm": 35.6247444152832,
      "learning_rate": 0.0001473217693710505,
      "loss": 0.142,
      "step": 156000
    },
    {
      "epoch": 0.5092536709653636,
      "grad_norm": 1.243888020515442,
      "learning_rate": 0.00014722389871039092,
      "loss": 0.3688,
      "step": 156100
    },
    {
      "epoch": 0.5095799065008955,
      "grad_norm": 1.4037904739379883,
      "learning_rate": 0.00014712602804973132,
      "loss": 0.6988,
      "step": 156200
    },
    {
      "epoch": 0.5099061420364275,
      "grad_norm": 0.01733315736055374,
      "learning_rate": 0.00014702815738907175,
      "loss": 0.5364,
      "step": 156300
    },
    {
      "epoch": 0.5102323775719594,
      "grad_norm": 0.00037630635779350996,
      "learning_rate": 0.00014693028672841218,
      "loss": 0.2025,
      "step": 156400
    },
    {
      "epoch": 0.5105586131074914,
      "grad_norm": 34.54367446899414,
      "learning_rate": 0.00014683241606775258,
      "loss": 0.284,
      "step": 156500
    },
    {
      "epoch": 0.5108848486430233,
      "grad_norm": 0.052131809294223785,
      "learning_rate": 0.000146734545407093,
      "loss": 0.195,
      "step": 156600
    },
    {
      "epoch": 0.5112110841785552,
      "grad_norm": 0.00018789661407936364,
      "learning_rate": 0.00014663667474643343,
      "loss": 0.3118,
      "step": 156700
    },
    {
      "epoch": 0.5115373197140872,
      "grad_norm": 0.0005168450297787786,
      "learning_rate": 0.00014653880408577383,
      "loss": 0.4051,
      "step": 156800
    },
    {
      "epoch": 0.5118635552496191,
      "grad_norm": 75.94157409667969,
      "learning_rate": 0.00014644093342511426,
      "loss": 0.3534,
      "step": 156900
    },
    {
      "epoch": 0.512189790785151,
      "grad_norm": 0.0011542628053575754,
      "learning_rate": 0.00014634306276445466,
      "loss": 0.58,
      "step": 157000
    },
    {
      "epoch": 0.512516026320683,
      "grad_norm": 4.8023905754089355,
      "learning_rate": 0.0001462451921037951,
      "loss": 0.1414,
      "step": 157100
    },
    {
      "epoch": 0.512842261856215,
      "grad_norm": 0.00022946667741052806,
      "learning_rate": 0.0001461473214431355,
      "loss": 0.4225,
      "step": 157200
    },
    {
      "epoch": 0.5131684973917469,
      "grad_norm": 0.0007740551955066621,
      "learning_rate": 0.00014604945078247592,
      "loss": 0.253,
      "step": 157300
    },
    {
      "epoch": 0.5134947329272789,
      "grad_norm": 0.5905770659446716,
      "learning_rate": 0.00014595158012181635,
      "loss": 0.4568,
      "step": 157400
    },
    {
      "epoch": 0.5138209684628108,
      "grad_norm": 0.002204586984589696,
      "learning_rate": 0.00014585370946115675,
      "loss": 0.3857,
      "step": 157500
    },
    {
      "epoch": 0.5141472039983427,
      "grad_norm": 0.09421014785766602,
      "learning_rate": 0.00014575583880049718,
      "loss": 0.4193,
      "step": 157600
    },
    {
      "epoch": 0.5144734395338747,
      "grad_norm": 0.00021280007786117494,
      "learning_rate": 0.0001456579681398376,
      "loss": 0.3869,
      "step": 157700
    },
    {
      "epoch": 0.5147996750694066,
      "grad_norm": 0.006324377376586199,
      "learning_rate": 0.000145560097479178,
      "loss": 0.2565,
      "step": 157800
    },
    {
      "epoch": 0.5151259106049385,
      "grad_norm": 49.8858757019043,
      "learning_rate": 0.0001454622268185184,
      "loss": 0.5292,
      "step": 157900
    },
    {
      "epoch": 0.5154521461404705,
      "grad_norm": 0.00034618881181813776,
      "learning_rate": 0.00014536435615785883,
      "loss": 0.2884,
      "step": 158000
    },
    {
      "epoch": 0.5157783816760024,
      "grad_norm": 0.008083201013505459,
      "learning_rate": 0.00014526648549719926,
      "loss": 0.3828,
      "step": 158100
    },
    {
      "epoch": 0.5161046172115343,
      "grad_norm": 0.24746736884117126,
      "learning_rate": 0.00014516861483653966,
      "loss": 0.2763,
      "step": 158200
    },
    {
      "epoch": 0.5164308527470663,
      "grad_norm": 0.0011485370341688395,
      "learning_rate": 0.0001450707441758801,
      "loss": 0.4257,
      "step": 158300
    },
    {
      "epoch": 0.5167570882825983,
      "grad_norm": 0.96661376953125,
      "learning_rate": 0.00014497287351522052,
      "loss": 0.4824,
      "step": 158400
    },
    {
      "epoch": 0.5170833238181302,
      "grad_norm": 0.00311669590882957,
      "learning_rate": 0.00014487500285456092,
      "loss": 0.2071,
      "step": 158500
    },
    {
      "epoch": 0.5174095593536622,
      "grad_norm": 11.775991439819336,
      "learning_rate": 0.00014477713219390135,
      "loss": 0.2532,
      "step": 158600
    },
    {
      "epoch": 0.5177357948891941,
      "grad_norm": 0.0442865751683712,
      "learning_rate": 0.00014467926153324177,
      "loss": 0.2542,
      "step": 158700
    },
    {
      "epoch": 0.5180620304247261,
      "grad_norm": 45.129417419433594,
      "learning_rate": 0.00014458139087258218,
      "loss": 0.2271,
      "step": 158800
    },
    {
      "epoch": 0.518388265960258,
      "grad_norm": 7.5448150634765625,
      "learning_rate": 0.00014448352021192258,
      "loss": 0.3631,
      "step": 158900
    },
    {
      "epoch": 0.5187145014957899,
      "grad_norm": 0.004843545611947775,
      "learning_rate": 0.000144385649551263,
      "loss": 0.3633,
      "step": 159000
    },
    {
      "epoch": 0.5190407370313219,
      "grad_norm": 0.01569327339529991,
      "learning_rate": 0.00014428777889060343,
      "loss": 0.4867,
      "step": 159100
    },
    {
      "epoch": 0.5193669725668538,
      "grad_norm": 0.00516876857727766,
      "learning_rate": 0.00014418990822994383,
      "loss": 0.4061,
      "step": 159200
    },
    {
      "epoch": 0.5196932081023857,
      "grad_norm": 54.113040924072266,
      "learning_rate": 0.00014409203756928426,
      "loss": 0.2029,
      "step": 159300
    },
    {
      "epoch": 0.5200194436379177,
      "grad_norm": 0.0022780762519687414,
      "learning_rate": 0.0001439941669086247,
      "loss": 0.3477,
      "step": 159400
    },
    {
      "epoch": 0.5203456791734496,
      "grad_norm": 0.0006322204135358334,
      "learning_rate": 0.0001438962962479651,
      "loss": 0.3841,
      "step": 159500
    },
    {
      "epoch": 0.5206719147089816,
      "grad_norm": 0.01599644310772419,
      "learning_rate": 0.00014379842558730552,
      "loss": 0.4385,
      "step": 159600
    },
    {
      "epoch": 0.5209981502445136,
      "grad_norm": 39.196231842041016,
      "learning_rate": 0.00014370055492664595,
      "loss": 0.3608,
      "step": 159700
    },
    {
      "epoch": 0.5213243857800455,
      "grad_norm": 0.11689994484186172,
      "learning_rate": 0.00014360268426598635,
      "loss": 0.5116,
      "step": 159800
    },
    {
      "epoch": 0.5216506213155774,
      "grad_norm": 0.0014325863448902965,
      "learning_rate": 0.00014350481360532675,
      "loss": 0.4239,
      "step": 159900
    },
    {
      "epoch": 0.5219768568511094,
      "grad_norm": 0.0175640806555748,
      "learning_rate": 0.00014340694294466717,
      "loss": 0.443,
      "step": 160000
    },
    {
      "epoch": 0.5223030923866413,
      "grad_norm": 0.01412611547857523,
      "learning_rate": 0.0001433090722840076,
      "loss": 0.2996,
      "step": 160100
    },
    {
      "epoch": 0.5226293279221732,
      "grad_norm": 47.9322395324707,
      "learning_rate": 0.000143211201623348,
      "loss": 0.2184,
      "step": 160200
    },
    {
      "epoch": 0.5229555634577052,
      "grad_norm": 0.008206643164157867,
      "learning_rate": 0.00014311333096268843,
      "loss": 0.2185,
      "step": 160300
    },
    {
      "epoch": 0.5232817989932371,
      "grad_norm": 0.0004709753266070038,
      "learning_rate": 0.00014301546030202886,
      "loss": 0.4194,
      "step": 160400
    },
    {
      "epoch": 0.523608034528769,
      "grad_norm": 0.3918028771877289,
      "learning_rate": 0.00014291758964136926,
      "loss": 0.455,
      "step": 160500
    },
    {
      "epoch": 0.523934270064301,
      "grad_norm": 0.003167060436680913,
      "learning_rate": 0.00014281971898070966,
      "loss": 0.4304,
      "step": 160600
    },
    {
      "epoch": 0.524260505599833,
      "grad_norm": 0.0027108651120215654,
      "learning_rate": 0.00014272184832005012,
      "loss": 0.3689,
      "step": 160700
    },
    {
      "epoch": 0.524586741135365,
      "grad_norm": 0.0011983197182416916,
      "learning_rate": 0.00014262397765939052,
      "loss": 0.4066,
      "step": 160800
    },
    {
      "epoch": 0.5249129766708969,
      "grad_norm": 0.6756105422973633,
      "learning_rate": 0.00014252610699873092,
      "loss": 0.1819,
      "step": 160900
    },
    {
      "epoch": 0.5252392122064288,
      "grad_norm": 6.205921090440825e-05,
      "learning_rate": 0.00014242823633807135,
      "loss": 0.3626,
      "step": 161000
    },
    {
      "epoch": 0.5255654477419608,
      "grad_norm": 0.0005013004411011934,
      "learning_rate": 0.00014233036567741177,
      "loss": 0.4306,
      "step": 161100
    },
    {
      "epoch": 0.5258916832774927,
      "grad_norm": 9.22617244720459,
      "learning_rate": 0.00014223249501675217,
      "loss": 0.2242,
      "step": 161200
    },
    {
      "epoch": 0.5262179188130246,
      "grad_norm": 65.91259765625,
      "learning_rate": 0.0001421346243560926,
      "loss": 0.4348,
      "step": 161300
    },
    {
      "epoch": 0.5265441543485566,
      "grad_norm": 0.0006541903130710125,
      "learning_rate": 0.00014203675369543303,
      "loss": 0.3919,
      "step": 161400
    },
    {
      "epoch": 0.5268703898840885,
      "grad_norm": 0.3403210937976837,
      "learning_rate": 0.00014193888303477343,
      "loss": 0.2265,
      "step": 161500
    },
    {
      "epoch": 0.5271966254196204,
      "grad_norm": 0.07012750208377838,
      "learning_rate": 0.00014184101237411383,
      "loss": 0.367,
      "step": 161600
    },
    {
      "epoch": 0.5275228609551524,
      "grad_norm": 0.005394165404140949,
      "learning_rate": 0.00014174314171345426,
      "loss": 0.2973,
      "step": 161700
    },
    {
      "epoch": 0.5278490964906843,
      "grad_norm": 0.25905346870422363,
      "learning_rate": 0.0001416452710527947,
      "loss": 0.3311,
      "step": 161800
    },
    {
      "epoch": 0.5281753320262162,
      "grad_norm": 0.007704759016633034,
      "learning_rate": 0.0001415474003921351,
      "loss": 0.2758,
      "step": 161900
    },
    {
      "epoch": 0.5285015675617483,
      "grad_norm": 0.0005974896484985948,
      "learning_rate": 0.00014144952973147552,
      "loss": 0.3258,
      "step": 162000
    },
    {
      "epoch": 0.5288278030972802,
      "grad_norm": 83.67984008789062,
      "learning_rate": 0.00014135165907081594,
      "loss": 0.3173,
      "step": 162100
    },
    {
      "epoch": 0.5291540386328121,
      "grad_norm": 0.011287911795079708,
      "learning_rate": 0.00014125378841015634,
      "loss": 0.3675,
      "step": 162200
    },
    {
      "epoch": 0.5294802741683441,
      "grad_norm": 0.0006241937517188489,
      "learning_rate": 0.00014115591774949677,
      "loss": 0.4525,
      "step": 162300
    },
    {
      "epoch": 0.529806509703876,
      "grad_norm": 0.002383218379691243,
      "learning_rate": 0.0001410580470888372,
      "loss": 0.3551,
      "step": 162400
    },
    {
      "epoch": 0.5301327452394079,
      "grad_norm": 48.407100677490234,
      "learning_rate": 0.0001409601764281776,
      "loss": 0.3265,
      "step": 162500
    },
    {
      "epoch": 0.5304589807749399,
      "grad_norm": 0.0006557239685207605,
      "learning_rate": 0.000140862305767518,
      "loss": 0.3387,
      "step": 162600
    },
    {
      "epoch": 0.5307852163104718,
      "grad_norm": 0.0006972663104534149,
      "learning_rate": 0.00014076443510685843,
      "loss": 0.2075,
      "step": 162700
    },
    {
      "epoch": 0.5311114518460037,
      "grad_norm": 0.011184414848685265,
      "learning_rate": 0.00014066656444619886,
      "loss": 0.3964,
      "step": 162800
    },
    {
      "epoch": 0.5314376873815357,
      "grad_norm": 0.005460903514176607,
      "learning_rate": 0.00014056869378553926,
      "loss": 0.4371,
      "step": 162900
    },
    {
      "epoch": 0.5317639229170676,
      "grad_norm": 0.002169826300814748,
      "learning_rate": 0.0001404708231248797,
      "loss": 0.238,
      "step": 163000
    },
    {
      "epoch": 0.5320901584525997,
      "grad_norm": 50.09201431274414,
      "learning_rate": 0.00014037295246422011,
      "loss": 0.4865,
      "step": 163100
    },
    {
      "epoch": 0.5324163939881316,
      "grad_norm": 23.82480812072754,
      "learning_rate": 0.00014027508180356052,
      "loss": 0.4917,
      "step": 163200
    },
    {
      "epoch": 0.5327426295236635,
      "grad_norm": 0.06040745973587036,
      "learning_rate": 0.00014017721114290094,
      "loss": 0.3161,
      "step": 163300
    },
    {
      "epoch": 0.5330688650591955,
      "grad_norm": 0.1320941001176834,
      "learning_rate": 0.00014007934048224137,
      "loss": 0.3235,
      "step": 163400
    },
    {
      "epoch": 0.5333951005947274,
      "grad_norm": 0.011008662171661854,
      "learning_rate": 0.00013998146982158177,
      "loss": 0.5605,
      "step": 163500
    },
    {
      "epoch": 0.5337213361302593,
      "grad_norm": 0.0185276847332716,
      "learning_rate": 0.00013988359916092217,
      "loss": 0.2872,
      "step": 163600
    },
    {
      "epoch": 0.5340475716657913,
      "grad_norm": 0.0005931323976255953,
      "learning_rate": 0.0001397857285002626,
      "loss": 0.3278,
      "step": 163700
    },
    {
      "epoch": 0.5343738072013232,
      "grad_norm": 7.095152977854013e-05,
      "learning_rate": 0.00013968785783960303,
      "loss": 0.391,
      "step": 163800
    },
    {
      "epoch": 0.5347000427368551,
      "grad_norm": 0.6713041067123413,
      "learning_rate": 0.00013958998717894343,
      "loss": 0.3562,
      "step": 163900
    },
    {
      "epoch": 0.5350262782723871,
      "grad_norm": 25.458242416381836,
      "learning_rate": 0.00013949211651828386,
      "loss": 0.2217,
      "step": 164000
    },
    {
      "epoch": 0.535352513807919,
      "grad_norm": 18.75287437438965,
      "learning_rate": 0.00013939424585762429,
      "loss": 0.2631,
      "step": 164100
    },
    {
      "epoch": 0.5356787493434509,
      "grad_norm": 59.12357711791992,
      "learning_rate": 0.00013929637519696469,
      "loss": 0.3227,
      "step": 164200
    },
    {
      "epoch": 0.536004984878983,
      "grad_norm": 0.0002654624404385686,
      "learning_rate": 0.00013919850453630511,
      "loss": 0.4818,
      "step": 164300
    },
    {
      "epoch": 0.5363312204145149,
      "grad_norm": 0.0007627186714671552,
      "learning_rate": 0.00013910063387564554,
      "loss": 0.3143,
      "step": 164400
    },
    {
      "epoch": 0.5366574559500468,
      "grad_norm": 41.61249542236328,
      "learning_rate": 0.00013900276321498594,
      "loss": 0.182,
      "step": 164500
    },
    {
      "epoch": 0.5369836914855788,
      "grad_norm": 0.0071561215445399284,
      "learning_rate": 0.00013890489255432634,
      "loss": 0.2411,
      "step": 164600
    },
    {
      "epoch": 0.5373099270211107,
      "grad_norm": 0.0010337173007428646,
      "learning_rate": 0.00013880702189366677,
      "loss": 0.3824,
      "step": 164700
    },
    {
      "epoch": 0.5376361625566426,
      "grad_norm": 0.0026297469157725573,
      "learning_rate": 0.0001387091512330072,
      "loss": 0.3911,
      "step": 164800
    },
    {
      "epoch": 0.5379623980921746,
      "grad_norm": 0.004545759875327349,
      "learning_rate": 0.0001386112805723476,
      "loss": 0.2914,
      "step": 164900
    },
    {
      "epoch": 0.5382886336277065,
      "grad_norm": 46.58826446533203,
      "learning_rate": 0.00013851340991168803,
      "loss": 0.4291,
      "step": 165000
    },
    {
      "epoch": 0.5386148691632385,
      "grad_norm": 0.00013104898971505463,
      "learning_rate": 0.00013841553925102846,
      "loss": 0.3449,
      "step": 165100
    },
    {
      "epoch": 0.5389411046987704,
      "grad_norm": 1.641149640083313,
      "learning_rate": 0.00013831766859036886,
      "loss": 0.4093,
      "step": 165200
    },
    {
      "epoch": 0.5392673402343023,
      "grad_norm": 0.00011303695646347478,
      "learning_rate": 0.00013821979792970928,
      "loss": 0.2899,
      "step": 165300
    },
    {
      "epoch": 0.5395935757698344,
      "grad_norm": 0.1189790740609169,
      "learning_rate": 0.00013812192726904969,
      "loss": 0.4209,
      "step": 165400
    },
    {
      "epoch": 0.5399198113053663,
      "grad_norm": 13.096323013305664,
      "learning_rate": 0.0001380240566083901,
      "loss": 0.3357,
      "step": 165500
    },
    {
      "epoch": 0.5402460468408982,
      "grad_norm": 0.0013182174880057573,
      "learning_rate": 0.00013792618594773054,
      "loss": 0.4145,
      "step": 165600
    },
    {
      "epoch": 0.5405722823764302,
      "grad_norm": 4.7437042667297646e-05,
      "learning_rate": 0.00013782831528707094,
      "loss": 0.0705,
      "step": 165700
    },
    {
      "epoch": 0.5408985179119621,
      "grad_norm": 5.457561016082764,
      "learning_rate": 0.00013773044462641137,
      "loss": 0.3054,
      "step": 165800
    },
    {
      "epoch": 0.541224753447494,
      "grad_norm": 0.0034320028498768806,
      "learning_rate": 0.00013763257396575177,
      "loss": 0.4406,
      "step": 165900
    },
    {
      "epoch": 0.541550988983026,
      "grad_norm": 0.015653524547815323,
      "learning_rate": 0.0001375347033050922,
      "loss": 0.2393,
      "step": 166000
    },
    {
      "epoch": 0.5418772245185579,
      "grad_norm": 2.2892634868621826,
      "learning_rate": 0.00013743683264443263,
      "loss": 0.6195,
      "step": 166100
    },
    {
      "epoch": 0.5422034600540898,
      "grad_norm": 0.02037142962217331,
      "learning_rate": 0.00013733896198377303,
      "loss": 0.3263,
      "step": 166200
    },
    {
      "epoch": 0.5425296955896218,
      "grad_norm": 21.650144577026367,
      "learning_rate": 0.00013724109132311346,
      "loss": 0.3149,
      "step": 166300
    },
    {
      "epoch": 0.5428559311251537,
      "grad_norm": 0.0031819252762943506,
      "learning_rate": 0.00013714322066245386,
      "loss": 0.3053,
      "step": 166400
    },
    {
      "epoch": 0.5431821666606856,
      "grad_norm": 0.002921269042417407,
      "learning_rate": 0.00013704535000179428,
      "loss": 0.4552,
      "step": 166500
    },
    {
      "epoch": 0.5435084021962177,
      "grad_norm": 0.37184879183769226,
      "learning_rate": 0.0001369474793411347,
      "loss": 0.3459,
      "step": 166600
    },
    {
      "epoch": 0.5438346377317496,
      "grad_norm": 0.00012253649765625596,
      "learning_rate": 0.0001368496086804751,
      "loss": 0.3461,
      "step": 166700
    },
    {
      "epoch": 0.5441608732672815,
      "grad_norm": 0.038221947848796844,
      "learning_rate": 0.00013675173801981554,
      "loss": 0.3954,
      "step": 166800
    },
    {
      "epoch": 0.5444871088028135,
      "grad_norm": 0.0036649073008447886,
      "learning_rate": 0.00013665386735915594,
      "loss": 0.3045,
      "step": 166900
    },
    {
      "epoch": 0.5448133443383454,
      "grad_norm": 15.875740051269531,
      "learning_rate": 0.00013655599669849637,
      "loss": 0.4062,
      "step": 167000
    },
    {
      "epoch": 0.5451395798738773,
      "grad_norm": 12.204124450683594,
      "learning_rate": 0.0001364581260378368,
      "loss": 0.5045,
      "step": 167100
    },
    {
      "epoch": 0.5454658154094093,
      "grad_norm": 0.0038521275855600834,
      "learning_rate": 0.0001363602553771772,
      "loss": 0.5397,
      "step": 167200
    },
    {
      "epoch": 0.5457920509449412,
      "grad_norm": 28.72463607788086,
      "learning_rate": 0.00013626238471651763,
      "loss": 0.4527,
      "step": 167300
    },
    {
      "epoch": 0.5461182864804732,
      "grad_norm": 0.0006963898194953799,
      "learning_rate": 0.00013616451405585803,
      "loss": 0.3413,
      "step": 167400
    },
    {
      "epoch": 0.5464445220160051,
      "grad_norm": 0.006755585316568613,
      "learning_rate": 0.00013606664339519845,
      "loss": 0.3319,
      "step": 167500
    },
    {
      "epoch": 0.546770757551537,
      "grad_norm": 0.000661709753330797,
      "learning_rate": 0.00013596877273453888,
      "loss": 0.204,
      "step": 167600
    },
    {
      "epoch": 0.547096993087069,
      "grad_norm": 0.0005537489778362215,
      "learning_rate": 0.00013587090207387928,
      "loss": 0.5469,
      "step": 167700
    },
    {
      "epoch": 0.547423228622601,
      "grad_norm": 1.2855579853057861,
      "learning_rate": 0.0001357730314132197,
      "loss": 0.5362,
      "step": 167800
    },
    {
      "epoch": 0.5477494641581329,
      "grad_norm": 0.0017800612840801477,
      "learning_rate": 0.0001356751607525601,
      "loss": 0.3204,
      "step": 167900
    },
    {
      "epoch": 0.5480756996936649,
      "grad_norm": 7.553229079348966e-05,
      "learning_rate": 0.00013557729009190054,
      "loss": 0.3545,
      "step": 168000
    },
    {
      "epoch": 0.5484019352291968,
      "grad_norm": 0.007873239926993847,
      "learning_rate": 0.00013547941943124094,
      "loss": 0.3512,
      "step": 168100
    },
    {
      "epoch": 0.5487281707647287,
      "grad_norm": 0.01749865710735321,
      "learning_rate": 0.00013538154877058137,
      "loss": 0.4779,
      "step": 168200
    },
    {
      "epoch": 0.5490544063002607,
      "grad_norm": 0.0011708984384313226,
      "learning_rate": 0.0001352836781099218,
      "loss": 0.2966,
      "step": 168300
    },
    {
      "epoch": 0.5493806418357926,
      "grad_norm": 68.81944274902344,
      "learning_rate": 0.0001351858074492622,
      "loss": 0.3077,
      "step": 168400
    },
    {
      "epoch": 0.5497068773713245,
      "grad_norm": 0.002243833150714636,
      "learning_rate": 0.00013508793678860263,
      "loss": 0.4466,
      "step": 168500
    },
    {
      "epoch": 0.5500331129068565,
      "grad_norm": 0.030919566750526428,
      "learning_rate": 0.00013499006612794305,
      "loss": 0.2521,
      "step": 168600
    },
    {
      "epoch": 0.5503593484423884,
      "grad_norm": 0.03363409265875816,
      "learning_rate": 0.00013489219546728345,
      "loss": 0.3137,
      "step": 168700
    },
    {
      "epoch": 0.5506855839779203,
      "grad_norm": 0.05958423390984535,
      "learning_rate": 0.00013479432480662388,
      "loss": 0.3386,
      "step": 168800
    },
    {
      "epoch": 0.5510118195134523,
      "grad_norm": 0.011192189529538155,
      "learning_rate": 0.00013469645414596428,
      "loss": 0.3829,
      "step": 168900
    },
    {
      "epoch": 0.5513380550489843,
      "grad_norm": 0.15570586919784546,
      "learning_rate": 0.0001345985834853047,
      "loss": 0.2516,
      "step": 169000
    },
    {
      "epoch": 0.5516642905845162,
      "grad_norm": 0.9043486714363098,
      "learning_rate": 0.0001345007128246451,
      "loss": 0.3617,
      "step": 169100
    },
    {
      "epoch": 0.5519905261200482,
      "grad_norm": 0.013696075417101383,
      "learning_rate": 0.00013440284216398554,
      "loss": 0.2457,
      "step": 169200
    },
    {
      "epoch": 0.5523167616555801,
      "grad_norm": 0.010175167582929134,
      "learning_rate": 0.00013430497150332597,
      "loss": 0.3896,
      "step": 169300
    },
    {
      "epoch": 0.552642997191112,
      "grad_norm": 0.0036221228074282408,
      "learning_rate": 0.00013420710084266637,
      "loss": 0.3151,
      "step": 169400
    },
    {
      "epoch": 0.552969232726644,
      "grad_norm": 0.0030624023638665676,
      "learning_rate": 0.0001341092301820068,
      "loss": 0.3552,
      "step": 169500
    },
    {
      "epoch": 0.5532954682621759,
      "grad_norm": 37.97901153564453,
      "learning_rate": 0.00013401135952134722,
      "loss": 0.5008,
      "step": 169600
    },
    {
      "epoch": 0.5536217037977079,
      "grad_norm": 0.0026092862244695425,
      "learning_rate": 0.00013391348886068762,
      "loss": 0.3114,
      "step": 169700
    },
    {
      "epoch": 0.5539479393332398,
      "grad_norm": 0.0024829532485455275,
      "learning_rate": 0.00013381561820002805,
      "loss": 0.2667,
      "step": 169800
    },
    {
      "epoch": 0.5542741748687717,
      "grad_norm": 0.00035836760071106255,
      "learning_rate": 0.00013371774753936848,
      "loss": 0.2699,
      "step": 169900
    },
    {
      "epoch": 0.5546004104043037,
      "grad_norm": 0.04559493064880371,
      "learning_rate": 0.00013361987687870888,
      "loss": 0.525,
      "step": 170000
    },
    {
      "epoch": 0.5549266459398356,
      "grad_norm": 65.09120178222656,
      "learning_rate": 0.00013352200621804928,
      "loss": 0.3835,
      "step": 170100
    },
    {
      "epoch": 0.5552528814753676,
      "grad_norm": 0.007597626652568579,
      "learning_rate": 0.0001334241355573897,
      "loss": 0.4428,
      "step": 170200
    },
    {
      "epoch": 0.5555791170108996,
      "grad_norm": 21.72420883178711,
      "learning_rate": 0.00013332626489673014,
      "loss": 0.2125,
      "step": 170300
    },
    {
      "epoch": 0.5559053525464315,
      "grad_norm": 32.69823455810547,
      "learning_rate": 0.00013322839423607054,
      "loss": 0.3103,
      "step": 170400
    },
    {
      "epoch": 0.5562315880819634,
      "grad_norm": 25.112777709960938,
      "learning_rate": 0.00013313052357541097,
      "loss": 0.2996,
      "step": 170500
    },
    {
      "epoch": 0.5565578236174954,
      "grad_norm": 0.0031203352846205235,
      "learning_rate": 0.0001330326529147514,
      "loss": 0.3791,
      "step": 170600
    },
    {
      "epoch": 0.5568840591530273,
      "grad_norm": 67.07047271728516,
      "learning_rate": 0.0001329347822540918,
      "loss": 0.3969,
      "step": 170700
    },
    {
      "epoch": 0.5572102946885592,
      "grad_norm": 0.3104591369628906,
      "learning_rate": 0.00013283691159343222,
      "loss": 0.3509,
      "step": 170800
    },
    {
      "epoch": 0.5575365302240912,
      "grad_norm": 0.00048130046343430877,
      "learning_rate": 0.00013273904093277265,
      "loss": 0.3053,
      "step": 170900
    },
    {
      "epoch": 0.5578627657596231,
      "grad_norm": 0.619552493095398,
      "learning_rate": 0.00013264117027211305,
      "loss": 0.2647,
      "step": 171000
    },
    {
      "epoch": 0.558189001295155,
      "grad_norm": 8.056742808548734e-05,
      "learning_rate": 0.00013254329961145345,
      "loss": 0.2792,
      "step": 171100
    },
    {
      "epoch": 0.558515236830687,
      "grad_norm": 0.7188274264335632,
      "learning_rate": 0.00013244542895079388,
      "loss": 0.2268,
      "step": 171200
    },
    {
      "epoch": 0.558841472366219,
      "grad_norm": 10.637286186218262,
      "learning_rate": 0.0001323475582901343,
      "loss": 0.2196,
      "step": 171300
    },
    {
      "epoch": 0.5591677079017509,
      "grad_norm": 0.00036571294185705483,
      "learning_rate": 0.0001322496876294747,
      "loss": 0.3114,
      "step": 171400
    },
    {
      "epoch": 0.5594939434372829,
      "grad_norm": 38.91510772705078,
      "learning_rate": 0.00013215181696881514,
      "loss": 0.3681,
      "step": 171500
    },
    {
      "epoch": 0.5598201789728148,
      "grad_norm": 0.0021014453377574682,
      "learning_rate": 0.00013205394630815557,
      "loss": 0.311,
      "step": 171600
    },
    {
      "epoch": 0.5601464145083468,
      "grad_norm": 62.50294494628906,
      "learning_rate": 0.00013195607564749597,
      "loss": 0.4753,
      "step": 171700
    },
    {
      "epoch": 0.5604726500438787,
      "grad_norm": 0.04072234779596329,
      "learning_rate": 0.00013185820498683637,
      "loss": 0.2483,
      "step": 171800
    },
    {
      "epoch": 0.5607988855794106,
      "grad_norm": 0.0006499247392639518,
      "learning_rate": 0.0001317603343261768,
      "loss": 0.4264,
      "step": 171900
    },
    {
      "epoch": 0.5611251211149426,
      "grad_norm": 11.984750747680664,
      "learning_rate": 0.00013166246366551722,
      "loss": 0.3089,
      "step": 172000
    },
    {
      "epoch": 0.5614513566504745,
      "grad_norm": 0.3840184509754181,
      "learning_rate": 0.00013156459300485762,
      "loss": 0.2715,
      "step": 172100
    },
    {
      "epoch": 0.5617775921860064,
      "grad_norm": 0.00018697540508583188,
      "learning_rate": 0.00013146672234419805,
      "loss": 0.32,
      "step": 172200
    },
    {
      "epoch": 0.5621038277215384,
      "grad_norm": 2.3045194149017334,
      "learning_rate": 0.00013136885168353848,
      "loss": 0.2169,
      "step": 172300
    },
    {
      "epoch": 0.5624300632570703,
      "grad_norm": 0.008848004974424839,
      "learning_rate": 0.00013127098102287888,
      "loss": 0.4956,
      "step": 172400
    },
    {
      "epoch": 0.5627562987926022,
      "grad_norm": 0.007759935222566128,
      "learning_rate": 0.0001311731103622193,
      "loss": 0.2886,
      "step": 172500
    },
    {
      "epoch": 0.5630825343281343,
      "grad_norm": 0.0032650469802320004,
      "learning_rate": 0.00013107523970155974,
      "loss": 0.3827,
      "step": 172600
    },
    {
      "epoch": 0.5634087698636662,
      "grad_norm": 0.000920548161957413,
      "learning_rate": 0.00013097736904090014,
      "loss": 0.2147,
      "step": 172700
    },
    {
      "epoch": 0.5637350053991981,
      "grad_norm": 2.3303009584196843e-05,
      "learning_rate": 0.00013087949838024054,
      "loss": 0.2851,
      "step": 172800
    },
    {
      "epoch": 0.5640612409347301,
      "grad_norm": 0.0008339118212461472,
      "learning_rate": 0.00013078162771958097,
      "loss": 0.4422,
      "step": 172900
    },
    {
      "epoch": 0.564387476470262,
      "grad_norm": 0.012816928327083588,
      "learning_rate": 0.0001306837570589214,
      "loss": 0.347,
      "step": 173000
    },
    {
      "epoch": 0.5647137120057939,
      "grad_norm": 0.0012060640146955848,
      "learning_rate": 0.0001305858863982618,
      "loss": 0.2452,
      "step": 173100
    },
    {
      "epoch": 0.5650399475413259,
      "grad_norm": 0.13972613215446472,
      "learning_rate": 0.00013048801573760222,
      "loss": 0.1761,
      "step": 173200
    },
    {
      "epoch": 0.5653661830768578,
      "grad_norm": 12.092694282531738,
      "learning_rate": 0.00013039014507694265,
      "loss": 0.3864,
      "step": 173300
    },
    {
      "epoch": 0.5656924186123897,
      "grad_norm": 0.02094961516559124,
      "learning_rate": 0.00013029227441628305,
      "loss": 0.1554,
      "step": 173400
    },
    {
      "epoch": 0.5660186541479217,
      "grad_norm": 11.759566307067871,
      "learning_rate": 0.00013019440375562348,
      "loss": 0.4126,
      "step": 173500
    },
    {
      "epoch": 0.5663448896834536,
      "grad_norm": 0.0025110712740570307,
      "learning_rate": 0.0001300965330949639,
      "loss": 0.4367,
      "step": 173600
    },
    {
      "epoch": 0.5666711252189855,
      "grad_norm": 0.0033890865743160248,
      "learning_rate": 0.0001299986624343043,
      "loss": 0.264,
      "step": 173700
    },
    {
      "epoch": 0.5669973607545176,
      "grad_norm": 0.0020779024343937635,
      "learning_rate": 0.0001299007917736447,
      "loss": 0.3677,
      "step": 173800
    },
    {
      "epoch": 0.5673235962900495,
      "grad_norm": 0.600735604763031,
      "learning_rate": 0.00012980292111298514,
      "loss": 0.4382,
      "step": 173900
    },
    {
      "epoch": 0.5676498318255815,
      "grad_norm": 0.00555655499920249,
      "learning_rate": 0.00012970505045232556,
      "loss": 0.279,
      "step": 174000
    },
    {
      "epoch": 0.5679760673611134,
      "grad_norm": 0.22154894471168518,
      "learning_rate": 0.00012960717979166597,
      "loss": 0.3882,
      "step": 174100
    },
    {
      "epoch": 0.5683023028966453,
      "grad_norm": 1.3110778331756592,
      "learning_rate": 0.0001295093091310064,
      "loss": 0.3477,
      "step": 174200
    },
    {
      "epoch": 0.5686285384321773,
      "grad_norm": 0.0013099474599584937,
      "learning_rate": 0.00012941143847034682,
      "loss": 0.2925,
      "step": 174300
    },
    {
      "epoch": 0.5689547739677092,
      "grad_norm": 0.005593449342995882,
      "learning_rate": 0.00012931356780968722,
      "loss": 0.3621,
      "step": 174400
    },
    {
      "epoch": 0.5692810095032411,
      "grad_norm": 0.04357033967971802,
      "learning_rate": 0.00012921569714902762,
      "loss": 0.587,
      "step": 174500
    },
    {
      "epoch": 0.5696072450387731,
      "grad_norm": 23.126184463500977,
      "learning_rate": 0.00012911782648836808,
      "loss": 0.1842,
      "step": 174600
    },
    {
      "epoch": 0.569933480574305,
      "grad_norm": 8.223351323977113e-05,
      "learning_rate": 0.00012901995582770848,
      "loss": 0.4312,
      "step": 174700
    },
    {
      "epoch": 0.570259716109837,
      "grad_norm": 46.752193450927734,
      "learning_rate": 0.00012892208516704888,
      "loss": 0.3869,
      "step": 174800
    },
    {
      "epoch": 0.570585951645369,
      "grad_norm": 0.0014119582483544946,
      "learning_rate": 0.0001288242145063893,
      "loss": 0.5506,
      "step": 174900
    },
    {
      "epoch": 0.5709121871809009,
      "grad_norm": 0.003074218286201358,
      "learning_rate": 0.00012872634384572973,
      "loss": 0.3479,
      "step": 175000
    },
    {
      "epoch": 0.5712384227164328,
      "grad_norm": 6.739552918588743e-05,
      "learning_rate": 0.00012862847318507014,
      "loss": 0.2009,
      "step": 175100
    },
    {
      "epoch": 0.5715646582519648,
      "grad_norm": 0.027380021288990974,
      "learning_rate": 0.00012853060252441056,
      "loss": 0.4752,
      "step": 175200
    },
    {
      "epoch": 0.5718908937874967,
      "grad_norm": 11.57398509979248,
      "learning_rate": 0.000128432731863751,
      "loss": 0.3655,
      "step": 175300
    },
    {
      "epoch": 0.5722171293230286,
      "grad_norm": 0.8469604253768921,
      "learning_rate": 0.0001283348612030914,
      "loss": 0.4589,
      "step": 175400
    },
    {
      "epoch": 0.5725433648585606,
      "grad_norm": 27.297574996948242,
      "learning_rate": 0.0001282369905424318,
      "loss": 0.3786,
      "step": 175500
    },
    {
      "epoch": 0.5728696003940925,
      "grad_norm": 0.24598249793052673,
      "learning_rate": 0.00012813911988177222,
      "loss": 0.3257,
      "step": 175600
    },
    {
      "epoch": 0.5731958359296244,
      "grad_norm": 0.04827055707573891,
      "learning_rate": 0.00012804124922111265,
      "loss": 0.1685,
      "step": 175700
    },
    {
      "epoch": 0.5735220714651564,
      "grad_norm": 19.020553588867188,
      "learning_rate": 0.00012794337856045305,
      "loss": 0.4195,
      "step": 175800
    },
    {
      "epoch": 0.5738483070006883,
      "grad_norm": 0.01588553562760353,
      "learning_rate": 0.00012784550789979348,
      "loss": 0.2209,
      "step": 175900
    },
    {
      "epoch": 0.5741745425362202,
      "grad_norm": 0.0015537722501903772,
      "learning_rate": 0.0001277476372391339,
      "loss": 0.2573,
      "step": 176000
    },
    {
      "epoch": 0.5745007780717523,
      "grad_norm": 38.98337173461914,
      "learning_rate": 0.0001276497665784743,
      "loss": 0.2987,
      "step": 176100
    },
    {
      "epoch": 0.5748270136072842,
      "grad_norm": 9.544436761643738e-05,
      "learning_rate": 0.00012755189591781473,
      "loss": 0.4189,
      "step": 176200
    },
    {
      "epoch": 0.5751532491428162,
      "grad_norm": 2.6248600988765247e-05,
      "learning_rate": 0.00012745402525715516,
      "loss": 0.5014,
      "step": 176300
    },
    {
      "epoch": 0.5754794846783481,
      "grad_norm": 0.0002428575389785692,
      "learning_rate": 0.00012735615459649556,
      "loss": 0.4081,
      "step": 176400
    },
    {
      "epoch": 0.57580572021388,
      "grad_norm": 0.007806078530848026,
      "learning_rate": 0.000127258283935836,
      "loss": 0.4366,
      "step": 176500
    },
    {
      "epoch": 0.576131955749412,
      "grad_norm": 1.669858829700388e-05,
      "learning_rate": 0.0001271604132751764,
      "loss": 0.376,
      "step": 176600
    },
    {
      "epoch": 0.5764581912849439,
      "grad_norm": 5.458704471588135,
      "learning_rate": 0.00012706254261451682,
      "loss": 0.3743,
      "step": 176700
    },
    {
      "epoch": 0.5767844268204758,
      "grad_norm": 0.001938865054398775,
      "learning_rate": 0.00012696467195385722,
      "loss": 0.4059,
      "step": 176800
    },
    {
      "epoch": 0.5771106623560078,
      "grad_norm": 5.736208186135627e-05,
      "learning_rate": 0.00012686680129319765,
      "loss": 0.2594,
      "step": 176900
    },
    {
      "epoch": 0.5774368978915397,
      "grad_norm": 0.010826416313648224,
      "learning_rate": 0.00012676893063253808,
      "loss": 0.3785,
      "step": 177000
    },
    {
      "epoch": 0.5777631334270716,
      "grad_norm": 0.042711060494184494,
      "learning_rate": 0.00012667105997187848,
      "loss": 0.3407,
      "step": 177100
    },
    {
      "epoch": 0.5780893689626037,
      "grad_norm": 0.001981383888050914,
      "learning_rate": 0.0001265731893112189,
      "loss": 0.4791,
      "step": 177200
    },
    {
      "epoch": 0.5784156044981356,
      "grad_norm": 0.07909760624170303,
      "learning_rate": 0.00012647531865055933,
      "loss": 0.3934,
      "step": 177300
    },
    {
      "epoch": 0.5787418400336675,
      "grad_norm": 0.0005245034699328244,
      "learning_rate": 0.00012637744798989973,
      "loss": 0.2553,
      "step": 177400
    },
    {
      "epoch": 0.5790680755691995,
      "grad_norm": 0.00024279930221382529,
      "learning_rate": 0.00012627957732924016,
      "loss": 0.3191,
      "step": 177500
    },
    {
      "epoch": 0.5793943111047314,
      "grad_norm": 0.0003885544720105827,
      "learning_rate": 0.00012618170666858056,
      "loss": 0.1597,
      "step": 177600
    },
    {
      "epoch": 0.5797205466402633,
      "grad_norm": 0.011036419309675694,
      "learning_rate": 0.000126083836007921,
      "loss": 0.2911,
      "step": 177700
    },
    {
      "epoch": 0.5800467821757953,
      "grad_norm": 0.48506873846054077,
      "learning_rate": 0.0001259859653472614,
      "loss": 0.3021,
      "step": 177800
    },
    {
      "epoch": 0.5803730177113272,
      "grad_norm": 1.6442490816116333,
      "learning_rate": 0.00012588809468660182,
      "loss": 0.3115,
      "step": 177900
    },
    {
      "epoch": 0.5806992532468591,
      "grad_norm": 0.0054204510524868965,
      "learning_rate": 0.00012579022402594225,
      "loss": 0.1392,
      "step": 178000
    },
    {
      "epoch": 0.5810254887823911,
      "grad_norm": 0.0003257494536228478,
      "learning_rate": 0.00012569235336528265,
      "loss": 0.2317,
      "step": 178100
    },
    {
      "epoch": 0.581351724317923,
      "grad_norm": 32.07198715209961,
      "learning_rate": 0.00012559448270462308,
      "loss": 0.2623,
      "step": 178200
    },
    {
      "epoch": 0.581677959853455,
      "grad_norm": 0.0006913407705724239,
      "learning_rate": 0.0001254966120439635,
      "loss": 0.24,
      "step": 178300
    },
    {
      "epoch": 0.582004195388987,
      "grad_norm": 0.0005342021468095481,
      "learning_rate": 0.0001253987413833039,
      "loss": 0.4137,
      "step": 178400
    },
    {
      "epoch": 0.5823304309245189,
      "grad_norm": 0.0004940718645229936,
      "learning_rate": 0.00012530087072264433,
      "loss": 0.3397,
      "step": 178500
    },
    {
      "epoch": 0.5826566664600509,
      "grad_norm": 0.0001226106978720054,
      "learning_rate": 0.00012520300006198473,
      "loss": 0.2686,
      "step": 178600
    },
    {
      "epoch": 0.5829829019955828,
      "grad_norm": 0.007878195494413376,
      "learning_rate": 0.00012510512940132516,
      "loss": 0.345,
      "step": 178700
    },
    {
      "epoch": 0.5833091375311147,
      "grad_norm": 0.20832766592502594,
      "learning_rate": 0.00012500725874066556,
      "loss": 0.3748,
      "step": 178800
    },
    {
      "epoch": 0.5836353730666467,
      "grad_norm": 0.0003278386720921844,
      "learning_rate": 0.000124909388080006,
      "loss": 0.4404,
      "step": 178900
    },
    {
      "epoch": 0.5839616086021786,
      "grad_norm": 0.042560648173093796,
      "learning_rate": 0.00012481151741934642,
      "loss": 0.1513,
      "step": 179000
    },
    {
      "epoch": 0.5842878441377105,
      "grad_norm": 0.07539700716733932,
      "learning_rate": 0.00012471364675868682,
      "loss": 0.2528,
      "step": 179100
    },
    {
      "epoch": 0.5846140796732425,
      "grad_norm": 54.27755355834961,
      "learning_rate": 0.00012461577609802725,
      "loss": 0.2862,
      "step": 179200
    },
    {
      "epoch": 0.5849403152087744,
      "grad_norm": 0.0014137834077700973,
      "learning_rate": 0.00012451790543736765,
      "loss": 0.2485,
      "step": 179300
    },
    {
      "epoch": 0.5852665507443063,
      "grad_norm": 97.30562591552734,
      "learning_rate": 0.00012442003477670808,
      "loss": 0.3768,
      "step": 179400
    },
    {
      "epoch": 0.5855927862798384,
      "grad_norm": 0.034857019782066345,
      "learning_rate": 0.0001243221641160485,
      "loss": 0.1896,
      "step": 179500
    },
    {
      "epoch": 0.5859190218153703,
      "grad_norm": 0.010729930363595486,
      "learning_rate": 0.0001242242934553889,
      "loss": 0.2715,
      "step": 179600
    },
    {
      "epoch": 0.5862452573509022,
      "grad_norm": 0.002161194570362568,
      "learning_rate": 0.00012412642279472933,
      "loss": 0.2587,
      "step": 179700
    },
    {
      "epoch": 0.5865714928864342,
      "grad_norm": 0.0008644962799735367,
      "learning_rate": 0.00012402855213406976,
      "loss": 0.3601,
      "step": 179800
    },
    {
      "epoch": 0.5868977284219661,
      "grad_norm": 0.005726533010601997,
      "learning_rate": 0.00012393068147341016,
      "loss": 0.267,
      "step": 179900
    },
    {
      "epoch": 0.587223963957498,
      "grad_norm": 1.0558353662490845,
      "learning_rate": 0.0001238328108127506,
      "loss": 0.3109,
      "step": 180000
    },
    {
      "epoch": 0.58755019949303,
      "grad_norm": 0.0011605978943407536,
      "learning_rate": 0.000123734940152091,
      "loss": 0.215,
      "step": 180100
    },
    {
      "epoch": 0.5878764350285619,
      "grad_norm": 0.0028934888541698456,
      "learning_rate": 0.00012363706949143142,
      "loss": 0.3587,
      "step": 180200
    },
    {
      "epoch": 0.5882026705640938,
      "grad_norm": 0.00044367252849042416,
      "learning_rate": 0.00012353919883077182,
      "loss": 0.2775,
      "step": 180300
    },
    {
      "epoch": 0.5885289060996258,
      "grad_norm": 26.640422821044922,
      "learning_rate": 0.00012344132817011225,
      "loss": 0.2082,
      "step": 180400
    },
    {
      "epoch": 0.5888551416351577,
      "grad_norm": 0.014839714393019676,
      "learning_rate": 0.00012334345750945267,
      "loss": 0.3818,
      "step": 180500
    },
    {
      "epoch": 0.5891813771706897,
      "grad_norm": 0.0013319917488843203,
      "learning_rate": 0.00012324558684879307,
      "loss": 0.434,
      "step": 180600
    },
    {
      "epoch": 0.5895076127062217,
      "grad_norm": 0.1591653823852539,
      "learning_rate": 0.0001231477161881335,
      "loss": 0.3555,
      "step": 180700
    },
    {
      "epoch": 0.5898338482417536,
      "grad_norm": 25.114551544189453,
      "learning_rate": 0.00012304984552747393,
      "loss": 0.5696,
      "step": 180800
    },
    {
      "epoch": 0.5901600837772856,
      "grad_norm": 0.00046453598770312965,
      "learning_rate": 0.00012295197486681433,
      "loss": 0.338,
      "step": 180900
    },
    {
      "epoch": 0.5904863193128175,
      "grad_norm": 105.8384017944336,
      "learning_rate": 0.00012285410420615476,
      "loss": 0.2198,
      "step": 181000
    },
    {
      "epoch": 0.5908125548483494,
      "grad_norm": 0.0008559446432627738,
      "learning_rate": 0.00012275623354549516,
      "loss": 0.4033,
      "step": 181100
    },
    {
      "epoch": 0.5911387903838814,
      "grad_norm": 0.0004255959647707641,
      "learning_rate": 0.0001226583628848356,
      "loss": 0.182,
      "step": 181200
    },
    {
      "epoch": 0.5914650259194133,
      "grad_norm": 44.57097244262695,
      "learning_rate": 0.000122560492224176,
      "loss": 0.3016,
      "step": 181300
    },
    {
      "epoch": 0.5917912614549452,
      "grad_norm": 0.4940738379955292,
      "learning_rate": 0.00012246262156351642,
      "loss": 0.4015,
      "step": 181400
    },
    {
      "epoch": 0.5921174969904772,
      "grad_norm": 38.9740104675293,
      "learning_rate": 0.00012236475090285684,
      "loss": 0.3169,
      "step": 181500
    },
    {
      "epoch": 0.5924437325260091,
      "grad_norm": 0.09870408475399017,
      "learning_rate": 0.00012226688024219725,
      "loss": 0.195,
      "step": 181600
    },
    {
      "epoch": 0.592769968061541,
      "grad_norm": 0.0006824063020758331,
      "learning_rate": 0.00012216900958153767,
      "loss": 0.3729,
      "step": 181700
    },
    {
      "epoch": 0.593096203597073,
      "grad_norm": 0.000902793079148978,
      "learning_rate": 0.0001220711389208781,
      "loss": 0.3046,
      "step": 181800
    },
    {
      "epoch": 0.593422439132605,
      "grad_norm": 0.00019285775488242507,
      "learning_rate": 0.0001219732682602185,
      "loss": 0.3136,
      "step": 181900
    },
    {
      "epoch": 0.5937486746681369,
      "grad_norm": 0.06652594357728958,
      "learning_rate": 0.00012187539759955892,
      "loss": 0.1715,
      "step": 182000
    },
    {
      "epoch": 0.5940749102036689,
      "grad_norm": 0.0003646048135124147,
      "learning_rate": 0.00012177752693889933,
      "loss": 0.3026,
      "step": 182100
    },
    {
      "epoch": 0.5944011457392008,
      "grad_norm": 11.76259994506836,
      "learning_rate": 0.00012167965627823976,
      "loss": 0.3781,
      "step": 182200
    },
    {
      "epoch": 0.5947273812747327,
      "grad_norm": 0.004980073776096106,
      "learning_rate": 0.00012158178561758017,
      "loss": 0.3243,
      "step": 182300
    },
    {
      "epoch": 0.5950536168102647,
      "grad_norm": 40.54780197143555,
      "learning_rate": 0.00012148391495692059,
      "loss": 0.5055,
      "step": 182400
    },
    {
      "epoch": 0.5953798523457966,
      "grad_norm": 0.06335864961147308,
      "learning_rate": 0.00012138604429626102,
      "loss": 0.2437,
      "step": 182500
    },
    {
      "epoch": 0.5957060878813286,
      "grad_norm": 0.4540809690952301,
      "learning_rate": 0.00012128817363560142,
      "loss": 0.4381,
      "step": 182600
    },
    {
      "epoch": 0.5960323234168605,
      "grad_norm": 0.005720858462154865,
      "learning_rate": 0.00012119030297494183,
      "loss": 0.1867,
      "step": 182700
    },
    {
      "epoch": 0.5963585589523924,
      "grad_norm": 0.0019731149077415466,
      "learning_rate": 0.00012109243231428226,
      "loss": 0.3612,
      "step": 182800
    },
    {
      "epoch": 0.5966847944879244,
      "grad_norm": 0.000434290268458426,
      "learning_rate": 0.00012099456165362267,
      "loss": 0.3967,
      "step": 182900
    },
    {
      "epoch": 0.5970110300234563,
      "grad_norm": 67.83424377441406,
      "learning_rate": 0.00012089669099296309,
      "loss": 0.2682,
      "step": 183000
    },
    {
      "epoch": 0.5973372655589883,
      "grad_norm": 18.01362419128418,
      "learning_rate": 0.00012079882033230351,
      "loss": 0.2743,
      "step": 183100
    },
    {
      "epoch": 0.5976635010945203,
      "grad_norm": 0.00547516206279397,
      "learning_rate": 0.00012070094967164393,
      "loss": 0.2931,
      "step": 183200
    },
    {
      "epoch": 0.5979897366300522,
      "grad_norm": 62.10245895385742,
      "learning_rate": 0.00012060307901098434,
      "loss": 0.3378,
      "step": 183300
    },
    {
      "epoch": 0.5983159721655841,
      "grad_norm": 0.0018163294298574328,
      "learning_rate": 0.00012050520835032474,
      "loss": 0.2799,
      "step": 183400
    },
    {
      "epoch": 0.5986422077011161,
      "grad_norm": 0.0066961427219212055,
      "learning_rate": 0.00012040733768966517,
      "loss": 0.305,
      "step": 183500
    },
    {
      "epoch": 0.598968443236648,
      "grad_norm": 2.1041925720055588e-05,
      "learning_rate": 0.00012030946702900559,
      "loss": 0.279,
      "step": 183600
    },
    {
      "epoch": 0.5992946787721799,
      "grad_norm": 0.000695916241966188,
      "learning_rate": 0.000120211596368346,
      "loss": 0.5104,
      "step": 183700
    },
    {
      "epoch": 0.5996209143077119,
      "grad_norm": 0.0021480987779796124,
      "learning_rate": 0.00012011372570768643,
      "loss": 0.4483,
      "step": 183800
    },
    {
      "epoch": 0.5999471498432438,
      "grad_norm": 0.00038757725269533694,
      "learning_rate": 0.00012001585504702684,
      "loss": 0.284,
      "step": 183900
    },
    {
      "epoch": 0.6002733853787757,
      "grad_norm": 0.0002347790141357109,
      "learning_rate": 0.00011991798438636726,
      "loss": 0.2233,
      "step": 184000
    },
    {
      "epoch": 0.6005996209143077,
      "grad_norm": 79.02684020996094,
      "learning_rate": 0.00011982011372570769,
      "loss": 0.2734,
      "step": 184100
    },
    {
      "epoch": 0.6009258564498396,
      "grad_norm": 0.03011353313922882,
      "learning_rate": 0.0001197222430650481,
      "loss": 0.3463,
      "step": 184200
    },
    {
      "epoch": 0.6012520919853716,
      "grad_norm": 0.13426810503005981,
      "learning_rate": 0.00011962437240438851,
      "loss": 0.3189,
      "step": 184300
    },
    {
      "epoch": 0.6015783275209036,
      "grad_norm": 0.0003684539988171309,
      "learning_rate": 0.00011952650174372891,
      "loss": 0.4369,
      "step": 184400
    },
    {
      "epoch": 0.6019045630564355,
      "grad_norm": 21.506900787353516,
      "learning_rate": 0.00011942863108306934,
      "loss": 0.3226,
      "step": 184500
    },
    {
      "epoch": 0.6022307985919674,
      "grad_norm": 0.0003425043250899762,
      "learning_rate": 0.00011933076042240976,
      "loss": 0.1964,
      "step": 184600
    },
    {
      "epoch": 0.6025570341274994,
      "grad_norm": 0.005834600888192654,
      "learning_rate": 0.00011923288976175017,
      "loss": 0.2795,
      "step": 184700
    },
    {
      "epoch": 0.6028832696630313,
      "grad_norm": 0.0006708840373903513,
      "learning_rate": 0.0001191350191010906,
      "loss": 0.3111,
      "step": 184800
    },
    {
      "epoch": 0.6032095051985633,
      "grad_norm": 0.0007933633751235902,
      "learning_rate": 0.00011903714844043101,
      "loss": 0.3362,
      "step": 184900
    },
    {
      "epoch": 0.6035357407340952,
      "grad_norm": 8.435165364062414e-05,
      "learning_rate": 0.00011893927777977143,
      "loss": 0.1659,
      "step": 185000
    },
    {
      "epoch": 0.6038619762696271,
      "grad_norm": 0.06850215047597885,
      "learning_rate": 0.00011884140711911186,
      "loss": 0.0985,
      "step": 185100
    },
    {
      "epoch": 0.6041882118051591,
      "grad_norm": 0.000629743211902678,
      "learning_rate": 0.00011874353645845227,
      "loss": 0.3463,
      "step": 185200
    },
    {
      "epoch": 0.604514447340691,
      "grad_norm": 0.012671124190092087,
      "learning_rate": 0.00011864566579779267,
      "loss": 0.2538,
      "step": 185300
    },
    {
      "epoch": 0.604840682876223,
      "grad_norm": 0.0030717498157173395,
      "learning_rate": 0.00011854779513713309,
      "loss": 0.2379,
      "step": 185400
    },
    {
      "epoch": 0.605166918411755,
      "grad_norm": 0.2583216428756714,
      "learning_rate": 0.00011844992447647351,
      "loss": 0.2402,
      "step": 185500
    },
    {
      "epoch": 0.6054931539472869,
      "grad_norm": 0.009159360080957413,
      "learning_rate": 0.00011835205381581393,
      "loss": 0.2502,
      "step": 185600
    },
    {
      "epoch": 0.6058193894828188,
      "grad_norm": 23.77691078186035,
      "learning_rate": 0.00011825418315515434,
      "loss": 0.2636,
      "step": 185700
    },
    {
      "epoch": 0.6061456250183508,
      "grad_norm": 5.515474796295166,
      "learning_rate": 0.00011815631249449477,
      "loss": 0.4139,
      "step": 185800
    },
    {
      "epoch": 0.6064718605538827,
      "grad_norm": 0.007417479995638132,
      "learning_rate": 0.00011805844183383518,
      "loss": 0.3555,
      "step": 185900
    },
    {
      "epoch": 0.6067980960894146,
      "grad_norm": 0.009135324507951736,
      "learning_rate": 0.0001179605711731756,
      "loss": 0.1691,
      "step": 186000
    },
    {
      "epoch": 0.6071243316249466,
      "grad_norm": 1.6405444145202637,
      "learning_rate": 0.00011786270051251603,
      "loss": 0.4308,
      "step": 186100
    },
    {
      "epoch": 0.6074505671604785,
      "grad_norm": 0.0014189196517691016,
      "learning_rate": 0.00011776482985185644,
      "loss": 0.4236,
      "step": 186200
    },
    {
      "epoch": 0.6077768026960104,
      "grad_norm": 68.18275451660156,
      "learning_rate": 0.00011766695919119684,
      "loss": 0.3511,
      "step": 186300
    },
    {
      "epoch": 0.6081030382315424,
      "grad_norm": 0.002328460803255439,
      "learning_rate": 0.00011756908853053726,
      "loss": 0.2054,
      "step": 186400
    },
    {
      "epoch": 0.6084292737670743,
      "grad_norm": 0.00020256963034626096,
      "learning_rate": 0.00011747121786987768,
      "loss": 0.4062,
      "step": 186500
    },
    {
      "epoch": 0.6087555093026062,
      "grad_norm": 4.404118953971192e-05,
      "learning_rate": 0.0001173733472092181,
      "loss": 0.2859,
      "step": 186600
    },
    {
      "epoch": 0.6090817448381383,
      "grad_norm": 0.1512550711631775,
      "learning_rate": 0.00011727547654855851,
      "loss": 0.259,
      "step": 186700
    },
    {
      "epoch": 0.6094079803736702,
      "grad_norm": 0.33786237239837646,
      "learning_rate": 0.00011717760588789894,
      "loss": 0.4285,
      "step": 186800
    },
    {
      "epoch": 0.6097342159092021,
      "grad_norm": 0.00011068062303820625,
      "learning_rate": 0.00011707973522723936,
      "loss": 0.216,
      "step": 186900
    },
    {
      "epoch": 0.6100604514447341,
      "grad_norm": 65.8927001953125,
      "learning_rate": 0.00011698186456657977,
      "loss": 0.2075,
      "step": 187000
    },
    {
      "epoch": 0.610386686980266,
      "grad_norm": 0.0023271520622074604,
      "learning_rate": 0.0001168839939059202,
      "loss": 0.2993,
      "step": 187100
    },
    {
      "epoch": 0.610712922515798,
      "grad_norm": 87.91071319580078,
      "learning_rate": 0.0001167861232452606,
      "loss": 0.3835,
      "step": 187200
    },
    {
      "epoch": 0.6110391580513299,
      "grad_norm": 0.9404948949813843,
      "learning_rate": 0.00011668825258460101,
      "loss": 0.2375,
      "step": 187300
    },
    {
      "epoch": 0.6113653935868618,
      "grad_norm": 0.1191788986325264,
      "learning_rate": 0.00011659038192394144,
      "loss": 0.4086,
      "step": 187400
    },
    {
      "epoch": 0.6116916291223938,
      "grad_norm": 0.0004305367183405906,
      "learning_rate": 0.00011649251126328185,
      "loss": 0.2412,
      "step": 187500
    },
    {
      "epoch": 0.6120178646579257,
      "grad_norm": 23.880525588989258,
      "learning_rate": 0.00011639464060262227,
      "loss": 0.4761,
      "step": 187600
    },
    {
      "epoch": 0.6123441001934576,
      "grad_norm": 0.29327744245529175,
      "learning_rate": 0.00011629676994196268,
      "loss": 0.2908,
      "step": 187700
    },
    {
      "epoch": 0.6126703357289897,
      "grad_norm": 0.002319318475201726,
      "learning_rate": 0.00011619889928130311,
      "loss": 0.2392,
      "step": 187800
    },
    {
      "epoch": 0.6129965712645216,
      "grad_norm": 10.92854118347168,
      "learning_rate": 0.00011610102862064353,
      "loss": 0.2496,
      "step": 187900
    },
    {
      "epoch": 0.6133228068000535,
      "grad_norm": 0.015433477237820625,
      "learning_rate": 0.00011600315795998393,
      "loss": 0.1525,
      "step": 188000
    },
    {
      "epoch": 0.6136490423355855,
      "grad_norm": 0.00045791026786901057,
      "learning_rate": 0.00011590528729932437,
      "loss": 0.4114,
      "step": 188100
    },
    {
      "epoch": 0.6139752778711174,
      "grad_norm": 61.61650466918945,
      "learning_rate": 0.00011580741663866477,
      "loss": 0.6252,
      "step": 188200
    },
    {
      "epoch": 0.6143015134066493,
      "grad_norm": 8.916119259083644e-05,
      "learning_rate": 0.00011570954597800518,
      "loss": 0.2735,
      "step": 188300
    },
    {
      "epoch": 0.6146277489421813,
      "grad_norm": 93.56681060791016,
      "learning_rate": 0.00011561167531734561,
      "loss": 0.3265,
      "step": 188400
    },
    {
      "epoch": 0.6149539844777132,
      "grad_norm": 0.05809846892952919,
      "learning_rate": 0.00011551380465668603,
      "loss": 0.4102,
      "step": 188500
    },
    {
      "epoch": 0.6152802200132451,
      "grad_norm": 0.0004371037648525089,
      "learning_rate": 0.00011541593399602644,
      "loss": 0.1595,
      "step": 188600
    },
    {
      "epoch": 0.6156064555487771,
      "grad_norm": 0.00016089847486000508,
      "learning_rate": 0.00011531806333536685,
      "loss": 0.2494,
      "step": 188700
    },
    {
      "epoch": 0.615932691084309,
      "grad_norm": 0.21604007482528687,
      "learning_rate": 0.00011522019267470728,
      "loss": 0.2988,
      "step": 188800
    },
    {
      "epoch": 0.6162589266198409,
      "grad_norm": 0.08182404935359955,
      "learning_rate": 0.0001151223220140477,
      "loss": 0.2825,
      "step": 188900
    },
    {
      "epoch": 0.616585162155373,
      "grad_norm": 0.017400100827217102,
      "learning_rate": 0.0001150244513533881,
      "loss": 0.3486,
      "step": 189000
    },
    {
      "epoch": 0.6169113976909049,
      "grad_norm": 0.0020798896439373493,
      "learning_rate": 0.00011492658069272853,
      "loss": 0.2539,
      "step": 189100
    },
    {
      "epoch": 0.6172376332264369,
      "grad_norm": 0.00021997990552335978,
      "learning_rate": 0.00011482871003206894,
      "loss": 0.2912,
      "step": 189200
    },
    {
      "epoch": 0.6175638687619688,
      "grad_norm": 2.062084674835205,
      "learning_rate": 0.00011473083937140935,
      "loss": 0.2543,
      "step": 189300
    },
    {
      "epoch": 0.6178901042975007,
      "grad_norm": 3.2369332313537598,
      "learning_rate": 0.00011463296871074978,
      "loss": 0.3711,
      "step": 189400
    },
    {
      "epoch": 0.6182163398330327,
      "grad_norm": 0.0008045419817790389,
      "learning_rate": 0.0001145350980500902,
      "loss": 0.2634,
      "step": 189500
    },
    {
      "epoch": 0.6185425753685646,
      "grad_norm": 0.0009160826448351145,
      "learning_rate": 0.00011443722738943061,
      "loss": 0.2244,
      "step": 189600
    },
    {
      "epoch": 0.6188688109040965,
      "grad_norm": 49.27180862426758,
      "learning_rate": 0.00011433935672877103,
      "loss": 0.2945,
      "step": 189700
    },
    {
      "epoch": 0.6191950464396285,
      "grad_norm": 52.119468688964844,
      "learning_rate": 0.00011424148606811145,
      "loss": 0.612,
      "step": 189800
    },
    {
      "epoch": 0.6195212819751604,
      "grad_norm": 0.0012878748821094632,
      "learning_rate": 0.00011414361540745185,
      "loss": 0.1788,
      "step": 189900
    },
    {
      "epoch": 0.6198475175106923,
      "grad_norm": 0.19360597431659698,
      "learning_rate": 0.00011404574474679227,
      "loss": 0.2658,
      "step": 190000
    },
    {
      "epoch": 0.6201737530462244,
      "grad_norm": 0.007785504218190908,
      "learning_rate": 0.0001139478740861327,
      "loss": 0.3181,
      "step": 190100
    },
    {
      "epoch": 0.6204999885817563,
      "grad_norm": 0.9363018870353699,
      "learning_rate": 0.00011385000342547311,
      "loss": 0.2677,
      "step": 190200
    },
    {
      "epoch": 0.6208262241172882,
      "grad_norm": 0.4855334758758545,
      "learning_rate": 0.00011375213276481352,
      "loss": 0.335,
      "step": 190300
    },
    {
      "epoch": 0.6211524596528202,
      "grad_norm": 0.0003815285745076835,
      "learning_rate": 0.00011365426210415395,
      "loss": 0.1501,
      "step": 190400
    },
    {
      "epoch": 0.6214786951883521,
      "grad_norm": 0.00017828679119702429,
      "learning_rate": 0.00011355639144349437,
      "loss": 0.2099,
      "step": 190500
    },
    {
      "epoch": 0.621804930723884,
      "grad_norm": 102.06494903564453,
      "learning_rate": 0.00011345852078283478,
      "loss": 0.1999,
      "step": 190600
    },
    {
      "epoch": 0.622131166259416,
      "grad_norm": 0.0939895510673523,
      "learning_rate": 0.00011336065012217521,
      "loss": 0.198,
      "step": 190700
    },
    {
      "epoch": 0.6224574017949479,
      "grad_norm": 67.8176040649414,
      "learning_rate": 0.00011326277946151562,
      "loss": 0.4923,
      "step": 190800
    },
    {
      "epoch": 0.6227836373304798,
      "grad_norm": 0.0010937118204310536,
      "learning_rate": 0.00011316490880085602,
      "loss": 0.2563,
      "step": 190900
    },
    {
      "epoch": 0.6231098728660118,
      "grad_norm": 0.10803971439599991,
      "learning_rate": 0.00011306703814019644,
      "loss": 0.287,
      "step": 191000
    },
    {
      "epoch": 0.6234361084015437,
      "grad_norm": 82.43292999267578,
      "learning_rate": 0.00011296916747953687,
      "loss": 0.2237,
      "step": 191100
    },
    {
      "epoch": 0.6237623439370756,
      "grad_norm": 0.003990644123405218,
      "learning_rate": 0.00011287129681887728,
      "loss": 0.3011,
      "step": 191200
    },
    {
      "epoch": 0.6240885794726077,
      "grad_norm": 0.000987186562269926,
      "learning_rate": 0.0001127734261582177,
      "loss": 0.2448,
      "step": 191300
    },
    {
      "epoch": 0.6244148150081396,
      "grad_norm": 0.0014828526182100177,
      "learning_rate": 0.00011267555549755812,
      "loss": 0.3037,
      "step": 191400
    },
    {
      "epoch": 0.6247410505436716,
      "grad_norm": 0.17824584245681763,
      "learning_rate": 0.00011257768483689854,
      "loss": 0.2045,
      "step": 191500
    },
    {
      "epoch": 0.6250672860792035,
      "grad_norm": 8.453167915344238,
      "learning_rate": 0.00011247981417623895,
      "loss": 0.2001,
      "step": 191600
    },
    {
      "epoch": 0.6253935216147354,
      "grad_norm": 26.920875549316406,
      "learning_rate": 0.00011238194351557938,
      "loss": 0.2641,
      "step": 191700
    },
    {
      "epoch": 0.6257197571502674,
      "grad_norm": 0.0014876935165375471,
      "learning_rate": 0.0001122840728549198,
      "loss": 0.2283,
      "step": 191800
    },
    {
      "epoch": 0.6260459926857993,
      "grad_norm": 0.00991788413375616,
      "learning_rate": 0.0001121862021942602,
      "loss": 0.3498,
      "step": 191900
    },
    {
      "epoch": 0.6263722282213312,
      "grad_norm": 0.2828475832939148,
      "learning_rate": 0.00011208833153360061,
      "loss": 0.3202,
      "step": 192000
    },
    {
      "epoch": 0.6266984637568632,
      "grad_norm": 0.008552865125238895,
      "learning_rate": 0.00011199046087294104,
      "loss": 0.3748,
      "step": 192100
    },
    {
      "epoch": 0.6270246992923951,
      "grad_norm": 0.0023200646974146366,
      "learning_rate": 0.00011189259021228145,
      "loss": 0.2404,
      "step": 192200
    },
    {
      "epoch": 0.627350934827927,
      "grad_norm": 0.00150646164547652,
      "learning_rate": 0.00011179471955162187,
      "loss": 0.3301,
      "step": 192300
    },
    {
      "epoch": 0.627677170363459,
      "grad_norm": 0.007410735357552767,
      "learning_rate": 0.0001116968488909623,
      "loss": 0.3276,
      "step": 192400
    },
    {
      "epoch": 0.628003405898991,
      "grad_norm": 0.0006423107115551829,
      "learning_rate": 0.00011159897823030271,
      "loss": 0.2711,
      "step": 192500
    },
    {
      "epoch": 0.6283296414345229,
      "grad_norm": 0.00024013849906623363,
      "learning_rate": 0.00011150110756964312,
      "loss": 0.2197,
      "step": 192600
    },
    {
      "epoch": 0.6286558769700549,
      "grad_norm": 0.006629259791225195,
      "learning_rate": 0.00011140323690898355,
      "loss": 0.1943,
      "step": 192700
    },
    {
      "epoch": 0.6289821125055868,
      "grad_norm": 0.008757355622947216,
      "learning_rate": 0.00011130536624832395,
      "loss": 0.3541,
      "step": 192800
    },
    {
      "epoch": 0.6293083480411187,
      "grad_norm": 0.0017491556936874986,
      "learning_rate": 0.00011120749558766437,
      "loss": 0.1638,
      "step": 192900
    },
    {
      "epoch": 0.6296345835766507,
      "grad_norm": 0.0012101826723665,
      "learning_rate": 0.00011110962492700478,
      "loss": 0.128,
      "step": 193000
    },
    {
      "epoch": 0.6299608191121826,
      "grad_norm": 0.001224844716489315,
      "learning_rate": 0.00011101175426634521,
      "loss": 0.4498,
      "step": 193100
    },
    {
      "epoch": 0.6302870546477145,
      "grad_norm": 0.0014026035787537694,
      "learning_rate": 0.00011091388360568562,
      "loss": 0.5531,
      "step": 193200
    },
    {
      "epoch": 0.6306132901832465,
      "grad_norm": 0.012524428777396679,
      "learning_rate": 0.00011081601294502604,
      "loss": 0.5513,
      "step": 193300
    },
    {
      "epoch": 0.6309395257187784,
      "grad_norm": 0.003012702101841569,
      "learning_rate": 0.00011071814228436646,
      "loss": 0.2794,
      "step": 193400
    },
    {
      "epoch": 0.6312657612543104,
      "grad_norm": 0.0003723155241459608,
      "learning_rate": 0.00011062027162370688,
      "loss": 0.2368,
      "step": 193500
    },
    {
      "epoch": 0.6315919967898423,
      "grad_norm": 0.0050456528551876545,
      "learning_rate": 0.00011052240096304728,
      "loss": 0.1911,
      "step": 193600
    },
    {
      "epoch": 0.6319182323253743,
      "grad_norm": 3.276501178741455,
      "learning_rate": 0.00011042453030238772,
      "loss": 0.2654,
      "step": 193700
    },
    {
      "epoch": 0.6322444678609063,
      "grad_norm": 0.0004828467790503055,
      "learning_rate": 0.00011032665964172812,
      "loss": 0.2355,
      "step": 193800
    },
    {
      "epoch": 0.6325707033964382,
      "grad_norm": 2.833425760269165,
      "learning_rate": 0.00011022878898106854,
      "loss": 0.181,
      "step": 193900
    },
    {
      "epoch": 0.6328969389319701,
      "grad_norm": 0.0013241415144875646,
      "learning_rate": 0.00011013091832040896,
      "loss": 0.3046,
      "step": 194000
    },
    {
      "epoch": 0.6332231744675021,
      "grad_norm": 0.00020756815501954406,
      "learning_rate": 0.00011003304765974938,
      "loss": 0.2128,
      "step": 194100
    },
    {
      "epoch": 0.633549410003034,
      "grad_norm": 7.557133358204737e-05,
      "learning_rate": 0.00010993517699908979,
      "loss": 0.3544,
      "step": 194200
    },
    {
      "epoch": 0.6338756455385659,
      "grad_norm": 0.0013152551837265491,
      "learning_rate": 0.00010983730633843021,
      "loss": 0.193,
      "step": 194300
    },
    {
      "epoch": 0.6342018810740979,
      "grad_norm": 0.0017257765866816044,
      "learning_rate": 0.00010973943567777064,
      "loss": 0.318,
      "step": 194400
    },
    {
      "epoch": 0.6345281166096298,
      "grad_norm": 0.002308413153514266,
      "learning_rate": 0.00010964156501711105,
      "loss": 0.1679,
      "step": 194500
    },
    {
      "epoch": 0.6348543521451617,
      "grad_norm": 0.0005207653157413006,
      "learning_rate": 0.00010954369435645145,
      "loss": 0.1917,
      "step": 194600
    },
    {
      "epoch": 0.6351805876806937,
      "grad_norm": 0.0017561030108481646,
      "learning_rate": 0.00010944582369579188,
      "loss": 0.4233,
      "step": 194700
    },
    {
      "epoch": 0.6355068232162256,
      "grad_norm": 0.01055774837732315,
      "learning_rate": 0.00010934795303513229,
      "loss": 0.3221,
      "step": 194800
    },
    {
      "epoch": 0.6358330587517576,
      "grad_norm": 0.0010727347107604146,
      "learning_rate": 0.00010925008237447271,
      "loss": 0.3126,
      "step": 194900
    },
    {
      "epoch": 0.6361592942872896,
      "grad_norm": 0.0023956685326993465,
      "learning_rate": 0.00010915221171381314,
      "loss": 0.1764,
      "step": 195000
    },
    {
      "epoch": 0.6364855298228215,
      "grad_norm": 0.00017277747974731028,
      "learning_rate": 0.00010905434105315355,
      "loss": 0.3292,
      "step": 195100
    },
    {
      "epoch": 0.6368117653583534,
      "grad_norm": 3.836014002445154e-05,
      "learning_rate": 0.00010895647039249396,
      "loss": 0.2792,
      "step": 195200
    },
    {
      "epoch": 0.6371380008938854,
      "grad_norm": 12.356660842895508,
      "learning_rate": 0.00010885859973183438,
      "loss": 0.4764,
      "step": 195300
    },
    {
      "epoch": 0.6374642364294173,
      "grad_norm": 6.737105846405029,
      "learning_rate": 0.0001087607290711748,
      "loss": 0.2372,
      "step": 195400
    },
    {
      "epoch": 0.6377904719649492,
      "grad_norm": 0.0015814139042049646,
      "learning_rate": 0.00010866285841051521,
      "loss": 0.2277,
      "step": 195500
    },
    {
      "epoch": 0.6381167075004812,
      "grad_norm": 0.008172761648893356,
      "learning_rate": 0.00010856498774985562,
      "loss": 0.1435,
      "step": 195600
    },
    {
      "epoch": 0.6384429430360131,
      "grad_norm": 0.0007289909408427775,
      "learning_rate": 0.00010846711708919605,
      "loss": 0.3151,
      "step": 195700
    },
    {
      "epoch": 0.6387691785715451,
      "grad_norm": 7.561800885014236e-05,
      "learning_rate": 0.00010836924642853646,
      "loss": 0.3324,
      "step": 195800
    },
    {
      "epoch": 0.639095414107077,
      "grad_norm": 0.009460773319005966,
      "learning_rate": 0.00010827137576787688,
      "loss": 0.3657,
      "step": 195900
    },
    {
      "epoch": 0.639421649642609,
      "grad_norm": 3.76900577545166,
      "learning_rate": 0.0001081735051072173,
      "loss": 0.1971,
      "step": 196000
    },
    {
      "epoch": 0.639747885178141,
      "grad_norm": 0.0022673532366752625,
      "learning_rate": 0.00010807563444655772,
      "loss": 0.199,
      "step": 196100
    },
    {
      "epoch": 0.6400741207136729,
      "grad_norm": 0.0003888733626808971,
      "learning_rate": 0.00010797776378589813,
      "loss": 0.124,
      "step": 196200
    },
    {
      "epoch": 0.6404003562492048,
      "grad_norm": 0.010800418443977833,
      "learning_rate": 0.00010787989312523855,
      "loss": 0.2456,
      "step": 196300
    },
    {
      "epoch": 0.6407265917847368,
      "grad_norm": 0.0080009950324893,
      "learning_rate": 0.00010778202246457898,
      "loss": 0.2912,
      "step": 196400
    },
    {
      "epoch": 0.6410528273202687,
      "grad_norm": 0.0032198389526456594,
      "learning_rate": 0.00010768415180391938,
      "loss": 0.3866,
      "step": 196500
    },
    {
      "epoch": 0.6413790628558006,
      "grad_norm": 0.3851846754550934,
      "learning_rate": 0.00010758628114325979,
      "loss": 0.275,
      "step": 196600
    },
    {
      "epoch": 0.6417052983913326,
      "grad_norm": 0.0038602983113378286,
      "learning_rate": 0.00010748841048260022,
      "loss": 0.4453,
      "step": 196700
    },
    {
      "epoch": 0.6420315339268645,
      "grad_norm": 0.00020936469081789255,
      "learning_rate": 0.00010739053982194063,
      "loss": 0.2353,
      "step": 196800
    },
    {
      "epoch": 0.6423577694623964,
      "grad_norm": 0.021175464615225792,
      "learning_rate": 0.00010729266916128105,
      "loss": 0.2555,
      "step": 196900
    },
    {
      "epoch": 0.6426840049979284,
      "grad_norm": 0.00032415223540738225,
      "learning_rate": 0.00010719479850062148,
      "loss": 0.2699,
      "step": 197000
    },
    {
      "epoch": 0.6430102405334603,
      "grad_norm": 0.00026519273524172604,
      "learning_rate": 0.00010709692783996189,
      "loss": 0.4228,
      "step": 197100
    },
    {
      "epoch": 0.6433364760689922,
      "grad_norm": 0.0014597970293834805,
      "learning_rate": 0.0001069990571793023,
      "loss": 0.3278,
      "step": 197200
    },
    {
      "epoch": 0.6436627116045243,
      "grad_norm": 0.004310572054237127,
      "learning_rate": 0.00010690118651864273,
      "loss": 0.3585,
      "step": 197300
    },
    {
      "epoch": 0.6439889471400562,
      "grad_norm": 68.36940002441406,
      "learning_rate": 0.00010680331585798313,
      "loss": 0.1526,
      "step": 197400
    },
    {
      "epoch": 0.6443151826755881,
      "grad_norm": 0.3086264133453369,
      "learning_rate": 0.00010670544519732355,
      "loss": 0.2275,
      "step": 197500
    },
    {
      "epoch": 0.6446414182111201,
      "grad_norm": 0.18869197368621826,
      "learning_rate": 0.00010660757453666396,
      "loss": 0.3579,
      "step": 197600
    },
    {
      "epoch": 0.644967653746652,
      "grad_norm": 0.5278206467628479,
      "learning_rate": 0.00010650970387600439,
      "loss": 0.2642,
      "step": 197700
    },
    {
      "epoch": 0.6452938892821839,
      "grad_norm": 7.103050231933594,
      "learning_rate": 0.0001064118332153448,
      "loss": 0.2104,
      "step": 197800
    },
    {
      "epoch": 0.6456201248177159,
      "grad_norm": 0.0008641016902402043,
      "learning_rate": 0.00010631396255468522,
      "loss": 0.2469,
      "step": 197900
    },
    {
      "epoch": 0.6459463603532478,
      "grad_norm": 0.001002355944365263,
      "learning_rate": 0.00010621609189402565,
      "loss": 0.3051,
      "step": 198000
    },
    {
      "epoch": 0.6462725958887798,
      "grad_norm": 28.898834228515625,
      "learning_rate": 0.00010611822123336606,
      "loss": 0.4445,
      "step": 198100
    },
    {
      "epoch": 0.6465988314243117,
      "grad_norm": 0.4346427917480469,
      "learning_rate": 0.00010602035057270648,
      "loss": 0.08,
      "step": 198200
    },
    {
      "epoch": 0.6469250669598436,
      "grad_norm": 5.346010208129883,
      "learning_rate": 0.0001059224799120469,
      "loss": 0.3842,
      "step": 198300
    },
    {
      "epoch": 0.6472513024953757,
      "grad_norm": 0.011558815836906433,
      "learning_rate": 0.0001058246092513873,
      "loss": 0.3121,
      "step": 198400
    },
    {
      "epoch": 0.6475775380309076,
      "grad_norm": 0.0007100793882273138,
      "learning_rate": 0.00010572673859072772,
      "loss": 0.2465,
      "step": 198500
    },
    {
      "epoch": 0.6479037735664395,
      "grad_norm": 0.0020542051643133163,
      "learning_rate": 0.00010562886793006813,
      "loss": 0.2694,
      "step": 198600
    },
    {
      "epoch": 0.6482300091019715,
      "grad_norm": 0.003149262862280011,
      "learning_rate": 0.00010553099726940856,
      "loss": 0.1998,
      "step": 198700
    },
    {
      "epoch": 0.6485562446375034,
      "grad_norm": 0.000707742408849299,
      "learning_rate": 0.00010543312660874898,
      "loss": 0.3101,
      "step": 198800
    },
    {
      "epoch": 0.6488824801730353,
      "grad_norm": 0.10628014802932739,
      "learning_rate": 0.00010533525594808939,
      "loss": 0.3764,
      "step": 198900
    },
    {
      "epoch": 0.6492087157085673,
      "grad_norm": 0.10302428901195526,
      "learning_rate": 0.00010523738528742982,
      "loss": 0.3152,
      "step": 199000
    },
    {
      "epoch": 0.6495349512440992,
      "grad_norm": 0.0010047879768535495,
      "learning_rate": 0.00010513951462677023,
      "loss": 0.4102,
      "step": 199100
    },
    {
      "epoch": 0.6498611867796311,
      "grad_norm": 0.000652722897939384,
      "learning_rate": 0.00010504164396611063,
      "loss": 0.3128,
      "step": 199200
    },
    {
      "epoch": 0.6501874223151631,
      "grad_norm": 52.78568649291992,
      "learning_rate": 0.00010494377330545106,
      "loss": 0.4006,
      "step": 199300
    },
    {
      "epoch": 0.650513657850695,
      "grad_norm": 12.971061706542969,
      "learning_rate": 0.00010484590264479148,
      "loss": 0.2657,
      "step": 199400
    },
    {
      "epoch": 0.6508398933862269,
      "grad_norm": 5.642472267150879,
      "learning_rate": 0.00010474803198413189,
      "loss": 0.1787,
      "step": 199500
    },
    {
      "epoch": 0.651166128921759,
      "grad_norm": 0.009773620404303074,
      "learning_rate": 0.0001046501613234723,
      "loss": 0.419,
      "step": 199600
    },
    {
      "epoch": 0.6514923644572909,
      "grad_norm": 0.047586169093847275,
      "learning_rate": 0.00010455229066281273,
      "loss": 0.2389,
      "step": 199700
    },
    {
      "epoch": 0.6518185999928228,
      "grad_norm": 0.00033050833735615015,
      "learning_rate": 0.00010445442000215315,
      "loss": 0.3235,
      "step": 199800
    },
    {
      "epoch": 0.6521448355283548,
      "grad_norm": 0.00447121262550354,
      "learning_rate": 0.00010435654934149356,
      "loss": 0.118,
      "step": 199900
    },
    {
      "epoch": 0.6524710710638867,
      "grad_norm": 0.01758592762053013,
      "learning_rate": 0.00010425867868083399,
      "loss": 0.2577,
      "step": 200000
    },
    {
      "epoch": 0.6527973065994187,
      "grad_norm": 0.0012617027387022972,
      "learning_rate": 0.0001041608080201744,
      "loss": 0.2763,
      "step": 200100
    },
    {
      "epoch": 0.6531235421349506,
      "grad_norm": 0.00021517567802220583,
      "learning_rate": 0.0001040629373595148,
      "loss": 0.2218,
      "step": 200200
    },
    {
      "epoch": 0.6534497776704825,
      "grad_norm": 0.00019456278823781759,
      "learning_rate": 0.00010396506669885523,
      "loss": 0.2298,
      "step": 200300
    },
    {
      "epoch": 0.6537760132060145,
      "grad_norm": 36.357513427734375,
      "learning_rate": 0.00010386719603819565,
      "loss": 0.3658,
      "step": 200400
    },
    {
      "epoch": 0.6541022487415464,
      "grad_norm": 19.416221618652344,
      "learning_rate": 0.00010376932537753606,
      "loss": 0.2532,
      "step": 200500
    },
    {
      "epoch": 0.6544284842770783,
      "grad_norm": 0.21812625229358673,
      "learning_rate": 0.00010367145471687647,
      "loss": 0.2616,
      "step": 200600
    },
    {
      "epoch": 0.6547547198126104,
      "grad_norm": 0.08104509860277176,
      "learning_rate": 0.0001035735840562169,
      "loss": 0.3275,
      "step": 200700
    },
    {
      "epoch": 0.6550809553481423,
      "grad_norm": 0.0010319551220163703,
      "learning_rate": 0.00010347571339555732,
      "loss": 0.2101,
      "step": 200800
    },
    {
      "epoch": 0.6554071908836742,
      "grad_norm": 17.97955894470215,
      "learning_rate": 0.00010337784273489773,
      "loss": 0.2263,
      "step": 200900
    },
    {
      "epoch": 0.6557334264192062,
      "grad_norm": 0.0037770483177155256,
      "learning_rate": 0.00010327997207423816,
      "loss": 0.3554,
      "step": 201000
    },
    {
      "epoch": 0.6560596619547381,
      "grad_norm": 0.0033550155349075794,
      "learning_rate": 0.00010318210141357856,
      "loss": 0.1863,
      "step": 201100
    },
    {
      "epoch": 0.65638589749027,
      "grad_norm": 34.805198669433594,
      "learning_rate": 0.00010308423075291897,
      "loss": 0.2973,
      "step": 201200
    },
    {
      "epoch": 0.656712133025802,
      "grad_norm": 0.00011011568858521059,
      "learning_rate": 0.0001029863600922594,
      "loss": 0.1651,
      "step": 201300
    },
    {
      "epoch": 0.6570383685613339,
      "grad_norm": 40.29532241821289,
      "learning_rate": 0.00010288848943159982,
      "loss": 0.3507,
      "step": 201400
    },
    {
      "epoch": 0.6573646040968658,
      "grad_norm": 0.00026563077699393034,
      "learning_rate": 0.00010279061877094023,
      "loss": 0.1857,
      "step": 201500
    },
    {
      "epoch": 0.6576908396323978,
      "grad_norm": 0.00043000804726034403,
      "learning_rate": 0.00010269274811028066,
      "loss": 0.314,
      "step": 201600
    },
    {
      "epoch": 0.6580170751679297,
      "grad_norm": 0.0002828107972163707,
      "learning_rate": 0.00010259487744962107,
      "loss": 0.2982,
      "step": 201700
    },
    {
      "epoch": 0.6583433107034616,
      "grad_norm": 0.0014263598714023829,
      "learning_rate": 0.00010249700678896149,
      "loss": 0.4311,
      "step": 201800
    },
    {
      "epoch": 0.6586695462389937,
      "grad_norm": 11.421917915344238,
      "learning_rate": 0.00010239913612830189,
      "loss": 0.4126,
      "step": 201900
    },
    {
      "epoch": 0.6589957817745256,
      "grad_norm": 0.008490684442222118,
      "learning_rate": 0.00010230126546764233,
      "loss": 0.3454,
      "step": 202000
    },
    {
      "epoch": 0.6593220173100575,
      "grad_norm": 0.0004048561386298388,
      "learning_rate": 0.00010220339480698273,
      "loss": 0.1183,
      "step": 202100
    },
    {
      "epoch": 0.6596482528455895,
      "grad_norm": 0.0004997557261958718,
      "learning_rate": 0.00010210552414632315,
      "loss": 0.2304,
      "step": 202200
    },
    {
      "epoch": 0.6599744883811214,
      "grad_norm": 0.000412983528804034,
      "learning_rate": 0.00010200765348566357,
      "loss": 0.2657,
      "step": 202300
    },
    {
      "epoch": 0.6603007239166534,
      "grad_norm": 0.004174367990344763,
      "learning_rate": 0.00010190978282500399,
      "loss": 0.3001,
      "step": 202400
    },
    {
      "epoch": 0.6606269594521853,
      "grad_norm": 0.02384933829307556,
      "learning_rate": 0.0001018119121643444,
      "loss": 0.3761,
      "step": 202500
    },
    {
      "epoch": 0.6609531949877172,
      "grad_norm": 27.56591033935547,
      "learning_rate": 0.00010171404150368483,
      "loss": 0.3843,
      "step": 202600
    },
    {
      "epoch": 0.6612794305232492,
      "grad_norm": 7.199530227808282e-05,
      "learning_rate": 0.00010161617084302524,
      "loss": 0.3576,
      "step": 202700
    },
    {
      "epoch": 0.6616056660587811,
      "grad_norm": 3.406693935394287,
      "learning_rate": 0.00010151830018236566,
      "loss": 0.2361,
      "step": 202800
    },
    {
      "epoch": 0.661931901594313,
      "grad_norm": 0.0038349642418324947,
      "learning_rate": 0.00010142042952170606,
      "loss": 0.1983,
      "step": 202900
    },
    {
      "epoch": 0.662258137129845,
      "grad_norm": 0.005374026484787464,
      "learning_rate": 0.00010132255886104649,
      "loss": 0.2888,
      "step": 203000
    },
    {
      "epoch": 0.662584372665377,
      "grad_norm": 0.007040610536932945,
      "learning_rate": 0.0001012246882003869,
      "loss": 0.3051,
      "step": 203100
    },
    {
      "epoch": 0.6629106082009089,
      "grad_norm": 0.03199814260005951,
      "learning_rate": 0.00010112681753972732,
      "loss": 0.2641,
      "step": 203200
    },
    {
      "epoch": 0.6632368437364409,
      "grad_norm": 10.90766716003418,
      "learning_rate": 0.00010102894687906774,
      "loss": 0.1696,
      "step": 203300
    },
    {
      "epoch": 0.6635630792719728,
      "grad_norm": 0.0016092619625851512,
      "learning_rate": 0.00010093107621840816,
      "loss": 0.3409,
      "step": 203400
    },
    {
      "epoch": 0.6638893148075047,
      "grad_norm": 0.3046572804450989,
      "learning_rate": 0.00010083320555774857,
      "loss": 0.2593,
      "step": 203500
    },
    {
      "epoch": 0.6642155503430367,
      "grad_norm": 33.51805114746094,
      "learning_rate": 0.000100735334897089,
      "loss": 0.2252,
      "step": 203600
    },
    {
      "epoch": 0.6645417858785686,
      "grad_norm": 0.014617863111197948,
      "learning_rate": 0.00010063746423642941,
      "loss": 0.2862,
      "step": 203700
    },
    {
      "epoch": 0.6648680214141005,
      "grad_norm": 0.0003718286461662501,
      "learning_rate": 0.00010053959357576983,
      "loss": 0.2572,
      "step": 203800
    },
    {
      "epoch": 0.6651942569496325,
      "grad_norm": 55.97602844238281,
      "learning_rate": 0.00010044172291511023,
      "loss": 0.3695,
      "step": 203900
    },
    {
      "epoch": 0.6655204924851644,
      "grad_norm": 0.0028332225047051907,
      "learning_rate": 0.00010034385225445066,
      "loss": 0.2929,
      "step": 204000
    },
    {
      "epoch": 0.6658467280206963,
      "grad_norm": 27.65674591064453,
      "learning_rate": 0.00010024598159379107,
      "loss": 0.2545,
      "step": 204100
    },
    {
      "epoch": 0.6661729635562283,
      "grad_norm": 0.004092859569936991,
      "learning_rate": 0.00010014811093313149,
      "loss": 0.2438,
      "step": 204200
    },
    {
      "epoch": 0.6664991990917603,
      "grad_norm": 0.02205807901918888,
      "learning_rate": 0.00010005024027247191,
      "loss": 0.3084,
      "step": 204300
    },
    {
      "epoch": 0.6668254346272922,
      "grad_norm": 0.002877726685255766,
      "learning_rate": 9.995236961181233e-05,
      "loss": 0.2443,
      "step": 204400
    },
    {
      "epoch": 0.6671516701628242,
      "grad_norm": 7.970275328261778e-05,
      "learning_rate": 9.985449895115274e-05,
      "loss": 0.3059,
      "step": 204500
    },
    {
      "epoch": 0.6674779056983561,
      "grad_norm": 0.0010002164635807276,
      "learning_rate": 9.975662829049317e-05,
      "loss": 0.2356,
      "step": 204600
    },
    {
      "epoch": 0.6678041412338881,
      "grad_norm": 0.0018476721597835422,
      "learning_rate": 9.965875762983359e-05,
      "loss": 0.1617,
      "step": 204700
    },
    {
      "epoch": 0.66813037676942,
      "grad_norm": 0.00047555947094224393,
      "learning_rate": 9.956088696917399e-05,
      "loss": 0.3749,
      "step": 204800
    },
    {
      "epoch": 0.6684566123049519,
      "grad_norm": 0.004216703120619059,
      "learning_rate": 9.946301630851441e-05,
      "loss": 0.3118,
      "step": 204900
    },
    {
      "epoch": 0.6687828478404839,
      "grad_norm": 55.631874084472656,
      "learning_rate": 9.936514564785483e-05,
      "loss": 0.3924,
      "step": 205000
    },
    {
      "epoch": 0.6691090833760158,
      "grad_norm": 0.001134473248384893,
      "learning_rate": 9.926727498719524e-05,
      "loss": 0.2444,
      "step": 205100
    },
    {
      "epoch": 0.6694353189115477,
      "grad_norm": 42.16548538208008,
      "learning_rate": 9.916940432653566e-05,
      "loss": 0.324,
      "step": 205200
    },
    {
      "epoch": 0.6697615544470797,
      "grad_norm": 0.00781427975744009,
      "learning_rate": 9.907153366587608e-05,
      "loss": 0.3745,
      "step": 205300
    },
    {
      "epoch": 0.6700877899826116,
      "grad_norm": 0.0059600831009447575,
      "learning_rate": 9.89736630052165e-05,
      "loss": 0.1966,
      "step": 205400
    },
    {
      "epoch": 0.6704140255181436,
      "grad_norm": 0.42656928300857544,
      "learning_rate": 9.887579234455691e-05,
      "loss": 0.3221,
      "step": 205500
    },
    {
      "epoch": 0.6707402610536756,
      "grad_norm": 3.748699600691907e-05,
      "learning_rate": 9.877792168389734e-05,
      "loss": 0.2087,
      "step": 205600
    },
    {
      "epoch": 0.6710664965892075,
      "grad_norm": 0.0008391819428652525,
      "learning_rate": 9.868005102323776e-05,
      "loss": 0.3812,
      "step": 205700
    },
    {
      "epoch": 0.6713927321247394,
      "grad_norm": 0.004429838620126247,
      "learning_rate": 9.858218036257816e-05,
      "loss": 0.1728,
      "step": 205800
    },
    {
      "epoch": 0.6717189676602714,
      "grad_norm": 0.15841814875602722,
      "learning_rate": 9.848430970191858e-05,
      "loss": 0.3674,
      "step": 205900
    },
    {
      "epoch": 0.6720452031958033,
      "grad_norm": 32.57268142700195,
      "learning_rate": 9.8386439041259e-05,
      "loss": 0.2288,
      "step": 206000
    },
    {
      "epoch": 0.6723714387313352,
      "grad_norm": 0.004697424825280905,
      "learning_rate": 9.828856838059941e-05,
      "loss": 0.2458,
      "step": 206100
    },
    {
      "epoch": 0.6726976742668672,
      "grad_norm": 0.003841191530227661,
      "learning_rate": 9.819069771993983e-05,
      "loss": 0.4334,
      "step": 206200
    },
    {
      "epoch": 0.6730239098023991,
      "grad_norm": 2.4281071091536433e-05,
      "learning_rate": 9.809282705928026e-05,
      "loss": 0.2287,
      "step": 206300
    },
    {
      "epoch": 0.673350145337931,
      "grad_norm": 25.05457878112793,
      "learning_rate": 9.799495639862067e-05,
      "loss": 0.1715,
      "step": 206400
    },
    {
      "epoch": 0.673676380873463,
      "grad_norm": 0.018828975036740303,
      "learning_rate": 9.789708573796108e-05,
      "loss": 0.1973,
      "step": 206500
    },
    {
      "epoch": 0.674002616408995,
      "grad_norm": 0.0025078044272959232,
      "learning_rate": 9.779921507730151e-05,
      "loss": 0.3312,
      "step": 206600
    },
    {
      "epoch": 0.674328851944527,
      "grad_norm": 0.018332464620471,
      "learning_rate": 9.770134441664191e-05,
      "loss": 0.2318,
      "step": 206700
    },
    {
      "epoch": 0.6746550874800589,
      "grad_norm": 8.75878620147705,
      "learning_rate": 9.760347375598233e-05,
      "loss": 0.5241,
      "step": 206800
    },
    {
      "epoch": 0.6749813230155908,
      "grad_norm": 5.68491268157959,
      "learning_rate": 9.750560309532276e-05,
      "loss": 0.1701,
      "step": 206900
    },
    {
      "epoch": 0.6753075585511228,
      "grad_norm": 0.00015290062583517283,
      "learning_rate": 9.740773243466317e-05,
      "loss": 0.2248,
      "step": 207000
    },
    {
      "epoch": 0.6756337940866547,
      "grad_norm": 0.0012940771412104368,
      "learning_rate": 9.730986177400358e-05,
      "loss": 0.4395,
      "step": 207100
    },
    {
      "epoch": 0.6759600296221866,
      "grad_norm": 2.8755555152893066,
      "learning_rate": 9.7211991113344e-05,
      "loss": 0.2487,
      "step": 207200
    },
    {
      "epoch": 0.6762862651577186,
      "grad_norm": 0.0016762774903327227,
      "learning_rate": 9.711412045268443e-05,
      "loss": 0.2462,
      "step": 207300
    },
    {
      "epoch": 0.6766125006932505,
      "grad_norm": 4.746551036834717,
      "learning_rate": 9.701624979202484e-05,
      "loss": 0.4366,
      "step": 207400
    },
    {
      "epoch": 0.6769387362287824,
      "grad_norm": 0.48507583141326904,
      "learning_rate": 9.691837913136524e-05,
      "loss": 0.2717,
      "step": 207500
    },
    {
      "epoch": 0.6772649717643144,
      "grad_norm": 0.7250427603721619,
      "learning_rate": 9.682050847070568e-05,
      "loss": 0.2861,
      "step": 207600
    },
    {
      "epoch": 0.6775912072998463,
      "grad_norm": 0.007132173981517553,
      "learning_rate": 9.672263781004608e-05,
      "loss": 0.313,
      "step": 207700
    },
    {
      "epoch": 0.6779174428353782,
      "grad_norm": 0.0024881307035684586,
      "learning_rate": 9.66247671493865e-05,
      "loss": 0.1926,
      "step": 207800
    },
    {
      "epoch": 0.6782436783709103,
      "grad_norm": 0.020016375929117203,
      "learning_rate": 9.652689648872693e-05,
      "loss": 0.2743,
      "step": 207900
    },
    {
      "epoch": 0.6785699139064422,
      "grad_norm": 2.03208589553833,
      "learning_rate": 9.642902582806734e-05,
      "loss": 0.3898,
      "step": 208000
    },
    {
      "epoch": 0.6788961494419741,
      "grad_norm": 0.00039706967072561383,
      "learning_rate": 9.633115516740775e-05,
      "loss": 0.2104,
      "step": 208100
    },
    {
      "epoch": 0.6792223849775061,
      "grad_norm": 0.0009534108685329556,
      "learning_rate": 9.623328450674818e-05,
      "loss": 0.2683,
      "step": 208200
    },
    {
      "epoch": 0.679548620513038,
      "grad_norm": 155.49037170410156,
      "learning_rate": 9.61354138460886e-05,
      "loss": 0.3705,
      "step": 208300
    },
    {
      "epoch": 0.6798748560485699,
      "grad_norm": 0.3450392484664917,
      "learning_rate": 9.603754318542901e-05,
      "loss": 0.3576,
      "step": 208400
    },
    {
      "epoch": 0.6802010915841019,
      "grad_norm": 0.010464789345860481,
      "learning_rate": 9.593967252476941e-05,
      "loss": 0.1361,
      "step": 208500
    },
    {
      "epoch": 0.6805273271196338,
      "grad_norm": 1.563477962918114e-05,
      "learning_rate": 9.584180186410984e-05,
      "loss": 0.1607,
      "step": 208600
    },
    {
      "epoch": 0.6808535626551657,
      "grad_norm": 0.0016033992869779468,
      "learning_rate": 9.574393120345025e-05,
      "loss": 0.0865,
      "step": 208700
    },
    {
      "epoch": 0.6811797981906977,
      "grad_norm": 7.708101475145668e-05,
      "learning_rate": 9.564606054279067e-05,
      "loss": 0.2149,
      "step": 208800
    },
    {
      "epoch": 0.6815060337262296,
      "grad_norm": 0.1629418581724167,
      "learning_rate": 9.55481898821311e-05,
      "loss": 0.2743,
      "step": 208900
    },
    {
      "epoch": 0.6818322692617617,
      "grad_norm": 0.0034353979863226414,
      "learning_rate": 9.545031922147151e-05,
      "loss": 0.3691,
      "step": 209000
    },
    {
      "epoch": 0.6821585047972936,
      "grad_norm": 0.00018560333410277963,
      "learning_rate": 9.535244856081193e-05,
      "loss": 0.2477,
      "step": 209100
    },
    {
      "epoch": 0.6824847403328255,
      "grad_norm": 40.52784729003906,
      "learning_rate": 9.525457790015235e-05,
      "loss": 0.1828,
      "step": 209200
    },
    {
      "epoch": 0.6828109758683575,
      "grad_norm": 108.4771728515625,
      "learning_rate": 9.515670723949277e-05,
      "loss": 0.2693,
      "step": 209300
    },
    {
      "epoch": 0.6831372114038894,
      "grad_norm": 0.005905242636799812,
      "learning_rate": 9.505883657883317e-05,
      "loss": 0.2395,
      "step": 209400
    },
    {
      "epoch": 0.6834634469394213,
      "grad_norm": 0.0009256526245735586,
      "learning_rate": 9.496096591817358e-05,
      "loss": 0.3115,
      "step": 209500
    },
    {
      "epoch": 0.6837896824749533,
      "grad_norm": 3.959125388064422e-05,
      "learning_rate": 9.486309525751401e-05,
      "loss": 0.1662,
      "step": 209600
    },
    {
      "epoch": 0.6841159180104852,
      "grad_norm": 0.013233194127678871,
      "learning_rate": 9.476522459685443e-05,
      "loss": 0.3846,
      "step": 209700
    },
    {
      "epoch": 0.6844421535460171,
      "grad_norm": 0.016408924013376236,
      "learning_rate": 9.466735393619484e-05,
      "loss": 0.1996,
      "step": 209800
    },
    {
      "epoch": 0.6847683890815491,
      "grad_norm": 9.279577352572232e-05,
      "learning_rate": 9.456948327553527e-05,
      "loss": 0.1478,
      "step": 209900
    },
    {
      "epoch": 0.685094624617081,
      "grad_norm": 0.002811916172504425,
      "learning_rate": 9.447161261487568e-05,
      "loss": 0.1743,
      "step": 210000
    },
    {
      "epoch": 0.6854208601526129,
      "grad_norm": 0.0021475362591445446,
      "learning_rate": 9.43737419542161e-05,
      "loss": 0.3456,
      "step": 210100
    },
    {
      "epoch": 0.685747095688145,
      "grad_norm": 0.011513661593198776,
      "learning_rate": 9.427587129355652e-05,
      "loss": 0.1719,
      "step": 210200
    },
    {
      "epoch": 0.6860733312236769,
      "grad_norm": 1.544379301776644e-05,
      "learning_rate": 9.417800063289694e-05,
      "loss": 0.1355,
      "step": 210300
    },
    {
      "epoch": 0.6863995667592088,
      "grad_norm": 0.0009453946258872747,
      "learning_rate": 9.408012997223734e-05,
      "loss": 0.2332,
      "step": 210400
    },
    {
      "epoch": 0.6867258022947408,
      "grad_norm": 0.00037146342219784856,
      "learning_rate": 9.398225931157775e-05,
      "loss": 0.2505,
      "step": 210500
    },
    {
      "epoch": 0.6870520378302727,
      "grad_norm": 0.16661810874938965,
      "learning_rate": 9.388438865091818e-05,
      "loss": 0.1568,
      "step": 210600
    },
    {
      "epoch": 0.6873782733658046,
      "grad_norm": 0.007873116992413998,
      "learning_rate": 9.37865179902586e-05,
      "loss": 0.2966,
      "step": 210700
    },
    {
      "epoch": 0.6877045089013366,
      "grad_norm": 3.580642078304663e-05,
      "learning_rate": 9.368864732959901e-05,
      "loss": 0.3668,
      "step": 210800
    },
    {
      "epoch": 0.6880307444368685,
      "grad_norm": 3.79410594177898e-05,
      "learning_rate": 9.359077666893944e-05,
      "loss": 0.2589,
      "step": 210900
    },
    {
      "epoch": 0.6883569799724005,
      "grad_norm": 0.009094608947634697,
      "learning_rate": 9.349290600827985e-05,
      "loss": 0.2905,
      "step": 211000
    },
    {
      "epoch": 0.6886832155079324,
      "grad_norm": 0.0003156159946229309,
      "learning_rate": 9.339503534762027e-05,
      "loss": 0.2458,
      "step": 211100
    },
    {
      "epoch": 0.6890094510434643,
      "grad_norm": 0.37988775968551636,
      "learning_rate": 9.32971646869607e-05,
      "loss": 0.3798,
      "step": 211200
    },
    {
      "epoch": 0.6893356865789964,
      "grad_norm": 0.00039322132943198085,
      "learning_rate": 9.31992940263011e-05,
      "loss": 0.2373,
      "step": 211300
    },
    {
      "epoch": 0.6896619221145283,
      "grad_norm": 38.79066467285156,
      "learning_rate": 9.310142336564151e-05,
      "loss": 0.307,
      "step": 211400
    },
    {
      "epoch": 0.6899881576500602,
      "grad_norm": 0.0001592205953784287,
      "learning_rate": 9.300355270498194e-05,
      "loss": 0.2763,
      "step": 211500
    },
    {
      "epoch": 0.6903143931855922,
      "grad_norm": 18.86322021484375,
      "learning_rate": 9.290568204432235e-05,
      "loss": 0.3096,
      "step": 211600
    },
    {
      "epoch": 0.6906406287211241,
      "grad_norm": 0.000800458190497011,
      "learning_rate": 9.280781138366277e-05,
      "loss": 0.4116,
      "step": 211700
    },
    {
      "epoch": 0.690966864256656,
      "grad_norm": 0.0007724673487246037,
      "learning_rate": 9.270994072300318e-05,
      "loss": 0.2815,
      "step": 211800
    },
    {
      "epoch": 0.691293099792188,
      "grad_norm": 0.00038860709173604846,
      "learning_rate": 9.261207006234361e-05,
      "loss": 0.2198,
      "step": 211900
    },
    {
      "epoch": 0.6916193353277199,
      "grad_norm": 10.716374397277832,
      "learning_rate": 9.251419940168402e-05,
      "loss": 0.21,
      "step": 212000
    },
    {
      "epoch": 0.6919455708632518,
      "grad_norm": 0.0011063272831961513,
      "learning_rate": 9.241632874102444e-05,
      "loss": 0.2594,
      "step": 212100
    },
    {
      "epoch": 0.6922718063987838,
      "grad_norm": 0.00010366075730416924,
      "learning_rate": 9.231845808036487e-05,
      "loss": 0.4441,
      "step": 212200
    },
    {
      "epoch": 0.6925980419343157,
      "grad_norm": 0.01238044910132885,
      "learning_rate": 9.222058741970527e-05,
      "loss": 0.264,
      "step": 212300
    },
    {
      "epoch": 0.6929242774698476,
      "grad_norm": 0.0002824479597620666,
      "learning_rate": 9.212271675904568e-05,
      "loss": 0.2575,
      "step": 212400
    },
    {
      "epoch": 0.6932505130053797,
      "grad_norm": 0.00012458399578463286,
      "learning_rate": 9.202484609838611e-05,
      "loss": 0.1359,
      "step": 212500
    },
    {
      "epoch": 0.6935767485409116,
      "grad_norm": 0.001160063548013568,
      "learning_rate": 9.192697543772652e-05,
      "loss": 0.2847,
      "step": 212600
    },
    {
      "epoch": 0.6939029840764435,
      "grad_norm": 0.00848848931491375,
      "learning_rate": 9.182910477706694e-05,
      "loss": 0.2561,
      "step": 212700
    },
    {
      "epoch": 0.6942292196119755,
      "grad_norm": 0.6558387875556946,
      "learning_rate": 9.173123411640735e-05,
      "loss": 0.1289,
      "step": 212800
    },
    {
      "epoch": 0.6945554551475074,
      "grad_norm": 4.8550733481533825e-05,
      "learning_rate": 9.163336345574778e-05,
      "loss": 0.1271,
      "step": 212900
    },
    {
      "epoch": 0.6948816906830393,
      "grad_norm": 0.03276495262980461,
      "learning_rate": 9.15354927950882e-05,
      "loss": 0.3022,
      "step": 213000
    },
    {
      "epoch": 0.6952079262185713,
      "grad_norm": 0.0003763483837246895,
      "learning_rate": 9.14376221344286e-05,
      "loss": 0.1287,
      "step": 213100
    },
    {
      "epoch": 0.6955341617541032,
      "grad_norm": 0.00013855082215741277,
      "learning_rate": 9.133975147376904e-05,
      "loss": 0.3005,
      "step": 213200
    },
    {
      "epoch": 0.6958603972896352,
      "grad_norm": 0.0011499536922201514,
      "learning_rate": 9.124188081310944e-05,
      "loss": 0.1766,
      "step": 213300
    },
    {
      "epoch": 0.6961866328251671,
      "grad_norm": 0.003129900898784399,
      "learning_rate": 9.114401015244985e-05,
      "loss": 0.2857,
      "step": 213400
    },
    {
      "epoch": 0.696512868360699,
      "grad_norm": 10.922292709350586,
      "learning_rate": 9.104613949179028e-05,
      "loss": 0.2583,
      "step": 213500
    },
    {
      "epoch": 0.696839103896231,
      "grad_norm": 0.00020698271691799164,
      "learning_rate": 9.09482688311307e-05,
      "loss": 0.26,
      "step": 213600
    },
    {
      "epoch": 0.697165339431763,
      "grad_norm": 72.38762664794922,
      "learning_rate": 9.085039817047111e-05,
      "loss": 0.1285,
      "step": 213700
    },
    {
      "epoch": 0.6974915749672949,
      "grad_norm": 0.0009826020104810596,
      "learning_rate": 9.075252750981152e-05,
      "loss": 0.1601,
      "step": 213800
    },
    {
      "epoch": 0.6978178105028269,
      "grad_norm": 24.80143165588379,
      "learning_rate": 9.065465684915195e-05,
      "loss": 0.3953,
      "step": 213900
    },
    {
      "epoch": 0.6981440460383588,
      "grad_norm": 2.165026903152466,
      "learning_rate": 9.055678618849236e-05,
      "loss": 0.1186,
      "step": 214000
    },
    {
      "epoch": 0.6984702815738907,
      "grad_norm": 0.009731605648994446,
      "learning_rate": 9.045891552783277e-05,
      "loss": 0.2689,
      "step": 214100
    },
    {
      "epoch": 0.6987965171094227,
      "grad_norm": 0.001209527486935258,
      "learning_rate": 9.03610448671732e-05,
      "loss": 0.3658,
      "step": 214200
    },
    {
      "epoch": 0.6991227526449546,
      "grad_norm": 21.072265625,
      "learning_rate": 9.026317420651361e-05,
      "loss": 0.0751,
      "step": 214300
    },
    {
      "epoch": 0.6994489881804865,
      "grad_norm": 0.0004124428378418088,
      "learning_rate": 9.016530354585402e-05,
      "loss": 0.2707,
      "step": 214400
    },
    {
      "epoch": 0.6997752237160185,
      "grad_norm": 2.781809598673135e-05,
      "learning_rate": 9.006743288519445e-05,
      "loss": 0.2948,
      "step": 214500
    },
    {
      "epoch": 0.7001014592515504,
      "grad_norm": 22.384700775146484,
      "learning_rate": 8.996956222453486e-05,
      "loss": 0.2951,
      "step": 214600
    },
    {
      "epoch": 0.7004276947870823,
      "grad_norm": 0.1616458147764206,
      "learning_rate": 8.987169156387528e-05,
      "loss": 0.158,
      "step": 214700
    },
    {
      "epoch": 0.7007539303226143,
      "grad_norm": 8.583692397223786e-05,
      "learning_rate": 8.97738209032157e-05,
      "loss": 0.2755,
      "step": 214800
    },
    {
      "epoch": 0.7010801658581463,
      "grad_norm": 0.5260376930236816,
      "learning_rate": 8.967595024255612e-05,
      "loss": 0.2097,
      "step": 214900
    },
    {
      "epoch": 0.7014064013936782,
      "grad_norm": 0.003916817717254162,
      "learning_rate": 8.957807958189652e-05,
      "loss": 0.2446,
      "step": 215000
    },
    {
      "epoch": 0.7017326369292102,
      "grad_norm": 0.00041441855137236416,
      "learning_rate": 8.948020892123694e-05,
      "loss": 0.3266,
      "step": 215100
    },
    {
      "epoch": 0.7020588724647421,
      "grad_norm": 0.024456776678562164,
      "learning_rate": 8.938233826057736e-05,
      "loss": 0.2626,
      "step": 215200
    },
    {
      "epoch": 0.702385108000274,
      "grad_norm": 0.00026366213569417596,
      "learning_rate": 8.928446759991778e-05,
      "loss": 0.3024,
      "step": 215300
    },
    {
      "epoch": 0.702711343535806,
      "grad_norm": 0.0007713943487033248,
      "learning_rate": 8.918659693925819e-05,
      "loss": 0.1993,
      "step": 215400
    },
    {
      "epoch": 0.7030375790713379,
      "grad_norm": 0.003436464350670576,
      "learning_rate": 8.908872627859862e-05,
      "loss": 0.3705,
      "step": 215500
    },
    {
      "epoch": 0.7033638146068699,
      "grad_norm": 1.1258376616751775e-05,
      "learning_rate": 8.899085561793903e-05,
      "loss": 0.1416,
      "step": 215600
    },
    {
      "epoch": 0.7036900501424018,
      "grad_norm": 0.005243172403424978,
      "learning_rate": 8.889298495727945e-05,
      "loss": 0.2158,
      "step": 215700
    },
    {
      "epoch": 0.7040162856779337,
      "grad_norm": 0.15103715658187866,
      "learning_rate": 8.879511429661988e-05,
      "loss": 0.3178,
      "step": 215800
    },
    {
      "epoch": 0.7043425212134657,
      "grad_norm": 81.80183410644531,
      "learning_rate": 8.869724363596029e-05,
      "loss": 0.3431,
      "step": 215900
    },
    {
      "epoch": 0.7046687567489976,
      "grad_norm": 16.582380294799805,
      "learning_rate": 8.859937297530069e-05,
      "loss": 0.2701,
      "step": 216000
    },
    {
      "epoch": 0.7049949922845296,
      "grad_norm": 9.926417260430753e-05,
      "learning_rate": 8.85015023146411e-05,
      "loss": 0.3666,
      "step": 216100
    },
    {
      "epoch": 0.7053212278200616,
      "grad_norm": 0.00013530308206100017,
      "learning_rate": 8.840363165398153e-05,
      "loss": 0.3156,
      "step": 216200
    },
    {
      "epoch": 0.7056474633555935,
      "grad_norm": 0.0003437803534325212,
      "learning_rate": 8.830576099332195e-05,
      "loss": 0.322,
      "step": 216300
    },
    {
      "epoch": 0.7059736988911254,
      "grad_norm": 0.0009030070505104959,
      "learning_rate": 8.820789033266236e-05,
      "loss": 0.3741,
      "step": 216400
    },
    {
      "epoch": 0.7062999344266574,
      "grad_norm": 0.00016670144395902753,
      "learning_rate": 8.811001967200279e-05,
      "loss": 0.2406,
      "step": 216500
    },
    {
      "epoch": 0.7066261699621893,
      "grad_norm": 0.017914989963173866,
      "learning_rate": 8.80121490113432e-05,
      "loss": 0.3409,
      "step": 216600
    },
    {
      "epoch": 0.7069524054977212,
      "grad_norm": 0.012522190809249878,
      "learning_rate": 8.791427835068362e-05,
      "loss": 0.2629,
      "step": 216700
    },
    {
      "epoch": 0.7072786410332532,
      "grad_norm": 0.00039005905273370445,
      "learning_rate": 8.781640769002405e-05,
      "loss": 0.2219,
      "step": 216800
    },
    {
      "epoch": 0.7076048765687851,
      "grad_norm": 12.110583305358887,
      "learning_rate": 8.771853702936445e-05,
      "loss": 0.1434,
      "step": 216900
    },
    {
      "epoch": 0.707931112104317,
      "grad_norm": 0.00012885914475191385,
      "learning_rate": 8.762066636870486e-05,
      "loss": 0.277,
      "step": 217000
    },
    {
      "epoch": 0.708257347639849,
      "grad_norm": 0.00027627803501673043,
      "learning_rate": 8.752279570804528e-05,
      "loss": 0.233,
      "step": 217100
    },
    {
      "epoch": 0.708583583175381,
      "grad_norm": 0.7454637885093689,
      "learning_rate": 8.74249250473857e-05,
      "loss": 0.1535,
      "step": 217200
    },
    {
      "epoch": 0.7089098187109129,
      "grad_norm": 58.570884704589844,
      "learning_rate": 8.732705438672612e-05,
      "loss": 0.3595,
      "step": 217300
    },
    {
      "epoch": 0.7092360542464449,
      "grad_norm": 0.00010869735706364736,
      "learning_rate": 8.722918372606653e-05,
      "loss": 0.2595,
      "step": 217400
    },
    {
      "epoch": 0.7095622897819768,
      "grad_norm": 0.39197564125061035,
      "learning_rate": 8.713131306540696e-05,
      "loss": 0.2409,
      "step": 217500
    },
    {
      "epoch": 0.7098885253175088,
      "grad_norm": 0.001986440736800432,
      "learning_rate": 8.703344240474738e-05,
      "loss": 0.3351,
      "step": 217600
    },
    {
      "epoch": 0.7102147608530407,
      "grad_norm": 0.0012781169498339295,
      "learning_rate": 8.693557174408779e-05,
      "loss": 0.1821,
      "step": 217700
    },
    {
      "epoch": 0.7105409963885726,
      "grad_norm": 29.710124969482422,
      "learning_rate": 8.683770108342822e-05,
      "loss": 0.2244,
      "step": 217800
    },
    {
      "epoch": 0.7108672319241046,
      "grad_norm": 0.006966801825910807,
      "learning_rate": 8.673983042276862e-05,
      "loss": 0.4289,
      "step": 217900
    },
    {
      "epoch": 0.7111934674596365,
      "grad_norm": 25.020750045776367,
      "learning_rate": 8.664195976210903e-05,
      "loss": 0.2627,
      "step": 218000
    },
    {
      "epoch": 0.7115197029951684,
      "grad_norm": 0.03598681837320328,
      "learning_rate": 8.654408910144945e-05,
      "loss": 0.2131,
      "step": 218100
    },
    {
      "epoch": 0.7118459385307004,
      "grad_norm": 34.02101516723633,
      "learning_rate": 8.644621844078988e-05,
      "loss": 0.2981,
      "step": 218200
    },
    {
      "epoch": 0.7121721740662323,
      "grad_norm": 0.0008823774405755103,
      "learning_rate": 8.634834778013029e-05,
      "loss": 0.3261,
      "step": 218300
    },
    {
      "epoch": 0.7124984096017642,
      "grad_norm": 51.17433166503906,
      "learning_rate": 8.62504771194707e-05,
      "loss": 0.2339,
      "step": 218400
    },
    {
      "epoch": 0.7128246451372963,
      "grad_norm": 0.0020034455228596926,
      "learning_rate": 8.615260645881113e-05,
      "loss": 0.267,
      "step": 218500
    },
    {
      "epoch": 0.7131508806728282,
      "grad_norm": 0.005773329176008701,
      "learning_rate": 8.605473579815155e-05,
      "loss": 0.3614,
      "step": 218600
    },
    {
      "epoch": 0.7134771162083601,
      "grad_norm": 0.0004915609606541693,
      "learning_rate": 8.595686513749195e-05,
      "loss": 0.2106,
      "step": 218700
    },
    {
      "epoch": 0.7138033517438921,
      "grad_norm": 23.78916358947754,
      "learning_rate": 8.585899447683238e-05,
      "loss": 0.2875,
      "step": 218800
    },
    {
      "epoch": 0.714129587279424,
      "grad_norm": 0.009337901137769222,
      "learning_rate": 8.576112381617279e-05,
      "loss": 0.352,
      "step": 218900
    },
    {
      "epoch": 0.7144558228149559,
      "grad_norm": 0.0002291160199092701,
      "learning_rate": 8.56632531555132e-05,
      "loss": 0.1913,
      "step": 219000
    },
    {
      "epoch": 0.7147820583504879,
      "grad_norm": 0.0034400459844619036,
      "learning_rate": 8.556538249485363e-05,
      "loss": 0.3373,
      "step": 219100
    },
    {
      "epoch": 0.7151082938860198,
      "grad_norm": 0.0009324205457232893,
      "learning_rate": 8.546751183419405e-05,
      "loss": 0.3808,
      "step": 219200
    },
    {
      "epoch": 0.7154345294215517,
      "grad_norm": 0.0008529513725079596,
      "learning_rate": 8.536964117353446e-05,
      "loss": 0.4761,
      "step": 219300
    },
    {
      "epoch": 0.7157607649570837,
      "grad_norm": 0.0007129883742891252,
      "learning_rate": 8.527177051287488e-05,
      "loss": 0.201,
      "step": 219400
    },
    {
      "epoch": 0.7160870004926156,
      "grad_norm": 19.182247161865234,
      "learning_rate": 8.51738998522153e-05,
      "loss": 0.1756,
      "step": 219500
    },
    {
      "epoch": 0.7164132360281475,
      "grad_norm": 34.64329147338867,
      "learning_rate": 8.507602919155572e-05,
      "loss": 0.2532,
      "step": 219600
    },
    {
      "epoch": 0.7167394715636796,
      "grad_norm": 0.003029488492757082,
      "learning_rate": 8.497815853089612e-05,
      "loss": 0.245,
      "step": 219700
    },
    {
      "epoch": 0.7170657070992115,
      "grad_norm": 21.75232696533203,
      "learning_rate": 8.488028787023655e-05,
      "loss": 0.1758,
      "step": 219800
    },
    {
      "epoch": 0.7173919426347435,
      "grad_norm": 11.878244400024414,
      "learning_rate": 8.478241720957696e-05,
      "loss": 0.3317,
      "step": 219900
    },
    {
      "epoch": 0.7177181781702754,
      "grad_norm": 0.04040681943297386,
      "learning_rate": 8.468454654891738e-05,
      "loss": 0.2789,
      "step": 220000
    },
    {
      "epoch": 0.7180444137058073,
      "grad_norm": 0.000660108111333102,
      "learning_rate": 8.45866758882578e-05,
      "loss": 0.4865,
      "step": 220100
    },
    {
      "epoch": 0.7183706492413393,
      "grad_norm": 93.5547866821289,
      "learning_rate": 8.448880522759822e-05,
      "loss": 0.2864,
      "step": 220200
    },
    {
      "epoch": 0.7186968847768712,
      "grad_norm": 0.000135870257508941,
      "learning_rate": 8.439093456693863e-05,
      "loss": 0.2803,
      "step": 220300
    },
    {
      "epoch": 0.7190231203124031,
      "grad_norm": 94.60612487792969,
      "learning_rate": 8.429306390627905e-05,
      "loss": 0.3646,
      "step": 220400
    },
    {
      "epoch": 0.7193493558479351,
      "grad_norm": 0.06479011476039886,
      "learning_rate": 8.419519324561947e-05,
      "loss": 0.1713,
      "step": 220500
    },
    {
      "epoch": 0.719675591383467,
      "grad_norm": 0.00012232709559611976,
      "learning_rate": 8.409732258495987e-05,
      "loss": 0.1666,
      "step": 220600
    },
    {
      "epoch": 0.7200018269189989,
      "grad_norm": 3.01804256439209,
      "learning_rate": 8.399945192430029e-05,
      "loss": 0.2116,
      "step": 220700
    },
    {
      "epoch": 0.720328062454531,
      "grad_norm": 0.00209561618976295,
      "learning_rate": 8.390158126364072e-05,
      "loss": 0.2663,
      "step": 220800
    },
    {
      "epoch": 0.7206542979900629,
      "grad_norm": 7.276635733433068e-05,
      "learning_rate": 8.380371060298113e-05,
      "loss": 0.39,
      "step": 220900
    },
    {
      "epoch": 0.7209805335255948,
      "grad_norm": 0.001518234028480947,
      "learning_rate": 8.370583994232155e-05,
      "loss": 0.2502,
      "step": 221000
    },
    {
      "epoch": 0.7213067690611268,
      "grad_norm": 12.15084457397461,
      "learning_rate": 8.360796928166197e-05,
      "loss": 0.1731,
      "step": 221100
    },
    {
      "epoch": 0.7216330045966587,
      "grad_norm": 0.001642731949687004,
      "learning_rate": 8.351009862100239e-05,
      "loss": 0.3598,
      "step": 221200
    },
    {
      "epoch": 0.7219592401321906,
      "grad_norm": 0.00013535958714783192,
      "learning_rate": 8.34122279603428e-05,
      "loss": 0.2999,
      "step": 221300
    },
    {
      "epoch": 0.7222854756677226,
      "grad_norm": 0.00021337218640837818,
      "learning_rate": 8.33143572996832e-05,
      "loss": 0.3157,
      "step": 221400
    },
    {
      "epoch": 0.7226117112032545,
      "grad_norm": 0.0020541376434266567,
      "learning_rate": 8.321648663902364e-05,
      "loss": 0.2097,
      "step": 221500
    },
    {
      "epoch": 0.7229379467387864,
      "grad_norm": 0.4126574695110321,
      "learning_rate": 8.311861597836405e-05,
      "loss": 0.3312,
      "step": 221600
    },
    {
      "epoch": 0.7232641822743184,
      "grad_norm": 0.04725535213947296,
      "learning_rate": 8.302074531770446e-05,
      "loss": 0.2193,
      "step": 221700
    },
    {
      "epoch": 0.7235904178098503,
      "grad_norm": 0.005471192765980959,
      "learning_rate": 8.292287465704489e-05,
      "loss": 0.1861,
      "step": 221800
    },
    {
      "epoch": 0.7239166533453822,
      "grad_norm": 0.00037191982846707106,
      "learning_rate": 8.28250039963853e-05,
      "loss": 0.4866,
      "step": 221900
    },
    {
      "epoch": 0.7242428888809143,
      "grad_norm": 31.40043830871582,
      "learning_rate": 8.272713333572572e-05,
      "loss": 0.3246,
      "step": 222000
    },
    {
      "epoch": 0.7245691244164462,
      "grad_norm": 0.8771570324897766,
      "learning_rate": 8.262926267506614e-05,
      "loss": 0.1687,
      "step": 222100
    },
    {
      "epoch": 0.7248953599519782,
      "grad_norm": 0.0032866711262613535,
      "learning_rate": 8.253139201440656e-05,
      "loss": 0.2541,
      "step": 222200
    },
    {
      "epoch": 0.7252215954875101,
      "grad_norm": 0.3076735734939575,
      "learning_rate": 8.243352135374697e-05,
      "loss": 0.3523,
      "step": 222300
    },
    {
      "epoch": 0.725547831023042,
      "grad_norm": 0.0009263053070753813,
      "learning_rate": 8.23356506930874e-05,
      "loss": 0.0302,
      "step": 222400
    },
    {
      "epoch": 0.725874066558574,
      "grad_norm": 0.00018997275037690997,
      "learning_rate": 8.22377800324278e-05,
      "loss": 0.182,
      "step": 222500
    },
    {
      "epoch": 0.7262003020941059,
      "grad_norm": 0.003315357491374016,
      "learning_rate": 8.213990937176822e-05,
      "loss": 0.2554,
      "step": 222600
    },
    {
      "epoch": 0.7265265376296378,
      "grad_norm": 19.849933624267578,
      "learning_rate": 8.204203871110863e-05,
      "loss": 0.1742,
      "step": 222700
    },
    {
      "epoch": 0.7268527731651698,
      "grad_norm": 0.02489626780152321,
      "learning_rate": 8.194416805044906e-05,
      "loss": 0.2203,
      "step": 222800
    },
    {
      "epoch": 0.7271790087007017,
      "grad_norm": 0.00014162137813400477,
      "learning_rate": 8.184629738978947e-05,
      "loss": 0.0934,
      "step": 222900
    },
    {
      "epoch": 0.7275052442362336,
      "grad_norm": 4.215932858642191e-05,
      "learning_rate": 8.174842672912989e-05,
      "loss": 0.197,
      "step": 223000
    },
    {
      "epoch": 0.7278314797717657,
      "grad_norm": 2.9494986534118652,
      "learning_rate": 8.165055606847032e-05,
      "loss": 0.539,
      "step": 223100
    },
    {
      "epoch": 0.7281577153072976,
      "grad_norm": 0.006644619163125753,
      "learning_rate": 8.155268540781073e-05,
      "loss": 0.265,
      "step": 223200
    },
    {
      "epoch": 0.7284839508428295,
      "grad_norm": 0.00023661325394641608,
      "learning_rate": 8.145481474715114e-05,
      "loss": 0.2182,
      "step": 223300
    },
    {
      "epoch": 0.7288101863783615,
      "grad_norm": 0.0005039884708821774,
      "learning_rate": 8.135694408649157e-05,
      "loss": 0.1382,
      "step": 223400
    },
    {
      "epoch": 0.7291364219138934,
      "grad_norm": 0.0023953455965965986,
      "learning_rate": 8.125907342583197e-05,
      "loss": 0.232,
      "step": 223500
    },
    {
      "epoch": 0.7294626574494253,
      "grad_norm": 0.004116622731089592,
      "learning_rate": 8.116120276517239e-05,
      "loss": 0.1421,
      "step": 223600
    },
    {
      "epoch": 0.7297888929849573,
      "grad_norm": 0.0007632290362380445,
      "learning_rate": 8.10633321045128e-05,
      "loss": 0.2713,
      "step": 223700
    },
    {
      "epoch": 0.7301151285204892,
      "grad_norm": 0.0008369339047931135,
      "learning_rate": 8.096546144385323e-05,
      "loss": 0.2696,
      "step": 223800
    },
    {
      "epoch": 0.7304413640560211,
      "grad_norm": 0.0006834419327788055,
      "learning_rate": 8.086759078319364e-05,
      "loss": 0.2348,
      "step": 223900
    },
    {
      "epoch": 0.7307675995915531,
      "grad_norm": 14.693517684936523,
      "learning_rate": 8.076972012253406e-05,
      "loss": 0.2518,
      "step": 224000
    },
    {
      "epoch": 0.731093835127085,
      "grad_norm": 0.0008267865632660687,
      "learning_rate": 8.067184946187449e-05,
      "loss": 0.2097,
      "step": 224100
    },
    {
      "epoch": 0.731420070662617,
      "grad_norm": 0.0010400221217423677,
      "learning_rate": 8.05739788012149e-05,
      "loss": 0.2815,
      "step": 224200
    },
    {
      "epoch": 0.731746306198149,
      "grad_norm": 0.0005028062150813639,
      "learning_rate": 8.04761081405553e-05,
      "loss": 0.3281,
      "step": 224300
    },
    {
      "epoch": 0.7320725417336809,
      "grad_norm": 0.43286189436912537,
      "learning_rate": 8.037823747989573e-05,
      "loss": 0.2814,
      "step": 224400
    },
    {
      "epoch": 0.7323987772692129,
      "grad_norm": 75.162109375,
      "learning_rate": 8.028036681923614e-05,
      "loss": 0.1412,
      "step": 224500
    },
    {
      "epoch": 0.7327250128047448,
      "grad_norm": 0.00029000197537243366,
      "learning_rate": 8.018249615857656e-05,
      "loss": 0.4256,
      "step": 224600
    },
    {
      "epoch": 0.7330512483402767,
      "grad_norm": 0.004177656956017017,
      "learning_rate": 8.008462549791697e-05,
      "loss": 0.2008,
      "step": 224700
    },
    {
      "epoch": 0.7333774838758087,
      "grad_norm": 0.03135823830962181,
      "learning_rate": 7.99867548372574e-05,
      "loss": 0.2413,
      "step": 224800
    },
    {
      "epoch": 0.7337037194113406,
      "grad_norm": 9.9041166305542,
      "learning_rate": 7.988888417659781e-05,
      "loss": 0.322,
      "step": 224900
    },
    {
      "epoch": 0.7340299549468725,
      "grad_norm": 64.76924133300781,
      "learning_rate": 7.979101351593823e-05,
      "loss": 0.1554,
      "step": 225000
    },
    {
      "epoch": 0.7343561904824045,
      "grad_norm": 0.0031692690681666136,
      "learning_rate": 7.969314285527866e-05,
      "loss": 0.2526,
      "step": 225100
    },
    {
      "epoch": 0.7346824260179364,
      "grad_norm": 0.00027969773509539664,
      "learning_rate": 7.959527219461907e-05,
      "loss": 0.2736,
      "step": 225200
    },
    {
      "epoch": 0.7350086615534683,
      "grad_norm": 0.0003374938387423754,
      "learning_rate": 7.949740153395947e-05,
      "loss": 0.1353,
      "step": 225300
    },
    {
      "epoch": 0.7353348970890003,
      "grad_norm": 0.001776670222170651,
      "learning_rate": 7.93995308732999e-05,
      "loss": 0.146,
      "step": 225400
    },
    {
      "epoch": 0.7356611326245323,
      "grad_norm": 0.0018728503491729498,
      "learning_rate": 7.930166021264031e-05,
      "loss": 0.2119,
      "step": 225500
    },
    {
      "epoch": 0.7359873681600642,
      "grad_norm": 0.0016391604440286756,
      "learning_rate": 7.920378955198073e-05,
      "loss": 0.2183,
      "step": 225600
    },
    {
      "epoch": 0.7363136036955962,
      "grad_norm": 1.5899968275334686e-05,
      "learning_rate": 7.910591889132116e-05,
      "loss": 0.2442,
      "step": 225700
    },
    {
      "epoch": 0.7366398392311281,
      "grad_norm": 0.0003334483481012285,
      "learning_rate": 7.900804823066157e-05,
      "loss": 0.432,
      "step": 225800
    },
    {
      "epoch": 0.73696607476666,
      "grad_norm": 0.014374435879290104,
      "learning_rate": 7.891017757000198e-05,
      "loss": 0.2201,
      "step": 225900
    },
    {
      "epoch": 0.737292310302192,
      "grad_norm": 0.029079077765345573,
      "learning_rate": 7.88123069093424e-05,
      "loss": 0.1807,
      "step": 226000
    },
    {
      "epoch": 0.7376185458377239,
      "grad_norm": 0.004086942411959171,
      "learning_rate": 7.871443624868283e-05,
      "loss": 0.4414,
      "step": 226100
    },
    {
      "epoch": 0.7379447813732558,
      "grad_norm": 0.005189414136111736,
      "learning_rate": 7.861656558802323e-05,
      "loss": 0.2481,
      "step": 226200
    },
    {
      "epoch": 0.7382710169087878,
      "grad_norm": 0.009214583784341812,
      "learning_rate": 7.851869492736364e-05,
      "loss": 0.3692,
      "step": 226300
    },
    {
      "epoch": 0.7385972524443197,
      "grad_norm": 0.34106817841529846,
      "learning_rate": 7.842082426670407e-05,
      "loss": 0.192,
      "step": 226400
    },
    {
      "epoch": 0.7389234879798517,
      "grad_norm": 0.000613657059147954,
      "learning_rate": 7.832295360604448e-05,
      "loss": 0.2513,
      "step": 226500
    },
    {
      "epoch": 0.7392497235153836,
      "grad_norm": 0.5651647448539734,
      "learning_rate": 7.82250829453849e-05,
      "loss": 0.2006,
      "step": 226600
    },
    {
      "epoch": 0.7395759590509156,
      "grad_norm": 0.018907228484749794,
      "learning_rate": 7.812721228472533e-05,
      "loss": 0.263,
      "step": 226700
    },
    {
      "epoch": 0.7399021945864476,
      "grad_norm": 4.766292840940878e-05,
      "learning_rate": 7.802934162406574e-05,
      "loss": 0.2559,
      "step": 226800
    },
    {
      "epoch": 0.7402284301219795,
      "grad_norm": 1.9422968626022339,
      "learning_rate": 7.793147096340616e-05,
      "loss": 0.0755,
      "step": 226900
    },
    {
      "epoch": 0.7405546656575114,
      "grad_norm": 0.012485607527196407,
      "learning_rate": 7.783360030274656e-05,
      "loss": 0.2591,
      "step": 227000
    },
    {
      "epoch": 0.7408809011930434,
      "grad_norm": 9.526054054731503e-05,
      "learning_rate": 7.7735729642087e-05,
      "loss": 0.242,
      "step": 227100
    },
    {
      "epoch": 0.7412071367285753,
      "grad_norm": 0.0007538067293353379,
      "learning_rate": 7.76378589814274e-05,
      "loss": 0.2495,
      "step": 227200
    },
    {
      "epoch": 0.7415333722641072,
      "grad_norm": 0.003771559800952673,
      "learning_rate": 7.753998832076781e-05,
      "loss": 0.2238,
      "step": 227300
    },
    {
      "epoch": 0.7418596077996392,
      "grad_norm": 0.0010095186298713088,
      "learning_rate": 7.744211766010824e-05,
      "loss": 0.3332,
      "step": 227400
    },
    {
      "epoch": 0.7421858433351711,
      "grad_norm": 0.0034256447106599808,
      "learning_rate": 7.734424699944866e-05,
      "loss": 0.1958,
      "step": 227500
    },
    {
      "epoch": 0.742512078870703,
      "grad_norm": 130.93539428710938,
      "learning_rate": 7.724637633878907e-05,
      "loss": 0.258,
      "step": 227600
    },
    {
      "epoch": 0.742838314406235,
      "grad_norm": 12.04008960723877,
      "learning_rate": 7.71485056781295e-05,
      "loss": 0.2655,
      "step": 227700
    },
    {
      "epoch": 0.743164549941767,
      "grad_norm": 7.521951675415039,
      "learning_rate": 7.705063501746991e-05,
      "loss": 0.2479,
      "step": 227800
    },
    {
      "epoch": 0.7434907854772989,
      "grad_norm": 0.0014880792004987597,
      "learning_rate": 7.695276435681033e-05,
      "loss": 0.1649,
      "step": 227900
    },
    {
      "epoch": 0.7438170210128309,
      "grad_norm": 0.00032813631696626544,
      "learning_rate": 7.685489369615073e-05,
      "loss": 0.5116,
      "step": 228000
    },
    {
      "epoch": 0.7441432565483628,
      "grad_norm": 20.855024337768555,
      "learning_rate": 7.675702303549115e-05,
      "loss": 0.123,
      "step": 228100
    },
    {
      "epoch": 0.7444694920838947,
      "grad_norm": 15.005559921264648,
      "learning_rate": 7.665915237483157e-05,
      "loss": 0.3356,
      "step": 228200
    },
    {
      "epoch": 0.7447957276194267,
      "grad_norm": 0.4804683327674866,
      "learning_rate": 7.656128171417198e-05,
      "loss": 0.2179,
      "step": 228300
    },
    {
      "epoch": 0.7451219631549586,
      "grad_norm": 0.019877493381500244,
      "learning_rate": 7.646341105351241e-05,
      "loss": 0.2329,
      "step": 228400
    },
    {
      "epoch": 0.7454481986904906,
      "grad_norm": 23.848926544189453,
      "learning_rate": 7.636554039285283e-05,
      "loss": 0.2841,
      "step": 228500
    },
    {
      "epoch": 0.7457744342260225,
      "grad_norm": 7.123495578765869,
      "learning_rate": 7.626766973219324e-05,
      "loss": 0.2846,
      "step": 228600
    },
    {
      "epoch": 0.7461006697615544,
      "grad_norm": 0.00011449824523879215,
      "learning_rate": 7.616979907153367e-05,
      "loss": 0.4625,
      "step": 228700
    },
    {
      "epoch": 0.7464269052970864,
      "grad_norm": 4.764282493852079e-05,
      "learning_rate": 7.607192841087408e-05,
      "loss": 0.1686,
      "step": 228800
    },
    {
      "epoch": 0.7467531408326183,
      "grad_norm": 0.005371068604290485,
      "learning_rate": 7.597405775021448e-05,
      "loss": 0.3246,
      "step": 228900
    },
    {
      "epoch": 0.7470793763681502,
      "grad_norm": 30.056785583496094,
      "learning_rate": 7.587618708955492e-05,
      "loss": 0.3147,
      "step": 229000
    },
    {
      "epoch": 0.7474056119036823,
      "grad_norm": 0.036251913756132126,
      "learning_rate": 7.577831642889533e-05,
      "loss": 0.2835,
      "step": 229100
    },
    {
      "epoch": 0.7477318474392142,
      "grad_norm": 0.0013430491089820862,
      "learning_rate": 7.568044576823574e-05,
      "loss": 0.2445,
      "step": 229200
    },
    {
      "epoch": 0.7480580829747461,
      "grad_norm": 0.005515133496373892,
      "learning_rate": 7.558257510757615e-05,
      "loss": 0.2287,
      "step": 229300
    },
    {
      "epoch": 0.7483843185102781,
      "grad_norm": 1.0215709209442139,
      "learning_rate": 7.548470444691658e-05,
      "loss": 0.2561,
      "step": 229400
    },
    {
      "epoch": 0.74871055404581,
      "grad_norm": 0.0008310263510793447,
      "learning_rate": 7.5386833786257e-05,
      "loss": 0.1701,
      "step": 229500
    },
    {
      "epoch": 0.7490367895813419,
      "grad_norm": 0.022870952263474464,
      "learning_rate": 7.528896312559741e-05,
      "loss": 0.279,
      "step": 229600
    },
    {
      "epoch": 0.7493630251168739,
      "grad_norm": 31.794679641723633,
      "learning_rate": 7.519109246493784e-05,
      "loss": 0.2641,
      "step": 229700
    },
    {
      "epoch": 0.7496892606524058,
      "grad_norm": 49.12193298339844,
      "learning_rate": 7.509322180427825e-05,
      "loss": 0.3054,
      "step": 229800
    },
    {
      "epoch": 0.7500154961879377,
      "grad_norm": 11.263917922973633,
      "learning_rate": 7.499535114361865e-05,
      "loss": 0.2094,
      "step": 229900
    },
    {
      "epoch": 0.7503417317234697,
      "grad_norm": 37.01054000854492,
      "learning_rate": 7.489748048295908e-05,
      "loss": 0.1499,
      "step": 230000
    },
    {
      "epoch": 0.7506679672590016,
      "grad_norm": 8.885098941391334e-05,
      "learning_rate": 7.47996098222995e-05,
      "loss": 0.2939,
      "step": 230100
    },
    {
      "epoch": 0.7509942027945335,
      "grad_norm": 0.03880428522825241,
      "learning_rate": 7.470173916163991e-05,
      "loss": 0.1537,
      "step": 230200
    },
    {
      "epoch": 0.7513204383300656,
      "grad_norm": 15.880742073059082,
      "learning_rate": 7.460386850098034e-05,
      "loss": 0.3176,
      "step": 230300
    },
    {
      "epoch": 0.7516466738655975,
      "grad_norm": 0.034135330468416214,
      "learning_rate": 7.450599784032074e-05,
      "loss": 0.2422,
      "step": 230400
    },
    {
      "epoch": 0.7519729094011294,
      "grad_norm": 0.009023088961839676,
      "learning_rate": 7.440812717966117e-05,
      "loss": 0.1826,
      "step": 230500
    },
    {
      "epoch": 0.7522991449366614,
      "grad_norm": 0.0059377155266702175,
      "learning_rate": 7.431025651900158e-05,
      "loss": 0.1158,
      "step": 230600
    },
    {
      "epoch": 0.7526253804721933,
      "grad_norm": 0.003783427644520998,
      "learning_rate": 7.4212385858342e-05,
      "loss": 0.3286,
      "step": 230700
    },
    {
      "epoch": 0.7529516160077253,
      "grad_norm": 0.0007154093473218381,
      "learning_rate": 7.411451519768241e-05,
      "loss": 0.2392,
      "step": 230800
    },
    {
      "epoch": 0.7532778515432572,
      "grad_norm": 0.0003833117661997676,
      "learning_rate": 7.401664453702284e-05,
      "loss": 0.2529,
      "step": 230900
    },
    {
      "epoch": 0.7536040870787891,
      "grad_norm": 0.3197759985923767,
      "learning_rate": 7.391877387636325e-05,
      "loss": 0.1681,
      "step": 231000
    },
    {
      "epoch": 0.7539303226143211,
      "grad_norm": 0.00226118927821517,
      "learning_rate": 7.382090321570367e-05,
      "loss": 0.2483,
      "step": 231100
    },
    {
      "epoch": 0.754256558149853,
      "grad_norm": 0.0002059117250610143,
      "learning_rate": 7.372303255504408e-05,
      "loss": 0.1589,
      "step": 231200
    },
    {
      "epoch": 0.754582793685385,
      "grad_norm": 0.0745047777891159,
      "learning_rate": 7.36251618943845e-05,
      "loss": 0.2413,
      "step": 231300
    },
    {
      "epoch": 0.754909029220917,
      "grad_norm": 0.00017315323930233717,
      "learning_rate": 7.352729123372492e-05,
      "loss": 0.1988,
      "step": 231400
    },
    {
      "epoch": 0.7552352647564489,
      "grad_norm": 0.006837516091763973,
      "learning_rate": 7.342942057306534e-05,
      "loss": 0.2455,
      "step": 231500
    },
    {
      "epoch": 0.7555615002919808,
      "grad_norm": 35.09440994262695,
      "learning_rate": 7.333154991240575e-05,
      "loss": 0.1715,
      "step": 231600
    },
    {
      "epoch": 0.7558877358275128,
      "grad_norm": 0.0002117668482242152,
      "learning_rate": 7.323367925174617e-05,
      "loss": 0.1403,
      "step": 231700
    },
    {
      "epoch": 0.7562139713630447,
      "grad_norm": 17.268396377563477,
      "learning_rate": 7.313580859108658e-05,
      "loss": 0.3979,
      "step": 231800
    },
    {
      "epoch": 0.7565402068985766,
      "grad_norm": 0.0005068727768957615,
      "learning_rate": 7.303793793042701e-05,
      "loss": 0.2406,
      "step": 231900
    },
    {
      "epoch": 0.7568664424341086,
      "grad_norm": 0.009549999609589577,
      "learning_rate": 7.294006726976742e-05,
      "loss": 0.2865,
      "step": 232000
    },
    {
      "epoch": 0.7571926779696405,
      "grad_norm": 0.00020697954460047185,
      "learning_rate": 7.284219660910784e-05,
      "loss": 0.2664,
      "step": 232100
    },
    {
      "epoch": 0.7575189135051724,
      "grad_norm": 0.1196027547121048,
      "learning_rate": 7.274432594844825e-05,
      "loss": 0.2177,
      "step": 232200
    },
    {
      "epoch": 0.7578451490407044,
      "grad_norm": 0.001090468605980277,
      "learning_rate": 7.264645528778867e-05,
      "loss": 0.215,
      "step": 232300
    },
    {
      "epoch": 0.7581713845762363,
      "grad_norm": 1.0797083377838135,
      "learning_rate": 7.25485846271291e-05,
      "loss": 0.2414,
      "step": 232400
    },
    {
      "epoch": 0.7584976201117682,
      "grad_norm": 30.03496551513672,
      "learning_rate": 7.245071396646951e-05,
      "loss": 0.1732,
      "step": 232500
    },
    {
      "epoch": 0.7588238556473003,
      "grad_norm": 0.009035260416567326,
      "learning_rate": 7.235284330580992e-05,
      "loss": 0.2797,
      "step": 232600
    },
    {
      "epoch": 0.7591500911828322,
      "grad_norm": 0.0027907919138669968,
      "learning_rate": 7.225497264515034e-05,
      "loss": 0.258,
      "step": 232700
    },
    {
      "epoch": 0.7594763267183641,
      "grad_norm": 0.0020536729134619236,
      "learning_rate": 7.215710198449075e-05,
      "loss": 0.1184,
      "step": 232800
    },
    {
      "epoch": 0.7598025622538961,
      "grad_norm": 0.07008406519889832,
      "learning_rate": 7.205923132383118e-05,
      "loss": 0.1922,
      "step": 232900
    },
    {
      "epoch": 0.760128797789428,
      "grad_norm": 0.012158270925283432,
      "learning_rate": 7.19613606631716e-05,
      "loss": 0.302,
      "step": 233000
    },
    {
      "epoch": 0.76045503332496,
      "grad_norm": 0.004401211626827717,
      "learning_rate": 7.186349000251201e-05,
      "loss": 0.2041,
      "step": 233100
    },
    {
      "epoch": 0.7607812688604919,
      "grad_norm": 0.0030079015996307135,
      "learning_rate": 7.176561934185242e-05,
      "loss": 0.2076,
      "step": 233200
    },
    {
      "epoch": 0.7611075043960238,
      "grad_norm": 0.02533235214650631,
      "learning_rate": 7.166774868119284e-05,
      "loss": 0.0631,
      "step": 233300
    },
    {
      "epoch": 0.7614337399315558,
      "grad_norm": 0.00010085613757837564,
      "learning_rate": 7.156987802053326e-05,
      "loss": 0.3466,
      "step": 233400
    },
    {
      "epoch": 0.7617599754670877,
      "grad_norm": 0.007763563189655542,
      "learning_rate": 7.147200735987368e-05,
      "loss": 0.1731,
      "step": 233500
    },
    {
      "epoch": 0.7620862110026196,
      "grad_norm": 29.26079559326172,
      "learning_rate": 7.13741366992141e-05,
      "loss": 0.313,
      "step": 233600
    },
    {
      "epoch": 0.7624124465381517,
      "grad_norm": 0.007847300730645657,
      "learning_rate": 7.127626603855451e-05,
      "loss": 0.2716,
      "step": 233700
    },
    {
      "epoch": 0.7627386820736836,
      "grad_norm": 0.004086196888238192,
      "learning_rate": 7.117839537789492e-05,
      "loss": 0.3078,
      "step": 233800
    },
    {
      "epoch": 0.7630649176092155,
      "grad_norm": 0.007537920493632555,
      "learning_rate": 7.108052471723535e-05,
      "loss": 0.3551,
      "step": 233900
    },
    {
      "epoch": 0.7633911531447475,
      "grad_norm": 0.0013312333030626178,
      "learning_rate": 7.098265405657576e-05,
      "loss": 0.4073,
      "step": 234000
    },
    {
      "epoch": 0.7637173886802794,
      "grad_norm": 5.6128254072973505e-05,
      "learning_rate": 7.088478339591618e-05,
      "loss": 0.2328,
      "step": 234100
    },
    {
      "epoch": 0.7640436242158113,
      "grad_norm": 0.0011992177460342646,
      "learning_rate": 7.07869127352566e-05,
      "loss": 0.2008,
      "step": 234200
    },
    {
      "epoch": 0.7643698597513433,
      "grad_norm": 0.4812100827693939,
      "learning_rate": 7.068904207459701e-05,
      "loss": 0.153,
      "step": 234300
    },
    {
      "epoch": 0.7646960952868752,
      "grad_norm": 0.00012003799201920629,
      "learning_rate": 7.059117141393744e-05,
      "loss": 0.0926,
      "step": 234400
    },
    {
      "epoch": 0.7650223308224071,
      "grad_norm": 0.006155795883387327,
      "learning_rate": 7.049330075327784e-05,
      "loss": 0.1126,
      "step": 234500
    },
    {
      "epoch": 0.7653485663579391,
      "grad_norm": 0.005382790695875883,
      "learning_rate": 7.039543009261826e-05,
      "loss": 0.1827,
      "step": 234600
    },
    {
      "epoch": 0.765674801893471,
      "grad_norm": 0.000558254832867533,
      "learning_rate": 7.029755943195868e-05,
      "loss": 0.279,
      "step": 234700
    },
    {
      "epoch": 0.7660010374290029,
      "grad_norm": 0.0010433938587084413,
      "learning_rate": 7.019968877129909e-05,
      "loss": 0.1244,
      "step": 234800
    },
    {
      "epoch": 0.766327272964535,
      "grad_norm": 7.938911585370079e-05,
      "learning_rate": 7.010181811063952e-05,
      "loss": 0.193,
      "step": 234900
    },
    {
      "epoch": 0.7666535085000669,
      "grad_norm": 0.0001107563657569699,
      "learning_rate": 7.000394744997992e-05,
      "loss": 0.1573,
      "step": 235000
    },
    {
      "epoch": 0.7669797440355989,
      "grad_norm": 17.886573791503906,
      "learning_rate": 6.990607678932035e-05,
      "loss": 0.2085,
      "step": 235100
    },
    {
      "epoch": 0.7673059795711308,
      "grad_norm": 0.0008507397142238915,
      "learning_rate": 6.980820612866076e-05,
      "loss": 0.1735,
      "step": 235200
    },
    {
      "epoch": 0.7676322151066627,
      "grad_norm": 0.00011142448056489229,
      "learning_rate": 6.971033546800118e-05,
      "loss": 0.2454,
      "step": 235300
    },
    {
      "epoch": 0.7679584506421947,
      "grad_norm": 0.842956006526947,
      "learning_rate": 6.96124648073416e-05,
      "loss": 0.3424,
      "step": 235400
    },
    {
      "epoch": 0.7682846861777266,
      "grad_norm": 0.013115673325955868,
      "learning_rate": 6.951459414668201e-05,
      "loss": 0.1028,
      "step": 235500
    },
    {
      "epoch": 0.7686109217132585,
      "grad_norm": 0.2887866497039795,
      "learning_rate": 6.941672348602244e-05,
      "loss": 0.2559,
      "step": 235600
    },
    {
      "epoch": 0.7689371572487905,
      "grad_norm": 0.02337116003036499,
      "learning_rate": 6.931885282536285e-05,
      "loss": 0.3244,
      "step": 235700
    },
    {
      "epoch": 0.7692633927843224,
      "grad_norm": 50.59181594848633,
      "learning_rate": 6.922098216470326e-05,
      "loss": 0.2345,
      "step": 235800
    },
    {
      "epoch": 0.7695896283198543,
      "grad_norm": 3.507468500174582e-05,
      "learning_rate": 6.912311150404369e-05,
      "loss": 0.1135,
      "step": 235900
    },
    {
      "epoch": 0.7699158638553864,
      "grad_norm": 0.00013534935715142637,
      "learning_rate": 6.902524084338409e-05,
      "loss": 0.27,
      "step": 236000
    },
    {
      "epoch": 0.7702420993909183,
      "grad_norm": 0.11954888701438904,
      "learning_rate": 6.892737018272452e-05,
      "loss": 0.2522,
      "step": 236100
    },
    {
      "epoch": 0.7705683349264502,
      "grad_norm": 0.003549628658220172,
      "learning_rate": 6.882949952206493e-05,
      "loss": 0.1758,
      "step": 236200
    },
    {
      "epoch": 0.7708945704619822,
      "grad_norm": 0.04774291068315506,
      "learning_rate": 6.873162886140535e-05,
      "loss": 0.3104,
      "step": 236300
    },
    {
      "epoch": 0.7712208059975141,
      "grad_norm": 14.833099365234375,
      "learning_rate": 6.863375820074576e-05,
      "loss": 0.2691,
      "step": 236400
    },
    {
      "epoch": 0.771547041533046,
      "grad_norm": 79.1934814453125,
      "learning_rate": 6.853588754008618e-05,
      "loss": 0.1235,
      "step": 236500
    },
    {
      "epoch": 0.771873277068578,
      "grad_norm": 28.787363052368164,
      "learning_rate": 6.84380168794266e-05,
      "loss": 0.2037,
      "step": 236600
    },
    {
      "epoch": 0.7721995126041099,
      "grad_norm": 0.0002930776681751013,
      "learning_rate": 6.834014621876702e-05,
      "loss": 0.1021,
      "step": 236700
    },
    {
      "epoch": 0.7725257481396418,
      "grad_norm": 0.00104525126516819,
      "learning_rate": 6.824227555810743e-05,
      "loss": 0.2055,
      "step": 236800
    },
    {
      "epoch": 0.7728519836751738,
      "grad_norm": 0.83305823802948,
      "learning_rate": 6.814440489744785e-05,
      "loss": 0.2919,
      "step": 236900
    },
    {
      "epoch": 0.7731782192107057,
      "grad_norm": 0.22159382700920105,
      "learning_rate": 6.804653423678826e-05,
      "loss": 0.3,
      "step": 237000
    },
    {
      "epoch": 0.7735044547462376,
      "grad_norm": 11.306313514709473,
      "learning_rate": 6.794866357612869e-05,
      "loss": 0.4887,
      "step": 237100
    },
    {
      "epoch": 0.7738306902817697,
      "grad_norm": 0.001340630929917097,
      "learning_rate": 6.78507929154691e-05,
      "loss": 0.2112,
      "step": 237200
    },
    {
      "epoch": 0.7741569258173016,
      "grad_norm": 0.013195802457630634,
      "learning_rate": 6.775292225480952e-05,
      "loss": 0.269,
      "step": 237300
    },
    {
      "epoch": 0.7744831613528336,
      "grad_norm": 5.408092498779297,
      "learning_rate": 6.765505159414993e-05,
      "loss": 0.1799,
      "step": 237400
    },
    {
      "epoch": 0.7748093968883655,
      "grad_norm": 0.0016260177362710238,
      "learning_rate": 6.755718093349035e-05,
      "loss": 0.2661,
      "step": 237500
    },
    {
      "epoch": 0.7751356324238974,
      "grad_norm": 0.0001021596253849566,
      "learning_rate": 6.745931027283078e-05,
      "loss": 0.2172,
      "step": 237600
    },
    {
      "epoch": 0.7754618679594294,
      "grad_norm": 0.0021348833106458187,
      "learning_rate": 6.736143961217119e-05,
      "loss": 0.2722,
      "step": 237700
    },
    {
      "epoch": 0.7757881034949613,
      "grad_norm": 0.002807378536090255,
      "learning_rate": 6.72635689515116e-05,
      "loss": 0.2194,
      "step": 237800
    },
    {
      "epoch": 0.7761143390304932,
      "grad_norm": 11.60436725616455,
      "learning_rate": 6.716569829085202e-05,
      "loss": 0.281,
      "step": 237900
    },
    {
      "epoch": 0.7764405745660252,
      "grad_norm": 3.6322901248931885,
      "learning_rate": 6.706782763019245e-05,
      "loss": 0.1965,
      "step": 238000
    },
    {
      "epoch": 0.7767668101015571,
      "grad_norm": 1.0659897327423096,
      "learning_rate": 6.696995696953286e-05,
      "loss": 0.1648,
      "step": 238100
    },
    {
      "epoch": 0.777093045637089,
      "grad_norm": 12.031137466430664,
      "learning_rate": 6.687208630887328e-05,
      "loss": 0.1543,
      "step": 238200
    },
    {
      "epoch": 0.777419281172621,
      "grad_norm": 8.961739540100098,
      "learning_rate": 6.677421564821369e-05,
      "loss": 0.3252,
      "step": 238300
    },
    {
      "epoch": 0.777745516708153,
      "grad_norm": 6.0466678405646235e-05,
      "learning_rate": 6.66763449875541e-05,
      "loss": 0.2926,
      "step": 238400
    },
    {
      "epoch": 0.7780717522436849,
      "grad_norm": 1.3847025911672972e-05,
      "learning_rate": 6.657847432689453e-05,
      "loss": 0.2588,
      "step": 238500
    },
    {
      "epoch": 0.7783979877792169,
      "grad_norm": 0.0016734603559598327,
      "learning_rate": 6.648060366623495e-05,
      "loss": 0.2645,
      "step": 238600
    },
    {
      "epoch": 0.7787242233147488,
      "grad_norm": 22.400897979736328,
      "learning_rate": 6.638273300557536e-05,
      "loss": 0.3499,
      "step": 238700
    },
    {
      "epoch": 0.7790504588502807,
      "grad_norm": 0.000603389460593462,
      "learning_rate": 6.628486234491578e-05,
      "loss": 0.3601,
      "step": 238800
    },
    {
      "epoch": 0.7793766943858127,
      "grad_norm": 57.53248977661133,
      "learning_rate": 6.618699168425619e-05,
      "loss": 0.2639,
      "step": 238900
    },
    {
      "epoch": 0.7797029299213446,
      "grad_norm": 0.16797900199890137,
      "learning_rate": 6.608912102359662e-05,
      "loss": 0.1855,
      "step": 239000
    },
    {
      "epoch": 0.7800291654568765,
      "grad_norm": 0.0029008437413722277,
      "learning_rate": 6.599125036293703e-05,
      "loss": 0.2092,
      "step": 239100
    },
    {
      "epoch": 0.7803554009924085,
      "grad_norm": 0.0005327760009095073,
      "learning_rate": 6.589337970227745e-05,
      "loss": 0.365,
      "step": 239200
    },
    {
      "epoch": 0.7806816365279404,
      "grad_norm": 0.0011394070461392403,
      "learning_rate": 6.579550904161786e-05,
      "loss": 0.2139,
      "step": 239300
    },
    {
      "epoch": 0.7810078720634723,
      "grad_norm": 11.929561614990234,
      "learning_rate": 6.569763838095828e-05,
      "loss": 0.2689,
      "step": 239400
    },
    {
      "epoch": 0.7813341075990043,
      "grad_norm": 0.001304643345065415,
      "learning_rate": 6.55997677202987e-05,
      "loss": 0.2839,
      "step": 239500
    },
    {
      "epoch": 0.7816603431345363,
      "grad_norm": 7.733698294032365e-05,
      "learning_rate": 6.55018970596391e-05,
      "loss": 0.1826,
      "step": 239600
    },
    {
      "epoch": 0.7819865786700683,
      "grad_norm": 0.001298699644394219,
      "learning_rate": 6.540402639897953e-05,
      "loss": 0.2836,
      "step": 239700
    },
    {
      "epoch": 0.7823128142056002,
      "grad_norm": 0.0012333031045272946,
      "learning_rate": 6.530615573831995e-05,
      "loss": 0.1661,
      "step": 239800
    },
    {
      "epoch": 0.7826390497411321,
      "grad_norm": 0.000216378626646474,
      "learning_rate": 6.520828507766036e-05,
      "loss": 0.3061,
      "step": 239900
    },
    {
      "epoch": 0.7829652852766641,
      "grad_norm": 0.004410454537719488,
      "learning_rate": 6.511041441700079e-05,
      "loss": 0.2327,
      "step": 240000
    },
    {
      "epoch": 0.783291520812196,
      "grad_norm": 2.6928117222269066e-05,
      "learning_rate": 6.501254375634119e-05,
      "loss": 0.268,
      "step": 240100
    },
    {
      "epoch": 0.7836177563477279,
      "grad_norm": 0.12491846829652786,
      "learning_rate": 6.491467309568162e-05,
      "loss": 0.4131,
      "step": 240200
    },
    {
      "epoch": 0.7839439918832599,
      "grad_norm": 0.00027931458316743374,
      "learning_rate": 6.481680243502203e-05,
      "loss": 0.1748,
      "step": 240300
    },
    {
      "epoch": 0.7842702274187918,
      "grad_norm": 0.006839183624833822,
      "learning_rate": 6.471893177436245e-05,
      "loss": 0.0813,
      "step": 240400
    },
    {
      "epoch": 0.7845964629543237,
      "grad_norm": 0.00012178423639852554,
      "learning_rate": 6.462106111370287e-05,
      "loss": 0.1715,
      "step": 240500
    },
    {
      "epoch": 0.7849226984898557,
      "grad_norm": 22.376249313354492,
      "learning_rate": 6.452319045304327e-05,
      "loss": 0.2047,
      "step": 240600
    },
    {
      "epoch": 0.7852489340253876,
      "grad_norm": 0.7756397724151611,
      "learning_rate": 6.44253197923837e-05,
      "loss": 0.3007,
      "step": 240700
    },
    {
      "epoch": 0.7855751695609196,
      "grad_norm": 0.0006185693200677633,
      "learning_rate": 6.432744913172412e-05,
      "loss": 0.3577,
      "step": 240800
    },
    {
      "epoch": 0.7859014050964516,
      "grad_norm": 3.330628896947019e-05,
      "learning_rate": 6.422957847106453e-05,
      "loss": 0.3565,
      "step": 240900
    },
    {
      "epoch": 0.7862276406319835,
      "grad_norm": 0.002107044914737344,
      "learning_rate": 6.413170781040496e-05,
      "loss": 0.3905,
      "step": 241000
    },
    {
      "epoch": 0.7865538761675154,
      "grad_norm": 0.0013514403253793716,
      "learning_rate": 6.403383714974536e-05,
      "loss": 0.3851,
      "step": 241100
    },
    {
      "epoch": 0.7868801117030474,
      "grad_norm": 0.009614206850528717,
      "learning_rate": 6.393596648908579e-05,
      "loss": 0.2333,
      "step": 241200
    },
    {
      "epoch": 0.7872063472385793,
      "grad_norm": 0.002019109670072794,
      "learning_rate": 6.38380958284262e-05,
      "loss": 0.1133,
      "step": 241300
    },
    {
      "epoch": 0.7875325827741112,
      "grad_norm": 0.0003151313867419958,
      "learning_rate": 6.374022516776662e-05,
      "loss": 0.1637,
      "step": 241400
    },
    {
      "epoch": 0.7878588183096432,
      "grad_norm": 1.948763370513916,
      "learning_rate": 6.364235450710703e-05,
      "loss": 0.2843,
      "step": 241500
    },
    {
      "epoch": 0.7881850538451751,
      "grad_norm": 4.339994484325871e-05,
      "learning_rate": 6.354448384644745e-05,
      "loss": 0.1803,
      "step": 241600
    },
    {
      "epoch": 0.7885112893807071,
      "grad_norm": 31.396419525146484,
      "learning_rate": 6.344661318578787e-05,
      "loss": 0.2292,
      "step": 241700
    },
    {
      "epoch": 0.788837524916239,
      "grad_norm": 0.004515048116445541,
      "learning_rate": 6.334874252512829e-05,
      "loss": 0.1908,
      "step": 241800
    },
    {
      "epoch": 0.789163760451771,
      "grad_norm": 0.017150353640317917,
      "learning_rate": 6.32508718644687e-05,
      "loss": 0.2913,
      "step": 241900
    },
    {
      "epoch": 0.789489995987303,
      "grad_norm": 0.000212043451028876,
      "learning_rate": 6.315300120380912e-05,
      "loss": 0.2442,
      "step": 242000
    },
    {
      "epoch": 0.7898162315228349,
      "grad_norm": 0.0011086200829595327,
      "learning_rate": 6.305513054314953e-05,
      "loss": 0.1621,
      "step": 242100
    },
    {
      "epoch": 0.7901424670583668,
      "grad_norm": 0.001640142290852964,
      "learning_rate": 6.295725988248996e-05,
      "loss": 0.2142,
      "step": 242200
    },
    {
      "epoch": 0.7904687025938988,
      "grad_norm": 3.466778798610903e-05,
      "learning_rate": 6.285938922183037e-05,
      "loss": 0.1262,
      "step": 242300
    },
    {
      "epoch": 0.7907949381294307,
      "grad_norm": 79.3997573852539,
      "learning_rate": 6.276151856117079e-05,
      "loss": 0.3233,
      "step": 242400
    },
    {
      "epoch": 0.7911211736649626,
      "grad_norm": 0.00222058710642159,
      "learning_rate": 6.26636479005112e-05,
      "loss": 0.2582,
      "step": 242500
    },
    {
      "epoch": 0.7914474092004946,
      "grad_norm": 0.0003027921775355935,
      "learning_rate": 6.256577723985162e-05,
      "loss": 0.2078,
      "step": 242600
    },
    {
      "epoch": 0.7917736447360265,
      "grad_norm": 0.0009420855203643441,
      "learning_rate": 6.246790657919204e-05,
      "loss": 0.2078,
      "step": 242700
    },
    {
      "epoch": 0.7920998802715584,
      "grad_norm": 2.935067459475249e-05,
      "learning_rate": 6.237003591853246e-05,
      "loss": 0.2674,
      "step": 242800
    },
    {
      "epoch": 0.7924261158070904,
      "grad_norm": 0.009713779203593731,
      "learning_rate": 6.227216525787287e-05,
      "loss": 0.1569,
      "step": 242900
    },
    {
      "epoch": 0.7927523513426223,
      "grad_norm": 0.0016300742281600833,
      "learning_rate": 6.217429459721329e-05,
      "loss": 0.2143,
      "step": 243000
    },
    {
      "epoch": 0.7930785868781542,
      "grad_norm": 0.002179831499233842,
      "learning_rate": 6.20764239365537e-05,
      "loss": 0.284,
      "step": 243100
    },
    {
      "epoch": 0.7934048224136863,
      "grad_norm": 0.0006046879570931196,
      "learning_rate": 6.197855327589413e-05,
      "loss": 0.25,
      "step": 243200
    },
    {
      "epoch": 0.7937310579492182,
      "grad_norm": 0.004898545332252979,
      "learning_rate": 6.188068261523454e-05,
      "loss": 0.3533,
      "step": 243300
    },
    {
      "epoch": 0.7940572934847501,
      "grad_norm": 11.638626098632812,
      "learning_rate": 6.178281195457496e-05,
      "loss": 0.237,
      "step": 243400
    },
    {
      "epoch": 0.7943835290202821,
      "grad_norm": 1.8249007553094998e-05,
      "learning_rate": 6.168494129391537e-05,
      "loss": 0.125,
      "step": 243500
    },
    {
      "epoch": 0.794709764555814,
      "grad_norm": 0.0004992237663827837,
      "learning_rate": 6.158707063325579e-05,
      "loss": 0.2332,
      "step": 243600
    },
    {
      "epoch": 0.7950360000913459,
      "grad_norm": 0.00012444541789591312,
      "learning_rate": 6.148919997259621e-05,
      "loss": 0.1937,
      "step": 243700
    },
    {
      "epoch": 0.7953622356268779,
      "grad_norm": 0.08841702342033386,
      "learning_rate": 6.139132931193663e-05,
      "loss": 0.2385,
      "step": 243800
    },
    {
      "epoch": 0.7956884711624098,
      "grad_norm": 0.21507377922534943,
      "learning_rate": 6.129345865127704e-05,
      "loss": 0.2403,
      "step": 243900
    },
    {
      "epoch": 0.7960147066979418,
      "grad_norm": 0.0015701774973422289,
      "learning_rate": 6.119558799061746e-05,
      "loss": 0.3025,
      "step": 244000
    },
    {
      "epoch": 0.7963409422334737,
      "grad_norm": 0.0027004722505807877,
      "learning_rate": 6.109771732995787e-05,
      "loss": 0.303,
      "step": 244100
    },
    {
      "epoch": 0.7966671777690056,
      "grad_norm": 0.001332049840129912,
      "learning_rate": 6.0999846669298293e-05,
      "loss": 0.1218,
      "step": 244200
    },
    {
      "epoch": 0.7969934133045377,
      "grad_norm": 0.010900981724262238,
      "learning_rate": 6.0901976008638715e-05,
      "loss": 0.3147,
      "step": 244300
    },
    {
      "epoch": 0.7973196488400696,
      "grad_norm": 0.003137187799438834,
      "learning_rate": 6.080410534797913e-05,
      "loss": 0.3017,
      "step": 244400
    },
    {
      "epoch": 0.7976458843756015,
      "grad_norm": 0.002739780815318227,
      "learning_rate": 6.070623468731954e-05,
      "loss": 0.2727,
      "step": 244500
    },
    {
      "epoch": 0.7979721199111335,
      "grad_norm": 0.0006325019639916718,
      "learning_rate": 6.060836402665996e-05,
      "loss": 0.2735,
      "step": 244600
    },
    {
      "epoch": 0.7982983554466654,
      "grad_norm": 0.012549078091979027,
      "learning_rate": 6.051049336600038e-05,
      "loss": 0.2473,
      "step": 244700
    },
    {
      "epoch": 0.7986245909821973,
      "grad_norm": 4.268415432306938e-05,
      "learning_rate": 6.04126227053408e-05,
      "loss": 0.3541,
      "step": 244800
    },
    {
      "epoch": 0.7989508265177293,
      "grad_norm": 5.3615243814419955e-05,
      "learning_rate": 6.0314752044681214e-05,
      "loss": 0.2297,
      "step": 244900
    },
    {
      "epoch": 0.7992770620532612,
      "grad_norm": 0.01814458891749382,
      "learning_rate": 6.021688138402163e-05,
      "loss": 0.2785,
      "step": 245000
    },
    {
      "epoch": 0.7996032975887931,
      "grad_norm": 0.04379820451140404,
      "learning_rate": 6.011901072336205e-05,
      "loss": 0.3592,
      "step": 245100
    },
    {
      "epoch": 0.7999295331243251,
      "grad_norm": 2.0858100469922647e-05,
      "learning_rate": 6.0021140062702464e-05,
      "loss": 0.315,
      "step": 245200
    },
    {
      "epoch": 0.800255768659857,
      "grad_norm": 0.00039189407834783196,
      "learning_rate": 5.9923269402042885e-05,
      "loss": 0.2933,
      "step": 245300
    },
    {
      "epoch": 0.8005820041953889,
      "grad_norm": 0.16898393630981445,
      "learning_rate": 5.982539874138329e-05,
      "loss": 0.1169,
      "step": 245400
    },
    {
      "epoch": 0.800908239730921,
      "grad_norm": 6.0460912209236994e-05,
      "learning_rate": 5.9727528080723714e-05,
      "loss": 0.2254,
      "step": 245500
    },
    {
      "epoch": 0.8012344752664529,
      "grad_norm": 13.960670471191406,
      "learning_rate": 5.9629657420064135e-05,
      "loss": 0.2418,
      "step": 245600
    },
    {
      "epoch": 0.8015607108019848,
      "grad_norm": 0.1822216808795929,
      "learning_rate": 5.953178675940455e-05,
      "loss": 0.1897,
      "step": 245700
    },
    {
      "epoch": 0.8018869463375168,
      "grad_norm": 0.031017759814858437,
      "learning_rate": 5.943391609874497e-05,
      "loss": 0.0894,
      "step": 245800
    },
    {
      "epoch": 0.8022131818730487,
      "grad_norm": 0.00243196077644825,
      "learning_rate": 5.933604543808538e-05,
      "loss": 0.2419,
      "step": 245900
    },
    {
      "epoch": 0.8025394174085807,
      "grad_norm": 0.017986776307225227,
      "learning_rate": 5.92381747774258e-05,
      "loss": 0.2144,
      "step": 246000
    },
    {
      "epoch": 0.8028656529441126,
      "grad_norm": 0.4341258704662323,
      "learning_rate": 5.914030411676622e-05,
      "loss": 0.3031,
      "step": 246100
    },
    {
      "epoch": 0.8031918884796445,
      "grad_norm": 0.0002888229500968009,
      "learning_rate": 5.9042433456106635e-05,
      "loss": 0.2476,
      "step": 246200
    },
    {
      "epoch": 0.8035181240151765,
      "grad_norm": 2.732024950091727e-05,
      "learning_rate": 5.8944562795447056e-05,
      "loss": 0.1471,
      "step": 246300
    },
    {
      "epoch": 0.8038443595507084,
      "grad_norm": 0.011445030570030212,
      "learning_rate": 5.8846692134787464e-05,
      "loss": 0.2201,
      "step": 246400
    },
    {
      "epoch": 0.8041705950862403,
      "grad_norm": 0.004519140813499689,
      "learning_rate": 5.8748821474127885e-05,
      "loss": 0.1968,
      "step": 246500
    },
    {
      "epoch": 0.8044968306217724,
      "grad_norm": 0.001386823016218841,
      "learning_rate": 5.8650950813468306e-05,
      "loss": 0.1356,
      "step": 246600
    },
    {
      "epoch": 0.8048230661573043,
      "grad_norm": 15.057024955749512,
      "learning_rate": 5.855308015280872e-05,
      "loss": 0.2969,
      "step": 246700
    },
    {
      "epoch": 0.8051493016928362,
      "grad_norm": 0.0026657136622816324,
      "learning_rate": 5.845520949214914e-05,
      "loss": 0.2764,
      "step": 246800
    },
    {
      "epoch": 0.8054755372283682,
      "grad_norm": 3.148663745378144e-05,
      "learning_rate": 5.835733883148955e-05,
      "loss": 0.1899,
      "step": 246900
    },
    {
      "epoch": 0.8058017727639001,
      "grad_norm": 5.923286153119989e-05,
      "learning_rate": 5.825946817082997e-05,
      "loss": 0.1719,
      "step": 247000
    },
    {
      "epoch": 0.806128008299432,
      "grad_norm": 0.000575405778363347,
      "learning_rate": 5.816159751017039e-05,
      "loss": 0.154,
      "step": 247100
    },
    {
      "epoch": 0.806454243834964,
      "grad_norm": 4.583904956234619e-05,
      "learning_rate": 5.8063726849510805e-05,
      "loss": 0.1774,
      "step": 247200
    },
    {
      "epoch": 0.8067804793704959,
      "grad_norm": 4.8811922169988975e-05,
      "learning_rate": 5.796585618885122e-05,
      "loss": 0.3151,
      "step": 247300
    },
    {
      "epoch": 0.8071067149060278,
      "grad_norm": 0.00030941941076889634,
      "learning_rate": 5.7867985528191634e-05,
      "loss": 0.2266,
      "step": 247400
    },
    {
      "epoch": 0.8074329504415598,
      "grad_norm": 0.0010963573586195707,
      "learning_rate": 5.7770114867532055e-05,
      "loss": 0.3112,
      "step": 247500
    },
    {
      "epoch": 0.8077591859770917,
      "grad_norm": 7.785746856825426e-05,
      "learning_rate": 5.7672244206872476e-05,
      "loss": 0.1168,
      "step": 247600
    },
    {
      "epoch": 0.8080854215126236,
      "grad_norm": 0.00021832025959156454,
      "learning_rate": 5.757437354621289e-05,
      "loss": 0.2109,
      "step": 247700
    },
    {
      "epoch": 0.8084116570481557,
      "grad_norm": 0.004684352781623602,
      "learning_rate": 5.7476502885553305e-05,
      "loss": 0.2161,
      "step": 247800
    },
    {
      "epoch": 0.8087378925836876,
      "grad_norm": 0.003145157126709819,
      "learning_rate": 5.737863222489372e-05,
      "loss": 0.1733,
      "step": 247900
    },
    {
      "epoch": 0.8090641281192195,
      "grad_norm": 7.416892913170159e-05,
      "learning_rate": 5.728076156423414e-05,
      "loss": 0.1132,
      "step": 248000
    },
    {
      "epoch": 0.8093903636547515,
      "grad_norm": 0.012491829693317413,
      "learning_rate": 5.718289090357456e-05,
      "loss": 0.4157,
      "step": 248100
    },
    {
      "epoch": 0.8097165991902834,
      "grad_norm": 0.012646992690861225,
      "learning_rate": 5.708502024291497e-05,
      "loss": 0.4035,
      "step": 248200
    },
    {
      "epoch": 0.8100428347258154,
      "grad_norm": 0.009068344719707966,
      "learning_rate": 5.698714958225539e-05,
      "loss": 0.2034,
      "step": 248300
    },
    {
      "epoch": 0.8103690702613473,
      "grad_norm": 0.0009879268473014235,
      "learning_rate": 5.688927892159581e-05,
      "loss": 0.2586,
      "step": 248400
    },
    {
      "epoch": 0.8106953057968792,
      "grad_norm": 9.931668281555176,
      "learning_rate": 5.6791408260936226e-05,
      "loss": 0.1713,
      "step": 248500
    },
    {
      "epoch": 0.8110215413324112,
      "grad_norm": 0.35871875286102295,
      "learning_rate": 5.669353760027665e-05,
      "loss": 0.2124,
      "step": 248600
    },
    {
      "epoch": 0.8113477768679431,
      "grad_norm": 0.0013450802071020007,
      "learning_rate": 5.6595666939617055e-05,
      "loss": 0.174,
      "step": 248700
    },
    {
      "epoch": 0.811674012403475,
      "grad_norm": 0.0006086913635954261,
      "learning_rate": 5.6497796278957476e-05,
      "loss": 0.2557,
      "step": 248800
    },
    {
      "epoch": 0.812000247939007,
      "grad_norm": 0.012965118512511253,
      "learning_rate": 5.63999256182979e-05,
      "loss": 0.1681,
      "step": 248900
    },
    {
      "epoch": 0.812326483474539,
      "grad_norm": 3.0540101528167725,
      "learning_rate": 5.630205495763831e-05,
      "loss": 0.2102,
      "step": 249000
    },
    {
      "epoch": 0.8126527190100709,
      "grad_norm": 0.2599819600582123,
      "learning_rate": 5.620418429697873e-05,
      "loss": 0.1945,
      "step": 249100
    },
    {
      "epoch": 0.8129789545456029,
      "grad_norm": 0.005046030040830374,
      "learning_rate": 5.610631363631914e-05,
      "loss": 0.2182,
      "step": 249200
    },
    {
      "epoch": 0.8133051900811348,
      "grad_norm": 0.004254581872373819,
      "learning_rate": 5.600844297565956e-05,
      "loss": 0.1345,
      "step": 249300
    },
    {
      "epoch": 0.8136314256166667,
      "grad_norm": 0.0038967267610132694,
      "learning_rate": 5.591057231499998e-05,
      "loss": 0.1515,
      "step": 249400
    },
    {
      "epoch": 0.8139576611521987,
      "grad_norm": 0.0025362682063132524,
      "learning_rate": 5.58127016543404e-05,
      "loss": 0.1452,
      "step": 249500
    },
    {
      "epoch": 0.8142838966877306,
      "grad_norm": 0.02383686788380146,
      "learning_rate": 5.571483099368082e-05,
      "loss": 0.2096,
      "step": 249600
    },
    {
      "epoch": 0.8146101322232625,
      "grad_norm": 0.0006967608933337033,
      "learning_rate": 5.5616960333021225e-05,
      "loss": 0.2262,
      "step": 249700
    },
    {
      "epoch": 0.8149363677587945,
      "grad_norm": 0.0009076597052626312,
      "learning_rate": 5.5519089672361647e-05,
      "loss": 0.1542,
      "step": 249800
    },
    {
      "epoch": 0.8152626032943264,
      "grad_norm": 0.0029807419050484896,
      "learning_rate": 5.542121901170207e-05,
      "loss": 0.1685,
      "step": 249900
    },
    {
      "epoch": 0.8155888388298583,
      "grad_norm": 1.5523592233657837,
      "learning_rate": 5.532334835104248e-05,
      "loss": 0.2088,
      "step": 250000
    },
    {
      "epoch": 0.8159150743653903,
      "grad_norm": 0.0012111536925658584,
      "learning_rate": 5.5225477690382896e-05,
      "loss": 0.0479,
      "step": 250100
    },
    {
      "epoch": 0.8162413099009223,
      "grad_norm": 0.002076380653306842,
      "learning_rate": 5.512760702972331e-05,
      "loss": 0.1124,
      "step": 250200
    },
    {
      "epoch": 0.8165675454364542,
      "grad_norm": 20.5142765045166,
      "learning_rate": 5.502973636906373e-05,
      "loss": 0.3196,
      "step": 250300
    },
    {
      "epoch": 0.8168937809719862,
      "grad_norm": 0.0072681233286857605,
      "learning_rate": 5.493186570840415e-05,
      "loss": 0.2002,
      "step": 250400
    },
    {
      "epoch": 0.8172200165075181,
      "grad_norm": 0.00017525791190564632,
      "learning_rate": 5.483399504774456e-05,
      "loss": 0.347,
      "step": 250500
    },
    {
      "epoch": 0.8175462520430501,
      "grad_norm": 0.00035304517950862646,
      "learning_rate": 5.473612438708498e-05,
      "loss": 0.2422,
      "step": 250600
    },
    {
      "epoch": 0.817872487578582,
      "grad_norm": 0.0008741050260141492,
      "learning_rate": 5.4638253726425396e-05,
      "loss": 0.2681,
      "step": 250700
    },
    {
      "epoch": 0.8181987231141139,
      "grad_norm": 0.0001734656107146293,
      "learning_rate": 5.454038306576582e-05,
      "loss": 0.2257,
      "step": 250800
    },
    {
      "epoch": 0.8185249586496459,
      "grad_norm": 0.00045451379264704883,
      "learning_rate": 5.444251240510624e-05,
      "loss": 0.2137,
      "step": 250900
    },
    {
      "epoch": 0.8188511941851778,
      "grad_norm": 1.0017212629318237,
      "learning_rate": 5.4344641744446646e-05,
      "loss": 0.2914,
      "step": 251000
    },
    {
      "epoch": 0.8191774297207097,
      "grad_norm": 0.8910033702850342,
      "learning_rate": 5.424677108378707e-05,
      "loss": 0.2036,
      "step": 251100
    },
    {
      "epoch": 0.8195036652562417,
      "grad_norm": 0.0028154233004897833,
      "learning_rate": 5.414890042312748e-05,
      "loss": 0.1583,
      "step": 251200
    },
    {
      "epoch": 0.8198299007917736,
      "grad_norm": 0.3651133179664612,
      "learning_rate": 5.40510297624679e-05,
      "loss": 0.2005,
      "step": 251300
    },
    {
      "epoch": 0.8201561363273056,
      "grad_norm": 0.003330281237140298,
      "learning_rate": 5.3953159101808324e-05,
      "loss": 0.1835,
      "step": 251400
    },
    {
      "epoch": 0.8204823718628376,
      "grad_norm": 27.310791015625,
      "learning_rate": 5.385528844114873e-05,
      "loss": 0.4068,
      "step": 251500
    },
    {
      "epoch": 0.8208086073983695,
      "grad_norm": 0.0008710123947821558,
      "learning_rate": 5.375741778048915e-05,
      "loss": 0.3226,
      "step": 251600
    },
    {
      "epoch": 0.8211348429339014,
      "grad_norm": 0.050451818853616714,
      "learning_rate": 5.3659547119829574e-05,
      "loss": 0.2831,
      "step": 251700
    },
    {
      "epoch": 0.8214610784694334,
      "grad_norm": 0.004723293241113424,
      "learning_rate": 5.356167645916999e-05,
      "loss": 0.1747,
      "step": 251800
    },
    {
      "epoch": 0.8217873140049653,
      "grad_norm": 0.0011927932500839233,
      "learning_rate": 5.346380579851041e-05,
      "loss": 0.349,
      "step": 251900
    },
    {
      "epoch": 0.8221135495404972,
      "grad_norm": 0.002355797914788127,
      "learning_rate": 5.336593513785082e-05,
      "loss": 0.0785,
      "step": 252000
    },
    {
      "epoch": 0.8224397850760292,
      "grad_norm": 0.00040033168625086546,
      "learning_rate": 5.326806447719124e-05,
      "loss": 0.3202,
      "step": 252100
    },
    {
      "epoch": 0.8227660206115611,
      "grad_norm": 6.566703814314678e-05,
      "learning_rate": 5.317019381653166e-05,
      "loss": 0.2781,
      "step": 252200
    },
    {
      "epoch": 0.823092256147093,
      "grad_norm": 2.916789162554778e-05,
      "learning_rate": 5.307232315587207e-05,
      "loss": 0.2301,
      "step": 252300
    },
    {
      "epoch": 0.823418491682625,
      "grad_norm": 0.1516738086938858,
      "learning_rate": 5.2974452495212494e-05,
      "loss": 0.2893,
      "step": 252400
    },
    {
      "epoch": 0.823744727218157,
      "grad_norm": 0.0005080791306681931,
      "learning_rate": 5.28765818345529e-05,
      "loss": 0.2101,
      "step": 252500
    },
    {
      "epoch": 0.824070962753689,
      "grad_norm": 1.5087144674907904e-05,
      "learning_rate": 5.277871117389332e-05,
      "loss": 0.1594,
      "step": 252600
    },
    {
      "epoch": 0.8243971982892209,
      "grad_norm": 0.00013825467613060027,
      "learning_rate": 5.2680840513233744e-05,
      "loss": 0.1122,
      "step": 252700
    },
    {
      "epoch": 0.8247234338247528,
      "grad_norm": 71.97638702392578,
      "learning_rate": 5.258296985257416e-05,
      "loss": 0.1478,
      "step": 252800
    },
    {
      "epoch": 0.8250496693602848,
      "grad_norm": 74.71836853027344,
      "learning_rate": 5.248509919191457e-05,
      "loss": 0.3707,
      "step": 252900
    },
    {
      "epoch": 0.8253759048958167,
      "grad_norm": 0.0007953094318509102,
      "learning_rate": 5.238722853125499e-05,
      "loss": 0.1574,
      "step": 253000
    },
    {
      "epoch": 0.8257021404313486,
      "grad_norm": 0.01354602724313736,
      "learning_rate": 5.228935787059541e-05,
      "loss": 0.1965,
      "step": 253100
    },
    {
      "epoch": 0.8260283759668806,
      "grad_norm": 12.021105766296387,
      "learning_rate": 5.219148720993583e-05,
      "loss": 0.2117,
      "step": 253200
    },
    {
      "epoch": 0.8263546115024125,
      "grad_norm": 0.000242699432419613,
      "learning_rate": 5.209361654927624e-05,
      "loss": 0.0541,
      "step": 253300
    },
    {
      "epoch": 0.8266808470379444,
      "grad_norm": 0.00010610996832838282,
      "learning_rate": 5.199574588861666e-05,
      "loss": 0.1494,
      "step": 253400
    },
    {
      "epoch": 0.8270070825734764,
      "grad_norm": 40.2363395690918,
      "learning_rate": 5.189787522795707e-05,
      "loss": 0.3872,
      "step": 253500
    },
    {
      "epoch": 0.8273333181090083,
      "grad_norm": 0.0007906511891633272,
      "learning_rate": 5.1800004567297494e-05,
      "loss": 0.2673,
      "step": 253600
    },
    {
      "epoch": 0.8276595536445402,
      "grad_norm": 0.00027885151212103665,
      "learning_rate": 5.1702133906637915e-05,
      "loss": 0.2494,
      "step": 253700
    },
    {
      "epoch": 0.8279857891800723,
      "grad_norm": 40.41588592529297,
      "learning_rate": 5.160426324597832e-05,
      "loss": 0.15,
      "step": 253800
    },
    {
      "epoch": 0.8283120247156042,
      "grad_norm": 0.0007963691605255008,
      "learning_rate": 5.1506392585318744e-05,
      "loss": 0.2221,
      "step": 253900
    },
    {
      "epoch": 0.8286382602511361,
      "grad_norm": 0.0003255402552895248,
      "learning_rate": 5.140852192465916e-05,
      "loss": 0.237,
      "step": 254000
    },
    {
      "epoch": 0.8289644957866681,
      "grad_norm": 0.03219890594482422,
      "learning_rate": 5.131065126399958e-05,
      "loss": 0.2029,
      "step": 254100
    },
    {
      "epoch": 0.8292907313222,
      "grad_norm": 0.008174649439752102,
      "learning_rate": 5.121278060334e-05,
      "loss": 0.2281,
      "step": 254200
    },
    {
      "epoch": 0.8296169668577319,
      "grad_norm": 8.992956161499023,
      "learning_rate": 5.111490994268041e-05,
      "loss": 0.3748,
      "step": 254300
    },
    {
      "epoch": 0.8299432023932639,
      "grad_norm": 0.001979124266654253,
      "learning_rate": 5.101703928202083e-05,
      "loss": 0.0781,
      "step": 254400
    },
    {
      "epoch": 0.8302694379287958,
      "grad_norm": 27.67847442626953,
      "learning_rate": 5.091916862136124e-05,
      "loss": 0.2842,
      "step": 254500
    },
    {
      "epoch": 0.8305956734643277,
      "grad_norm": 0.000311273499391973,
      "learning_rate": 5.0821297960701664e-05,
      "loss": 0.2439,
      "step": 254600
    },
    {
      "epoch": 0.8309219089998597,
      "grad_norm": 0.21501004695892334,
      "learning_rate": 5.0723427300042086e-05,
      "loss": 0.2292,
      "step": 254700
    },
    {
      "epoch": 0.8312481445353916,
      "grad_norm": 0.00022001318575348705,
      "learning_rate": 5.062555663938249e-05,
      "loss": 0.1449,
      "step": 254800
    },
    {
      "epoch": 0.8315743800709237,
      "grad_norm": 0.0002010736643569544,
      "learning_rate": 5.0527685978722914e-05,
      "loss": 0.2738,
      "step": 254900
    },
    {
      "epoch": 0.8319006156064556,
      "grad_norm": 60.88075256347656,
      "learning_rate": 5.042981531806333e-05,
      "loss": 0.2235,
      "step": 255000
    },
    {
      "epoch": 0.8322268511419875,
      "grad_norm": 4.1505809349473566e-05,
      "learning_rate": 5.033194465740375e-05,
      "loss": 0.1488,
      "step": 255100
    },
    {
      "epoch": 0.8325530866775195,
      "grad_norm": 0.011040099896490574,
      "learning_rate": 5.0234073996744164e-05,
      "loss": 0.2731,
      "step": 255200
    },
    {
      "epoch": 0.8328793222130514,
      "grad_norm": 0.0032815972808748484,
      "learning_rate": 5.013620333608458e-05,
      "loss": 0.2698,
      "step": 255300
    },
    {
      "epoch": 0.8332055577485833,
      "grad_norm": 0.0033328274730592966,
      "learning_rate": 5.0038332675425e-05,
      "loss": 0.1442,
      "step": 255400
    },
    {
      "epoch": 0.8335317932841153,
      "grad_norm": 0.00013875825970899314,
      "learning_rate": 4.994046201476542e-05,
      "loss": 0.2391,
      "step": 255500
    },
    {
      "epoch": 0.8338580288196472,
      "grad_norm": 4.15452741435729e-05,
      "learning_rate": 4.9842591354105835e-05,
      "loss": 0.2725,
      "step": 255600
    },
    {
      "epoch": 0.8341842643551791,
      "grad_norm": 0.0011802575318142772,
      "learning_rate": 4.974472069344625e-05,
      "loss": 0.2678,
      "step": 255700
    },
    {
      "epoch": 0.8345104998907111,
      "grad_norm": 26.93595314025879,
      "learning_rate": 4.9646850032786664e-05,
      "loss": 0.3135,
      "step": 255800
    },
    {
      "epoch": 0.834836735426243,
      "grad_norm": 0.00019493128638714552,
      "learning_rate": 4.9548979372127085e-05,
      "loss": 0.1535,
      "step": 255900
    },
    {
      "epoch": 0.8351629709617749,
      "grad_norm": 2.5711910724639893,
      "learning_rate": 4.9451108711467506e-05,
      "loss": 0.2368,
      "step": 256000
    },
    {
      "epoch": 0.835489206497307,
      "grad_norm": 0.026939887553453445,
      "learning_rate": 4.9353238050807914e-05,
      "loss": 0.1656,
      "step": 256100
    },
    {
      "epoch": 0.8358154420328389,
      "grad_norm": 0.10422061383724213,
      "learning_rate": 4.9255367390148335e-05,
      "loss": 0.2144,
      "step": 256200
    },
    {
      "epoch": 0.8361416775683708,
      "grad_norm": 0.0004019137704744935,
      "learning_rate": 4.915749672948875e-05,
      "loss": 0.2782,
      "step": 256300
    },
    {
      "epoch": 0.8364679131039028,
      "grad_norm": 0.0023017991334199905,
      "learning_rate": 4.905962606882917e-05,
      "loss": 0.143,
      "step": 256400
    },
    {
      "epoch": 0.8367941486394347,
      "grad_norm": 0.008752101100981236,
      "learning_rate": 4.896175540816959e-05,
      "loss": 0.0955,
      "step": 256500
    },
    {
      "epoch": 0.8371203841749666,
      "grad_norm": 0.0008373026503250003,
      "learning_rate": 4.886388474751e-05,
      "loss": 0.1923,
      "step": 256600
    },
    {
      "epoch": 0.8374466197104986,
      "grad_norm": 0.004493408370763063,
      "learning_rate": 4.876601408685042e-05,
      "loss": 0.3697,
      "step": 256700
    },
    {
      "epoch": 0.8377728552460305,
      "grad_norm": 5.311206405167468e-05,
      "learning_rate": 4.8668143426190835e-05,
      "loss": 0.1955,
      "step": 256800
    },
    {
      "epoch": 0.8380990907815625,
      "grad_norm": 0.6795583963394165,
      "learning_rate": 4.8570272765531256e-05,
      "loss": 0.3219,
      "step": 256900
    },
    {
      "epoch": 0.8384253263170944,
      "grad_norm": 0.0017009663861244917,
      "learning_rate": 4.847240210487168e-05,
      "loss": 0.3023,
      "step": 257000
    },
    {
      "epoch": 0.8387515618526263,
      "grad_norm": 0.17452000081539154,
      "learning_rate": 4.8374531444212084e-05,
      "loss": 0.3056,
      "step": 257100
    },
    {
      "epoch": 0.8390777973881584,
      "grad_norm": 0.0011866568820551038,
      "learning_rate": 4.8276660783552506e-05,
      "loss": 0.1961,
      "step": 257200
    },
    {
      "epoch": 0.8394040329236903,
      "grad_norm": 0.002182424534112215,
      "learning_rate": 4.817879012289292e-05,
      "loss": 0.1921,
      "step": 257300
    },
    {
      "epoch": 0.8397302684592222,
      "grad_norm": 9.742572729010135e-05,
      "learning_rate": 4.808091946223334e-05,
      "loss": 0.3303,
      "step": 257400
    },
    {
      "epoch": 0.8400565039947542,
      "grad_norm": 9.801939010620117,
      "learning_rate": 4.798304880157376e-05,
      "loss": 0.1849,
      "step": 257500
    },
    {
      "epoch": 0.8403827395302861,
      "grad_norm": 0.12040504068136215,
      "learning_rate": 4.788517814091417e-05,
      "loss": 0.2876,
      "step": 257600
    },
    {
      "epoch": 0.840708975065818,
      "grad_norm": 0.0052267564460635185,
      "learning_rate": 4.778730748025459e-05,
      "loss": 0.1047,
      "step": 257700
    },
    {
      "epoch": 0.84103521060135,
      "grad_norm": 0.0005202805041335523,
      "learning_rate": 4.7689436819595005e-05,
      "loss": 0.3807,
      "step": 257800
    },
    {
      "epoch": 0.8413614461368819,
      "grad_norm": 2.3273072242736816,
      "learning_rate": 4.7591566158935426e-05,
      "loss": 0.163,
      "step": 257900
    },
    {
      "epoch": 0.8416876816724138,
      "grad_norm": 0.007986808195710182,
      "learning_rate": 4.749369549827584e-05,
      "loss": 0.3311,
      "step": 258000
    },
    {
      "epoch": 0.8420139172079458,
      "grad_norm": 0.0010141062084585428,
      "learning_rate": 4.7395824837616255e-05,
      "loss": 0.0614,
      "step": 258100
    },
    {
      "epoch": 0.8423401527434777,
      "grad_norm": 12.307477951049805,
      "learning_rate": 4.7297954176956676e-05,
      "loss": 0.4012,
      "step": 258200
    },
    {
      "epoch": 0.8426663882790096,
      "grad_norm": 0.00010164326522499323,
      "learning_rate": 4.720008351629709e-05,
      "loss": 0.3176,
      "step": 258300
    },
    {
      "epoch": 0.8429926238145417,
      "grad_norm": 8.35972823551856e-05,
      "learning_rate": 4.710221285563751e-05,
      "loss": 0.1489,
      "step": 258400
    },
    {
      "epoch": 0.8433188593500736,
      "grad_norm": 0.016876665875315666,
      "learning_rate": 4.7004342194977926e-05,
      "loss": 0.2333,
      "step": 258500
    },
    {
      "epoch": 0.8436450948856055,
      "grad_norm": 7.209002797026187e-05,
      "learning_rate": 4.690647153431834e-05,
      "loss": 0.2461,
      "step": 258600
    },
    {
      "epoch": 0.8439713304211375,
      "grad_norm": 0.49728724360466003,
      "learning_rate": 4.680860087365876e-05,
      "loss": 0.2039,
      "step": 258700
    },
    {
      "epoch": 0.8442975659566694,
      "grad_norm": 2.3453989342669956e-05,
      "learning_rate": 4.671073021299918e-05,
      "loss": 0.2752,
      "step": 258800
    },
    {
      "epoch": 0.8446238014922013,
      "grad_norm": 63.783817291259766,
      "learning_rate": 4.661285955233959e-05,
      "loss": 0.3118,
      "step": 258900
    },
    {
      "epoch": 0.8449500370277333,
      "grad_norm": 0.0011407325509935617,
      "learning_rate": 4.651498889168001e-05,
      "loss": 0.2484,
      "step": 259000
    },
    {
      "epoch": 0.8452762725632652,
      "grad_norm": 8.915621583582833e-05,
      "learning_rate": 4.6417118231020426e-05,
      "loss": 0.1734,
      "step": 259100
    },
    {
      "epoch": 0.8456025080987972,
      "grad_norm": 9.058034629561007e-05,
      "learning_rate": 4.631924757036085e-05,
      "loss": 0.3324,
      "step": 259200
    },
    {
      "epoch": 0.8459287436343291,
      "grad_norm": 0.0035610946360975504,
      "learning_rate": 4.622137690970127e-05,
      "loss": 0.3283,
      "step": 259300
    },
    {
      "epoch": 0.846254979169861,
      "grad_norm": 0.000113530462840572,
      "learning_rate": 4.6123506249041676e-05,
      "loss": 0.2038,
      "step": 259400
    },
    {
      "epoch": 0.846581214705393,
      "grad_norm": 0.0017235338455066085,
      "learning_rate": 4.60256355883821e-05,
      "loss": 0.293,
      "step": 259500
    },
    {
      "epoch": 0.846907450240925,
      "grad_norm": 0.0007856555166654289,
      "learning_rate": 4.592776492772251e-05,
      "loss": 0.0899,
      "step": 259600
    },
    {
      "epoch": 0.8472336857764569,
      "grad_norm": 24.35289192199707,
      "learning_rate": 4.582989426706293e-05,
      "loss": 0.1275,
      "step": 259700
    },
    {
      "epoch": 0.8475599213119889,
      "grad_norm": 0.000135074311401695,
      "learning_rate": 4.573202360640335e-05,
      "loss": 0.2613,
      "step": 259800
    },
    {
      "epoch": 0.8478861568475208,
      "grad_norm": 8.374203753191978e-05,
      "learning_rate": 4.563415294574376e-05,
      "loss": 0.1609,
      "step": 259900
    },
    {
      "epoch": 0.8482123923830527,
      "grad_norm": 0.000182874413440004,
      "learning_rate": 4.553628228508418e-05,
      "loss": 0.23,
      "step": 260000
    },
    {
      "epoch": 0.8485386279185847,
      "grad_norm": 0.00019100568897556514,
      "learning_rate": 4.5438411624424596e-05,
      "loss": 0.1357,
      "step": 260100
    },
    {
      "epoch": 0.8488648634541166,
      "grad_norm": 0.00031303774449042976,
      "learning_rate": 4.534054096376502e-05,
      "loss": 0.1672,
      "step": 260200
    },
    {
      "epoch": 0.8491910989896485,
      "grad_norm": 0.030405299738049507,
      "learning_rate": 4.524267030310544e-05,
      "loss": 0.3024,
      "step": 260300
    },
    {
      "epoch": 0.8495173345251805,
      "grad_norm": 0.002906326437368989,
      "learning_rate": 4.5144799642445846e-05,
      "loss": 0.0597,
      "step": 260400
    },
    {
      "epoch": 0.8498435700607124,
      "grad_norm": 0.0002385764237260446,
      "learning_rate": 4.504692898178627e-05,
      "loss": 0.4776,
      "step": 260500
    },
    {
      "epoch": 0.8501698055962443,
      "grad_norm": 3.5012195110321045,
      "learning_rate": 4.494905832112668e-05,
      "loss": 0.1768,
      "step": 260600
    },
    {
      "epoch": 0.8504960411317763,
      "grad_norm": 6.235024193301797e-05,
      "learning_rate": 4.48511876604671e-05,
      "loss": 0.1871,
      "step": 260700
    },
    {
      "epoch": 0.8508222766673083,
      "grad_norm": 0.29155099391937256,
      "learning_rate": 4.475331699980752e-05,
      "loss": 0.1575,
      "step": 260800
    },
    {
      "epoch": 0.8511485122028402,
      "grad_norm": 0.014134799130260944,
      "learning_rate": 4.465544633914793e-05,
      "loss": 0.1593,
      "step": 260900
    },
    {
      "epoch": 0.8514747477383722,
      "grad_norm": 0.0029070808086544275,
      "learning_rate": 4.455757567848835e-05,
      "loss": 0.3204,
      "step": 261000
    },
    {
      "epoch": 0.8518009832739041,
      "grad_norm": 1.690547651378438e-05,
      "learning_rate": 4.445970501782877e-05,
      "loss": 0.1854,
      "step": 261100
    },
    {
      "epoch": 0.852127218809436,
      "grad_norm": 0.002323223277926445,
      "learning_rate": 4.436183435716918e-05,
      "loss": 0.1218,
      "step": 261200
    },
    {
      "epoch": 0.852453454344968,
      "grad_norm": 0.010295534506440163,
      "learning_rate": 4.42639636965096e-05,
      "loss": 0.1709,
      "step": 261300
    },
    {
      "epoch": 0.8527796898804999,
      "grad_norm": 8.809232531348243e-05,
      "learning_rate": 4.416609303585002e-05,
      "loss": 0.0983,
      "step": 261400
    },
    {
      "epoch": 0.8531059254160319,
      "grad_norm": 0.00807754322886467,
      "learning_rate": 4.406822237519044e-05,
      "loss": 0.2514,
      "step": 261500
    },
    {
      "epoch": 0.8534321609515638,
      "grad_norm": 3.136889063171111e-05,
      "learning_rate": 4.397035171453085e-05,
      "loss": 0.207,
      "step": 261600
    },
    {
      "epoch": 0.8537583964870957,
      "grad_norm": 0.00043090563849546015,
      "learning_rate": 4.387248105387127e-05,
      "loss": 0.3364,
      "step": 261700
    },
    {
      "epoch": 0.8540846320226277,
      "grad_norm": 0.008431640453636646,
      "learning_rate": 4.377461039321169e-05,
      "loss": 0.2261,
      "step": 261800
    },
    {
      "epoch": 0.8544108675581596,
      "grad_norm": 13.913962364196777,
      "learning_rate": 4.36767397325521e-05,
      "loss": 0.2583,
      "step": 261900
    },
    {
      "epoch": 0.8547371030936916,
      "grad_norm": 0.019752521067857742,
      "learning_rate": 4.3578869071892523e-05,
      "loss": 0.2014,
      "step": 262000
    },
    {
      "epoch": 0.8550633386292236,
      "grad_norm": 0.005051163490861654,
      "learning_rate": 4.348099841123293e-05,
      "loss": 0.1558,
      "step": 262100
    },
    {
      "epoch": 0.8553895741647555,
      "grad_norm": 0.0002867205475922674,
      "learning_rate": 4.338312775057335e-05,
      "loss": 0.2324,
      "step": 262200
    },
    {
      "epoch": 0.8557158097002874,
      "grad_norm": 0.0010610284516587853,
      "learning_rate": 4.328525708991377e-05,
      "loss": 0.2619,
      "step": 262300
    },
    {
      "epoch": 0.8560420452358194,
      "grad_norm": 0.0001585289282957092,
      "learning_rate": 4.318738642925419e-05,
      "loss": 0.138,
      "step": 262400
    },
    {
      "epoch": 0.8563682807713513,
      "grad_norm": 0.00244390731677413,
      "learning_rate": 4.308951576859461e-05,
      "loss": 0.1728,
      "step": 262500
    },
    {
      "epoch": 0.8566945163068832,
      "grad_norm": 0.0001885417732410133,
      "learning_rate": 4.299164510793503e-05,
      "loss": 0.2943,
      "step": 262600
    },
    {
      "epoch": 0.8570207518424152,
      "grad_norm": 0.00010234052751911804,
      "learning_rate": 4.289377444727544e-05,
      "loss": 0.4062,
      "step": 262700
    },
    {
      "epoch": 0.8573469873779471,
      "grad_norm": 0.0007976280176080763,
      "learning_rate": 4.279590378661586e-05,
      "loss": 0.1594,
      "step": 262800
    },
    {
      "epoch": 0.857673222913479,
      "grad_norm": 0.004560471512377262,
      "learning_rate": 4.269803312595627e-05,
      "loss": 0.1854,
      "step": 262900
    },
    {
      "epoch": 0.857999458449011,
      "grad_norm": 0.0009553871350362897,
      "learning_rate": 4.2600162465296694e-05,
      "loss": 0.2863,
      "step": 263000
    },
    {
      "epoch": 0.858325693984543,
      "grad_norm": 16.8734188079834,
      "learning_rate": 4.2502291804637115e-05,
      "loss": 0.1947,
      "step": 263100
    },
    {
      "epoch": 0.8586519295200749,
      "grad_norm": 0.020895907655358315,
      "learning_rate": 4.240442114397752e-05,
      "loss": 0.2393,
      "step": 263200
    },
    {
      "epoch": 0.8589781650556069,
      "grad_norm": 0.0005608436185866594,
      "learning_rate": 4.2306550483317944e-05,
      "loss": 0.1292,
      "step": 263300
    },
    {
      "epoch": 0.8593044005911388,
      "grad_norm": 0.0009733327315188944,
      "learning_rate": 4.220867982265836e-05,
      "loss": 0.2675,
      "step": 263400
    },
    {
      "epoch": 0.8596306361266708,
      "grad_norm": 0.0006159415352158248,
      "learning_rate": 4.211080916199878e-05,
      "loss": 0.1704,
      "step": 263500
    },
    {
      "epoch": 0.8599568716622027,
      "grad_norm": 0.015348260290920734,
      "learning_rate": 4.2012938501339194e-05,
      "loss": 0.2578,
      "step": 263600
    },
    {
      "epoch": 0.8602831071977346,
      "grad_norm": 0.0019268965115770698,
      "learning_rate": 4.191506784067961e-05,
      "loss": 0.3049,
      "step": 263700
    },
    {
      "epoch": 0.8606093427332666,
      "grad_norm": 1.7129096984863281,
      "learning_rate": 4.181719718002003e-05,
      "loss": 0.3159,
      "step": 263800
    },
    {
      "epoch": 0.8609355782687985,
      "grad_norm": 0.0002768193662632257,
      "learning_rate": 4.1719326519360444e-05,
      "loss": 0.2342,
      "step": 263900
    },
    {
      "epoch": 0.8612618138043304,
      "grad_norm": 0.0004935445031151175,
      "learning_rate": 4.162145585870086e-05,
      "loss": 0.1086,
      "step": 264000
    },
    {
      "epoch": 0.8615880493398624,
      "grad_norm": 0.00019921008788514882,
      "learning_rate": 4.152358519804128e-05,
      "loss": 0.2007,
      "step": 264100
    },
    {
      "epoch": 0.8619142848753943,
      "grad_norm": 0.0024352793116122484,
      "learning_rate": 4.1425714537381694e-05,
      "loss": 0.1405,
      "step": 264200
    },
    {
      "epoch": 0.8622405204109262,
      "grad_norm": 3.796368764596991e-05,
      "learning_rate": 4.1327843876722115e-05,
      "loss": 0.1111,
      "step": 264300
    },
    {
      "epoch": 0.8625667559464583,
      "grad_norm": 40.1026725769043,
      "learning_rate": 4.122997321606253e-05,
      "loss": 0.2866,
      "step": 264400
    },
    {
      "epoch": 0.8628929914819902,
      "grad_norm": 0.0015693618915975094,
      "learning_rate": 4.1132102555402943e-05,
      "loss": 0.1714,
      "step": 264500
    },
    {
      "epoch": 0.8632192270175221,
      "grad_norm": 7.739415013929829e-05,
      "learning_rate": 4.1034231894743365e-05,
      "loss": 0.3778,
      "step": 264600
    },
    {
      "epoch": 0.8635454625530541,
      "grad_norm": 0.013711448758840561,
      "learning_rate": 4.093636123408378e-05,
      "loss": 0.2076,
      "step": 264700
    },
    {
      "epoch": 0.863871698088586,
      "grad_norm": 0.021441923454403877,
      "learning_rate": 4.08384905734242e-05,
      "loss": 0.0943,
      "step": 264800
    },
    {
      "epoch": 0.8641979336241179,
      "grad_norm": 0.0734293982386589,
      "learning_rate": 4.074061991276461e-05,
      "loss": 0.3586,
      "step": 264900
    },
    {
      "epoch": 0.8645241691596499,
      "grad_norm": 0.011443155817687511,
      "learning_rate": 4.064274925210503e-05,
      "loss": 0.0972,
      "step": 265000
    },
    {
      "epoch": 0.8648504046951818,
      "grad_norm": 0.0013486190000548959,
      "learning_rate": 4.054487859144545e-05,
      "loss": 0.2071,
      "step": 265100
    },
    {
      "epoch": 0.8651766402307137,
      "grad_norm": 13.317938804626465,
      "learning_rate": 4.0447007930785864e-05,
      "loss": 0.4076,
      "step": 265200
    },
    {
      "epoch": 0.8655028757662457,
      "grad_norm": 0.0036939254496246576,
      "learning_rate": 4.0349137270126285e-05,
      "loss": 0.3471,
      "step": 265300
    },
    {
      "epoch": 0.8658291113017776,
      "grad_norm": 0.00014218372234608978,
      "learning_rate": 4.025126660946669e-05,
      "loss": 0.1935,
      "step": 265400
    },
    {
      "epoch": 0.8661553468373095,
      "grad_norm": 8.066134614637122e-05,
      "learning_rate": 4.0153395948807114e-05,
      "loss": 0.2165,
      "step": 265500
    },
    {
      "epoch": 0.8664815823728416,
      "grad_norm": 0.0009759859531186521,
      "learning_rate": 4.0055525288147535e-05,
      "loss": 0.2044,
      "step": 265600
    },
    {
      "epoch": 0.8668078179083735,
      "grad_norm": 0.6042236089706421,
      "learning_rate": 3.995765462748795e-05,
      "loss": 0.1102,
      "step": 265700
    },
    {
      "epoch": 0.8671340534439055,
      "grad_norm": 0.0004285690374672413,
      "learning_rate": 3.985978396682837e-05,
      "loss": 0.1839,
      "step": 265800
    },
    {
      "epoch": 0.8674602889794374,
      "grad_norm": 12.148168563842773,
      "learning_rate": 3.9761913306168785e-05,
      "loss": 0.2329,
      "step": 265900
    },
    {
      "epoch": 0.8677865245149693,
      "grad_norm": 0.00017982468125410378,
      "learning_rate": 3.96640426455092e-05,
      "loss": 0.3558,
      "step": 266000
    },
    {
      "epoch": 0.8681127600505013,
      "grad_norm": 38.560699462890625,
      "learning_rate": 3.956617198484962e-05,
      "loss": 0.1769,
      "step": 266100
    },
    {
      "epoch": 0.8684389955860332,
      "grad_norm": 84.48249053955078,
      "learning_rate": 3.9468301324190035e-05,
      "loss": 0.2466,
      "step": 266200
    },
    {
      "epoch": 0.8687652311215651,
      "grad_norm": 0.12158233672380447,
      "learning_rate": 3.9370430663530456e-05,
      "loss": 0.1326,
      "step": 266300
    },
    {
      "epoch": 0.8690914666570971,
      "grad_norm": 0.26972660422325134,
      "learning_rate": 3.927256000287087e-05,
      "loss": 0.2267,
      "step": 266400
    },
    {
      "epoch": 0.869417702192629,
      "grad_norm": 0.0009954903507605195,
      "learning_rate": 3.9174689342211285e-05,
      "loss": 0.3739,
      "step": 266500
    },
    {
      "epoch": 0.8697439377281609,
      "grad_norm": 0.33160755038261414,
      "learning_rate": 3.9076818681551706e-05,
      "loss": 0.1623,
      "step": 266600
    },
    {
      "epoch": 0.870070173263693,
      "grad_norm": 0.002221079310402274,
      "learning_rate": 3.897894802089212e-05,
      "loss": 0.2354,
      "step": 266700
    },
    {
      "epoch": 0.8703964087992249,
      "grad_norm": 0.0012828272301703691,
      "learning_rate": 3.8881077360232535e-05,
      "loss": 0.16,
      "step": 266800
    },
    {
      "epoch": 0.8707226443347568,
      "grad_norm": 0.07761844247579575,
      "learning_rate": 3.8783206699572956e-05,
      "loss": 0.1433,
      "step": 266900
    },
    {
      "epoch": 0.8710488798702888,
      "grad_norm": 1.5118800547497813e-05,
      "learning_rate": 3.868533603891337e-05,
      "loss": 0.3316,
      "step": 267000
    },
    {
      "epoch": 0.8713751154058207,
      "grad_norm": 19.126922607421875,
      "learning_rate": 3.858746537825379e-05,
      "loss": 0.1989,
      "step": 267100
    },
    {
      "epoch": 0.8717013509413526,
      "grad_norm": 11.60247802734375,
      "learning_rate": 3.84895947175942e-05,
      "loss": 0.2733,
      "step": 267200
    },
    {
      "epoch": 0.8720275864768846,
      "grad_norm": 4.699522105511278e-05,
      "learning_rate": 3.839172405693462e-05,
      "loss": 0.1897,
      "step": 267300
    },
    {
      "epoch": 0.8723538220124165,
      "grad_norm": 0.0035455890465527773,
      "learning_rate": 3.829385339627504e-05,
      "loss": 0.1863,
      "step": 267400
    },
    {
      "epoch": 0.8726800575479484,
      "grad_norm": 7.09619271219708e-05,
      "learning_rate": 3.8195982735615455e-05,
      "loss": 0.3597,
      "step": 267500
    },
    {
      "epoch": 0.8730062930834804,
      "grad_norm": 0.03167615085840225,
      "learning_rate": 3.8098112074955877e-05,
      "loss": 0.3229,
      "step": 267600
    },
    {
      "epoch": 0.8733325286190123,
      "grad_norm": 0.0011022427352145314,
      "learning_rate": 3.8000241414296284e-05,
      "loss": 0.2157,
      "step": 267700
    },
    {
      "epoch": 0.8736587641545442,
      "grad_norm": 0.0007428723038174212,
      "learning_rate": 3.7902370753636705e-05,
      "loss": 0.203,
      "step": 267800
    },
    {
      "epoch": 0.8739849996900763,
      "grad_norm": 11.715108871459961,
      "learning_rate": 3.7804500092977126e-05,
      "loss": 0.1676,
      "step": 267900
    },
    {
      "epoch": 0.8743112352256082,
      "grad_norm": 0.016666147857904434,
      "learning_rate": 3.770662943231754e-05,
      "loss": 0.1445,
      "step": 268000
    },
    {
      "epoch": 0.8746374707611402,
      "grad_norm": 0.00029507422004826367,
      "learning_rate": 3.760875877165796e-05,
      "loss": 0.1832,
      "step": 268100
    },
    {
      "epoch": 0.8749637062966721,
      "grad_norm": 0.47538748383522034,
      "learning_rate": 3.751088811099837e-05,
      "loss": 0.2447,
      "step": 268200
    },
    {
      "epoch": 0.875289941832204,
      "grad_norm": 0.0399896539747715,
      "learning_rate": 3.741301745033879e-05,
      "loss": 0.0978,
      "step": 268300
    },
    {
      "epoch": 0.875616177367736,
      "grad_norm": 5.320606231689453,
      "learning_rate": 3.7315146789679205e-05,
      "loss": 0.2414,
      "step": 268400
    },
    {
      "epoch": 0.8759424129032679,
      "grad_norm": 0.0004587498551700264,
      "learning_rate": 3.7217276129019626e-05,
      "loss": 0.2443,
      "step": 268500
    },
    {
      "epoch": 0.8762686484387998,
      "grad_norm": 0.00014126468158792704,
      "learning_rate": 3.711940546836005e-05,
      "loss": 0.2535,
      "step": 268600
    },
    {
      "epoch": 0.8765948839743318,
      "grad_norm": 0.00022111806902103126,
      "learning_rate": 3.702153480770046e-05,
      "loss": 0.1785,
      "step": 268700
    },
    {
      "epoch": 0.8769211195098637,
      "grad_norm": 0.0015814697835594416,
      "learning_rate": 3.6923664147040876e-05,
      "loss": 0.1991,
      "step": 268800
    },
    {
      "epoch": 0.8772473550453956,
      "grad_norm": 0.011877370066940784,
      "learning_rate": 3.682579348638129e-05,
      "loss": 0.1055,
      "step": 268900
    },
    {
      "epoch": 0.8775735905809277,
      "grad_norm": 56.84873962402344,
      "learning_rate": 3.672792282572171e-05,
      "loss": 0.3196,
      "step": 269000
    },
    {
      "epoch": 0.8778998261164596,
      "grad_norm": 5.4347901823348366e-06,
      "learning_rate": 3.663005216506213e-05,
      "loss": 0.2567,
      "step": 269100
    },
    {
      "epoch": 0.8782260616519915,
      "grad_norm": 0.0020173280499875546,
      "learning_rate": 3.653218150440255e-05,
      "loss": 0.2986,
      "step": 269200
    },
    {
      "epoch": 0.8785522971875235,
      "grad_norm": 0.0005755203892476857,
      "learning_rate": 3.643431084374296e-05,
      "loss": 0.3234,
      "step": 269300
    },
    {
      "epoch": 0.8788785327230554,
      "grad_norm": 0.6562125086784363,
      "learning_rate": 3.6336440183083376e-05,
      "loss": 0.1653,
      "step": 269400
    },
    {
      "epoch": 0.8792047682585873,
      "grad_norm": 0.0009688390418887138,
      "learning_rate": 3.62385695224238e-05,
      "loss": 0.1225,
      "step": 269500
    },
    {
      "epoch": 0.8795310037941193,
      "grad_norm": 0.00019535196770448238,
      "learning_rate": 3.614069886176421e-05,
      "loss": 0.3158,
      "step": 269600
    },
    {
      "epoch": 0.8798572393296512,
      "grad_norm": 0.006652816664427519,
      "learning_rate": 3.604282820110463e-05,
      "loss": 0.103,
      "step": 269700
    },
    {
      "epoch": 0.8801834748651831,
      "grad_norm": 0.005826193373650312,
      "learning_rate": 3.594495754044505e-05,
      "loss": 0.1871,
      "step": 269800
    },
    {
      "epoch": 0.8805097104007151,
      "grad_norm": 42.8786735534668,
      "learning_rate": 3.584708687978546e-05,
      "loss": 0.1313,
      "step": 269900
    },
    {
      "epoch": 0.880835945936247,
      "grad_norm": 0.518867015838623,
      "learning_rate": 3.574921621912588e-05,
      "loss": 0.2859,
      "step": 270000
    },
    {
      "epoch": 0.881162181471779,
      "grad_norm": 0.004023743327707052,
      "learning_rate": 3.5651345558466297e-05,
      "loss": 0.1554,
      "step": 270100
    },
    {
      "epoch": 0.881488417007311,
      "grad_norm": 104.79326629638672,
      "learning_rate": 3.555347489780672e-05,
      "loss": 0.2528,
      "step": 270200
    },
    {
      "epoch": 0.8818146525428429,
      "grad_norm": 0.00490884343162179,
      "learning_rate": 3.545560423714713e-05,
      "loss": 0.4736,
      "step": 270300
    },
    {
      "epoch": 0.8821408880783749,
      "grad_norm": 0.0008386379922740161,
      "learning_rate": 3.535773357648755e-05,
      "loss": 0.2409,
      "step": 270400
    },
    {
      "epoch": 0.8824671236139068,
      "grad_norm": 45.512451171875,
      "learning_rate": 3.525986291582797e-05,
      "loss": 0.1479,
      "step": 270500
    },
    {
      "epoch": 0.8827933591494387,
      "grad_norm": 0.000860783678945154,
      "learning_rate": 3.516199225516838e-05,
      "loss": 0.1461,
      "step": 270600
    },
    {
      "epoch": 0.8831195946849707,
      "grad_norm": 0.0033138918224722147,
      "learning_rate": 3.50641215945088e-05,
      "loss": 0.1672,
      "step": 270700
    },
    {
      "epoch": 0.8834458302205026,
      "grad_norm": 57.90337371826172,
      "learning_rate": 3.496625093384922e-05,
      "loss": 0.1986,
      "step": 270800
    },
    {
      "epoch": 0.8837720657560345,
      "grad_norm": 0.0007586029241792858,
      "learning_rate": 3.486838027318964e-05,
      "loss": 0.1126,
      "step": 270900
    },
    {
      "epoch": 0.8840983012915665,
      "grad_norm": 49.33658218383789,
      "learning_rate": 3.477050961253005e-05,
      "loss": 0.2068,
      "step": 271000
    },
    {
      "epoch": 0.8844245368270984,
      "grad_norm": 0.00023836341279093176,
      "learning_rate": 3.467263895187047e-05,
      "loss": 0.3459,
      "step": 271100
    },
    {
      "epoch": 0.8847507723626303,
      "grad_norm": 0.005535521544516087,
      "learning_rate": 3.457476829121088e-05,
      "loss": 0.3248,
      "step": 271200
    },
    {
      "epoch": 0.8850770078981623,
      "grad_norm": 45.81113815307617,
      "learning_rate": 3.44768976305513e-05,
      "loss": 0.2655,
      "step": 271300
    },
    {
      "epoch": 0.8854032434336943,
      "grad_norm": 0.0007534669130109251,
      "learning_rate": 3.4379026969891724e-05,
      "loss": 0.2752,
      "step": 271400
    },
    {
      "epoch": 0.8857294789692262,
      "grad_norm": 0.0037174082826822996,
      "learning_rate": 3.428115630923214e-05,
      "loss": 0.1876,
      "step": 271500
    },
    {
      "epoch": 0.8860557145047582,
      "grad_norm": 7.760324954986572,
      "learning_rate": 3.418328564857255e-05,
      "loss": 0.3307,
      "step": 271600
    },
    {
      "epoch": 0.8863819500402901,
      "grad_norm": 0.00020270053937565535,
      "learning_rate": 3.408541498791297e-05,
      "loss": 0.2246,
      "step": 271700
    },
    {
      "epoch": 0.886708185575822,
      "grad_norm": 0.0002137379633495584,
      "learning_rate": 3.398754432725339e-05,
      "loss": 0.209,
      "step": 271800
    },
    {
      "epoch": 0.887034421111354,
      "grad_norm": 11.677858352661133,
      "learning_rate": 3.38896736665938e-05,
      "loss": 0.1838,
      "step": 271900
    },
    {
      "epoch": 0.8873606566468859,
      "grad_norm": 1.5227683782577515,
      "learning_rate": 3.3791803005934224e-05,
      "loss": 0.3756,
      "step": 272000
    },
    {
      "epoch": 0.8876868921824178,
      "grad_norm": 53.25575637817383,
      "learning_rate": 3.369393234527464e-05,
      "loss": 0.1206,
      "step": 272100
    },
    {
      "epoch": 0.8880131277179498,
      "grad_norm": 0.4197939336299896,
      "learning_rate": 3.359606168461505e-05,
      "loss": 0.3045,
      "step": 272200
    },
    {
      "epoch": 0.8883393632534817,
      "grad_norm": 0.00784236192703247,
      "learning_rate": 3.349819102395547e-05,
      "loss": 0.2656,
      "step": 272300
    },
    {
      "epoch": 0.8886655987890137,
      "grad_norm": 0.025856537744402885,
      "learning_rate": 3.340032036329589e-05,
      "loss": 0.2407,
      "step": 272400
    },
    {
      "epoch": 0.8889918343245456,
      "grad_norm": 2.5262839699280448e-05,
      "learning_rate": 3.330244970263631e-05,
      "loss": 0.1579,
      "step": 272500
    },
    {
      "epoch": 0.8893180698600776,
      "grad_norm": 0.00014815424219705164,
      "learning_rate": 3.320457904197672e-05,
      "loss": 0.3102,
      "step": 272600
    },
    {
      "epoch": 0.8896443053956096,
      "grad_norm": 0.0006655327160842717,
      "learning_rate": 3.310670838131714e-05,
      "loss": 0.1734,
      "step": 272700
    },
    {
      "epoch": 0.8899705409311415,
      "grad_norm": 0.012032163329422474,
      "learning_rate": 3.300883772065756e-05,
      "loss": 0.2644,
      "step": 272800
    },
    {
      "epoch": 0.8902967764666734,
      "grad_norm": 0.0032157821115106344,
      "learning_rate": 3.291096705999797e-05,
      "loss": 0.1014,
      "step": 272900
    },
    {
      "epoch": 0.8906230120022054,
      "grad_norm": 0.8894277811050415,
      "learning_rate": 3.2813096399338394e-05,
      "loss": 0.1146,
      "step": 273000
    },
    {
      "epoch": 0.8909492475377373,
      "grad_norm": 0.15863150358200073,
      "learning_rate": 3.271522573867881e-05,
      "loss": 0.2334,
      "step": 273100
    },
    {
      "epoch": 0.8912754830732692,
      "grad_norm": 0.0018397325184196234,
      "learning_rate": 3.261735507801922e-05,
      "loss": 0.202,
      "step": 273200
    },
    {
      "epoch": 0.8916017186088012,
      "grad_norm": 3.2390682463301346e-05,
      "learning_rate": 3.2519484417359644e-05,
      "loss": 0.1541,
      "step": 273300
    },
    {
      "epoch": 0.8919279541443331,
      "grad_norm": 0.0002868626033887267,
      "learning_rate": 3.242161375670006e-05,
      "loss": 0.2501,
      "step": 273400
    },
    {
      "epoch": 0.892254189679865,
      "grad_norm": 0.031270090490579605,
      "learning_rate": 3.232374309604047e-05,
      "loss": 0.1622,
      "step": 273500
    },
    {
      "epoch": 0.892580425215397,
      "grad_norm": 0.006449952255934477,
      "learning_rate": 3.2225872435380894e-05,
      "loss": 0.1255,
      "step": 273600
    },
    {
      "epoch": 0.892906660750929,
      "grad_norm": 1.6960649490356445,
      "learning_rate": 3.2128001774721315e-05,
      "loss": 0.1921,
      "step": 273700
    },
    {
      "epoch": 0.8932328962864609,
      "grad_norm": 0.0014104448491707444,
      "learning_rate": 3.203013111406173e-05,
      "loss": 0.2806,
      "step": 273800
    },
    {
      "epoch": 0.8935591318219929,
      "grad_norm": 0.00022102172079030424,
      "learning_rate": 3.1932260453402144e-05,
      "loss": 0.204,
      "step": 273900
    },
    {
      "epoch": 0.8938853673575248,
      "grad_norm": 0.0008565213647671044,
      "learning_rate": 3.183438979274256e-05,
      "loss": 0.2347,
      "step": 274000
    },
    {
      "epoch": 0.8942116028930567,
      "grad_norm": 0.022954246029257774,
      "learning_rate": 3.173651913208298e-05,
      "loss": 0.1388,
      "step": 274100
    },
    {
      "epoch": 0.8945378384285887,
      "grad_norm": 1.2438201904296875,
      "learning_rate": 3.16386484714234e-05,
      "loss": 0.1708,
      "step": 274200
    },
    {
      "epoch": 0.8948640739641206,
      "grad_norm": 0.00014920912508387119,
      "learning_rate": 3.1540777810763815e-05,
      "loss": 0.1337,
      "step": 274300
    },
    {
      "epoch": 0.8951903094996526,
      "grad_norm": 0.005150752607733011,
      "learning_rate": 3.144290715010423e-05,
      "loss": 0.1099,
      "step": 274400
    },
    {
      "epoch": 0.8955165450351845,
      "grad_norm": 0.00020721649343613535,
      "learning_rate": 3.1345036489444643e-05,
      "loss": 0.1533,
      "step": 274500
    },
    {
      "epoch": 0.8958427805707164,
      "grad_norm": 0.001231953501701355,
      "learning_rate": 3.1247165828785065e-05,
      "loss": 0.1706,
      "step": 274600
    },
    {
      "epoch": 0.8961690161062484,
      "grad_norm": 0.008813099004328251,
      "learning_rate": 3.114929516812548e-05,
      "loss": 0.1033,
      "step": 274700
    },
    {
      "epoch": 0.8964952516417803,
      "grad_norm": 7.183039269875735e-05,
      "learning_rate": 3.10514245074659e-05,
      "loss": 0.28,
      "step": 274800
    },
    {
      "epoch": 0.8968214871773122,
      "grad_norm": 0.00012991322728339583,
      "learning_rate": 3.0953553846806314e-05,
      "loss": 0.2352,
      "step": 274900
    },
    {
      "epoch": 0.8971477227128443,
      "grad_norm": 0.001049446756951511,
      "learning_rate": 3.085568318614673e-05,
      "loss": 0.2389,
      "step": 275000
    },
    {
      "epoch": 0.8974739582483762,
      "grad_norm": 0.00018186525267083198,
      "learning_rate": 3.075781252548715e-05,
      "loss": 0.355,
      "step": 275100
    },
    {
      "epoch": 0.8978001937839081,
      "grad_norm": 0.0004571423924062401,
      "learning_rate": 3.0659941864827564e-05,
      "loss": 0.2499,
      "step": 275200
    },
    {
      "epoch": 0.8981264293194401,
      "grad_norm": 0.013688897714018822,
      "learning_rate": 3.0562071204167985e-05,
      "loss": 0.2106,
      "step": 275300
    },
    {
      "epoch": 0.898452664854972,
      "grad_norm": 0.02981770969927311,
      "learning_rate": 3.04642005435084e-05,
      "loss": 0.2092,
      "step": 275400
    },
    {
      "epoch": 0.8987789003905039,
      "grad_norm": 2.3293699996429496e-05,
      "learning_rate": 3.0366329882848814e-05,
      "loss": 0.1502,
      "step": 275500
    },
    {
      "epoch": 0.8991051359260359,
      "grad_norm": 0.28176233172416687,
      "learning_rate": 3.0268459222189235e-05,
      "loss": 0.1398,
      "step": 275600
    },
    {
      "epoch": 0.8994313714615678,
      "grad_norm": 0.0023503205738961697,
      "learning_rate": 3.0170588561529653e-05,
      "loss": 0.1228,
      "step": 275700
    },
    {
      "epoch": 0.8997576069970997,
      "grad_norm": 1.835067268984858e-05,
      "learning_rate": 3.0072717900870067e-05,
      "loss": 0.1679,
      "step": 275800
    },
    {
      "epoch": 0.9000838425326317,
      "grad_norm": 0.17256547510623932,
      "learning_rate": 2.9974847240210485e-05,
      "loss": 0.1406,
      "step": 275900
    },
    {
      "epoch": 0.9004100780681636,
      "grad_norm": 0.0006348143797367811,
      "learning_rate": 2.98769765795509e-05,
      "loss": 0.3184,
      "step": 276000
    },
    {
      "epoch": 0.9007363136036955,
      "grad_norm": 0.0004626328300219029,
      "learning_rate": 2.977910591889132e-05,
      "loss": 0.0506,
      "step": 276100
    },
    {
      "epoch": 0.9010625491392276,
      "grad_norm": 8.19946035335306e-06,
      "learning_rate": 2.9681235258231735e-05,
      "loss": 0.2516,
      "step": 276200
    },
    {
      "epoch": 0.9013887846747595,
      "grad_norm": 0.29309260845184326,
      "learning_rate": 2.9583364597572153e-05,
      "loss": 0.1868,
      "step": 276300
    },
    {
      "epoch": 0.9017150202102914,
      "grad_norm": 1.3704181909561157,
      "learning_rate": 2.9485493936912567e-05,
      "loss": 0.378,
      "step": 276400
    },
    {
      "epoch": 0.9020412557458234,
      "grad_norm": 0.00042139962897635996,
      "learning_rate": 2.9387623276252985e-05,
      "loss": 0.1837,
      "step": 276500
    },
    {
      "epoch": 0.9023674912813553,
      "grad_norm": 0.0022527070250362158,
      "learning_rate": 2.9289752615593406e-05,
      "loss": 0.2213,
      "step": 276600
    },
    {
      "epoch": 0.9026937268168873,
      "grad_norm": 41.854278564453125,
      "learning_rate": 2.919188195493382e-05,
      "loss": 0.1987,
      "step": 276700
    },
    {
      "epoch": 0.9030199623524192,
      "grad_norm": 0.04134255275130272,
      "learning_rate": 2.9094011294274238e-05,
      "loss": 0.2878,
      "step": 276800
    },
    {
      "epoch": 0.9033461978879511,
      "grad_norm": 0.000258694461081177,
      "learning_rate": 2.8996140633614652e-05,
      "loss": 0.2698,
      "step": 276900
    },
    {
      "epoch": 0.9036724334234831,
      "grad_norm": 0.004933690186589956,
      "learning_rate": 2.889826997295507e-05,
      "loss": 0.2821,
      "step": 277000
    },
    {
      "epoch": 0.903998668959015,
      "grad_norm": 0.0003589889674913138,
      "learning_rate": 2.880039931229549e-05,
      "loss": 0.0952,
      "step": 277100
    },
    {
      "epoch": 0.9043249044945469,
      "grad_norm": 0.00015865141176618636,
      "learning_rate": 2.8702528651635906e-05,
      "loss": 0.1735,
      "step": 277200
    },
    {
      "epoch": 0.904651140030079,
      "grad_norm": 0.0006067636422812939,
      "learning_rate": 2.8604657990976323e-05,
      "loss": 0.1922,
      "step": 277300
    },
    {
      "epoch": 0.9049773755656109,
      "grad_norm": 24.681804656982422,
      "learning_rate": 2.8506787330316738e-05,
      "loss": 0.159,
      "step": 277400
    },
    {
      "epoch": 0.9053036111011428,
      "grad_norm": 0.0008683932246640325,
      "learning_rate": 2.840891666965716e-05,
      "loss": 0.2202,
      "step": 277500
    },
    {
      "epoch": 0.9056298466366748,
      "grad_norm": 0.002931607188656926,
      "learning_rate": 2.8311046008997573e-05,
      "loss": 0.2586,
      "step": 277600
    },
    {
      "epoch": 0.9059560821722067,
      "grad_norm": 0.00012042484013363719,
      "learning_rate": 2.821317534833799e-05,
      "loss": 0.1744,
      "step": 277700
    },
    {
      "epoch": 0.9062823177077386,
      "grad_norm": 0.00043657064088620245,
      "learning_rate": 2.8115304687678405e-05,
      "loss": 0.0119,
      "step": 277800
    },
    {
      "epoch": 0.9066085532432706,
      "grad_norm": 0.0004583780246321112,
      "learning_rate": 2.8017434027018823e-05,
      "loss": 0.1616,
      "step": 277900
    },
    {
      "epoch": 0.9069347887788025,
      "grad_norm": 0.0013391072861850262,
      "learning_rate": 2.7919563366359244e-05,
      "loss": 0.2894,
      "step": 278000
    },
    {
      "epoch": 0.9072610243143344,
      "grad_norm": 0.0025930767878890038,
      "learning_rate": 2.782169270569966e-05,
      "loss": 0.2468,
      "step": 278100
    },
    {
      "epoch": 0.9075872598498664,
      "grad_norm": 0.006261387374252081,
      "learning_rate": 2.7723822045040076e-05,
      "loss": 0.1413,
      "step": 278200
    },
    {
      "epoch": 0.9079134953853983,
      "grad_norm": 0.011354867368936539,
      "learning_rate": 2.762595138438049e-05,
      "loss": 0.1486,
      "step": 278300
    },
    {
      "epoch": 0.9082397309209302,
      "grad_norm": 0.0006368542672134936,
      "learning_rate": 2.752808072372091e-05,
      "loss": 0.2899,
      "step": 278400
    },
    {
      "epoch": 0.9085659664564623,
      "grad_norm": 0.00012877993867732584,
      "learning_rate": 2.743021006306133e-05,
      "loss": 0.2961,
      "step": 278500
    },
    {
      "epoch": 0.9088922019919942,
      "grad_norm": 0.00026434045867063105,
      "learning_rate": 2.7332339402401744e-05,
      "loss": 0.2485,
      "step": 278600
    },
    {
      "epoch": 0.9092184375275261,
      "grad_norm": 0.0012492869282141328,
      "learning_rate": 2.723446874174216e-05,
      "loss": 0.2463,
      "step": 278700
    },
    {
      "epoch": 0.9095446730630581,
      "grad_norm": 0.0016054395819082856,
      "learning_rate": 2.7136598081082576e-05,
      "loss": 0.2125,
      "step": 278800
    },
    {
      "epoch": 0.90987090859859,
      "grad_norm": 0.0021721681114286184,
      "learning_rate": 2.7038727420422994e-05,
      "loss": 0.2174,
      "step": 278900
    },
    {
      "epoch": 0.910197144134122,
      "grad_norm": 0.001637840992771089,
      "learning_rate": 2.694085675976341e-05,
      "loss": 0.3101,
      "step": 279000
    },
    {
      "epoch": 0.9105233796696539,
      "grad_norm": 17.42409324645996,
      "learning_rate": 2.684298609910383e-05,
      "loss": 0.1235,
      "step": 279100
    },
    {
      "epoch": 0.9108496152051858,
      "grad_norm": 0.005438021384179592,
      "learning_rate": 2.6745115438444244e-05,
      "loss": 0.0956,
      "step": 279200
    },
    {
      "epoch": 0.9111758507407178,
      "grad_norm": 0.20989097654819489,
      "learning_rate": 2.664724477778466e-05,
      "loss": 0.2491,
      "step": 279300
    },
    {
      "epoch": 0.9115020862762497,
      "grad_norm": 0.0034985439851880074,
      "learning_rate": 2.6549374117125083e-05,
      "loss": 0.1215,
      "step": 279400
    },
    {
      "epoch": 0.9118283218117816,
      "grad_norm": 0.07567470520734787,
      "learning_rate": 2.6451503456465497e-05,
      "loss": 0.1906,
      "step": 279500
    },
    {
      "epoch": 0.9121545573473137,
      "grad_norm": 8.292359352111816,
      "learning_rate": 2.6353632795805915e-05,
      "loss": 0.1451,
      "step": 279600
    },
    {
      "epoch": 0.9124807928828456,
      "grad_norm": 0.00202918890863657,
      "learning_rate": 2.625576213514633e-05,
      "loss": 0.1849,
      "step": 279700
    },
    {
      "epoch": 0.9128070284183775,
      "grad_norm": 0.02232760190963745,
      "learning_rate": 2.6157891474486747e-05,
      "loss": 0.1976,
      "step": 279800
    },
    {
      "epoch": 0.9131332639539095,
      "grad_norm": 0.3027176260948181,
      "learning_rate": 2.6060020813827168e-05,
      "loss": 0.2179,
      "step": 279900
    },
    {
      "epoch": 0.9134594994894414,
      "grad_norm": 0.0013965705875307322,
      "learning_rate": 2.5962150153167582e-05,
      "loss": 0.1584,
      "step": 280000
    },
    {
      "epoch": 0.9137857350249733,
      "grad_norm": 0.000591817544773221,
      "learning_rate": 2.5864279492508e-05,
      "loss": 0.1918,
      "step": 280100
    },
    {
      "epoch": 0.9141119705605053,
      "grad_norm": 15.200303077697754,
      "learning_rate": 2.5766408831848414e-05,
      "loss": 0.2933,
      "step": 280200
    },
    {
      "epoch": 0.9144382060960372,
      "grad_norm": 9.625891834730282e-05,
      "learning_rate": 2.5668538171188832e-05,
      "loss": 0.1669,
      "step": 280300
    },
    {
      "epoch": 0.9147644416315691,
      "grad_norm": 0.000219389155972749,
      "learning_rate": 2.557066751052925e-05,
      "loss": 0.2786,
      "step": 280400
    },
    {
      "epoch": 0.9150906771671011,
      "grad_norm": 0.0009837044635787606,
      "learning_rate": 2.5472796849869668e-05,
      "loss": 0.1135,
      "step": 280500
    },
    {
      "epoch": 0.915416912702633,
      "grad_norm": 4.435650771483779e-05,
      "learning_rate": 2.5374926189210082e-05,
      "loss": 0.2243,
      "step": 280600
    },
    {
      "epoch": 0.9157431482381649,
      "grad_norm": 0.05688776820898056,
      "learning_rate": 2.52770555285505e-05,
      "loss": 0.2284,
      "step": 280700
    },
    {
      "epoch": 0.916069383773697,
      "grad_norm": 0.006148039363324642,
      "learning_rate": 2.517918486789092e-05,
      "loss": 0.1763,
      "step": 280800
    },
    {
      "epoch": 0.9163956193092289,
      "grad_norm": 0.0010026750387623906,
      "learning_rate": 2.5081314207231335e-05,
      "loss": 0.2272,
      "step": 280900
    },
    {
      "epoch": 0.9167218548447609,
      "grad_norm": 0.0010253764921799302,
      "learning_rate": 2.4983443546571753e-05,
      "loss": 0.1512,
      "step": 281000
    },
    {
      "epoch": 0.9170480903802928,
      "grad_norm": 5.09911187691614e-05,
      "learning_rate": 2.4885572885912167e-05,
      "loss": 0.3085,
      "step": 281100
    },
    {
      "epoch": 0.9173743259158247,
      "grad_norm": 38.07352828979492,
      "learning_rate": 2.4787702225252585e-05,
      "loss": 0.224,
      "step": 281200
    },
    {
      "epoch": 0.9177005614513567,
      "grad_norm": 0.007301805540919304,
      "learning_rate": 2.4689831564593006e-05,
      "loss": 0.1033,
      "step": 281300
    },
    {
      "epoch": 0.9180267969868886,
      "grad_norm": 0.00044351292308419943,
      "learning_rate": 2.459196090393342e-05,
      "loss": 0.2144,
      "step": 281400
    },
    {
      "epoch": 0.9183530325224205,
      "grad_norm": 0.0002045562578132376,
      "learning_rate": 2.4494090243273838e-05,
      "loss": 0.1727,
      "step": 281500
    },
    {
      "epoch": 0.9186792680579525,
      "grad_norm": 0.025930339470505714,
      "learning_rate": 2.4396219582614253e-05,
      "loss": 0.1925,
      "step": 281600
    },
    {
      "epoch": 0.9190055035934844,
      "grad_norm": 5.056618829257786e-05,
      "learning_rate": 2.429834892195467e-05,
      "loss": 0.1283,
      "step": 281700
    },
    {
      "epoch": 0.9193317391290163,
      "grad_norm": 0.007221020292490721,
      "learning_rate": 2.4200478261295088e-05,
      "loss": 0.1367,
      "step": 281800
    },
    {
      "epoch": 0.9196579746645483,
      "grad_norm": 0.00019433055422268808,
      "learning_rate": 2.4102607600635506e-05,
      "loss": 0.1462,
      "step": 281900
    },
    {
      "epoch": 0.9199842102000803,
      "grad_norm": 0.0004546199052128941,
      "learning_rate": 2.400473693997592e-05,
      "loss": 0.1991,
      "step": 282000
    },
    {
      "epoch": 0.9203104457356122,
      "grad_norm": 0.13902544975280762,
      "learning_rate": 2.3906866279316338e-05,
      "loss": 0.2388,
      "step": 282100
    },
    {
      "epoch": 0.9206366812711442,
      "grad_norm": 0.0003847144835162908,
      "learning_rate": 2.3808995618656752e-05,
      "loss": 0.2367,
      "step": 282200
    },
    {
      "epoch": 0.9209629168066761,
      "grad_norm": 15.382696151733398,
      "learning_rate": 2.3711124957997173e-05,
      "loss": 0.2506,
      "step": 282300
    },
    {
      "epoch": 0.921289152342208,
      "grad_norm": 0.03311801701784134,
      "learning_rate": 2.361325429733759e-05,
      "loss": 0.2624,
      "step": 282400
    },
    {
      "epoch": 0.92161538787774,
      "grad_norm": 0.006012732163071632,
      "learning_rate": 2.3515383636678006e-05,
      "loss": 0.1652,
      "step": 282500
    },
    {
      "epoch": 0.9219416234132719,
      "grad_norm": 0.0042701964266598225,
      "learning_rate": 2.3417512976018423e-05,
      "loss": 0.1817,
      "step": 282600
    },
    {
      "epoch": 0.9222678589488038,
      "grad_norm": 0.0004139644734095782,
      "learning_rate": 2.331964231535884e-05,
      "loss": 0.1384,
      "step": 282700
    },
    {
      "epoch": 0.9225940944843358,
      "grad_norm": 0.0007929588900879025,
      "learning_rate": 2.322177165469926e-05,
      "loss": 0.2293,
      "step": 282800
    },
    {
      "epoch": 0.9229203300198677,
      "grad_norm": 0.09862104803323746,
      "learning_rate": 2.3123900994039677e-05,
      "loss": 0.0873,
      "step": 282900
    },
    {
      "epoch": 0.9232465655553996,
      "grad_norm": 1.822690319386311e-05,
      "learning_rate": 2.302603033338009e-05,
      "loss": 0.1117,
      "step": 283000
    },
    {
      "epoch": 0.9235728010909316,
      "grad_norm": 0.000270408287178725,
      "learning_rate": 2.292815967272051e-05,
      "loss": 0.2303,
      "step": 283100
    },
    {
      "epoch": 0.9238990366264636,
      "grad_norm": 0.007798025384545326,
      "learning_rate": 2.2830289012060926e-05,
      "loss": 0.2947,
      "step": 283200
    },
    {
      "epoch": 0.9242252721619956,
      "grad_norm": 0.0017562038265168667,
      "learning_rate": 2.2732418351401344e-05,
      "loss": 0.1411,
      "step": 283300
    },
    {
      "epoch": 0.9245515076975275,
      "grad_norm": 0.06413286179304123,
      "learning_rate": 2.263454769074176e-05,
      "loss": 0.1902,
      "step": 283400
    },
    {
      "epoch": 0.9248777432330594,
      "grad_norm": 5.704256909666583e-05,
      "learning_rate": 2.2536677030082176e-05,
      "loss": 0.2536,
      "step": 283500
    },
    {
      "epoch": 0.9252039787685914,
      "grad_norm": 0.0057107675820589066,
      "learning_rate": 2.243880636942259e-05,
      "loss": 0.1691,
      "step": 283600
    },
    {
      "epoch": 0.9255302143041233,
      "grad_norm": 0.029080361127853394,
      "learning_rate": 2.2340935708763012e-05,
      "loss": 0.2905,
      "step": 283700
    },
    {
      "epoch": 0.9258564498396552,
      "grad_norm": 0.0009517325088381767,
      "learning_rate": 2.224306504810343e-05,
      "loss": 0.271,
      "step": 283800
    },
    {
      "epoch": 0.9261826853751872,
      "grad_norm": 0.00031734906951896846,
      "learning_rate": 2.2145194387443844e-05,
      "loss": 0.2319,
      "step": 283900
    },
    {
      "epoch": 0.9265089209107191,
      "grad_norm": 3.4114211302949116e-05,
      "learning_rate": 2.204732372678426e-05,
      "loss": 0.3427,
      "step": 284000
    },
    {
      "epoch": 0.926835156446251,
      "grad_norm": 0.004045541863888502,
      "learning_rate": 2.1949453066124676e-05,
      "loss": 0.1476,
      "step": 284100
    },
    {
      "epoch": 0.927161391981783,
      "grad_norm": 0.0007461047498509288,
      "learning_rate": 2.1851582405465097e-05,
      "loss": 0.1713,
      "step": 284200
    },
    {
      "epoch": 0.927487627517315,
      "grad_norm": 0.00100520474370569,
      "learning_rate": 2.1753711744805515e-05,
      "loss": 0.3302,
      "step": 284300
    },
    {
      "epoch": 0.9278138630528469,
      "grad_norm": 0.049318309873342514,
      "learning_rate": 2.165584108414593e-05,
      "loss": 0.262,
      "step": 284400
    },
    {
      "epoch": 0.9281400985883789,
      "grad_norm": 0.21441605687141418,
      "learning_rate": 2.1557970423486347e-05,
      "loss": 0.1417,
      "step": 284500
    },
    {
      "epoch": 0.9284663341239108,
      "grad_norm": 15.164246559143066,
      "learning_rate": 2.1460099762826765e-05,
      "loss": 0.2227,
      "step": 284600
    },
    {
      "epoch": 0.9287925696594427,
      "grad_norm": 8.18861008156091e-05,
      "learning_rate": 2.1362229102167182e-05,
      "loss": 0.2839,
      "step": 284700
    },
    {
      "epoch": 0.9291188051949747,
      "grad_norm": 0.004574968479573727,
      "learning_rate": 2.1264358441507597e-05,
      "loss": 0.1281,
      "step": 284800
    },
    {
      "epoch": 0.9294450407305066,
      "grad_norm": 0.0009486681665293872,
      "learning_rate": 2.1166487780848014e-05,
      "loss": 0.1856,
      "step": 284900
    },
    {
      "epoch": 0.9297712762660385,
      "grad_norm": 0.0008829455473460257,
      "learning_rate": 2.106861712018843e-05,
      "loss": 0.1795,
      "step": 285000
    },
    {
      "epoch": 0.9300975118015705,
      "grad_norm": 0.0005732905701734126,
      "learning_rate": 2.097074645952885e-05,
      "loss": 0.1634,
      "step": 285100
    },
    {
      "epoch": 0.9304237473371024,
      "grad_norm": 0.007916557602584362,
      "learning_rate": 2.0872875798869268e-05,
      "loss": 0.2522,
      "step": 285200
    },
    {
      "epoch": 0.9307499828726343,
      "grad_norm": 0.0023971081245690584,
      "learning_rate": 2.0775005138209682e-05,
      "loss": 0.2111,
      "step": 285300
    },
    {
      "epoch": 0.9310762184081663,
      "grad_norm": 0.00029314120183698833,
      "learning_rate": 2.06771344775501e-05,
      "loss": 0.1568,
      "step": 285400
    },
    {
      "epoch": 0.9314024539436982,
      "grad_norm": 0.00020142820721957833,
      "learning_rate": 2.0579263816890514e-05,
      "loss": 0.1676,
      "step": 285500
    },
    {
      "epoch": 0.9317286894792303,
      "grad_norm": 0.0002934308140538633,
      "learning_rate": 2.0481393156230935e-05,
      "loss": 0.1757,
      "step": 285600
    },
    {
      "epoch": 0.9320549250147622,
      "grad_norm": 0.0003866800107061863,
      "learning_rate": 2.038352249557135e-05,
      "loss": 0.3564,
      "step": 285700
    },
    {
      "epoch": 0.9323811605502941,
      "grad_norm": 0.0005952081992290914,
      "learning_rate": 2.0285651834911767e-05,
      "loss": 0.25,
      "step": 285800
    },
    {
      "epoch": 0.9327073960858261,
      "grad_norm": 4.604531932272948e-05,
      "learning_rate": 2.0187781174252185e-05,
      "loss": 0.3201,
      "step": 285900
    },
    {
      "epoch": 0.933033631621358,
      "grad_norm": 138.45684814453125,
      "learning_rate": 2.0089910513592603e-05,
      "loss": 0.4357,
      "step": 286000
    },
    {
      "epoch": 0.9333598671568899,
      "grad_norm": 0.00033250387059524655,
      "learning_rate": 1.999203985293302e-05,
      "loss": 0.2516,
      "step": 286100
    },
    {
      "epoch": 0.9336861026924219,
      "grad_norm": 0.0010167176369577646,
      "learning_rate": 1.9894169192273435e-05,
      "loss": 0.1642,
      "step": 286200
    },
    {
      "epoch": 0.9340123382279538,
      "grad_norm": 0.0015302039682865143,
      "learning_rate": 1.9796298531613853e-05,
      "loss": 0.2164,
      "step": 286300
    },
    {
      "epoch": 0.9343385737634857,
      "grad_norm": 0.39488449692726135,
      "learning_rate": 1.9698427870954267e-05,
      "loss": 0.1545,
      "step": 286400
    },
    {
      "epoch": 0.9346648092990177,
      "grad_norm": 0.0006039916770532727,
      "learning_rate": 1.9600557210294688e-05,
      "loss": 0.187,
      "step": 286500
    },
    {
      "epoch": 0.9349910448345496,
      "grad_norm": 0.044140104204416275,
      "learning_rate": 1.9502686549635106e-05,
      "loss": 0.2255,
      "step": 286600
    },
    {
      "epoch": 0.9353172803700815,
      "grad_norm": 3.859419302898459e-05,
      "learning_rate": 1.940481588897552e-05,
      "loss": 0.4045,
      "step": 286700
    },
    {
      "epoch": 0.9356435159056136,
      "grad_norm": 0.00028917548479512334,
      "learning_rate": 1.9306945228315938e-05,
      "loss": 0.2452,
      "step": 286800
    },
    {
      "epoch": 0.9359697514411455,
      "grad_norm": 0.0025747979525476694,
      "learning_rate": 1.9209074567656352e-05,
      "loss": 0.2376,
      "step": 286900
    },
    {
      "epoch": 0.9362959869766774,
      "grad_norm": 0.010272709652781487,
      "learning_rate": 1.9111203906996774e-05,
      "loss": 0.1879,
      "step": 287000
    },
    {
      "epoch": 0.9366222225122094,
      "grad_norm": 0.0001713394740363583,
      "learning_rate": 1.9013333246337188e-05,
      "loss": 0.2711,
      "step": 287100
    },
    {
      "epoch": 0.9369484580477413,
      "grad_norm": 19.379541397094727,
      "learning_rate": 1.8915462585677606e-05,
      "loss": 0.2493,
      "step": 287200
    },
    {
      "epoch": 0.9372746935832732,
      "grad_norm": 71.07854461669922,
      "learning_rate": 1.8817591925018023e-05,
      "loss": 0.386,
      "step": 287300
    },
    {
      "epoch": 0.9376009291188052,
      "grad_norm": 0.00010516360634937882,
      "learning_rate": 1.871972126435844e-05,
      "loss": 0.2671,
      "step": 287400
    },
    {
      "epoch": 0.9379271646543371,
      "grad_norm": 0.003226033877581358,
      "learning_rate": 1.8621850603698856e-05,
      "loss": 0.2314,
      "step": 287500
    },
    {
      "epoch": 0.9382534001898691,
      "grad_norm": 0.0023354587610810995,
      "learning_rate": 1.8523979943039273e-05,
      "loss": 0.1855,
      "step": 287600
    },
    {
      "epoch": 0.938579635725401,
      "grad_norm": 4.5828950533177704e-05,
      "learning_rate": 1.842610928237969e-05,
      "loss": 0.124,
      "step": 287700
    },
    {
      "epoch": 0.938905871260933,
      "grad_norm": 0.00037087363307364285,
      "learning_rate": 1.832823862172011e-05,
      "loss": 0.2106,
      "step": 287800
    },
    {
      "epoch": 0.939232106796465,
      "grad_norm": 12.63396167755127,
      "learning_rate": 1.8230367961060523e-05,
      "loss": 0.3282,
      "step": 287900
    },
    {
      "epoch": 0.9395583423319969,
      "grad_norm": 0.033825695514678955,
      "learning_rate": 1.813249730040094e-05,
      "loss": 0.1395,
      "step": 288000
    },
    {
      "epoch": 0.9398845778675288,
      "grad_norm": 19.22878646850586,
      "learning_rate": 1.803462663974136e-05,
      "loss": 0.2011,
      "step": 288100
    },
    {
      "epoch": 0.9402108134030608,
      "grad_norm": 26.624055862426758,
      "learning_rate": 1.7936755979081776e-05,
      "loss": 0.2706,
      "step": 288200
    },
    {
      "epoch": 0.9405370489385927,
      "grad_norm": 0.016909044235944748,
      "learning_rate": 1.7838885318422194e-05,
      "loss": 0.2265,
      "step": 288300
    },
    {
      "epoch": 0.9408632844741246,
      "grad_norm": 0.007908070459961891,
      "learning_rate": 1.774101465776261e-05,
      "loss": 0.2033,
      "step": 288400
    },
    {
      "epoch": 0.9411895200096566,
      "grad_norm": 0.0010068139526993036,
      "learning_rate": 1.7643143997103026e-05,
      "loss": 0.122,
      "step": 288500
    },
    {
      "epoch": 0.9415157555451885,
      "grad_norm": 2.126110553741455,
      "learning_rate": 1.7545273336443444e-05,
      "loss": 0.1589,
      "step": 288600
    },
    {
      "epoch": 0.9418419910807204,
      "grad_norm": 0.083989217877388,
      "learning_rate": 1.744740267578386e-05,
      "loss": 0.2337,
      "step": 288700
    },
    {
      "epoch": 0.9421682266162524,
      "grad_norm": 0.0001228959154104814,
      "learning_rate": 1.734953201512428e-05,
      "loss": 0.179,
      "step": 288800
    },
    {
      "epoch": 0.9424944621517843,
      "grad_norm": 0.08438323438167572,
      "learning_rate": 1.7251661354464694e-05,
      "loss": 0.1906,
      "step": 288900
    },
    {
      "epoch": 0.9428206976873162,
      "grad_norm": 0.0013622535625472665,
      "learning_rate": 1.715379069380511e-05,
      "loss": 0.1523,
      "step": 289000
    },
    {
      "epoch": 0.9431469332228483,
      "grad_norm": 0.00019447198428679258,
      "learning_rate": 1.705592003314553e-05,
      "loss": 0.2724,
      "step": 289100
    },
    {
      "epoch": 0.9434731687583802,
      "grad_norm": 0.006200865376740694,
      "learning_rate": 1.6958049372485947e-05,
      "loss": 0.1858,
      "step": 289200
    },
    {
      "epoch": 0.9437994042939121,
      "grad_norm": 9.919091098709032e-05,
      "learning_rate": 1.686017871182636e-05,
      "loss": 0.1747,
      "step": 289300
    },
    {
      "epoch": 0.9441256398294441,
      "grad_norm": 0.1532149761915207,
      "learning_rate": 1.676230805116678e-05,
      "loss": 0.1173,
      "step": 289400
    },
    {
      "epoch": 0.944451875364976,
      "grad_norm": 0.0015975615242496133,
      "learning_rate": 1.6664437390507197e-05,
      "loss": 0.2048,
      "step": 289500
    },
    {
      "epoch": 0.9447781109005079,
      "grad_norm": 0.0008806196856312454,
      "learning_rate": 1.6566566729847615e-05,
      "loss": 0.1605,
      "step": 289600
    },
    {
      "epoch": 0.9451043464360399,
      "grad_norm": 0.1781056523323059,
      "learning_rate": 1.6468696069188032e-05,
      "loss": 0.1133,
      "step": 289700
    },
    {
      "epoch": 0.9454305819715718,
      "grad_norm": 6.400796701200306e-05,
      "learning_rate": 1.6370825408528447e-05,
      "loss": 0.263,
      "step": 289800
    },
    {
      "epoch": 0.9457568175071038,
      "grad_norm": 6.773223140044138e-05,
      "learning_rate": 1.6272954747868865e-05,
      "loss": 0.2098,
      "step": 289900
    },
    {
      "epoch": 0.9460830530426357,
      "grad_norm": 0.0717431828379631,
      "learning_rate": 1.6175084087209282e-05,
      "loss": 0.0384,
      "step": 290000
    },
    {
      "epoch": 0.9464092885781676,
      "grad_norm": 30.26641845703125,
      "learning_rate": 1.6077213426549697e-05,
      "loss": 0.2599,
      "step": 290100
    },
    {
      "epoch": 0.9467355241136997,
      "grad_norm": 0.026079008355736732,
      "learning_rate": 1.5979342765890118e-05,
      "loss": 0.2497,
      "step": 290200
    },
    {
      "epoch": 0.9470617596492316,
      "grad_norm": 0.0003503144544083625,
      "learning_rate": 1.5881472105230532e-05,
      "loss": 0.1175,
      "step": 290300
    },
    {
      "epoch": 0.9473879951847635,
      "grad_norm": 0.0009136265143752098,
      "learning_rate": 1.578360144457095e-05,
      "loss": 0.223,
      "step": 290400
    },
    {
      "epoch": 0.9477142307202955,
      "grad_norm": 26.844547271728516,
      "learning_rate": 1.5685730783911368e-05,
      "loss": 0.128,
      "step": 290500
    },
    {
      "epoch": 0.9480404662558274,
      "grad_norm": 16.239408493041992,
      "learning_rate": 1.5587860123251782e-05,
      "loss": 0.2071,
      "step": 290600
    },
    {
      "epoch": 0.9483667017913593,
      "grad_norm": 0.00010639761603670195,
      "learning_rate": 1.54899894625922e-05,
      "loss": 0.1753,
      "step": 290700
    },
    {
      "epoch": 0.9486929373268913,
      "grad_norm": 0.0001995373604586348,
      "learning_rate": 1.5392118801932617e-05,
      "loss": 0.3288,
      "step": 290800
    },
    {
      "epoch": 0.9490191728624232,
      "grad_norm": 50.07257080078125,
      "learning_rate": 1.5294248141273035e-05,
      "loss": 0.1358,
      "step": 290900
    },
    {
      "epoch": 0.9493454083979551,
      "grad_norm": 0.005003189668059349,
      "learning_rate": 1.5196377480613451e-05,
      "loss": 0.2268,
      "step": 291000
    },
    {
      "epoch": 0.9496716439334871,
      "grad_norm": 0.00010504804231459275,
      "learning_rate": 1.509850681995387e-05,
      "loss": 0.1028,
      "step": 291100
    },
    {
      "epoch": 0.949997879469019,
      "grad_norm": 0.0010407231748104095,
      "learning_rate": 1.5000636159294287e-05,
      "loss": 0.2462,
      "step": 291200
    },
    {
      "epoch": 0.9503241150045509,
      "grad_norm": 0.0020214333198964596,
      "learning_rate": 1.4902765498634703e-05,
      "loss": 0.1858,
      "step": 291300
    },
    {
      "epoch": 0.950650350540083,
      "grad_norm": 0.00010946837574010715,
      "learning_rate": 1.480489483797512e-05,
      "loss": 0.1633,
      "step": 291400
    },
    {
      "epoch": 0.9509765860756149,
      "grad_norm": 0.004894933197647333,
      "learning_rate": 1.4707024177315537e-05,
      "loss": 0.2575,
      "step": 291500
    },
    {
      "epoch": 0.9513028216111468,
      "grad_norm": 12.013401985168457,
      "learning_rate": 1.4609153516655954e-05,
      "loss": 0.3287,
      "step": 291600
    },
    {
      "epoch": 0.9516290571466788,
      "grad_norm": 4.929521674057469e-05,
      "learning_rate": 1.451128285599637e-05,
      "loss": 0.1521,
      "step": 291700
    },
    {
      "epoch": 0.9519552926822107,
      "grad_norm": 0.013845410197973251,
      "learning_rate": 1.4413412195336788e-05,
      "loss": 0.1799,
      "step": 291800
    },
    {
      "epoch": 0.9522815282177427,
      "grad_norm": 0.00046672348980791867,
      "learning_rate": 1.4315541534677206e-05,
      "loss": 0.1421,
      "step": 291900
    },
    {
      "epoch": 0.9526077637532746,
      "grad_norm": 8.820769289741293e-05,
      "learning_rate": 1.4217670874017622e-05,
      "loss": 0.22,
      "step": 292000
    },
    {
      "epoch": 0.9529339992888065,
      "grad_norm": 0.2376943975687027,
      "learning_rate": 1.411980021335804e-05,
      "loss": 0.215,
      "step": 292100
    },
    {
      "epoch": 0.9532602348243385,
      "grad_norm": 17.107070922851562,
      "learning_rate": 1.4021929552698456e-05,
      "loss": 0.1162,
      "step": 292200
    },
    {
      "epoch": 0.9535864703598704,
      "grad_norm": 0.0013788462383672595,
      "learning_rate": 1.3924058892038873e-05,
      "loss": 0.2519,
      "step": 292300
    },
    {
      "epoch": 0.9539127058954023,
      "grad_norm": 91.2185287475586,
      "learning_rate": 1.382618823137929e-05,
      "loss": 0.1545,
      "step": 292400
    },
    {
      "epoch": 0.9542389414309344,
      "grad_norm": 0.0021499113645404577,
      "learning_rate": 1.3728317570719706e-05,
      "loss": 0.11,
      "step": 292500
    },
    {
      "epoch": 0.9545651769664663,
      "grad_norm": 0.00040657512727193534,
      "learning_rate": 1.3630446910060125e-05,
      "loss": 0.3601,
      "step": 292600
    },
    {
      "epoch": 0.9548914125019982,
      "grad_norm": 0.4680101275444031,
      "learning_rate": 1.3532576249400541e-05,
      "loss": 0.0608,
      "step": 292700
    },
    {
      "epoch": 0.9552176480375302,
      "grad_norm": 0.02899615280330181,
      "learning_rate": 1.3434705588740959e-05,
      "loss": 0.2528,
      "step": 292800
    },
    {
      "epoch": 0.9555438835730621,
      "grad_norm": 3.9725604437990114e-05,
      "learning_rate": 1.3336834928081375e-05,
      "loss": 0.1807,
      "step": 292900
    },
    {
      "epoch": 0.955870119108594,
      "grad_norm": 0.0010008858516812325,
      "learning_rate": 1.3238964267421793e-05,
      "loss": 0.1807,
      "step": 293000
    },
    {
      "epoch": 0.956196354644126,
      "grad_norm": 0.001300897798500955,
      "learning_rate": 1.3141093606762209e-05,
      "loss": 0.2612,
      "step": 293100
    },
    {
      "epoch": 0.9565225901796579,
      "grad_norm": 17.663345336914062,
      "learning_rate": 1.3043222946102625e-05,
      "loss": 0.1549,
      "step": 293200
    },
    {
      "epoch": 0.9568488257151898,
      "grad_norm": 0.23356996476650238,
      "learning_rate": 1.2945352285443042e-05,
      "loss": 0.0876,
      "step": 293300
    },
    {
      "epoch": 0.9571750612507218,
      "grad_norm": 0.002223411574959755,
      "learning_rate": 1.284748162478346e-05,
      "loss": 0.2285,
      "step": 293400
    },
    {
      "epoch": 0.9575012967862537,
      "grad_norm": 0.0003048311627935618,
      "learning_rate": 1.2749610964123878e-05,
      "loss": 0.1713,
      "step": 293500
    },
    {
      "epoch": 0.9578275323217856,
      "grad_norm": 0.002384396968409419,
      "learning_rate": 1.2651740303464294e-05,
      "loss": 0.1472,
      "step": 293600
    },
    {
      "epoch": 0.9581537678573177,
      "grad_norm": 11.691896438598633,
      "learning_rate": 1.2553869642804712e-05,
      "loss": 0.1916,
      "step": 293700
    },
    {
      "epoch": 0.9584800033928496,
      "grad_norm": 27.16185760498047,
      "learning_rate": 1.2455998982145128e-05,
      "loss": 0.2768,
      "step": 293800
    },
    {
      "epoch": 0.9588062389283815,
      "grad_norm": 0.0007323340396396816,
      "learning_rate": 1.2358128321485544e-05,
      "loss": 0.1101,
      "step": 293900
    },
    {
      "epoch": 0.9591324744639135,
      "grad_norm": 0.002512427745386958,
      "learning_rate": 1.2260257660825962e-05,
      "loss": 0.0949,
      "step": 294000
    },
    {
      "epoch": 0.9594587099994454,
      "grad_norm": 7.158051448641345e-05,
      "learning_rate": 1.216238700016638e-05,
      "loss": 0.4281,
      "step": 294100
    },
    {
      "epoch": 0.9597849455349774,
      "grad_norm": 0.006478280294686556,
      "learning_rate": 1.2064516339506797e-05,
      "loss": 0.1726,
      "step": 294200
    },
    {
      "epoch": 0.9601111810705093,
      "grad_norm": 0.8923130631446838,
      "learning_rate": 1.1966645678847213e-05,
      "loss": 0.1277,
      "step": 294300
    },
    {
      "epoch": 0.9604374166060412,
      "grad_norm": 0.0001207241220981814,
      "learning_rate": 1.1868775018187631e-05,
      "loss": 0.1271,
      "step": 294400
    },
    {
      "epoch": 0.9607636521415732,
      "grad_norm": 0.00022941612405702472,
      "learning_rate": 1.1770904357528047e-05,
      "loss": 0.2269,
      "step": 294500
    },
    {
      "epoch": 0.9610898876771051,
      "grad_norm": 7.575772906420752e-05,
      "learning_rate": 1.1673033696868463e-05,
      "loss": 0.1876,
      "step": 294600
    },
    {
      "epoch": 0.961416123212637,
      "grad_norm": 1.346146821975708,
      "learning_rate": 1.157516303620888e-05,
      "loss": 0.2651,
      "step": 294700
    },
    {
      "epoch": 0.961742358748169,
      "grad_norm": 0.0001522382372058928,
      "learning_rate": 1.1477292375549297e-05,
      "loss": 0.1449,
      "step": 294800
    },
    {
      "epoch": 0.962068594283701,
      "grad_norm": 3.139737600577064e-05,
      "learning_rate": 1.1379421714889716e-05,
      "loss": 0.1426,
      "step": 294900
    },
    {
      "epoch": 0.9623948298192329,
      "grad_norm": 0.0024974748957902193,
      "learning_rate": 1.1281551054230132e-05,
      "loss": 0.1434,
      "step": 295000
    },
    {
      "epoch": 0.9627210653547649,
      "grad_norm": 0.015424898825585842,
      "learning_rate": 1.1183680393570548e-05,
      "loss": 0.2135,
      "step": 295100
    },
    {
      "epoch": 0.9630473008902968,
      "grad_norm": 22.888254165649414,
      "learning_rate": 1.1085809732910966e-05,
      "loss": 0.2637,
      "step": 295200
    },
    {
      "epoch": 0.9633735364258287,
      "grad_norm": 0.0021595184225589037,
      "learning_rate": 1.0987939072251382e-05,
      "loss": 0.2449,
      "step": 295300
    },
    {
      "epoch": 0.9636997719613607,
      "grad_norm": 0.49927130341529846,
      "learning_rate": 1.08900684115918e-05,
      "loss": 0.1685,
      "step": 295400
    },
    {
      "epoch": 0.9640260074968926,
      "grad_norm": 0.0004439945623744279,
      "learning_rate": 1.0792197750932216e-05,
      "loss": 0.107,
      "step": 295500
    },
    {
      "epoch": 0.9643522430324245,
      "grad_norm": 0.0002708506945054978,
      "learning_rate": 1.0694327090272635e-05,
      "loss": 0.2796,
      "step": 295600
    },
    {
      "epoch": 0.9646784785679565,
      "grad_norm": 0.0010386771755293012,
      "learning_rate": 1.0596456429613051e-05,
      "loss": 0.2996,
      "step": 295700
    },
    {
      "epoch": 0.9650047141034884,
      "grad_norm": 0.018559729680418968,
      "learning_rate": 1.0498585768953467e-05,
      "loss": 0.2689,
      "step": 295800
    },
    {
      "epoch": 0.9653309496390203,
      "grad_norm": 0.00010651592310750857,
      "learning_rate": 1.0400715108293885e-05,
      "loss": 0.1395,
      "step": 295900
    },
    {
      "epoch": 0.9656571851745523,
      "grad_norm": 0.000538367428816855,
      "learning_rate": 1.0302844447634301e-05,
      "loss": 0.1283,
      "step": 296000
    },
    {
      "epoch": 0.9659834207100843,
      "grad_norm": 52.94204330444336,
      "learning_rate": 1.0204973786974719e-05,
      "loss": 0.249,
      "step": 296100
    },
    {
      "epoch": 0.9663096562456162,
      "grad_norm": 39.609039306640625,
      "learning_rate": 1.0107103126315135e-05,
      "loss": 0.2382,
      "step": 296200
    },
    {
      "epoch": 0.9666358917811482,
      "grad_norm": 0.00020866385602857918,
      "learning_rate": 1.0009232465655555e-05,
      "loss": 0.2204,
      "step": 296300
    },
    {
      "epoch": 0.9669621273166801,
      "grad_norm": 2.3457472707377747e-05,
      "learning_rate": 9.91136180499597e-06,
      "loss": 0.2005,
      "step": 296400
    },
    {
      "epoch": 0.9672883628522121,
      "grad_norm": 4.415107105160132e-05,
      "learning_rate": 9.813491144336387e-06,
      "loss": 0.1549,
      "step": 296500
    },
    {
      "epoch": 0.967614598387744,
      "grad_norm": 0.0001753763935994357,
      "learning_rate": 9.715620483676804e-06,
      "loss": 0.1305,
      "step": 296600
    },
    {
      "epoch": 0.9679408339232759,
      "grad_norm": 11.546092987060547,
      "learning_rate": 9.61774982301722e-06,
      "loss": 0.1147,
      "step": 296700
    },
    {
      "epoch": 0.9682670694588079,
      "grad_norm": 0.002948552602902055,
      "learning_rate": 9.519879162357638e-06,
      "loss": 0.2544,
      "step": 296800
    },
    {
      "epoch": 0.9685933049943398,
      "grad_norm": 0.00019489461556077003,
      "learning_rate": 9.422008501698054e-06,
      "loss": 0.2654,
      "step": 296900
    },
    {
      "epoch": 0.9689195405298717,
      "grad_norm": 0.00043487315997481346,
      "learning_rate": 9.324137841038472e-06,
      "loss": 0.0998,
      "step": 297000
    },
    {
      "epoch": 0.9692457760654037,
      "grad_norm": 0.0006842556176707149,
      "learning_rate": 9.22626718037889e-06,
      "loss": 0.0761,
      "step": 297100
    },
    {
      "epoch": 0.9695720116009356,
      "grad_norm": 0.034714195877313614,
      "learning_rate": 9.128396519719306e-06,
      "loss": 0.1073,
      "step": 297200
    },
    {
      "epoch": 0.9698982471364676,
      "grad_norm": 0.0017836791230365634,
      "learning_rate": 9.030525859059724e-06,
      "loss": 0.2229,
      "step": 297300
    },
    {
      "epoch": 0.9702244826719996,
      "grad_norm": 0.0005022947443649173,
      "learning_rate": 8.93265519840014e-06,
      "loss": 0.1461,
      "step": 297400
    },
    {
      "epoch": 0.9705507182075315,
      "grad_norm": 0.00014338819892145693,
      "learning_rate": 8.834784537740557e-06,
      "loss": 0.2157,
      "step": 297500
    },
    {
      "epoch": 0.9708769537430634,
      "grad_norm": 0.0005741995410062373,
      "learning_rate": 8.736913877080973e-06,
      "loss": 0.2883,
      "step": 297600
    },
    {
      "epoch": 0.9712031892785954,
      "grad_norm": 0.00018719655054155737,
      "learning_rate": 8.639043216421391e-06,
      "loss": 0.2564,
      "step": 297700
    },
    {
      "epoch": 0.9715294248141273,
      "grad_norm": 0.042479198426008224,
      "learning_rate": 8.541172555761809e-06,
      "loss": 0.1591,
      "step": 297800
    },
    {
      "epoch": 0.9718556603496592,
      "grad_norm": 0.0005308499094098806,
      "learning_rate": 8.443301895102225e-06,
      "loss": 0.2069,
      "step": 297900
    },
    {
      "epoch": 0.9721818958851912,
      "grad_norm": 0.00021170861145947129,
      "learning_rate": 8.345431234442641e-06,
      "loss": 0.23,
      "step": 298000
    },
    {
      "epoch": 0.9725081314207231,
      "grad_norm": 0.5987353920936584,
      "learning_rate": 8.247560573783059e-06,
      "loss": 0.3828,
      "step": 298100
    },
    {
      "epoch": 0.972834366956255,
      "grad_norm": 0.003401003545150161,
      "learning_rate": 8.149689913123476e-06,
      "loss": 0.2791,
      "step": 298200
    },
    {
      "epoch": 0.973160602491787,
      "grad_norm": 2.7384192435420118e-05,
      "learning_rate": 8.051819252463893e-06,
      "loss": 0.0844,
      "step": 298300
    },
    {
      "epoch": 0.973486838027319,
      "grad_norm": 0.6915665864944458,
      "learning_rate": 7.95394859180431e-06,
      "loss": 0.2463,
      "step": 298400
    },
    {
      "epoch": 0.973813073562851,
      "grad_norm": 0.0028817292768508196,
      "learning_rate": 7.856077931144728e-06,
      "loss": 0.1847,
      "step": 298500
    },
    {
      "epoch": 0.9741393090983829,
      "grad_norm": 0.00029234110843390226,
      "learning_rate": 7.758207270485144e-06,
      "loss": 0.1374,
      "step": 298600
    },
    {
      "epoch": 0.9744655446339148,
      "grad_norm": 11.977994918823242,
      "learning_rate": 7.66033660982556e-06,
      "loss": 0.1871,
      "step": 298700
    },
    {
      "epoch": 0.9747917801694468,
      "grad_norm": 0.0002908300084527582,
      "learning_rate": 7.562465949165978e-06,
      "loss": 0.141,
      "step": 298800
    },
    {
      "epoch": 0.9751180157049787,
      "grad_norm": 0.01685274951159954,
      "learning_rate": 7.464595288506395e-06,
      "loss": 0.094,
      "step": 298900
    },
    {
      "epoch": 0.9754442512405106,
      "grad_norm": 2.074613571166992,
      "learning_rate": 7.3667246278468125e-06,
      "loss": 0.1639,
      "step": 299000
    },
    {
      "epoch": 0.9757704867760426,
      "grad_norm": 0.003632719861343503,
      "learning_rate": 7.268853967187229e-06,
      "loss": 0.3055,
      "step": 299100
    },
    {
      "epoch": 0.9760967223115745,
      "grad_norm": 0.0001249752676812932,
      "learning_rate": 7.170983306527646e-06,
      "loss": 0.2311,
      "step": 299200
    },
    {
      "epoch": 0.9764229578471064,
      "grad_norm": 0.0011816064361482859,
      "learning_rate": 7.073112645868062e-06,
      "loss": 0.0698,
      "step": 299300
    },
    {
      "epoch": 0.9767491933826384,
      "grad_norm": 0.0018264318350702524,
      "learning_rate": 6.97524198520848e-06,
      "loss": 0.0661,
      "step": 299400
    },
    {
      "epoch": 0.9770754289181703,
      "grad_norm": 0.00018002274737227708,
      "learning_rate": 6.877371324548897e-06,
      "loss": 0.1186,
      "step": 299500
    },
    {
      "epoch": 0.9774016644537022,
      "grad_norm": 0.00012386744492687285,
      "learning_rate": 6.779500663889314e-06,
      "loss": 0.2706,
      "step": 299600
    },
    {
      "epoch": 0.9777278999892343,
      "grad_norm": 0.0009348040912300348,
      "learning_rate": 6.681630003229732e-06,
      "loss": 0.1644,
      "step": 299700
    },
    {
      "epoch": 0.9780541355247662,
      "grad_norm": 0.0020536535885185003,
      "learning_rate": 6.5837593425701485e-06,
      "loss": 0.2099,
      "step": 299800
    },
    {
      "epoch": 0.9783803710602981,
      "grad_norm": 4.382386684417725,
      "learning_rate": 6.4858886819105654e-06,
      "loss": 0.1724,
      "step": 299900
    },
    {
      "epoch": 0.9787066065958301,
      "grad_norm": 0.006757430732250214,
      "learning_rate": 6.3880180212509815e-06,
      "loss": 0.2504,
      "step": 300000
    },
    {
      "epoch": 0.979032842131362,
      "grad_norm": 0.0572437047958374,
      "learning_rate": 6.290147360591399e-06,
      "loss": 0.1362,
      "step": 300100
    },
    {
      "epoch": 0.9793590776668939,
      "grad_norm": 0.005927677731961012,
      "learning_rate": 6.192276699931816e-06,
      "loss": 0.1325,
      "step": 300200
    },
    {
      "epoch": 0.9796853132024259,
      "grad_norm": 0.0003196556936018169,
      "learning_rate": 6.094406039272233e-06,
      "loss": 0.4738,
      "step": 300300
    },
    {
      "epoch": 0.9800115487379578,
      "grad_norm": 2.117153417202644e-05,
      "learning_rate": 5.996535378612651e-06,
      "loss": 0.1884,
      "step": 300400
    },
    {
      "epoch": 0.9803377842734897,
      "grad_norm": 0.00039075477980077267,
      "learning_rate": 5.898664717953068e-06,
      "loss": 0.1574,
      "step": 300500
    },
    {
      "epoch": 0.9806640198090217,
      "grad_norm": 2.576084625616204e-05,
      "learning_rate": 5.800794057293484e-06,
      "loss": 0.2194,
      "step": 300600
    },
    {
      "epoch": 0.9809902553445536,
      "grad_norm": 0.001449492876417935,
      "learning_rate": 5.702923396633901e-06,
      "loss": 0.2944,
      "step": 300700
    },
    {
      "epoch": 0.9813164908800857,
      "grad_norm": 60.10441207885742,
      "learning_rate": 5.605052735974318e-06,
      "loss": 0.2237,
      "step": 300800
    },
    {
      "epoch": 0.9816427264156176,
      "grad_norm": 0.0003544283681549132,
      "learning_rate": 5.507182075314735e-06,
      "loss": 0.1166,
      "step": 300900
    },
    {
      "epoch": 0.9819689619511495,
      "grad_norm": 0.008055316284298897,
      "learning_rate": 5.409311414655152e-06,
      "loss": 0.2109,
      "step": 301000
    },
    {
      "epoch": 0.9822951974866815,
      "grad_norm": 4.687268257141113,
      "learning_rate": 5.31144075399557e-06,
      "loss": 0.1806,
      "step": 301100
    },
    {
      "epoch": 0.9826214330222134,
      "grad_norm": 0.0017613235395401716,
      "learning_rate": 5.213570093335987e-06,
      "loss": 0.0837,
      "step": 301200
    },
    {
      "epoch": 0.9829476685577453,
      "grad_norm": 0.0024036166723817587,
      "learning_rate": 5.115699432676403e-06,
      "loss": 0.2225,
      "step": 301300
    },
    {
      "epoch": 0.9832739040932773,
      "grad_norm": 4.919924685964361e-05,
      "learning_rate": 5.01782877201682e-06,
      "loss": 0.1656,
      "step": 301400
    },
    {
      "epoch": 0.9836001396288092,
      "grad_norm": 0.6065456867218018,
      "learning_rate": 4.9199581113572375e-06,
      "loss": 0.1595,
      "step": 301500
    },
    {
      "epoch": 0.9839263751643411,
      "grad_norm": 0.02920127473771572,
      "learning_rate": 4.822087450697654e-06,
      "loss": 0.084,
      "step": 301600
    },
    {
      "epoch": 0.9842526106998731,
      "grad_norm": 0.0001578065421199426,
      "learning_rate": 4.724216790038071e-06,
      "loss": 0.0969,
      "step": 301700
    },
    {
      "epoch": 0.984578846235405,
      "grad_norm": 3.886425110977143e-05,
      "learning_rate": 4.626346129378488e-06,
      "loss": 0.1637,
      "step": 301800
    },
    {
      "epoch": 0.9849050817709369,
      "grad_norm": 0.0002343648229725659,
      "learning_rate": 4.528475468718905e-06,
      "loss": 0.1028,
      "step": 301900
    },
    {
      "epoch": 0.985231317306469,
      "grad_norm": 0.000655576994176954,
      "learning_rate": 4.430604808059323e-06,
      "loss": 0.0939,
      "step": 302000
    },
    {
      "epoch": 0.9855575528420009,
      "grad_norm": 0.00046291144099086523,
      "learning_rate": 4.332734147399739e-06,
      "loss": 0.1925,
      "step": 302100
    },
    {
      "epoch": 0.9858837883775328,
      "grad_norm": 0.0001155463614850305,
      "learning_rate": 4.234863486740157e-06,
      "loss": 0.2404,
      "step": 302200
    },
    {
      "epoch": 0.9862100239130648,
      "grad_norm": 0.0002483789576217532,
      "learning_rate": 4.1369928260805736e-06,
      "loss": 0.198,
      "step": 302300
    },
    {
      "epoch": 0.9865362594485967,
      "grad_norm": 18.819252014160156,
      "learning_rate": 4.0391221654209905e-06,
      "loss": 0.1178,
      "step": 302400
    },
    {
      "epoch": 0.9868624949841286,
      "grad_norm": 2.1259771529003046e-05,
      "learning_rate": 3.941251504761407e-06,
      "loss": 0.1774,
      "step": 302500
    },
    {
      "epoch": 0.9871887305196606,
      "grad_norm": 105.83417510986328,
      "learning_rate": 3.843380844101824e-06,
      "loss": 0.1703,
      "step": 302600
    },
    {
      "epoch": 0.9875149660551925,
      "grad_norm": 0.38605883717536926,
      "learning_rate": 3.745510183442241e-06,
      "loss": 0.1555,
      "step": 302700
    },
    {
      "epoch": 0.9878412015907244,
      "grad_norm": 24.658674240112305,
      "learning_rate": 3.647639522782658e-06,
      "loss": 0.145,
      "step": 302800
    },
    {
      "epoch": 0.9881674371262564,
      "grad_norm": 0.7588868737220764,
      "learning_rate": 3.5497688621230754e-06,
      "loss": 0.2341,
      "step": 302900
    },
    {
      "epoch": 0.9884936726617883,
      "grad_norm": 4.556396015686914e-05,
      "learning_rate": 3.451898201463492e-06,
      "loss": 0.1209,
      "step": 303000
    },
    {
      "epoch": 0.9888199081973204,
      "grad_norm": 0.0005870984750799835,
      "learning_rate": 3.354027540803909e-06,
      "loss": 0.1392,
      "step": 303100
    },
    {
      "epoch": 0.9891461437328523,
      "grad_norm": 7.543806714238599e-05,
      "learning_rate": 3.2561568801443265e-06,
      "loss": 0.1003,
      "step": 303200
    },
    {
      "epoch": 0.9894723792683842,
      "grad_norm": 8.007320866454393e-05,
      "learning_rate": 3.1582862194847434e-06,
      "loss": 0.1368,
      "step": 303300
    },
    {
      "epoch": 0.9897986148039162,
      "grad_norm": 0.001668797805905342,
      "learning_rate": 3.0604155588251603e-06,
      "loss": 0.1914,
      "step": 303400
    },
    {
      "epoch": 0.9901248503394481,
      "grad_norm": 0.002712468383833766,
      "learning_rate": 2.962544898165577e-06,
      "loss": 0.1745,
      "step": 303500
    },
    {
      "epoch": 0.99045108587498,
      "grad_norm": 0.010965600609779358,
      "learning_rate": 2.8646742375059945e-06,
      "loss": 0.2813,
      "step": 303600
    },
    {
      "epoch": 0.990777321410512,
      "grad_norm": 6.04875895078294e-05,
      "learning_rate": 2.766803576846411e-06,
      "loss": 0.3217,
      "step": 303700
    },
    {
      "epoch": 0.9911035569460439,
      "grad_norm": 11.351035118103027,
      "learning_rate": 2.6689329161868283e-06,
      "loss": 0.1733,
      "step": 303800
    },
    {
      "epoch": 0.9914297924815758,
      "grad_norm": 3.77530996047426e-05,
      "learning_rate": 2.5710622555272456e-06,
      "loss": 0.2259,
      "step": 303900
    },
    {
      "epoch": 0.9917560280171078,
      "grad_norm": 0.004248736891895533,
      "learning_rate": 2.473191594867662e-06,
      "loss": 0.101,
      "step": 304000
    },
    {
      "epoch": 0.9920822635526397,
      "grad_norm": 11.605661392211914,
      "learning_rate": 2.3753209342080794e-06,
      "loss": 0.1523,
      "step": 304100
    },
    {
      "epoch": 0.9924084990881716,
      "grad_norm": 0.0007965911063365638,
      "learning_rate": 2.2774502735484963e-06,
      "loss": 0.2906,
      "step": 304200
    },
    {
      "epoch": 0.9927347346237037,
      "grad_norm": 6.81559176882729e-05,
      "learning_rate": 2.1795796128889132e-06,
      "loss": 0.2291,
      "step": 304300
    },
    {
      "epoch": 0.9930609701592356,
      "grad_norm": 0.0009159678593277931,
      "learning_rate": 2.08170895222933e-06,
      "loss": 0.1061,
      "step": 304400
    },
    {
      "epoch": 0.9933872056947675,
      "grad_norm": 0.0017068388406187296,
      "learning_rate": 1.9838382915697475e-06,
      "loss": 0.1452,
      "step": 304500
    },
    {
      "epoch": 0.9937134412302995,
      "grad_norm": 79.85750579833984,
      "learning_rate": 1.8859676309101643e-06,
      "loss": 0.1409,
      "step": 304600
    },
    {
      "epoch": 0.9940396767658314,
      "grad_norm": 3.2370931876357645e-05,
      "learning_rate": 1.7880969702505812e-06,
      "loss": 0.1824,
      "step": 304700
    },
    {
      "epoch": 0.9943659123013633,
      "grad_norm": 0.0005858330405317247,
      "learning_rate": 1.6902263095909984e-06,
      "loss": 0.2101,
      "step": 304800
    },
    {
      "epoch": 0.9946921478368953,
      "grad_norm": 0.0007522142259404063,
      "learning_rate": 1.5923556489314153e-06,
      "loss": 0.133,
      "step": 304900
    },
    {
      "epoch": 0.9950183833724272,
      "grad_norm": 3.049446240765974e-05,
      "learning_rate": 1.4944849882718324e-06,
      "loss": 0.1088,
      "step": 305000
    },
    {
      "epoch": 0.9953446189079592,
      "grad_norm": 0.009334330447018147,
      "learning_rate": 1.3966143276122495e-06,
      "loss": 0.1744,
      "step": 305100
    },
    {
      "epoch": 0.9956708544434911,
      "grad_norm": 0.02193557843565941,
      "learning_rate": 1.2987436669526664e-06,
      "loss": 0.2242,
      "step": 305200
    },
    {
      "epoch": 0.995997089979023,
      "grad_norm": 11.644556999206543,
      "learning_rate": 1.2008730062930835e-06,
      "loss": 0.3217,
      "step": 305300
    },
    {
      "epoch": 0.996323325514555,
      "grad_norm": 0.00014732837735209614,
      "learning_rate": 1.1030023456335004e-06,
      "loss": 0.2827,
      "step": 305400
    },
    {
      "epoch": 0.996649561050087,
      "grad_norm": 0.11397920548915863,
      "learning_rate": 1.0051316849739173e-06,
      "loss": 0.1884,
      "step": 305500
    },
    {
      "epoch": 0.9969757965856189,
      "grad_norm": 0.0015915463445708156,
      "learning_rate": 9.072610243143344e-07,
      "loss": 0.2519,
      "step": 305600
    },
    {
      "epoch": 0.9973020321211509,
      "grad_norm": 0.0014641577145084739,
      "learning_rate": 8.093903636547514e-07,
      "loss": 0.1379,
      "step": 305700
    },
    {
      "epoch": 0.9976282676566828,
      "grad_norm": 63.31126403808594,
      "learning_rate": 7.115197029951683e-07,
      "loss": 0.0941,
      "step": 305800
    },
    {
      "epoch": 0.9979545031922147,
      "grad_norm": 0.00022189576702658087,
      "learning_rate": 6.136490423355854e-07,
      "loss": 0.1373,
      "step": 305900
    },
    {
      "epoch": 0.9982807387277467,
      "grad_norm": 0.006967791356146336,
      "learning_rate": 5.157783816760024e-07,
      "loss": 0.1849,
      "step": 306000
    },
    {
      "epoch": 0.9986069742632786,
      "grad_norm": 0.019161192700266838,
      "learning_rate": 4.1790772101641937e-07,
      "loss": 0.141,
      "step": 306100
    },
    {
      "epoch": 0.9989332097988105,
      "grad_norm": 0.13733075559139252,
      "learning_rate": 3.2003706035683643e-07,
      "loss": 0.0859,
      "step": 306200
    },
    {
      "epoch": 0.9992594453343425,
      "grad_norm": 3.458029459579848e-05,
      "learning_rate": 2.2216639969725343e-07,
      "loss": 0.0682,
      "step": 306300
    },
    {
      "epoch": 0.9995856808698744,
      "grad_norm": 0.00045430485624819994,
      "learning_rate": 1.242957390376704e-07,
      "loss": 0.1724,
      "step": 306400
    },
    {
      "epoch": 0.9999119164054063,
      "grad_norm": 0.0004946646513417363,
      "learning_rate": 2.642507837808741e-08,
      "loss": 0.1284,
      "step": 306500
    }
  ],
  "logging_steps": 100,
  "max_steps": 306527,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2344207929729284e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
