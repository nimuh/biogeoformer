{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 303900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0003290556103981573,
      "grad_norm": 3.3425986766815186,
      "learning_rate": 0.0002999012833168805,
      "loss": 3.6221,
      "step": 100
    },
    {
      "epoch": 0.0006581112207963146,
      "grad_norm": 3.411405324935913,
      "learning_rate": 0.00029980256663376107,
      "loss": 3.618,
      "step": 200
    },
    {
      "epoch": 0.0009871668311944718,
      "grad_norm": 3.044045925140381,
      "learning_rate": 0.0002997038499506416,
      "loss": 3.6442,
      "step": 300
    },
    {
      "epoch": 0.0013162224415926291,
      "grad_norm": 3.9266295433044434,
      "learning_rate": 0.0002996051332675222,
      "loss": 3.6289,
      "step": 400
    },
    {
      "epoch": 0.0016452780519907865,
      "grad_norm": 3.3113033771514893,
      "learning_rate": 0.0002995064165844027,
      "loss": 3.543,
      "step": 500
    },
    {
      "epoch": 0.0019743336623889436,
      "grad_norm": 3.780421495437622,
      "learning_rate": 0.00029940769990128327,
      "loss": 3.473,
      "step": 600
    },
    {
      "epoch": 0.002303389272787101,
      "grad_norm": 6.750061511993408,
      "learning_rate": 0.00029930898321816387,
      "loss": 3.4053,
      "step": 700
    },
    {
      "epoch": 0.0026324448831852583,
      "grad_norm": 7.306906223297119,
      "learning_rate": 0.0002992102665350444,
      "loss": 3.3646,
      "step": 800
    },
    {
      "epoch": 0.0029615004935834156,
      "grad_norm": 13.260735511779785,
      "learning_rate": 0.0002991115498519249,
      "loss": 3.2244,
      "step": 900
    },
    {
      "epoch": 0.003290556103981573,
      "grad_norm": 6.857518196105957,
      "learning_rate": 0.0002990128331688055,
      "loss": 3.2657,
      "step": 1000
    },
    {
      "epoch": 0.0036196117143797303,
      "grad_norm": 13.379256248474121,
      "learning_rate": 0.00029891411648568606,
      "loss": 3.2066,
      "step": 1100
    },
    {
      "epoch": 0.003948667324777887,
      "grad_norm": 7.553727626800537,
      "learning_rate": 0.0002988153998025666,
      "loss": 3.0323,
      "step": 1200
    },
    {
      "epoch": 0.0042777229351760445,
      "grad_norm": 17.5195369720459,
      "learning_rate": 0.00029871668311944716,
      "loss": 3.1263,
      "step": 1300
    },
    {
      "epoch": 0.004606778545574202,
      "grad_norm": 10.588005065917969,
      "learning_rate": 0.0002986179664363277,
      "loss": 2.9724,
      "step": 1400
    },
    {
      "epoch": 0.004935834155972359,
      "grad_norm": 11.851820945739746,
      "learning_rate": 0.00029851924975320826,
      "loss": 3.0831,
      "step": 1500
    },
    {
      "epoch": 0.0052648897663705166,
      "grad_norm": 9.737142562866211,
      "learning_rate": 0.0002984205330700888,
      "loss": 3.0357,
      "step": 1600
    },
    {
      "epoch": 0.005593945376768674,
      "grad_norm": 7.751860618591309,
      "learning_rate": 0.00029832181638696936,
      "loss": 3.0169,
      "step": 1700
    },
    {
      "epoch": 0.005923000987166831,
      "grad_norm": 7.205183982849121,
      "learning_rate": 0.0002982230997038499,
      "loss": 2.9154,
      "step": 1800
    },
    {
      "epoch": 0.006252056597564989,
      "grad_norm": 9.941814422607422,
      "learning_rate": 0.00029812438302073046,
      "loss": 2.8816,
      "step": 1900
    },
    {
      "epoch": 0.006581112207963146,
      "grad_norm": 8.106120109558105,
      "learning_rate": 0.00029802566633761106,
      "loss": 2.9116,
      "step": 2000
    },
    {
      "epoch": 0.006910167818361303,
      "grad_norm": 9.075310707092285,
      "learning_rate": 0.0002979269496544916,
      "loss": 2.7326,
      "step": 2100
    },
    {
      "epoch": 0.007239223428759461,
      "grad_norm": 9.233043670654297,
      "learning_rate": 0.0002978282329713721,
      "loss": 2.8836,
      "step": 2200
    },
    {
      "epoch": 0.007568279039157618,
      "grad_norm": 9.98360538482666,
      "learning_rate": 0.0002977295162882527,
      "loss": 2.8013,
      "step": 2300
    },
    {
      "epoch": 0.007897334649555774,
      "grad_norm": 11.294422149658203,
      "learning_rate": 0.00029763079960513325,
      "loss": 2.7376,
      "step": 2400
    },
    {
      "epoch": 0.008226390259953932,
      "grad_norm": 19.121694564819336,
      "learning_rate": 0.0002975320829220138,
      "loss": 2.5115,
      "step": 2500
    },
    {
      "epoch": 0.008555445870352089,
      "grad_norm": 8.402485847473145,
      "learning_rate": 0.00029743336623889435,
      "loss": 2.653,
      "step": 2600
    },
    {
      "epoch": 0.008884501480750246,
      "grad_norm": 10.795069694519043,
      "learning_rate": 0.0002973346495557749,
      "loss": 2.7287,
      "step": 2700
    },
    {
      "epoch": 0.009213557091148404,
      "grad_norm": 6.799034595489502,
      "learning_rate": 0.00029723593287265545,
      "loss": 2.6188,
      "step": 2800
    },
    {
      "epoch": 0.009542612701546561,
      "grad_norm": 9.996133804321289,
      "learning_rate": 0.000297137216189536,
      "loss": 2.5503,
      "step": 2900
    },
    {
      "epoch": 0.009871668311944718,
      "grad_norm": 21.27964973449707,
      "learning_rate": 0.00029703849950641655,
      "loss": 2.5056,
      "step": 3000
    },
    {
      "epoch": 0.010200723922342876,
      "grad_norm": 14.547679901123047,
      "learning_rate": 0.00029693978282329715,
      "loss": 2.478,
      "step": 3100
    },
    {
      "epoch": 0.010529779532741033,
      "grad_norm": 27.777769088745117,
      "learning_rate": 0.00029684106614017764,
      "loss": 2.5405,
      "step": 3200
    },
    {
      "epoch": 0.01085883514313919,
      "grad_norm": 8.488455772399902,
      "learning_rate": 0.0002967423494570582,
      "loss": 2.7635,
      "step": 3300
    },
    {
      "epoch": 0.011187890753537348,
      "grad_norm": 1.4536406993865967,
      "learning_rate": 0.0002966436327739388,
      "loss": 2.5046,
      "step": 3400
    },
    {
      "epoch": 0.011516946363935505,
      "grad_norm": 6.309303283691406,
      "learning_rate": 0.00029654491609081934,
      "loss": 2.6324,
      "step": 3500
    },
    {
      "epoch": 0.011846001974333662,
      "grad_norm": 82.05940246582031,
      "learning_rate": 0.0002964461994076999,
      "loss": 2.309,
      "step": 3600
    },
    {
      "epoch": 0.01217505758473182,
      "grad_norm": 9.493013381958008,
      "learning_rate": 0.00029634748272458044,
      "loss": 2.3608,
      "step": 3700
    },
    {
      "epoch": 0.012504113195129977,
      "grad_norm": 6.385382652282715,
      "learning_rate": 0.000296248766041461,
      "loss": 2.3429,
      "step": 3800
    },
    {
      "epoch": 0.012833168805528134,
      "grad_norm": 7.122787952423096,
      "learning_rate": 0.00029615004935834154,
      "loss": 2.4274,
      "step": 3900
    },
    {
      "epoch": 0.013162224415926292,
      "grad_norm": 13.442729949951172,
      "learning_rate": 0.0002960513326752221,
      "loss": 2.351,
      "step": 4000
    },
    {
      "epoch": 0.01349128002632445,
      "grad_norm": 10.148660659790039,
      "learning_rate": 0.00029595261599210264,
      "loss": 2.2442,
      "step": 4100
    },
    {
      "epoch": 0.013820335636722606,
      "grad_norm": 24.05380630493164,
      "learning_rate": 0.0002958538993089832,
      "loss": 2.2745,
      "step": 4200
    },
    {
      "epoch": 0.014149391247120764,
      "grad_norm": 5.951088905334473,
      "learning_rate": 0.00029575518262586374,
      "loss": 2.0899,
      "step": 4300
    },
    {
      "epoch": 0.014478446857518921,
      "grad_norm": 28.192819595336914,
      "learning_rate": 0.00029565646594274434,
      "loss": 2.2389,
      "step": 4400
    },
    {
      "epoch": 0.014807502467917079,
      "grad_norm": 32.61393356323242,
      "learning_rate": 0.00029555774925962483,
      "loss": 2.1228,
      "step": 4500
    },
    {
      "epoch": 0.015136558078315236,
      "grad_norm": 14.800660133361816,
      "learning_rate": 0.0002954590325765054,
      "loss": 2.1769,
      "step": 4600
    },
    {
      "epoch": 0.015465613688713393,
      "grad_norm": 21.23210906982422,
      "learning_rate": 0.000295360315893386,
      "loss": 2.3447,
      "step": 4700
    },
    {
      "epoch": 0.01579466929911155,
      "grad_norm": 21.273685455322266,
      "learning_rate": 0.00029526159921026653,
      "loss": 2.1477,
      "step": 4800
    },
    {
      "epoch": 0.016123724909509706,
      "grad_norm": 11.491405487060547,
      "learning_rate": 0.00029516288252714703,
      "loss": 2.0681,
      "step": 4900
    },
    {
      "epoch": 0.016452780519907863,
      "grad_norm": 31.794221878051758,
      "learning_rate": 0.00029506416584402763,
      "loss": 2.0652,
      "step": 5000
    },
    {
      "epoch": 0.01678183613030602,
      "grad_norm": 36.29597854614258,
      "learning_rate": 0.0002949654491609082,
      "loss": 2.245,
      "step": 5100
    },
    {
      "epoch": 0.017110891740704178,
      "grad_norm": 15.870110511779785,
      "learning_rate": 0.00029486673247778873,
      "loss": 2.2254,
      "step": 5200
    },
    {
      "epoch": 0.017439947351102335,
      "grad_norm": 16.422428131103516,
      "learning_rate": 0.0002947680157946693,
      "loss": 2.0711,
      "step": 5300
    },
    {
      "epoch": 0.017769002961500493,
      "grad_norm": 15.62952709197998,
      "learning_rate": 0.0002946692991115498,
      "loss": 2.1304,
      "step": 5400
    },
    {
      "epoch": 0.01809805857189865,
      "grad_norm": 10.57160472869873,
      "learning_rate": 0.0002945705824284304,
      "loss": 1.8845,
      "step": 5500
    },
    {
      "epoch": 0.018427114182296808,
      "grad_norm": 21.41744613647461,
      "learning_rate": 0.0002944718657453109,
      "loss": 2.0053,
      "step": 5600
    },
    {
      "epoch": 0.018756169792694965,
      "grad_norm": 12.804826736450195,
      "learning_rate": 0.00029437314906219147,
      "loss": 1.9343,
      "step": 5700
    },
    {
      "epoch": 0.019085225403093122,
      "grad_norm": 2.130995273590088,
      "learning_rate": 0.000294274432379072,
      "loss": 1.7505,
      "step": 5800
    },
    {
      "epoch": 0.01941428101349128,
      "grad_norm": 13.536937713623047,
      "learning_rate": 0.00029417571569595257,
      "loss": 1.8791,
      "step": 5900
    },
    {
      "epoch": 0.019743336623889437,
      "grad_norm": 11.146214485168457,
      "learning_rate": 0.0002940769990128332,
      "loss": 1.794,
      "step": 6000
    },
    {
      "epoch": 0.020072392234287594,
      "grad_norm": 11.79865837097168,
      "learning_rate": 0.0002939782823297137,
      "loss": 2.1139,
      "step": 6100
    },
    {
      "epoch": 0.02040144784468575,
      "grad_norm": 7.493361949920654,
      "learning_rate": 0.0002938795656465942,
      "loss": 1.9253,
      "step": 6200
    },
    {
      "epoch": 0.02073050345508391,
      "grad_norm": 37.20787048339844,
      "learning_rate": 0.0002937808489634748,
      "loss": 1.8759,
      "step": 6300
    },
    {
      "epoch": 0.021059559065482066,
      "grad_norm": 21.654022216796875,
      "learning_rate": 0.00029368213228035537,
      "loss": 1.9526,
      "step": 6400
    },
    {
      "epoch": 0.021388614675880224,
      "grad_norm": 16.964468002319336,
      "learning_rate": 0.0002935834155972359,
      "loss": 1.8648,
      "step": 6500
    },
    {
      "epoch": 0.02171767028627838,
      "grad_norm": 16.92378807067871,
      "learning_rate": 0.00029348469891411647,
      "loss": 1.8015,
      "step": 6600
    },
    {
      "epoch": 0.022046725896676538,
      "grad_norm": 19.981975555419922,
      "learning_rate": 0.000293385982230997,
      "loss": 1.7759,
      "step": 6700
    },
    {
      "epoch": 0.022375781507074696,
      "grad_norm": 20.047637939453125,
      "learning_rate": 0.00029328726554787756,
      "loss": 1.8549,
      "step": 6800
    },
    {
      "epoch": 0.022704837117472853,
      "grad_norm": 0.2695969343185425,
      "learning_rate": 0.0002931885488647581,
      "loss": 1.7098,
      "step": 6900
    },
    {
      "epoch": 0.02303389272787101,
      "grad_norm": 27.843339920043945,
      "learning_rate": 0.00029308983218163866,
      "loss": 1.5983,
      "step": 7000
    },
    {
      "epoch": 0.023362948338269168,
      "grad_norm": 25.336286544799805,
      "learning_rate": 0.0002929911154985192,
      "loss": 1.88,
      "step": 7100
    },
    {
      "epoch": 0.023692003948667325,
      "grad_norm": 15.336946487426758,
      "learning_rate": 0.00029289239881539976,
      "loss": 1.9243,
      "step": 7200
    },
    {
      "epoch": 0.024021059559065482,
      "grad_norm": 0.999609649181366,
      "learning_rate": 0.0002927936821322803,
      "loss": 1.7529,
      "step": 7300
    },
    {
      "epoch": 0.02435011516946364,
      "grad_norm": 29.599103927612305,
      "learning_rate": 0.0002926949654491609,
      "loss": 1.5471,
      "step": 7400
    },
    {
      "epoch": 0.024679170779861797,
      "grad_norm": 16.06334686279297,
      "learning_rate": 0.0002925962487660414,
      "loss": 1.8741,
      "step": 7500
    },
    {
      "epoch": 0.025008226390259954,
      "grad_norm": 7.668832302093506,
      "learning_rate": 0.000292497532082922,
      "loss": 1.7149,
      "step": 7600
    },
    {
      "epoch": 0.02533728200065811,
      "grad_norm": 19.937543869018555,
      "learning_rate": 0.00029239881539980256,
      "loss": 1.7078,
      "step": 7700
    },
    {
      "epoch": 0.02566633761105627,
      "grad_norm": 19.112396240234375,
      "learning_rate": 0.0002923000987166831,
      "loss": 1.7592,
      "step": 7800
    },
    {
      "epoch": 0.025995393221454426,
      "grad_norm": 12.339037895202637,
      "learning_rate": 0.00029220138203356365,
      "loss": 1.6796,
      "step": 7900
    },
    {
      "epoch": 0.026324448831852584,
      "grad_norm": 32.71354293823242,
      "learning_rate": 0.0002921026653504442,
      "loss": 1.616,
      "step": 8000
    },
    {
      "epoch": 0.02665350444225074,
      "grad_norm": 31.122425079345703,
      "learning_rate": 0.00029200394866732475,
      "loss": 1.668,
      "step": 8100
    },
    {
      "epoch": 0.0269825600526489,
      "grad_norm": 17.62646484375,
      "learning_rate": 0.0002919052319842053,
      "loss": 1.4801,
      "step": 8200
    },
    {
      "epoch": 0.027311615663047056,
      "grad_norm": 17.44631576538086,
      "learning_rate": 0.00029180651530108585,
      "loss": 1.9,
      "step": 8300
    },
    {
      "epoch": 0.027640671273445213,
      "grad_norm": 15.433331489562988,
      "learning_rate": 0.00029170779861796645,
      "loss": 1.6634,
      "step": 8400
    },
    {
      "epoch": 0.02796972688384337,
      "grad_norm": 66.42214965820312,
      "learning_rate": 0.00029160908193484695,
      "loss": 1.6125,
      "step": 8500
    },
    {
      "epoch": 0.028298782494241528,
      "grad_norm": 6.407112121582031,
      "learning_rate": 0.0002915103652517275,
      "loss": 1.7148,
      "step": 8600
    },
    {
      "epoch": 0.028627838104639685,
      "grad_norm": 16.318178176879883,
      "learning_rate": 0.0002914116485686081,
      "loss": 1.6843,
      "step": 8700
    },
    {
      "epoch": 0.028956893715037842,
      "grad_norm": 11.462844848632812,
      "learning_rate": 0.0002913129318854886,
      "loss": 1.6169,
      "step": 8800
    },
    {
      "epoch": 0.029285949325436,
      "grad_norm": 12.716085433959961,
      "learning_rate": 0.00029121421520236914,
      "loss": 1.418,
      "step": 8900
    },
    {
      "epoch": 0.029615004935834157,
      "grad_norm": 11.09823989868164,
      "learning_rate": 0.00029111549851924975,
      "loss": 1.8296,
      "step": 9000
    },
    {
      "epoch": 0.029944060546232314,
      "grad_norm": 6.567119121551514,
      "learning_rate": 0.0002910167818361303,
      "loss": 1.5646,
      "step": 9100
    },
    {
      "epoch": 0.03027311615663047,
      "grad_norm": 1.7266786098480225,
      "learning_rate": 0.00029091806515301084,
      "loss": 1.2055,
      "step": 9200
    },
    {
      "epoch": 0.03060217176702863,
      "grad_norm": 1.1500450372695923,
      "learning_rate": 0.0002908193484698914,
      "loss": 1.4988,
      "step": 9300
    },
    {
      "epoch": 0.030931227377426786,
      "grad_norm": 10.830334663391113,
      "learning_rate": 0.00029072063178677194,
      "loss": 1.7177,
      "step": 9400
    },
    {
      "epoch": 0.03126028298782494,
      "grad_norm": 27.06150245666504,
      "learning_rate": 0.0002906219151036525,
      "loss": 1.5458,
      "step": 9500
    },
    {
      "epoch": 0.0315893385982231,
      "grad_norm": 6.674820423126221,
      "learning_rate": 0.00029052319842053304,
      "loss": 1.3402,
      "step": 9600
    },
    {
      "epoch": 0.031918394208621255,
      "grad_norm": 33.06451416015625,
      "learning_rate": 0.0002904244817374136,
      "loss": 1.3626,
      "step": 9700
    },
    {
      "epoch": 0.03224744981901941,
      "grad_norm": 15.002971649169922,
      "learning_rate": 0.00029032576505429414,
      "loss": 1.319,
      "step": 9800
    },
    {
      "epoch": 0.03257650542941757,
      "grad_norm": 6.491177082061768,
      "learning_rate": 0.0002902270483711747,
      "loss": 1.4513,
      "step": 9900
    },
    {
      "epoch": 0.03290556103981573,
      "grad_norm": 11.683862686157227,
      "learning_rate": 0.0002901283316880553,
      "loss": 1.4547,
      "step": 10000
    },
    {
      "epoch": 0.033234616650213884,
      "grad_norm": 62.201324462890625,
      "learning_rate": 0.00029002961500493584,
      "loss": 1.4436,
      "step": 10100
    },
    {
      "epoch": 0.03356367226061204,
      "grad_norm": 13.651885986328125,
      "learning_rate": 0.00028993089832181633,
      "loss": 1.7117,
      "step": 10200
    },
    {
      "epoch": 0.0338927278710102,
      "grad_norm": 11.976922035217285,
      "learning_rate": 0.00028983218163869693,
      "loss": 1.2079,
      "step": 10300
    },
    {
      "epoch": 0.034221783481408356,
      "grad_norm": 14.994481086730957,
      "learning_rate": 0.0002897334649555775,
      "loss": 1.54,
      "step": 10400
    },
    {
      "epoch": 0.034550839091806514,
      "grad_norm": 20.656267166137695,
      "learning_rate": 0.00028963474827245803,
      "loss": 1.6394,
      "step": 10500
    },
    {
      "epoch": 0.03487989470220467,
      "grad_norm": 29.773204803466797,
      "learning_rate": 0.0002895360315893386,
      "loss": 1.521,
      "step": 10600
    },
    {
      "epoch": 0.03520895031260283,
      "grad_norm": 29.916540145874023,
      "learning_rate": 0.00028943731490621913,
      "loss": 1.3911,
      "step": 10700
    },
    {
      "epoch": 0.035538005923000986,
      "grad_norm": 30.70392417907715,
      "learning_rate": 0.0002893385982230997,
      "loss": 1.3456,
      "step": 10800
    },
    {
      "epoch": 0.03586706153339914,
      "grad_norm": 9.431839942932129,
      "learning_rate": 0.00028923988153998023,
      "loss": 1.3724,
      "step": 10900
    },
    {
      "epoch": 0.0361961171437973,
      "grad_norm": 26.54722785949707,
      "learning_rate": 0.0002891411648568608,
      "loss": 1.5817,
      "step": 11000
    },
    {
      "epoch": 0.03652517275419546,
      "grad_norm": 0.7581886053085327,
      "learning_rate": 0.0002890424481737413,
      "loss": 1.3635,
      "step": 11100
    },
    {
      "epoch": 0.036854228364593615,
      "grad_norm": 20.91822624206543,
      "learning_rate": 0.0002889437314906219,
      "loss": 1.356,
      "step": 11200
    },
    {
      "epoch": 0.03718328397499177,
      "grad_norm": 17.362281799316406,
      "learning_rate": 0.0002888450148075024,
      "loss": 1.6313,
      "step": 11300
    },
    {
      "epoch": 0.03751233958538993,
      "grad_norm": 11.640913009643555,
      "learning_rate": 0.000288746298124383,
      "loss": 1.1739,
      "step": 11400
    },
    {
      "epoch": 0.03784139519578809,
      "grad_norm": 16.219100952148438,
      "learning_rate": 0.0002886475814412635,
      "loss": 1.534,
      "step": 11500
    },
    {
      "epoch": 0.038170450806186244,
      "grad_norm": 16.249635696411133,
      "learning_rate": 0.0002885488647581441,
      "loss": 1.5573,
      "step": 11600
    },
    {
      "epoch": 0.0384995064165844,
      "grad_norm": 20.87729835510254,
      "learning_rate": 0.00028845014807502467,
      "loss": 1.724,
      "step": 11700
    },
    {
      "epoch": 0.03882856202698256,
      "grad_norm": 1.9042989015579224,
      "learning_rate": 0.0002883514313919052,
      "loss": 1.4692,
      "step": 11800
    },
    {
      "epoch": 0.039157617637380716,
      "grad_norm": 1.6831614971160889,
      "learning_rate": 0.00028825271470878577,
      "loss": 1.3696,
      "step": 11900
    },
    {
      "epoch": 0.039486673247778874,
      "grad_norm": 26.222938537597656,
      "learning_rate": 0.0002881539980256663,
      "loss": 1.5213,
      "step": 12000
    },
    {
      "epoch": 0.03981572885817703,
      "grad_norm": 19.295442581176758,
      "learning_rate": 0.00028805528134254687,
      "loss": 1.3512,
      "step": 12100
    },
    {
      "epoch": 0.04014478446857519,
      "grad_norm": 0.10301394015550613,
      "learning_rate": 0.0002879565646594274,
      "loss": 1.2942,
      "step": 12200
    },
    {
      "epoch": 0.040473840078973346,
      "grad_norm": 25.803565979003906,
      "learning_rate": 0.00028785784797630797,
      "loss": 1.3198,
      "step": 12300
    },
    {
      "epoch": 0.0408028956893715,
      "grad_norm": 1.3752483129501343,
      "learning_rate": 0.0002877591312931885,
      "loss": 1.3044,
      "step": 12400
    },
    {
      "epoch": 0.04113195129976966,
      "grad_norm": 16.66523551940918,
      "learning_rate": 0.00028766041461006906,
      "loss": 1.3429,
      "step": 12500
    },
    {
      "epoch": 0.04146100691016782,
      "grad_norm": 50.29449462890625,
      "learning_rate": 0.0002875616979269496,
      "loss": 1.3089,
      "step": 12600
    },
    {
      "epoch": 0.041790062520565975,
      "grad_norm": 32.365299224853516,
      "learning_rate": 0.0002874629812438302,
      "loss": 1.7975,
      "step": 12700
    },
    {
      "epoch": 0.04211911813096413,
      "grad_norm": 40.40311813354492,
      "learning_rate": 0.0002873642645607107,
      "loss": 1.5359,
      "step": 12800
    },
    {
      "epoch": 0.04244817374136229,
      "grad_norm": 18.551210403442383,
      "learning_rate": 0.00028726554787759126,
      "loss": 1.2236,
      "step": 12900
    },
    {
      "epoch": 0.04277722935176045,
      "grad_norm": 0.3933342695236206,
      "learning_rate": 0.00028716683119447186,
      "loss": 1.6049,
      "step": 13000
    },
    {
      "epoch": 0.043106284962158604,
      "grad_norm": 10.664335250854492,
      "learning_rate": 0.0002870681145113524,
      "loss": 1.3357,
      "step": 13100
    },
    {
      "epoch": 0.04343534057255676,
      "grad_norm": 15.04274845123291,
      "learning_rate": 0.00028696939782823296,
      "loss": 1.4538,
      "step": 13200
    },
    {
      "epoch": 0.04376439618295492,
      "grad_norm": 34.989349365234375,
      "learning_rate": 0.0002868706811451135,
      "loss": 1.4717,
      "step": 13300
    },
    {
      "epoch": 0.044093451793353076,
      "grad_norm": 7.800326347351074,
      "learning_rate": 0.00028677196446199406,
      "loss": 1.2123,
      "step": 13400
    },
    {
      "epoch": 0.044422507403751234,
      "grad_norm": 11.793018341064453,
      "learning_rate": 0.0002866732477788746,
      "loss": 1.4051,
      "step": 13500
    },
    {
      "epoch": 0.04475156301414939,
      "grad_norm": 21.465749740600586,
      "learning_rate": 0.00028657453109575515,
      "loss": 1.1329,
      "step": 13600
    },
    {
      "epoch": 0.04508061862454755,
      "grad_norm": 33.646419525146484,
      "learning_rate": 0.0002864758144126357,
      "loss": 1.1742,
      "step": 13700
    },
    {
      "epoch": 0.045409674234945706,
      "grad_norm": 2.923598527908325,
      "learning_rate": 0.00028637709772951625,
      "loss": 1.3227,
      "step": 13800
    },
    {
      "epoch": 0.04573872984534386,
      "grad_norm": 8.365792274475098,
      "learning_rate": 0.0002862783810463968,
      "loss": 1.4189,
      "step": 13900
    },
    {
      "epoch": 0.04606778545574202,
      "grad_norm": 0.38039156794548035,
      "learning_rate": 0.0002861796643632774,
      "loss": 1.2734,
      "step": 14000
    },
    {
      "epoch": 0.04639684106614018,
      "grad_norm": 75.36430358886719,
      "learning_rate": 0.0002860809476801579,
      "loss": 1.3476,
      "step": 14100
    },
    {
      "epoch": 0.046725896676538335,
      "grad_norm": 11.399001121520996,
      "learning_rate": 0.00028598223099703845,
      "loss": 1.5469,
      "step": 14200
    },
    {
      "epoch": 0.04705495228693649,
      "grad_norm": 15.15304183959961,
      "learning_rate": 0.00028588351431391905,
      "loss": 1.226,
      "step": 14300
    },
    {
      "epoch": 0.04738400789733465,
      "grad_norm": 20.978702545166016,
      "learning_rate": 0.0002857847976307996,
      "loss": 1.6944,
      "step": 14400
    },
    {
      "epoch": 0.04771306350773281,
      "grad_norm": 4.7003326416015625,
      "learning_rate": 0.0002856860809476801,
      "loss": 1.3239,
      "step": 14500
    },
    {
      "epoch": 0.048042119118130965,
      "grad_norm": 13.174882888793945,
      "learning_rate": 0.0002855873642645607,
      "loss": 1.0625,
      "step": 14600
    },
    {
      "epoch": 0.04837117472852912,
      "grad_norm": 12.609126091003418,
      "learning_rate": 0.00028548864758144124,
      "loss": 1.1989,
      "step": 14700
    },
    {
      "epoch": 0.04870023033892728,
      "grad_norm": 27.605913162231445,
      "learning_rate": 0.0002853899308983218,
      "loss": 1.5482,
      "step": 14800
    },
    {
      "epoch": 0.04902928594932544,
      "grad_norm": 8.982068061828613,
      "learning_rate": 0.00028529121421520234,
      "loss": 1.5572,
      "step": 14900
    },
    {
      "epoch": 0.049358341559723594,
      "grad_norm": 0.021548807621002197,
      "learning_rate": 0.0002851924975320829,
      "loss": 1.3406,
      "step": 15000
    },
    {
      "epoch": 0.04968739717012175,
      "grad_norm": 15.14240550994873,
      "learning_rate": 0.00028509378084896344,
      "loss": 1.3192,
      "step": 15100
    },
    {
      "epoch": 0.05001645278051991,
      "grad_norm": 11.715792655944824,
      "learning_rate": 0.000284995064165844,
      "loss": 1.073,
      "step": 15200
    },
    {
      "epoch": 0.050345508390918066,
      "grad_norm": 13.168684005737305,
      "learning_rate": 0.00028489634748272454,
      "loss": 1.2864,
      "step": 15300
    },
    {
      "epoch": 0.05067456400131622,
      "grad_norm": 85.2613754272461,
      "learning_rate": 0.00028479763079960514,
      "loss": 1.163,
      "step": 15400
    },
    {
      "epoch": 0.05100361961171438,
      "grad_norm": 15.517080307006836,
      "learning_rate": 0.00028469891411648564,
      "loss": 1.3473,
      "step": 15500
    },
    {
      "epoch": 0.05133267522211254,
      "grad_norm": 22.59316062927246,
      "learning_rate": 0.00028460019743336624,
      "loss": 1.2772,
      "step": 15600
    },
    {
      "epoch": 0.051661730832510695,
      "grad_norm": 45.09769058227539,
      "learning_rate": 0.0002845014807502468,
      "loss": 1.2279,
      "step": 15700
    },
    {
      "epoch": 0.05199078644290885,
      "grad_norm": 18.16171646118164,
      "learning_rate": 0.00028440276406712734,
      "loss": 1.2286,
      "step": 15800
    },
    {
      "epoch": 0.05231984205330701,
      "grad_norm": 37.96984100341797,
      "learning_rate": 0.0002843040473840079,
      "loss": 1.2982,
      "step": 15900
    },
    {
      "epoch": 0.05264889766370517,
      "grad_norm": 0.015725186094641685,
      "learning_rate": 0.00028420533070088843,
      "loss": 1.1599,
      "step": 16000
    },
    {
      "epoch": 0.052977953274103325,
      "grad_norm": 3.2253551483154297,
      "learning_rate": 0.000284106614017769,
      "loss": 1.2901,
      "step": 16100
    },
    {
      "epoch": 0.05330700888450148,
      "grad_norm": 23.65766716003418,
      "learning_rate": 0.00028400789733464953,
      "loss": 1.3148,
      "step": 16200
    },
    {
      "epoch": 0.05363606449489964,
      "grad_norm": 24.57794952392578,
      "learning_rate": 0.0002839091806515301,
      "loss": 1.4708,
      "step": 16300
    },
    {
      "epoch": 0.0539651201052978,
      "grad_norm": 25.095705032348633,
      "learning_rate": 0.00028381046396841063,
      "loss": 1.2628,
      "step": 16400
    },
    {
      "epoch": 0.054294175715695954,
      "grad_norm": 4.666771411895752,
      "learning_rate": 0.0002837117472852912,
      "loss": 1.427,
      "step": 16500
    },
    {
      "epoch": 0.05462323132609411,
      "grad_norm": 3.946946620941162,
      "learning_rate": 0.0002836130306021717,
      "loss": 1.1729,
      "step": 16600
    },
    {
      "epoch": 0.05495228693649227,
      "grad_norm": 11.277512550354004,
      "learning_rate": 0.00028351431391905233,
      "loss": 1.1655,
      "step": 16700
    },
    {
      "epoch": 0.055281342546890426,
      "grad_norm": 25.62821388244629,
      "learning_rate": 0.0002834155972359328,
      "loss": 1.2624,
      "step": 16800
    },
    {
      "epoch": 0.05561039815728858,
      "grad_norm": 5.963542938232422,
      "learning_rate": 0.00028331688055281337,
      "loss": 1.2882,
      "step": 16900
    },
    {
      "epoch": 0.05593945376768674,
      "grad_norm": 75.31952667236328,
      "learning_rate": 0.000283218163869694,
      "loss": 1.1795,
      "step": 17000
    },
    {
      "epoch": 0.0562685093780849,
      "grad_norm": 5.202600479125977,
      "learning_rate": 0.0002831194471865745,
      "loss": 1.4874,
      "step": 17100
    },
    {
      "epoch": 0.056597564988483055,
      "grad_norm": 24.182775497436523,
      "learning_rate": 0.0002830207305034551,
      "loss": 1.4406,
      "step": 17200
    },
    {
      "epoch": 0.05692662059888121,
      "grad_norm": 24.6784725189209,
      "learning_rate": 0.0002829220138203356,
      "loss": 1.2844,
      "step": 17300
    },
    {
      "epoch": 0.05725567620927937,
      "grad_norm": 0.16801807284355164,
      "learning_rate": 0.00028282329713721617,
      "loss": 1.2012,
      "step": 17400
    },
    {
      "epoch": 0.05758473181967753,
      "grad_norm": 28.245952606201172,
      "learning_rate": 0.0002827245804540967,
      "loss": 1.4552,
      "step": 17500
    },
    {
      "epoch": 0.057913787430075685,
      "grad_norm": 23.23835563659668,
      "learning_rate": 0.00028262586377097727,
      "loss": 1.2754,
      "step": 17600
    },
    {
      "epoch": 0.05824284304047384,
      "grad_norm": 17.425188064575195,
      "learning_rate": 0.0002825271470878578,
      "loss": 1.2945,
      "step": 17700
    },
    {
      "epoch": 0.058571898650872,
      "grad_norm": 5.500092029571533,
      "learning_rate": 0.00028242843040473837,
      "loss": 1.105,
      "step": 17800
    },
    {
      "epoch": 0.05890095426127016,
      "grad_norm": 4.115002155303955,
      "learning_rate": 0.0002823297137216189,
      "loss": 1.1534,
      "step": 17900
    },
    {
      "epoch": 0.059230009871668314,
      "grad_norm": 7.95510196685791,
      "learning_rate": 0.0002822309970384995,
      "loss": 1.2736,
      "step": 18000
    },
    {
      "epoch": 0.05955906548206647,
      "grad_norm": 31.61880874633789,
      "learning_rate": 0.00028213228035538,
      "loss": 1.2136,
      "step": 18100
    },
    {
      "epoch": 0.05988812109246463,
      "grad_norm": 22.24627685546875,
      "learning_rate": 0.00028203356367226056,
      "loss": 1.1221,
      "step": 18200
    },
    {
      "epoch": 0.060217176702862786,
      "grad_norm": 18.085620880126953,
      "learning_rate": 0.00028193484698914116,
      "loss": 1.0761,
      "step": 18300
    },
    {
      "epoch": 0.06054623231326094,
      "grad_norm": 46.53879928588867,
      "learning_rate": 0.0002818361303060217,
      "loss": 1.3345,
      "step": 18400
    },
    {
      "epoch": 0.0608752879236591,
      "grad_norm": 26.288721084594727,
      "learning_rate": 0.0002817374136229022,
      "loss": 1.3845,
      "step": 18500
    },
    {
      "epoch": 0.06120434353405726,
      "grad_norm": 28.76847267150879,
      "learning_rate": 0.0002816386969397828,
      "loss": 1.1764,
      "step": 18600
    },
    {
      "epoch": 0.061533399144455415,
      "grad_norm": 0.21714918315410614,
      "learning_rate": 0.00028153998025666336,
      "loss": 1.0666,
      "step": 18700
    },
    {
      "epoch": 0.06186245475485357,
      "grad_norm": 0.9305259585380554,
      "learning_rate": 0.0002814412635735439,
      "loss": 1.2219,
      "step": 18800
    },
    {
      "epoch": 0.06219151036525173,
      "grad_norm": 24.147289276123047,
      "learning_rate": 0.00028134254689042446,
      "loss": 1.0811,
      "step": 18900
    },
    {
      "epoch": 0.06252056597564988,
      "grad_norm": 15.995984077453613,
      "learning_rate": 0.000281243830207305,
      "loss": 1.2408,
      "step": 19000
    },
    {
      "epoch": 0.06284962158604804,
      "grad_norm": 0.8669624924659729,
      "learning_rate": 0.00028114511352418556,
      "loss": 0.9611,
      "step": 19100
    },
    {
      "epoch": 0.0631786771964462,
      "grad_norm": 0.31168892979621887,
      "learning_rate": 0.0002810463968410661,
      "loss": 1.0816,
      "step": 19200
    },
    {
      "epoch": 0.06350773280684435,
      "grad_norm": 82.7625503540039,
      "learning_rate": 0.00028094768015794665,
      "loss": 1.1722,
      "step": 19300
    },
    {
      "epoch": 0.06383678841724251,
      "grad_norm": 9.121329307556152,
      "learning_rate": 0.0002808489634748272,
      "loss": 1.4681,
      "step": 19400
    },
    {
      "epoch": 0.06416584402764067,
      "grad_norm": 50.535430908203125,
      "learning_rate": 0.00028075024679170775,
      "loss": 1.4049,
      "step": 19500
    },
    {
      "epoch": 0.06449489963803882,
      "grad_norm": 0.260204941034317,
      "learning_rate": 0.00028065153010858835,
      "loss": 1.4829,
      "step": 19600
    },
    {
      "epoch": 0.06482395524843698,
      "grad_norm": 1.3903752565383911,
      "learning_rate": 0.0002805528134254689,
      "loss": 1.0189,
      "step": 19700
    },
    {
      "epoch": 0.06515301085883514,
      "grad_norm": 27.558958053588867,
      "learning_rate": 0.0002804540967423494,
      "loss": 1.0401,
      "step": 19800
    },
    {
      "epoch": 0.0654820664692333,
      "grad_norm": 0.30852508544921875,
      "learning_rate": 0.00028035538005923,
      "loss": 0.9045,
      "step": 19900
    },
    {
      "epoch": 0.06581112207963145,
      "grad_norm": 0.5290734767913818,
      "learning_rate": 0.00028025666337611055,
      "loss": 1.1025,
      "step": 20000
    },
    {
      "epoch": 0.06614017769002961,
      "grad_norm": 2.4508543014526367,
      "learning_rate": 0.0002801579466929911,
      "loss": 1.2435,
      "step": 20100
    },
    {
      "epoch": 0.06646923330042777,
      "grad_norm": 52.084068298339844,
      "learning_rate": 0.00028005923000987165,
      "loss": 1.1853,
      "step": 20200
    },
    {
      "epoch": 0.06679828891082593,
      "grad_norm": 0.028526853770017624,
      "learning_rate": 0.0002799605133267522,
      "loss": 1.1734,
      "step": 20300
    },
    {
      "epoch": 0.06712734452122408,
      "grad_norm": 24.98676109313965,
      "learning_rate": 0.00027986179664363274,
      "loss": 1.3751,
      "step": 20400
    },
    {
      "epoch": 0.06745640013162224,
      "grad_norm": 0.869499146938324,
      "learning_rate": 0.0002797630799605133,
      "loss": 1.3027,
      "step": 20500
    },
    {
      "epoch": 0.0677854557420204,
      "grad_norm": 4.44050931930542,
      "learning_rate": 0.00027966436327739384,
      "loss": 1.2132,
      "step": 20600
    },
    {
      "epoch": 0.06811451135241856,
      "grad_norm": 6.003668785095215,
      "learning_rate": 0.0002795656465942744,
      "loss": 1.3196,
      "step": 20700
    },
    {
      "epoch": 0.06844356696281671,
      "grad_norm": 30.519662857055664,
      "learning_rate": 0.00027946692991115494,
      "loss": 1.2091,
      "step": 20800
    },
    {
      "epoch": 0.06877262257321487,
      "grad_norm": 23.090627670288086,
      "learning_rate": 0.0002793682132280355,
      "loss": 0.9423,
      "step": 20900
    },
    {
      "epoch": 0.06910167818361303,
      "grad_norm": 0.45696669816970825,
      "learning_rate": 0.0002792694965449161,
      "loss": 1.1639,
      "step": 21000
    },
    {
      "epoch": 0.06943073379401118,
      "grad_norm": 6.902951717376709,
      "learning_rate": 0.0002791707798617966,
      "loss": 1.127,
      "step": 21100
    },
    {
      "epoch": 0.06975978940440934,
      "grad_norm": 50.511619567871094,
      "learning_rate": 0.0002790720631786772,
      "loss": 1.3776,
      "step": 21200
    },
    {
      "epoch": 0.0700888450148075,
      "grad_norm": 32.000244140625,
      "learning_rate": 0.00027897334649555774,
      "loss": 1.1687,
      "step": 21300
    },
    {
      "epoch": 0.07041790062520566,
      "grad_norm": 36.47346115112305,
      "learning_rate": 0.0002788746298124383,
      "loss": 1.2045,
      "step": 21400
    },
    {
      "epoch": 0.07074695623560381,
      "grad_norm": 26.185335159301758,
      "learning_rate": 0.00027877591312931883,
      "loss": 1.3288,
      "step": 21500
    },
    {
      "epoch": 0.07107601184600197,
      "grad_norm": 24.708894729614258,
      "learning_rate": 0.0002786771964461994,
      "loss": 1.0965,
      "step": 21600
    },
    {
      "epoch": 0.07140506745640013,
      "grad_norm": 8.798635482788086,
      "learning_rate": 0.00027857847976307993,
      "loss": 1.1761,
      "step": 21700
    },
    {
      "epoch": 0.07173412306679829,
      "grad_norm": 56.69868087768555,
      "learning_rate": 0.0002784797630799605,
      "loss": 1.0807,
      "step": 21800
    },
    {
      "epoch": 0.07206317867719644,
      "grad_norm": 0.06690903007984161,
      "learning_rate": 0.00027838104639684103,
      "loss": 0.9661,
      "step": 21900
    },
    {
      "epoch": 0.0723922342875946,
      "grad_norm": 0.02120756171643734,
      "learning_rate": 0.00027828232971372163,
      "loss": 0.9327,
      "step": 22000
    },
    {
      "epoch": 0.07272128989799276,
      "grad_norm": 0.00681279506534338,
      "learning_rate": 0.00027818361303060213,
      "loss": 1.1685,
      "step": 22100
    },
    {
      "epoch": 0.07305034550839092,
      "grad_norm": 16.482084274291992,
      "learning_rate": 0.0002780848963474827,
      "loss": 1.0164,
      "step": 22200
    },
    {
      "epoch": 0.07337940111878907,
      "grad_norm": 17.314401626586914,
      "learning_rate": 0.0002779861796643633,
      "loss": 1.4606,
      "step": 22300
    },
    {
      "epoch": 0.07370845672918723,
      "grad_norm": 5.117361545562744,
      "learning_rate": 0.00027788746298124383,
      "loss": 1.2052,
      "step": 22400
    },
    {
      "epoch": 0.07403751233958539,
      "grad_norm": 16.867713928222656,
      "learning_rate": 0.0002777887462981243,
      "loss": 1.0836,
      "step": 22500
    },
    {
      "epoch": 0.07436656794998354,
      "grad_norm": 39.587486267089844,
      "learning_rate": 0.0002776900296150049,
      "loss": 1.1318,
      "step": 22600
    },
    {
      "epoch": 0.0746956235603817,
      "grad_norm": 38.62627410888672,
      "learning_rate": 0.0002775913129318855,
      "loss": 1.0926,
      "step": 22700
    },
    {
      "epoch": 0.07502467917077986,
      "grad_norm": 22.691701889038086,
      "learning_rate": 0.000277492596248766,
      "loss": 1.1502,
      "step": 22800
    },
    {
      "epoch": 0.07535373478117802,
      "grad_norm": 0.009940472431480885,
      "learning_rate": 0.00027739387956564657,
      "loss": 1.1174,
      "step": 22900
    },
    {
      "epoch": 0.07568279039157617,
      "grad_norm": 0.05685102939605713,
      "learning_rate": 0.0002772951628825271,
      "loss": 1.1164,
      "step": 23000
    },
    {
      "epoch": 0.07601184600197433,
      "grad_norm": 0.3134573698043823,
      "learning_rate": 0.00027719644619940767,
      "loss": 1.1439,
      "step": 23100
    },
    {
      "epoch": 0.07634090161237249,
      "grad_norm": 41.8845329284668,
      "learning_rate": 0.0002770977295162882,
      "loss": 1.3215,
      "step": 23200
    },
    {
      "epoch": 0.07666995722277065,
      "grad_norm": 32.225196838378906,
      "learning_rate": 0.00027699901283316877,
      "loss": 1.2941,
      "step": 23300
    },
    {
      "epoch": 0.0769990128331688,
      "grad_norm": 38.70964050292969,
      "learning_rate": 0.0002769002961500493,
      "loss": 1.114,
      "step": 23400
    },
    {
      "epoch": 0.07732806844356696,
      "grad_norm": 80.21131896972656,
      "learning_rate": 0.00027680157946692987,
      "loss": 1.1349,
      "step": 23500
    },
    {
      "epoch": 0.07765712405396512,
      "grad_norm": 9.761153221130371,
      "learning_rate": 0.00027670286278381047,
      "loss": 1.2689,
      "step": 23600
    },
    {
      "epoch": 0.07798617966436328,
      "grad_norm": 0.18529371917247772,
      "learning_rate": 0.000276604146100691,
      "loss": 0.9993,
      "step": 23700
    },
    {
      "epoch": 0.07831523527476143,
      "grad_norm": 46.00062942504883,
      "learning_rate": 0.0002765054294175715,
      "loss": 0.9617,
      "step": 23800
    },
    {
      "epoch": 0.07864429088515959,
      "grad_norm": 16.515409469604492,
      "learning_rate": 0.0002764067127344521,
      "loss": 0.883,
      "step": 23900
    },
    {
      "epoch": 0.07897334649555775,
      "grad_norm": 34.67168426513672,
      "learning_rate": 0.00027630799605133266,
      "loss": 1.1224,
      "step": 24000
    },
    {
      "epoch": 0.0793024021059559,
      "grad_norm": 0.6940349340438843,
      "learning_rate": 0.0002762092793682132,
      "loss": 1.1668,
      "step": 24100
    },
    {
      "epoch": 0.07963145771635406,
      "grad_norm": 57.9805908203125,
      "learning_rate": 0.00027611056268509376,
      "loss": 0.8608,
      "step": 24200
    },
    {
      "epoch": 0.07996051332675222,
      "grad_norm": 22.20753288269043,
      "learning_rate": 0.0002760118460019743,
      "loss": 1.011,
      "step": 24300
    },
    {
      "epoch": 0.08028956893715038,
      "grad_norm": 0.29074475169181824,
      "learning_rate": 0.00027591312931885486,
      "loss": 1.3234,
      "step": 24400
    },
    {
      "epoch": 0.08061862454754853,
      "grad_norm": 28.23038673400879,
      "learning_rate": 0.0002758144126357354,
      "loss": 1.0664,
      "step": 24500
    },
    {
      "epoch": 0.08094768015794669,
      "grad_norm": 0.048535820096731186,
      "learning_rate": 0.00027571569595261596,
      "loss": 1.1163,
      "step": 24600
    },
    {
      "epoch": 0.08127673576834485,
      "grad_norm": 7.0438618659973145,
      "learning_rate": 0.0002756169792694965,
      "loss": 1.2852,
      "step": 24700
    },
    {
      "epoch": 0.081605791378743,
      "grad_norm": 34.95845031738281,
      "learning_rate": 0.00027551826258637705,
      "loss": 1.3993,
      "step": 24800
    },
    {
      "epoch": 0.08193484698914116,
      "grad_norm": 6.127639293670654,
      "learning_rate": 0.0002754195459032576,
      "loss": 0.9435,
      "step": 24900
    },
    {
      "epoch": 0.08226390259953932,
      "grad_norm": 1.9987843036651611,
      "learning_rate": 0.0002753208292201382,
      "loss": 1.1708,
      "step": 25000
    },
    {
      "epoch": 0.08259295820993748,
      "grad_norm": 35.56844711303711,
      "learning_rate": 0.0002752221125370187,
      "loss": 1.056,
      "step": 25100
    },
    {
      "epoch": 0.08292201382033564,
      "grad_norm": 55.12843704223633,
      "learning_rate": 0.0002751233958538993,
      "loss": 0.9074,
      "step": 25200
    },
    {
      "epoch": 0.08325106943073379,
      "grad_norm": 21.09040069580078,
      "learning_rate": 0.00027502467917077985,
      "loss": 1.1311,
      "step": 25300
    },
    {
      "epoch": 0.08358012504113195,
      "grad_norm": 42.785301208496094,
      "learning_rate": 0.0002749259624876604,
      "loss": 1.0115,
      "step": 25400
    },
    {
      "epoch": 0.08390918065153011,
      "grad_norm": 7.804122447967529,
      "learning_rate": 0.00027482724580454095,
      "loss": 0.9762,
      "step": 25500
    },
    {
      "epoch": 0.08423823626192826,
      "grad_norm": 17.10700798034668,
      "learning_rate": 0.0002747285291214215,
      "loss": 0.8335,
      "step": 25600
    },
    {
      "epoch": 0.08456729187232642,
      "grad_norm": 0.29993122816085815,
      "learning_rate": 0.00027462981243830205,
      "loss": 1.0658,
      "step": 25700
    },
    {
      "epoch": 0.08489634748272458,
      "grad_norm": 30.513662338256836,
      "learning_rate": 0.0002745310957551826,
      "loss": 1.0446,
      "step": 25800
    },
    {
      "epoch": 0.08522540309312274,
      "grad_norm": 22.273357391357422,
      "learning_rate": 0.00027443237907206315,
      "loss": 1.1404,
      "step": 25900
    },
    {
      "epoch": 0.0855544587035209,
      "grad_norm": 29.18305015563965,
      "learning_rate": 0.0002743336623889437,
      "loss": 1.1832,
      "step": 26000
    },
    {
      "epoch": 0.08588351431391905,
      "grad_norm": 19.00503921508789,
      "learning_rate": 0.00027423494570582424,
      "loss": 1.0022,
      "step": 26100
    },
    {
      "epoch": 0.08621256992431721,
      "grad_norm": 29.45580291748047,
      "learning_rate": 0.0002741362290227048,
      "loss": 1.0964,
      "step": 26200
    },
    {
      "epoch": 0.08654162553471537,
      "grad_norm": 1.3029457330703735,
      "learning_rate": 0.0002740375123395854,
      "loss": 0.9168,
      "step": 26300
    },
    {
      "epoch": 0.08687068114511352,
      "grad_norm": 32.379547119140625,
      "learning_rate": 0.0002739387956564659,
      "loss": 0.8653,
      "step": 26400
    },
    {
      "epoch": 0.08719973675551168,
      "grad_norm": 66.28252410888672,
      "learning_rate": 0.00027384007897334644,
      "loss": 1.2652,
      "step": 26500
    },
    {
      "epoch": 0.08752879236590984,
      "grad_norm": 0.737153172492981,
      "learning_rate": 0.00027374136229022704,
      "loss": 0.9895,
      "step": 26600
    },
    {
      "epoch": 0.087857847976308,
      "grad_norm": 0.026779811829328537,
      "learning_rate": 0.0002736426456071076,
      "loss": 1.1779,
      "step": 26700
    },
    {
      "epoch": 0.08818690358670615,
      "grad_norm": 46.882904052734375,
      "learning_rate": 0.00027354392892398814,
      "loss": 1.018,
      "step": 26800
    },
    {
      "epoch": 0.08851595919710431,
      "grad_norm": 28.32533073425293,
      "learning_rate": 0.0002734452122408687,
      "loss": 1.1744,
      "step": 26900
    },
    {
      "epoch": 0.08884501480750247,
      "grad_norm": 0.047467973083257675,
      "learning_rate": 0.00027334649555774924,
      "loss": 1.2051,
      "step": 27000
    },
    {
      "epoch": 0.08917407041790062,
      "grad_norm": 19.850492477416992,
      "learning_rate": 0.0002732477788746298,
      "loss": 0.8007,
      "step": 27100
    },
    {
      "epoch": 0.08950312602829878,
      "grad_norm": 13.85080337524414,
      "learning_rate": 0.00027314906219151033,
      "loss": 1.021,
      "step": 27200
    },
    {
      "epoch": 0.08983218163869694,
      "grad_norm": 0.10562946647405624,
      "learning_rate": 0.0002730503455083909,
      "loss": 0.7471,
      "step": 27300
    },
    {
      "epoch": 0.0901612372490951,
      "grad_norm": 6.761717796325684,
      "learning_rate": 0.00027295162882527143,
      "loss": 0.829,
      "step": 27400
    },
    {
      "epoch": 0.09049029285949325,
      "grad_norm": 0.042734161019325256,
      "learning_rate": 0.000272852912142152,
      "loss": 0.8948,
      "step": 27500
    },
    {
      "epoch": 0.09081934846989141,
      "grad_norm": 12.572595596313477,
      "learning_rate": 0.0002727541954590326,
      "loss": 1.2455,
      "step": 27600
    },
    {
      "epoch": 0.09114840408028957,
      "grad_norm": 0.8714905381202698,
      "learning_rate": 0.00027265547877591313,
      "loss": 0.8397,
      "step": 27700
    },
    {
      "epoch": 0.09147745969068773,
      "grad_norm": 30.774402618408203,
      "learning_rate": 0.0002725567620927936,
      "loss": 1.1926,
      "step": 27800
    },
    {
      "epoch": 0.09180651530108588,
      "grad_norm": 17.092355728149414,
      "learning_rate": 0.00027245804540967423,
      "loss": 1.1636,
      "step": 27900
    },
    {
      "epoch": 0.09213557091148404,
      "grad_norm": 21.343233108520508,
      "learning_rate": 0.0002723593287265548,
      "loss": 1.0791,
      "step": 28000
    },
    {
      "epoch": 0.0924646265218822,
      "grad_norm": 15.824186325073242,
      "learning_rate": 0.00027226061204343533,
      "loss": 1.0077,
      "step": 28100
    },
    {
      "epoch": 0.09279368213228036,
      "grad_norm": 25.859241485595703,
      "learning_rate": 0.0002721618953603159,
      "loss": 0.8957,
      "step": 28200
    },
    {
      "epoch": 0.09312273774267851,
      "grad_norm": 0.055415499955415726,
      "learning_rate": 0.0002720631786771964,
      "loss": 0.7777,
      "step": 28300
    },
    {
      "epoch": 0.09345179335307667,
      "grad_norm": 0.01623404026031494,
      "learning_rate": 0.000271964461994077,
      "loss": 1.2971,
      "step": 28400
    },
    {
      "epoch": 0.09378084896347483,
      "grad_norm": 1.9157826900482178,
      "learning_rate": 0.0002718657453109575,
      "loss": 0.9339,
      "step": 28500
    },
    {
      "epoch": 0.09410990457387299,
      "grad_norm": 34.1822509765625,
      "learning_rate": 0.00027176702862783807,
      "loss": 1.1133,
      "step": 28600
    },
    {
      "epoch": 0.09443896018427114,
      "grad_norm": 0.006773377303034067,
      "learning_rate": 0.0002716683119447186,
      "loss": 0.8876,
      "step": 28700
    },
    {
      "epoch": 0.0947680157946693,
      "grad_norm": 1.352927327156067,
      "learning_rate": 0.00027156959526159917,
      "loss": 0.894,
      "step": 28800
    },
    {
      "epoch": 0.09509707140506746,
      "grad_norm": 1.722619652748108,
      "learning_rate": 0.00027147087857847977,
      "loss": 0.9099,
      "step": 28900
    },
    {
      "epoch": 0.09542612701546561,
      "grad_norm": 73.9580307006836,
      "learning_rate": 0.0002713721618953603,
      "loss": 1.0135,
      "step": 29000
    },
    {
      "epoch": 0.09575518262586377,
      "grad_norm": 0.8348667621612549,
      "learning_rate": 0.0002712734452122408,
      "loss": 1.1871,
      "step": 29100
    },
    {
      "epoch": 0.09608423823626193,
      "grad_norm": 78.05864715576172,
      "learning_rate": 0.0002711747285291214,
      "loss": 1.0905,
      "step": 29200
    },
    {
      "epoch": 0.09641329384666009,
      "grad_norm": 49.35911560058594,
      "learning_rate": 0.00027107601184600197,
      "loss": 1.0625,
      "step": 29300
    },
    {
      "epoch": 0.09674234945705824,
      "grad_norm": 0.00485367001965642,
      "learning_rate": 0.0002709772951628825,
      "loss": 1.056,
      "step": 29400
    },
    {
      "epoch": 0.0970714050674564,
      "grad_norm": 35.66923904418945,
      "learning_rate": 0.00027087857847976306,
      "loss": 0.9744,
      "step": 29500
    },
    {
      "epoch": 0.09740046067785456,
      "grad_norm": 1.1903010606765747,
      "learning_rate": 0.0002707798617966436,
      "loss": 0.7799,
      "step": 29600
    },
    {
      "epoch": 0.09772951628825272,
      "grad_norm": 12.449782371520996,
      "learning_rate": 0.00027068114511352416,
      "loss": 0.9172,
      "step": 29700
    },
    {
      "epoch": 0.09805857189865087,
      "grad_norm": 0.04577173292636871,
      "learning_rate": 0.0002705824284304047,
      "loss": 0.9806,
      "step": 29800
    },
    {
      "epoch": 0.09838762750904903,
      "grad_norm": 0.3773342967033386,
      "learning_rate": 0.00027048371174728526,
      "loss": 1.0007,
      "step": 29900
    },
    {
      "epoch": 0.09871668311944719,
      "grad_norm": 30.878000259399414,
      "learning_rate": 0.0002703849950641658,
      "loss": 1.1652,
      "step": 30000
    },
    {
      "epoch": 0.09904573872984535,
      "grad_norm": 0.018626336008310318,
      "learning_rate": 0.00027028627838104636,
      "loss": 1.0011,
      "step": 30100
    },
    {
      "epoch": 0.0993747943402435,
      "grad_norm": 14.553467750549316,
      "learning_rate": 0.0002701875616979269,
      "loss": 0.9077,
      "step": 30200
    },
    {
      "epoch": 0.09970384995064166,
      "grad_norm": 32.59054946899414,
      "learning_rate": 0.0002700888450148075,
      "loss": 0.6427,
      "step": 30300
    },
    {
      "epoch": 0.10003290556103982,
      "grad_norm": 2.6114814281463623,
      "learning_rate": 0.000269990128331688,
      "loss": 0.8202,
      "step": 30400
    },
    {
      "epoch": 0.10036196117143797,
      "grad_norm": 1.066384196281433,
      "learning_rate": 0.0002698914116485686,
      "loss": 0.9206,
      "step": 30500
    },
    {
      "epoch": 0.10069101678183613,
      "grad_norm": 0.26868852972984314,
      "learning_rate": 0.00026979269496544916,
      "loss": 1.0586,
      "step": 30600
    },
    {
      "epoch": 0.10102007239223429,
      "grad_norm": 25.68645668029785,
      "learning_rate": 0.0002696939782823297,
      "loss": 0.978,
      "step": 30700
    },
    {
      "epoch": 0.10134912800263245,
      "grad_norm": 0.5504429936408997,
      "learning_rate": 0.00026959526159921025,
      "loss": 0.7519,
      "step": 30800
    },
    {
      "epoch": 0.1016781836130306,
      "grad_norm": 15.607958793640137,
      "learning_rate": 0.0002694965449160908,
      "loss": 0.9549,
      "step": 30900
    },
    {
      "epoch": 0.10200723922342876,
      "grad_norm": 0.015046976506710052,
      "learning_rate": 0.00026939782823297135,
      "loss": 0.7963,
      "step": 31000
    },
    {
      "epoch": 0.10233629483382692,
      "grad_norm": 26.597412109375,
      "learning_rate": 0.0002692991115498519,
      "loss": 1.0317,
      "step": 31100
    },
    {
      "epoch": 0.10266535044422508,
      "grad_norm": 0.07668129354715347,
      "learning_rate": 0.00026920039486673245,
      "loss": 0.9102,
      "step": 31200
    },
    {
      "epoch": 0.10299440605462323,
      "grad_norm": 0.2961465120315552,
      "learning_rate": 0.000269101678183613,
      "loss": 0.8495,
      "step": 31300
    },
    {
      "epoch": 0.10332346166502139,
      "grad_norm": 30.942800521850586,
      "learning_rate": 0.00026900296150049355,
      "loss": 1.0141,
      "step": 31400
    },
    {
      "epoch": 0.10365251727541955,
      "grad_norm": 31.743486404418945,
      "learning_rate": 0.0002689042448173741,
      "loss": 0.9547,
      "step": 31500
    },
    {
      "epoch": 0.1039815728858177,
      "grad_norm": 26.247129440307617,
      "learning_rate": 0.0002688055281342547,
      "loss": 1.0535,
      "step": 31600
    },
    {
      "epoch": 0.10431062849621586,
      "grad_norm": 0.10111495852470398,
      "learning_rate": 0.0002687068114511352,
      "loss": 0.9896,
      "step": 31700
    },
    {
      "epoch": 0.10463968410661402,
      "grad_norm": 31.896272659301758,
      "learning_rate": 0.00026860809476801574,
      "loss": 0.9041,
      "step": 31800
    },
    {
      "epoch": 0.10496873971701218,
      "grad_norm": 2.864891767501831,
      "learning_rate": 0.00026850937808489634,
      "loss": 1.0408,
      "step": 31900
    },
    {
      "epoch": 0.10529779532741033,
      "grad_norm": 4.213115215301514,
      "learning_rate": 0.0002684106614017769,
      "loss": 1.0203,
      "step": 32000
    },
    {
      "epoch": 0.10562685093780849,
      "grad_norm": 41.059181213378906,
      "learning_rate": 0.00026831194471865744,
      "loss": 1.2693,
      "step": 32100
    },
    {
      "epoch": 0.10595590654820665,
      "grad_norm": 4.496204853057861,
      "learning_rate": 0.000268213228035538,
      "loss": 0.9605,
      "step": 32200
    },
    {
      "epoch": 0.1062849621586048,
      "grad_norm": 2.4058785438537598,
      "learning_rate": 0.00026811451135241854,
      "loss": 0.7794,
      "step": 32300
    },
    {
      "epoch": 0.10661401776900296,
      "grad_norm": 0.050092317163944244,
      "learning_rate": 0.0002680157946692991,
      "loss": 0.8533,
      "step": 32400
    },
    {
      "epoch": 0.10694307337940112,
      "grad_norm": 0.02121839113533497,
      "learning_rate": 0.00026791707798617964,
      "loss": 1.1601,
      "step": 32500
    },
    {
      "epoch": 0.10727212898979928,
      "grad_norm": 1.6258549690246582,
      "learning_rate": 0.0002678183613030602,
      "loss": 1.1044,
      "step": 32600
    },
    {
      "epoch": 0.10760118460019744,
      "grad_norm": 19.868730545043945,
      "learning_rate": 0.00026771964461994074,
      "loss": 0.9208,
      "step": 32700
    },
    {
      "epoch": 0.1079302402105956,
      "grad_norm": 56.386234283447266,
      "learning_rate": 0.0002676209279368213,
      "loss": 1.4443,
      "step": 32800
    },
    {
      "epoch": 0.10825929582099375,
      "grad_norm": 28.266666412353516,
      "learning_rate": 0.0002675222112537019,
      "loss": 0.9222,
      "step": 32900
    },
    {
      "epoch": 0.10858835143139191,
      "grad_norm": 0.8848844170570374,
      "learning_rate": 0.0002674234945705824,
      "loss": 1.0608,
      "step": 33000
    },
    {
      "epoch": 0.10891740704179007,
      "grad_norm": 0.00274060876108706,
      "learning_rate": 0.00026732477788746293,
      "loss": 1.0529,
      "step": 33100
    },
    {
      "epoch": 0.10924646265218822,
      "grad_norm": 38.37895202636719,
      "learning_rate": 0.00026722606120434353,
      "loss": 1.003,
      "step": 33200
    },
    {
      "epoch": 0.10957551826258638,
      "grad_norm": 68.47570037841797,
      "learning_rate": 0.0002671273445212241,
      "loss": 1.1015,
      "step": 33300
    },
    {
      "epoch": 0.10990457387298454,
      "grad_norm": 10.094945907592773,
      "learning_rate": 0.0002670286278381046,
      "loss": 1.1224,
      "step": 33400
    },
    {
      "epoch": 0.1102336294833827,
      "grad_norm": 0.2981680631637573,
      "learning_rate": 0.0002669299111549852,
      "loss": 1.0569,
      "step": 33500
    },
    {
      "epoch": 0.11056268509378085,
      "grad_norm": 17.79216194152832,
      "learning_rate": 0.00026683119447186573,
      "loss": 0.9147,
      "step": 33600
    },
    {
      "epoch": 0.11089174070417901,
      "grad_norm": 0.4816824495792389,
      "learning_rate": 0.0002667324777887463,
      "loss": 0.7552,
      "step": 33700
    },
    {
      "epoch": 0.11122079631457717,
      "grad_norm": 72.1223373413086,
      "learning_rate": 0.0002666337611056268,
      "loss": 0.9977,
      "step": 33800
    },
    {
      "epoch": 0.11154985192497532,
      "grad_norm": 0.0265927966684103,
      "learning_rate": 0.0002665350444225074,
      "loss": 0.9214,
      "step": 33900
    },
    {
      "epoch": 0.11187890753537348,
      "grad_norm": 2.4720449447631836,
      "learning_rate": 0.0002664363277393879,
      "loss": 0.8455,
      "step": 34000
    },
    {
      "epoch": 0.11220796314577164,
      "grad_norm": 12.015907287597656,
      "learning_rate": 0.00026633761105626847,
      "loss": 1.111,
      "step": 34100
    },
    {
      "epoch": 0.1125370187561698,
      "grad_norm": 2.5751864910125732,
      "learning_rate": 0.000266238894373149,
      "loss": 1.1362,
      "step": 34200
    },
    {
      "epoch": 0.11286607436656795,
      "grad_norm": 35.639835357666016,
      "learning_rate": 0.0002661401776900296,
      "loss": 1.0124,
      "step": 34300
    },
    {
      "epoch": 0.11319512997696611,
      "grad_norm": 66.31826782226562,
      "learning_rate": 0.0002660414610069101,
      "loss": 1.0599,
      "step": 34400
    },
    {
      "epoch": 0.11352418558736427,
      "grad_norm": 2.4123387336730957,
      "learning_rate": 0.0002659427443237907,
      "loss": 1.0091,
      "step": 34500
    },
    {
      "epoch": 0.11385324119776243,
      "grad_norm": 0.02289617992937565,
      "learning_rate": 0.00026584402764067127,
      "loss": 1.066,
      "step": 34600
    },
    {
      "epoch": 0.11418229680816058,
      "grad_norm": 0.7349450588226318,
      "learning_rate": 0.0002657453109575518,
      "loss": 0.8416,
      "step": 34700
    },
    {
      "epoch": 0.11451135241855874,
      "grad_norm": 0.0468321330845356,
      "learning_rate": 0.00026564659427443237,
      "loss": 0.9406,
      "step": 34800
    },
    {
      "epoch": 0.1148404080289569,
      "grad_norm": 0.5506474375724792,
      "learning_rate": 0.0002655478775913129,
      "loss": 0.9212,
      "step": 34900
    },
    {
      "epoch": 0.11516946363935505,
      "grad_norm": 31.424121856689453,
      "learning_rate": 0.00026544916090819347,
      "loss": 0.9084,
      "step": 35000
    },
    {
      "epoch": 0.11549851924975321,
      "grad_norm": 42.683773040771484,
      "learning_rate": 0.000265350444225074,
      "loss": 0.8514,
      "step": 35100
    },
    {
      "epoch": 0.11582757486015137,
      "grad_norm": 11.394269943237305,
      "learning_rate": 0.00026525172754195456,
      "loss": 0.9401,
      "step": 35200
    },
    {
      "epoch": 0.11615663047054953,
      "grad_norm": 26.692155838012695,
      "learning_rate": 0.0002651530108588351,
      "loss": 0.8418,
      "step": 35300
    },
    {
      "epoch": 0.11648568608094768,
      "grad_norm": 14.622775077819824,
      "learning_rate": 0.00026505429417571566,
      "loss": 0.9629,
      "step": 35400
    },
    {
      "epoch": 0.11681474169134584,
      "grad_norm": 31.275238037109375,
      "learning_rate": 0.0002649555774925962,
      "loss": 0.927,
      "step": 35500
    },
    {
      "epoch": 0.117143797301744,
      "grad_norm": 0.004024972207844257,
      "learning_rate": 0.0002648568608094768,
      "loss": 0.7281,
      "step": 35600
    },
    {
      "epoch": 0.11747285291214216,
      "grad_norm": 19.052579879760742,
      "learning_rate": 0.0002647581441263573,
      "loss": 0.9556,
      "step": 35700
    },
    {
      "epoch": 0.11780190852254031,
      "grad_norm": 0.0005968629266135395,
      "learning_rate": 0.00026465942744323786,
      "loss": 0.8245,
      "step": 35800
    },
    {
      "epoch": 0.11813096413293847,
      "grad_norm": 0.030321726575493813,
      "learning_rate": 0.00026456071076011846,
      "loss": 0.8039,
      "step": 35900
    },
    {
      "epoch": 0.11846001974333663,
      "grad_norm": 70.70208740234375,
      "learning_rate": 0.000264461994076999,
      "loss": 0.8726,
      "step": 36000
    },
    {
      "epoch": 0.11878907535373479,
      "grad_norm": 0.07518988847732544,
      "learning_rate": 0.00026436327739387956,
      "loss": 1.2313,
      "step": 36100
    },
    {
      "epoch": 0.11911813096413294,
      "grad_norm": 0.004199497867375612,
      "learning_rate": 0.0002642645607107601,
      "loss": 0.8938,
      "step": 36200
    },
    {
      "epoch": 0.1194471865745311,
      "grad_norm": 28.09827995300293,
      "learning_rate": 0.00026416584402764065,
      "loss": 1.0547,
      "step": 36300
    },
    {
      "epoch": 0.11977624218492926,
      "grad_norm": 0.8505516648292542,
      "learning_rate": 0.0002640671273445212,
      "loss": 1.1423,
      "step": 36400
    },
    {
      "epoch": 0.12010529779532741,
      "grad_norm": 30.702381134033203,
      "learning_rate": 0.00026396841066140175,
      "loss": 1.3078,
      "step": 36500
    },
    {
      "epoch": 0.12043435340572557,
      "grad_norm": 29.676471710205078,
      "learning_rate": 0.0002638696939782823,
      "loss": 0.9499,
      "step": 36600
    },
    {
      "epoch": 0.12076340901612373,
      "grad_norm": 0.9524828195571899,
      "learning_rate": 0.00026377097729516285,
      "loss": 0.8114,
      "step": 36700
    },
    {
      "epoch": 0.12109246462652189,
      "grad_norm": 0.11994986236095428,
      "learning_rate": 0.0002636722606120434,
      "loss": 1.1026,
      "step": 36800
    },
    {
      "epoch": 0.12142152023692004,
      "grad_norm": 0.7551356554031372,
      "learning_rate": 0.000263573543928924,
      "loss": 0.8556,
      "step": 36900
    },
    {
      "epoch": 0.1217505758473182,
      "grad_norm": 18.378469467163086,
      "learning_rate": 0.0002634748272458045,
      "loss": 0.8311,
      "step": 37000
    },
    {
      "epoch": 0.12207963145771636,
      "grad_norm": 54.595890045166016,
      "learning_rate": 0.00026337611056268505,
      "loss": 0.866,
      "step": 37100
    },
    {
      "epoch": 0.12240868706811452,
      "grad_norm": 40.74156951904297,
      "learning_rate": 0.00026327739387956565,
      "loss": 0.6938,
      "step": 37200
    },
    {
      "epoch": 0.12273774267851267,
      "grad_norm": 68.43450927734375,
      "learning_rate": 0.0002631786771964462,
      "loss": 1.0894,
      "step": 37300
    },
    {
      "epoch": 0.12306679828891083,
      "grad_norm": 0.5981807708740234,
      "learning_rate": 0.0002630799605133267,
      "loss": 0.8077,
      "step": 37400
    },
    {
      "epoch": 0.12339585389930899,
      "grad_norm": 114.40385437011719,
      "learning_rate": 0.0002629812438302073,
      "loss": 0.9594,
      "step": 37500
    },
    {
      "epoch": 0.12372490950970715,
      "grad_norm": 127.49518585205078,
      "learning_rate": 0.00026288252714708784,
      "loss": 0.9969,
      "step": 37600
    },
    {
      "epoch": 0.1240539651201053,
      "grad_norm": 0.10471474379301071,
      "learning_rate": 0.0002627838104639684,
      "loss": 0.9558,
      "step": 37700
    },
    {
      "epoch": 0.12438302073050346,
      "grad_norm": 35.70166015625,
      "learning_rate": 0.00026268509378084894,
      "loss": 1.1511,
      "step": 37800
    },
    {
      "epoch": 0.12471207634090162,
      "grad_norm": 38.20863342285156,
      "learning_rate": 0.0002625863770977295,
      "loss": 0.9426,
      "step": 37900
    },
    {
      "epoch": 0.12504113195129976,
      "grad_norm": 0.05477020516991615,
      "learning_rate": 0.00026248766041461004,
      "loss": 1.0559,
      "step": 38000
    },
    {
      "epoch": 0.12537018756169793,
      "grad_norm": 0.0013044486986473203,
      "learning_rate": 0.0002623889437314906,
      "loss": 0.7985,
      "step": 38100
    },
    {
      "epoch": 0.12569924317209608,
      "grad_norm": 0.4183287024497986,
      "learning_rate": 0.00026229022704837114,
      "loss": 0.9491,
      "step": 38200
    },
    {
      "epoch": 0.12602829878249425,
      "grad_norm": 50.20063400268555,
      "learning_rate": 0.0002621915103652517,
      "loss": 0.7203,
      "step": 38300
    },
    {
      "epoch": 0.1263573543928924,
      "grad_norm": 56.01333236694336,
      "learning_rate": 0.00026209279368213223,
      "loss": 1.2144,
      "step": 38400
    },
    {
      "epoch": 0.12668641000329056,
      "grad_norm": 0.04421975463628769,
      "learning_rate": 0.00026199407699901284,
      "loss": 0.729,
      "step": 38500
    },
    {
      "epoch": 0.1270154656136887,
      "grad_norm": 1.7320128679275513,
      "learning_rate": 0.0002618953603158934,
      "loss": 0.5434,
      "step": 38600
    },
    {
      "epoch": 0.12734452122408688,
      "grad_norm": 0.005157515872269869,
      "learning_rate": 0.0002617966436327739,
      "loss": 1.0387,
      "step": 38700
    },
    {
      "epoch": 0.12767357683448502,
      "grad_norm": 26.474712371826172,
      "learning_rate": 0.0002616979269496545,
      "loss": 0.8546,
      "step": 38800
    },
    {
      "epoch": 0.1280026324448832,
      "grad_norm": 23.317462921142578,
      "learning_rate": 0.00026159921026653503,
      "loss": 0.8143,
      "step": 38900
    },
    {
      "epoch": 0.12833168805528133,
      "grad_norm": 0.8112173080444336,
      "learning_rate": 0.0002615004935834156,
      "loss": 0.7941,
      "step": 39000
    },
    {
      "epoch": 0.1286607436656795,
      "grad_norm": 2.740246295928955,
      "learning_rate": 0.00026140177690029613,
      "loss": 0.9539,
      "step": 39100
    },
    {
      "epoch": 0.12898979927607765,
      "grad_norm": 0.0026430038269609213,
      "learning_rate": 0.0002613030602171767,
      "loss": 0.7768,
      "step": 39200
    },
    {
      "epoch": 0.12931885488647582,
      "grad_norm": 0.009990617632865906,
      "learning_rate": 0.00026120434353405723,
      "loss": 0.8323,
      "step": 39300
    },
    {
      "epoch": 0.12964791049687396,
      "grad_norm": 26.156335830688477,
      "learning_rate": 0.0002611056268509378,
      "loss": 0.8127,
      "step": 39400
    },
    {
      "epoch": 0.12997696610727213,
      "grad_norm": 0.04899971932172775,
      "learning_rate": 0.0002610069101678183,
      "loss": 1.0534,
      "step": 39500
    },
    {
      "epoch": 0.13030602171767028,
      "grad_norm": 49.5349235534668,
      "learning_rate": 0.00026090819348469893,
      "loss": 0.9697,
      "step": 39600
    },
    {
      "epoch": 0.13063507732806845,
      "grad_norm": 0.016288259997963905,
      "learning_rate": 0.0002608094768015794,
      "loss": 0.8363,
      "step": 39700
    },
    {
      "epoch": 0.1309641329384666,
      "grad_norm": 0.0022034391295164824,
      "learning_rate": 0.00026071076011845997,
      "loss": 1.148,
      "step": 39800
    },
    {
      "epoch": 0.13129318854886476,
      "grad_norm": 0.2540806829929352,
      "learning_rate": 0.0002606120434353406,
      "loss": 0.8086,
      "step": 39900
    },
    {
      "epoch": 0.1316222441592629,
      "grad_norm": 18.539953231811523,
      "learning_rate": 0.0002605133267522211,
      "loss": 0.6366,
      "step": 40000
    },
    {
      "epoch": 0.13195129976966108,
      "grad_norm": 2.110522508621216,
      "learning_rate": 0.00026041461006910167,
      "loss": 0.8263,
      "step": 40100
    },
    {
      "epoch": 0.13228035538005922,
      "grad_norm": 0.01595642790198326,
      "learning_rate": 0.0002603158933859822,
      "loss": 0.8628,
      "step": 40200
    },
    {
      "epoch": 0.1326094109904574,
      "grad_norm": 0.027302494272589684,
      "learning_rate": 0.00026021717670286277,
      "loss": 0.6367,
      "step": 40300
    },
    {
      "epoch": 0.13293846660085554,
      "grad_norm": 2.1432390213012695,
      "learning_rate": 0.0002601184600197433,
      "loss": 0.7799,
      "step": 40400
    },
    {
      "epoch": 0.1332675222112537,
      "grad_norm": 0.3085179030895233,
      "learning_rate": 0.00026001974333662387,
      "loss": 0.8132,
      "step": 40500
    },
    {
      "epoch": 0.13359657782165185,
      "grad_norm": 66.68473815917969,
      "learning_rate": 0.0002599210266535044,
      "loss": 0.793,
      "step": 40600
    },
    {
      "epoch": 0.13392563343205002,
      "grad_norm": 0.019238591194152832,
      "learning_rate": 0.00025982230997038497,
      "loss": 0.6817,
      "step": 40700
    },
    {
      "epoch": 0.13425468904244817,
      "grad_norm": 0.012943447567522526,
      "learning_rate": 0.0002597235932872655,
      "loss": 1.1491,
      "step": 40800
    },
    {
      "epoch": 0.13458374465284634,
      "grad_norm": 68.46199035644531,
      "learning_rate": 0.0002596248766041461,
      "loss": 0.7018,
      "step": 40900
    },
    {
      "epoch": 0.13491280026324448,
      "grad_norm": 55.4526481628418,
      "learning_rate": 0.0002595261599210266,
      "loss": 1.1627,
      "step": 41000
    },
    {
      "epoch": 0.13524185587364265,
      "grad_norm": 0.014761434867978096,
      "learning_rate": 0.00025942744323790716,
      "loss": 0.4496,
      "step": 41100
    },
    {
      "epoch": 0.1355709114840408,
      "grad_norm": 102.87086486816406,
      "learning_rate": 0.00025932872655478776,
      "loss": 1.0069,
      "step": 41200
    },
    {
      "epoch": 0.13589996709443897,
      "grad_norm": 39.8873176574707,
      "learning_rate": 0.0002592300098716683,
      "loss": 1.064,
      "step": 41300
    },
    {
      "epoch": 0.1362290227048371,
      "grad_norm": 0.014402062632143497,
      "learning_rate": 0.0002591312931885488,
      "loss": 0.8148,
      "step": 41400
    },
    {
      "epoch": 0.13655807831523528,
      "grad_norm": 9.668488502502441,
      "learning_rate": 0.0002590325765054294,
      "loss": 0.7352,
      "step": 41500
    },
    {
      "epoch": 0.13688713392563343,
      "grad_norm": 59.654972076416016,
      "learning_rate": 0.00025893385982230996,
      "loss": 1.0588,
      "step": 41600
    },
    {
      "epoch": 0.1372161895360316,
      "grad_norm": 0.006129059009253979,
      "learning_rate": 0.0002588351431391905,
      "loss": 0.7576,
      "step": 41700
    },
    {
      "epoch": 0.13754524514642974,
      "grad_norm": 17.082744598388672,
      "learning_rate": 0.00025873642645607106,
      "loss": 0.7904,
      "step": 41800
    },
    {
      "epoch": 0.1378743007568279,
      "grad_norm": 2.0401675701141357,
      "learning_rate": 0.0002586377097729516,
      "loss": 0.7653,
      "step": 41900
    },
    {
      "epoch": 0.13820335636722605,
      "grad_norm": 2.4823877811431885,
      "learning_rate": 0.00025853899308983215,
      "loss": 0.9577,
      "step": 42000
    },
    {
      "epoch": 0.13853241197762423,
      "grad_norm": 47.020668029785156,
      "learning_rate": 0.0002584402764067127,
      "loss": 0.9667,
      "step": 42100
    },
    {
      "epoch": 0.13886146758802237,
      "grad_norm": 26.870702743530273,
      "learning_rate": 0.00025834155972359325,
      "loss": 0.4864,
      "step": 42200
    },
    {
      "epoch": 0.13919052319842054,
      "grad_norm": 0.20750388503074646,
      "learning_rate": 0.0002582428430404738,
      "loss": 0.9139,
      "step": 42300
    },
    {
      "epoch": 0.13951957880881868,
      "grad_norm": 52.29610061645508,
      "learning_rate": 0.00025814412635735435,
      "loss": 0.8207,
      "step": 42400
    },
    {
      "epoch": 0.13984863441921686,
      "grad_norm": 7.401065826416016,
      "learning_rate": 0.00025804540967423495,
      "loss": 0.9598,
      "step": 42500
    },
    {
      "epoch": 0.140177690029615,
      "grad_norm": 43.63682556152344,
      "learning_rate": 0.0002579466929911155,
      "loss": 1.0022,
      "step": 42600
    },
    {
      "epoch": 0.14050674564001317,
      "grad_norm": 0.00045641581527888775,
      "learning_rate": 0.000257847976307996,
      "loss": 0.8885,
      "step": 42700
    },
    {
      "epoch": 0.1408358012504113,
      "grad_norm": 4.87493896484375,
      "learning_rate": 0.0002577492596248766,
      "loss": 0.884,
      "step": 42800
    },
    {
      "epoch": 0.14116485686080948,
      "grad_norm": 0.5591903328895569,
      "learning_rate": 0.00025765054294175715,
      "loss": 0.724,
      "step": 42900
    },
    {
      "epoch": 0.14149391247120763,
      "grad_norm": 71.06829071044922,
      "learning_rate": 0.0002575518262586377,
      "loss": 0.8592,
      "step": 43000
    },
    {
      "epoch": 0.1418229680816058,
      "grad_norm": 0.07991084456443787,
      "learning_rate": 0.00025745310957551824,
      "loss": 0.7904,
      "step": 43100
    },
    {
      "epoch": 0.14215202369200394,
      "grad_norm": 0.3773704469203949,
      "learning_rate": 0.0002573543928923988,
      "loss": 1.221,
      "step": 43200
    },
    {
      "epoch": 0.1424810793024021,
      "grad_norm": 1.4641917943954468,
      "learning_rate": 0.00025725567620927934,
      "loss": 0.81,
      "step": 43300
    },
    {
      "epoch": 0.14281013491280026,
      "grad_norm": 0.00932508334517479,
      "learning_rate": 0.0002571569595261599,
      "loss": 0.9623,
      "step": 43400
    },
    {
      "epoch": 0.14313919052319843,
      "grad_norm": 0.07235923409461975,
      "learning_rate": 0.00025705824284304044,
      "loss": 0.6075,
      "step": 43500
    },
    {
      "epoch": 0.14346824613359657,
      "grad_norm": 0.28885847330093384,
      "learning_rate": 0.000256959526159921,
      "loss": 1.0917,
      "step": 43600
    },
    {
      "epoch": 0.14379730174399474,
      "grad_norm": 0.2851313352584839,
      "learning_rate": 0.00025686080947680154,
      "loss": 0.7655,
      "step": 43700
    },
    {
      "epoch": 0.1441263573543929,
      "grad_norm": 63.518070220947266,
      "learning_rate": 0.0002567620927936821,
      "loss": 0.7485,
      "step": 43800
    },
    {
      "epoch": 0.14445541296479106,
      "grad_norm": 0.01455238088965416,
      "learning_rate": 0.0002566633761105627,
      "loss": 0.769,
      "step": 43900
    },
    {
      "epoch": 0.1447844685751892,
      "grad_norm": 34.89935302734375,
      "learning_rate": 0.0002565646594274432,
      "loss": 0.784,
      "step": 44000
    },
    {
      "epoch": 0.14511352418558737,
      "grad_norm": 4.571837425231934,
      "learning_rate": 0.0002564659427443238,
      "loss": 0.8738,
      "step": 44100
    },
    {
      "epoch": 0.14544257979598552,
      "grad_norm": 24.642606735229492,
      "learning_rate": 0.00025636722606120434,
      "loss": 0.9616,
      "step": 44200
    },
    {
      "epoch": 0.1457716354063837,
      "grad_norm": 0.0068356129340827465,
      "learning_rate": 0.0002562685093780849,
      "loss": 0.8113,
      "step": 44300
    },
    {
      "epoch": 0.14610069101678183,
      "grad_norm": 0.013837086968123913,
      "learning_rate": 0.00025616979269496543,
      "loss": 0.6071,
      "step": 44400
    },
    {
      "epoch": 0.14642974662718,
      "grad_norm": 0.0031246626749634743,
      "learning_rate": 0.000256071076011846,
      "loss": 0.8625,
      "step": 44500
    },
    {
      "epoch": 0.14675880223757815,
      "grad_norm": 0.24293948709964752,
      "learning_rate": 0.00025597235932872653,
      "loss": 0.9254,
      "step": 44600
    },
    {
      "epoch": 0.14708785784797632,
      "grad_norm": 32.26646041870117,
      "learning_rate": 0.0002558736426456071,
      "loss": 0.7351,
      "step": 44700
    },
    {
      "epoch": 0.14741691345837446,
      "grad_norm": 0.01669878140091896,
      "learning_rate": 0.00025577492596248763,
      "loss": 0.9418,
      "step": 44800
    },
    {
      "epoch": 0.14774596906877263,
      "grad_norm": 5.046017646789551,
      "learning_rate": 0.00025567620927936823,
      "loss": 0.6144,
      "step": 44900
    },
    {
      "epoch": 0.14807502467917077,
      "grad_norm": 5.891852855682373,
      "learning_rate": 0.0002555774925962487,
      "loss": 0.6899,
      "step": 45000
    },
    {
      "epoch": 0.14840408028956895,
      "grad_norm": 46.417152404785156,
      "learning_rate": 0.0002554787759131293,
      "loss": 0.6296,
      "step": 45100
    },
    {
      "epoch": 0.1487331358999671,
      "grad_norm": 0.07955387979745865,
      "learning_rate": 0.0002553800592300099,
      "loss": 0.6874,
      "step": 45200
    },
    {
      "epoch": 0.14906219151036526,
      "grad_norm": 0.1812397539615631,
      "learning_rate": 0.0002552813425468904,
      "loss": 0.899,
      "step": 45300
    },
    {
      "epoch": 0.1493912471207634,
      "grad_norm": 0.578154444694519,
      "learning_rate": 0.0002551826258637709,
      "loss": 0.9881,
      "step": 45400
    },
    {
      "epoch": 0.14972030273116158,
      "grad_norm": 16.24181365966797,
      "learning_rate": 0.0002550839091806515,
      "loss": 0.8789,
      "step": 45500
    },
    {
      "epoch": 0.15004935834155972,
      "grad_norm": 0.0011557297548279166,
      "learning_rate": 0.0002549851924975321,
      "loss": 0.7521,
      "step": 45600
    },
    {
      "epoch": 0.1503784139519579,
      "grad_norm": 60.63364791870117,
      "learning_rate": 0.0002548864758144126,
      "loss": 0.9399,
      "step": 45700
    },
    {
      "epoch": 0.15070746956235603,
      "grad_norm": 0.06362058967351913,
      "learning_rate": 0.00025478775913129317,
      "loss": 0.7224,
      "step": 45800
    },
    {
      "epoch": 0.1510365251727542,
      "grad_norm": 6.596934795379639,
      "learning_rate": 0.0002546890424481737,
      "loss": 0.6328,
      "step": 45900
    },
    {
      "epoch": 0.15136558078315235,
      "grad_norm": 0.0657157227396965,
      "learning_rate": 0.00025459032576505427,
      "loss": 0.8277,
      "step": 46000
    },
    {
      "epoch": 0.15169463639355052,
      "grad_norm": 0.002142220037057996,
      "learning_rate": 0.0002544916090819348,
      "loss": 0.7616,
      "step": 46100
    },
    {
      "epoch": 0.15202369200394866,
      "grad_norm": 42.67353820800781,
      "learning_rate": 0.00025439289239881537,
      "loss": 1.0733,
      "step": 46200
    },
    {
      "epoch": 0.15235274761434683,
      "grad_norm": 0.1787683218717575,
      "learning_rate": 0.0002542941757156959,
      "loss": 0.8819,
      "step": 46300
    },
    {
      "epoch": 0.15268180322474498,
      "grad_norm": 0.06412261724472046,
      "learning_rate": 0.00025419545903257646,
      "loss": 0.7926,
      "step": 46400
    },
    {
      "epoch": 0.15301085883514315,
      "grad_norm": 10.276673316955566,
      "learning_rate": 0.00025409674234945707,
      "loss": 0.8458,
      "step": 46500
    },
    {
      "epoch": 0.1533399144455413,
      "grad_norm": 0.0024978406727313995,
      "learning_rate": 0.0002539980256663376,
      "loss": 1.0802,
      "step": 46600
    },
    {
      "epoch": 0.15366897005593946,
      "grad_norm": 0.002261787885800004,
      "learning_rate": 0.0002538993089832181,
      "loss": 0.8841,
      "step": 46700
    },
    {
      "epoch": 0.1539980256663376,
      "grad_norm": 39.29524230957031,
      "learning_rate": 0.0002538005923000987,
      "loss": 0.6631,
      "step": 46800
    },
    {
      "epoch": 0.15432708127673578,
      "grad_norm": 5.199473857879639,
      "learning_rate": 0.00025370187561697926,
      "loss": 0.8471,
      "step": 46900
    },
    {
      "epoch": 0.15465613688713392,
      "grad_norm": 13.529533386230469,
      "learning_rate": 0.0002536031589338598,
      "loss": 0.8686,
      "step": 47000
    },
    {
      "epoch": 0.1549851924975321,
      "grad_norm": 0.5884288549423218,
      "learning_rate": 0.00025350444225074036,
      "loss": 0.795,
      "step": 47100
    },
    {
      "epoch": 0.15531424810793024,
      "grad_norm": 0.0031952711287885904,
      "learning_rate": 0.0002534057255676209,
      "loss": 0.7878,
      "step": 47200
    },
    {
      "epoch": 0.1556433037183284,
      "grad_norm": 0.004437153227627277,
      "learning_rate": 0.00025330700888450146,
      "loss": 0.6199,
      "step": 47300
    },
    {
      "epoch": 0.15597235932872655,
      "grad_norm": 44.17815399169922,
      "learning_rate": 0.000253208292201382,
      "loss": 0.9252,
      "step": 47400
    },
    {
      "epoch": 0.15630141493912472,
      "grad_norm": 0.81425940990448,
      "learning_rate": 0.00025310957551826256,
      "loss": 0.9418,
      "step": 47500
    },
    {
      "epoch": 0.15663047054952287,
      "grad_norm": 41.66532897949219,
      "learning_rate": 0.0002530108588351431,
      "loss": 0.87,
      "step": 47600
    },
    {
      "epoch": 0.15695952615992104,
      "grad_norm": 1.641737699508667,
      "learning_rate": 0.00025291214215202365,
      "loss": 0.8755,
      "step": 47700
    },
    {
      "epoch": 0.15728858177031918,
      "grad_norm": 0.04475783184170723,
      "learning_rate": 0.0002528134254689042,
      "loss": 0.7715,
      "step": 47800
    },
    {
      "epoch": 0.15761763738071735,
      "grad_norm": 0.46906036138534546,
      "learning_rate": 0.0002527147087857848,
      "loss": 0.7821,
      "step": 47900
    },
    {
      "epoch": 0.1579466929911155,
      "grad_norm": 0.3305809497833252,
      "learning_rate": 0.0002526159921026653,
      "loss": 0.4856,
      "step": 48000
    },
    {
      "epoch": 0.15827574860151367,
      "grad_norm": 0.0352504588663578,
      "learning_rate": 0.0002525172754195459,
      "loss": 0.7004,
      "step": 48100
    },
    {
      "epoch": 0.1586048042119118,
      "grad_norm": 0.950230598449707,
      "learning_rate": 0.00025241855873642645,
      "loss": 0.573,
      "step": 48200
    },
    {
      "epoch": 0.15893385982230998,
      "grad_norm": 24.946012496948242,
      "learning_rate": 0.000252319842053307,
      "loss": 0.8241,
      "step": 48300
    },
    {
      "epoch": 0.15926291543270812,
      "grad_norm": 0.04233994334936142,
      "learning_rate": 0.00025222112537018755,
      "loss": 0.9462,
      "step": 48400
    },
    {
      "epoch": 0.1595919710431063,
      "grad_norm": 0.017064552754163742,
      "learning_rate": 0.0002521224086870681,
      "loss": 0.7124,
      "step": 48500
    },
    {
      "epoch": 0.15992102665350444,
      "grad_norm": 91.73152160644531,
      "learning_rate": 0.00025202369200394865,
      "loss": 0.7599,
      "step": 48600
    },
    {
      "epoch": 0.1602500822639026,
      "grad_norm": 83.43700408935547,
      "learning_rate": 0.0002519249753208292,
      "loss": 0.7637,
      "step": 48700
    },
    {
      "epoch": 0.16057913787430075,
      "grad_norm": 0.04470737278461456,
      "learning_rate": 0.00025182625863770974,
      "loss": 0.7519,
      "step": 48800
    },
    {
      "epoch": 0.16090819348469892,
      "grad_norm": 20.011302947998047,
      "learning_rate": 0.0002517275419545903,
      "loss": 0.8408,
      "step": 48900
    },
    {
      "epoch": 0.16123724909509707,
      "grad_norm": 36.09291076660156,
      "learning_rate": 0.00025162882527147084,
      "loss": 1.1368,
      "step": 49000
    },
    {
      "epoch": 0.16156630470549524,
      "grad_norm": 39.92390060424805,
      "learning_rate": 0.0002515301085883514,
      "loss": 0.5902,
      "step": 49100
    },
    {
      "epoch": 0.16189536031589338,
      "grad_norm": 0.1240185797214508,
      "learning_rate": 0.000251431391905232,
      "loss": 0.8019,
      "step": 49200
    },
    {
      "epoch": 0.16222441592629155,
      "grad_norm": 31.128990173339844,
      "learning_rate": 0.0002513326752221125,
      "loss": 0.8744,
      "step": 49300
    },
    {
      "epoch": 0.1625534715366897,
      "grad_norm": 2.3553590774536133,
      "learning_rate": 0.00025123395853899304,
      "loss": 0.9376,
      "step": 49400
    },
    {
      "epoch": 0.16288252714708787,
      "grad_norm": 48.96177291870117,
      "learning_rate": 0.00025113524185587364,
      "loss": 1.0119,
      "step": 49500
    },
    {
      "epoch": 0.163211582757486,
      "grad_norm": 0.006567133590579033,
      "learning_rate": 0.0002510365251727542,
      "loss": 0.5709,
      "step": 49600
    },
    {
      "epoch": 0.16354063836788418,
      "grad_norm": 0.052855558693408966,
      "learning_rate": 0.00025093780848963474,
      "loss": 0.5578,
      "step": 49700
    },
    {
      "epoch": 0.16386969397828233,
      "grad_norm": 0.10171183943748474,
      "learning_rate": 0.0002508390918065153,
      "loss": 0.8064,
      "step": 49800
    },
    {
      "epoch": 0.1641987495886805,
      "grad_norm": 0.006377536803483963,
      "learning_rate": 0.00025074037512339583,
      "loss": 0.8618,
      "step": 49900
    },
    {
      "epoch": 0.16452780519907864,
      "grad_norm": 0.001867928309366107,
      "learning_rate": 0.0002506416584402764,
      "loss": 0.7556,
      "step": 50000
    },
    {
      "epoch": 0.1648568608094768,
      "grad_norm": 0.06812656670808792,
      "learning_rate": 0.00025054294175715693,
      "loss": 0.7745,
      "step": 50100
    },
    {
      "epoch": 0.16518591641987496,
      "grad_norm": 0.6424093246459961,
      "learning_rate": 0.0002504442250740375,
      "loss": 0.7076,
      "step": 50200
    },
    {
      "epoch": 0.16551497203027313,
      "grad_norm": 0.022871097549796104,
      "learning_rate": 0.00025034550839091803,
      "loss": 0.5658,
      "step": 50300
    },
    {
      "epoch": 0.16584402764067127,
      "grad_norm": 126.95492553710938,
      "learning_rate": 0.0002502467917077986,
      "loss": 0.9809,
      "step": 50400
    },
    {
      "epoch": 0.16617308325106944,
      "grad_norm": 0.002950911410152912,
      "learning_rate": 0.0002501480750246792,
      "loss": 0.869,
      "step": 50500
    },
    {
      "epoch": 0.16650213886146759,
      "grad_norm": 4.078858852386475,
      "learning_rate": 0.0002500493583415597,
      "loss": 0.7873,
      "step": 50600
    },
    {
      "epoch": 0.16683119447186576,
      "grad_norm": 0.015391635708510876,
      "learning_rate": 0.0002499506416584402,
      "loss": 0.4898,
      "step": 50700
    },
    {
      "epoch": 0.1671602500822639,
      "grad_norm": 0.10633089393377304,
      "learning_rate": 0.00024985192497532083,
      "loss": 0.7461,
      "step": 50800
    },
    {
      "epoch": 0.16748930569266207,
      "grad_norm": 0.022238923236727715,
      "learning_rate": 0.0002497532082922014,
      "loss": 0.7673,
      "step": 50900
    },
    {
      "epoch": 0.16781836130306022,
      "grad_norm": 0.03617442399263382,
      "learning_rate": 0.00024965449160908187,
      "loss": 0.5969,
      "step": 51000
    },
    {
      "epoch": 0.1681474169134584,
      "grad_norm": 0.01263337954878807,
      "learning_rate": 0.0002495557749259625,
      "loss": 0.4256,
      "step": 51100
    },
    {
      "epoch": 0.16847647252385653,
      "grad_norm": 18.62645721435547,
      "learning_rate": 0.000249457058242843,
      "loss": 0.7135,
      "step": 51200
    },
    {
      "epoch": 0.1688055281342547,
      "grad_norm": 0.05817180871963501,
      "learning_rate": 0.00024935834155972357,
      "loss": 0.6469,
      "step": 51300
    },
    {
      "epoch": 0.16913458374465284,
      "grad_norm": 5.224393367767334,
      "learning_rate": 0.0002492596248766041,
      "loss": 0.6187,
      "step": 51400
    },
    {
      "epoch": 0.16946363935505102,
      "grad_norm": 0.00344553729519248,
      "learning_rate": 0.00024916090819348467,
      "loss": 0.7693,
      "step": 51500
    },
    {
      "epoch": 0.16979269496544916,
      "grad_norm": 0.1556592881679535,
      "learning_rate": 0.0002490621915103652,
      "loss": 0.7849,
      "step": 51600
    },
    {
      "epoch": 0.17012175057584733,
      "grad_norm": 13.624731063842773,
      "learning_rate": 0.00024896347482724577,
      "loss": 0.9849,
      "step": 51700
    },
    {
      "epoch": 0.17045080618624547,
      "grad_norm": 7.865466117858887,
      "learning_rate": 0.0002488647581441263,
      "loss": 1.003,
      "step": 51800
    },
    {
      "epoch": 0.17077986179664364,
      "grad_norm": 1.0939457416534424,
      "learning_rate": 0.0002487660414610069,
      "loss": 0.6964,
      "step": 51900
    },
    {
      "epoch": 0.1711089174070418,
      "grad_norm": 25.258392333984375,
      "learning_rate": 0.0002486673247778874,
      "loss": 0.7247,
      "step": 52000
    },
    {
      "epoch": 0.17143797301743996,
      "grad_norm": 0.011581065133213997,
      "learning_rate": 0.000248568608094768,
      "loss": 0.5942,
      "step": 52100
    },
    {
      "epoch": 0.1717670286278381,
      "grad_norm": 0.012774951756000519,
      "learning_rate": 0.00024846989141164857,
      "loss": 0.7927,
      "step": 52200
    },
    {
      "epoch": 0.17209608423823627,
      "grad_norm": 4.249757289886475,
      "learning_rate": 0.00024837117472852906,
      "loss": 0.6952,
      "step": 52300
    },
    {
      "epoch": 0.17242513984863442,
      "grad_norm": 46.133785247802734,
      "learning_rate": 0.00024827245804540966,
      "loss": 0.747,
      "step": 52400
    },
    {
      "epoch": 0.1727541954590326,
      "grad_norm": 0.004116140305995941,
      "learning_rate": 0.0002481737413622902,
      "loss": 1.0565,
      "step": 52500
    },
    {
      "epoch": 0.17308325106943073,
      "grad_norm": 0.6554261445999146,
      "learning_rate": 0.00024807502467917076,
      "loss": 0.8605,
      "step": 52600
    },
    {
      "epoch": 0.1734123066798289,
      "grad_norm": 1.972075343132019,
      "learning_rate": 0.0002479763079960513,
      "loss": 0.7512,
      "step": 52700
    },
    {
      "epoch": 0.17374136229022705,
      "grad_norm": 0.039791930466890335,
      "learning_rate": 0.00024787759131293186,
      "loss": 0.7536,
      "step": 52800
    },
    {
      "epoch": 0.17407041790062522,
      "grad_norm": 38.12455749511719,
      "learning_rate": 0.0002477788746298124,
      "loss": 0.7752,
      "step": 52900
    },
    {
      "epoch": 0.17439947351102336,
      "grad_norm": 0.005316383671015501,
      "learning_rate": 0.00024768015794669296,
      "loss": 0.65,
      "step": 53000
    },
    {
      "epoch": 0.17472852912142153,
      "grad_norm": 0.0031471471302211285,
      "learning_rate": 0.0002475814412635735,
      "loss": 0.6388,
      "step": 53100
    },
    {
      "epoch": 0.17505758473181968,
      "grad_norm": 72.9429931640625,
      "learning_rate": 0.0002474827245804541,
      "loss": 0.9762,
      "step": 53200
    },
    {
      "epoch": 0.17538664034221785,
      "grad_norm": 45.423614501953125,
      "learning_rate": 0.0002473840078973346,
      "loss": 0.8241,
      "step": 53300
    },
    {
      "epoch": 0.175715695952616,
      "grad_norm": 0.025375261902809143,
      "learning_rate": 0.00024728529121421515,
      "loss": 0.6907,
      "step": 53400
    },
    {
      "epoch": 0.17604475156301416,
      "grad_norm": 18.187088012695312,
      "learning_rate": 0.00024718657453109575,
      "loss": 1.0203,
      "step": 53500
    },
    {
      "epoch": 0.1763738071734123,
      "grad_norm": 0.00022027554223313928,
      "learning_rate": 0.0002470878578479763,
      "loss": 0.9075,
      "step": 53600
    },
    {
      "epoch": 0.17670286278381048,
      "grad_norm": 0.055633995682001114,
      "learning_rate": 0.00024698914116485685,
      "loss": 0.8506,
      "step": 53700
    },
    {
      "epoch": 0.17703191839420862,
      "grad_norm": 0.0810067281126976,
      "learning_rate": 0.0002468904244817374,
      "loss": 0.4273,
      "step": 53800
    },
    {
      "epoch": 0.1773609740046068,
      "grad_norm": 123.92792510986328,
      "learning_rate": 0.00024679170779861795,
      "loss": 0.7235,
      "step": 53900
    },
    {
      "epoch": 0.17769002961500494,
      "grad_norm": 30.739553451538086,
      "learning_rate": 0.0002466929911154985,
      "loss": 0.8449,
      "step": 54000
    },
    {
      "epoch": 0.1780190852254031,
      "grad_norm": 0.01590682752430439,
      "learning_rate": 0.00024659427443237905,
      "loss": 0.5811,
      "step": 54100
    },
    {
      "epoch": 0.17834814083580125,
      "grad_norm": 0.07530462741851807,
      "learning_rate": 0.0002464955577492596,
      "loss": 0.6668,
      "step": 54200
    },
    {
      "epoch": 0.17867719644619942,
      "grad_norm": 59.363372802734375,
      "learning_rate": 0.00024639684106614015,
      "loss": 0.8176,
      "step": 54300
    },
    {
      "epoch": 0.17900625205659756,
      "grad_norm": 0.04176393151283264,
      "learning_rate": 0.0002462981243830207,
      "loss": 0.5291,
      "step": 54400
    },
    {
      "epoch": 0.17933530766699574,
      "grad_norm": 0.011590258218348026,
      "learning_rate": 0.0002461994076999013,
      "loss": 1.1809,
      "step": 54500
    },
    {
      "epoch": 0.17966436327739388,
      "grad_norm": 0.013655574060976505,
      "learning_rate": 0.0002461006910167818,
      "loss": 0.8581,
      "step": 54600
    },
    {
      "epoch": 0.17999341888779205,
      "grad_norm": 57.977386474609375,
      "learning_rate": 0.00024600197433366234,
      "loss": 0.5415,
      "step": 54700
    },
    {
      "epoch": 0.1803224744981902,
      "grad_norm": 0.028725014999508858,
      "learning_rate": 0.00024590325765054294,
      "loss": 0.7608,
      "step": 54800
    },
    {
      "epoch": 0.18065153010858837,
      "grad_norm": 0.2684735655784607,
      "learning_rate": 0.0002458045409674235,
      "loss": 0.6874,
      "step": 54900
    },
    {
      "epoch": 0.1809805857189865,
      "grad_norm": 0.0005463410634547472,
      "learning_rate": 0.000245705824284304,
      "loss": 1.001,
      "step": 55000
    },
    {
      "epoch": 0.18130964132938468,
      "grad_norm": 3.3231446743011475,
      "learning_rate": 0.0002456071076011846,
      "loss": 0.7781,
      "step": 55100
    },
    {
      "epoch": 0.18163869693978282,
      "grad_norm": 0.0031697305385023355,
      "learning_rate": 0.00024550839091806514,
      "loss": 0.801,
      "step": 55200
    },
    {
      "epoch": 0.18196775255018097,
      "grad_norm": 0.01776723563671112,
      "learning_rate": 0.0002454096742349457,
      "loss": 0.7322,
      "step": 55300
    },
    {
      "epoch": 0.18229680816057914,
      "grad_norm": 24.474933624267578,
      "learning_rate": 0.00024531095755182624,
      "loss": 1.0474,
      "step": 55400
    },
    {
      "epoch": 0.18262586377097728,
      "grad_norm": 95.19613647460938,
      "learning_rate": 0.0002452122408687068,
      "loss": 0.8021,
      "step": 55500
    },
    {
      "epoch": 0.18295491938137545,
      "grad_norm": 29.97640037536621,
      "learning_rate": 0.00024511352418558733,
      "loss": 0.6783,
      "step": 55600
    },
    {
      "epoch": 0.1832839749917736,
      "grad_norm": 74.0146484375,
      "learning_rate": 0.0002450148075024679,
      "loss": 0.6542,
      "step": 55700
    },
    {
      "epoch": 0.18361303060217177,
      "grad_norm": 0.37551912665367126,
      "learning_rate": 0.0002449160908193485,
      "loss": 0.7778,
      "step": 55800
    },
    {
      "epoch": 0.1839420862125699,
      "grad_norm": 63.056915283203125,
      "learning_rate": 0.000244817374136229,
      "loss": 0.6679,
      "step": 55900
    },
    {
      "epoch": 0.18427114182296808,
      "grad_norm": 6.149175643920898,
      "learning_rate": 0.00024471865745310953,
      "loss": 0.5856,
      "step": 56000
    },
    {
      "epoch": 0.18460019743336623,
      "grad_norm": 30.103755950927734,
      "learning_rate": 0.00024461994076999013,
      "loss": 0.8329,
      "step": 56100
    },
    {
      "epoch": 0.1849292530437644,
      "grad_norm": 105.91790771484375,
      "learning_rate": 0.0002445212240868707,
      "loss": 0.8414,
      "step": 56200
    },
    {
      "epoch": 0.18525830865416254,
      "grad_norm": 0.0014797445619478822,
      "learning_rate": 0.0002444225074037512,
      "loss": 0.6868,
      "step": 56300
    },
    {
      "epoch": 0.1855873642645607,
      "grad_norm": 38.62710952758789,
      "learning_rate": 0.0002443237907206318,
      "loss": 0.6358,
      "step": 56400
    },
    {
      "epoch": 0.18591641987495885,
      "grad_norm": 0.15745612978935242,
      "learning_rate": 0.00024422507403751233,
      "loss": 0.5269,
      "step": 56500
    },
    {
      "epoch": 0.18624547548535703,
      "grad_norm": 1.9067386388778687,
      "learning_rate": 0.00024412635735439288,
      "loss": 0.6619,
      "step": 56600
    },
    {
      "epoch": 0.18657453109575517,
      "grad_norm": 27.584774017333984,
      "learning_rate": 0.00024402764067127342,
      "loss": 0.7189,
      "step": 56700
    },
    {
      "epoch": 0.18690358670615334,
      "grad_norm": 101.29269409179688,
      "learning_rate": 0.00024392892398815397,
      "loss": 0.7815,
      "step": 56800
    },
    {
      "epoch": 0.18723264231655148,
      "grad_norm": 0.0020254659466445446,
      "learning_rate": 0.00024383020730503455,
      "loss": 0.8497,
      "step": 56900
    },
    {
      "epoch": 0.18756169792694966,
      "grad_norm": 0.04692427068948746,
      "learning_rate": 0.00024373149062191507,
      "loss": 0.648,
      "step": 57000
    },
    {
      "epoch": 0.1878907535373478,
      "grad_norm": 0.0032957184594124556,
      "learning_rate": 0.00024363277393879562,
      "loss": 0.9067,
      "step": 57100
    },
    {
      "epoch": 0.18821980914774597,
      "grad_norm": 54.69144058227539,
      "learning_rate": 0.0002435340572556762,
      "loss": 0.4325,
      "step": 57200
    },
    {
      "epoch": 0.1885488647581441,
      "grad_norm": 67.6297607421875,
      "learning_rate": 0.00024343534057255674,
      "loss": 0.5253,
      "step": 57300
    },
    {
      "epoch": 0.18887792036854228,
      "grad_norm": 0.0124215641990304,
      "learning_rate": 0.00024333662388943732,
      "loss": 0.5873,
      "step": 57400
    },
    {
      "epoch": 0.18920697597894043,
      "grad_norm": 0.05808942764997482,
      "learning_rate": 0.00024323790720631784,
      "loss": 0.8767,
      "step": 57500
    },
    {
      "epoch": 0.1895360315893386,
      "grad_norm": 0.0455077663064003,
      "learning_rate": 0.0002431391905231984,
      "loss": 0.7393,
      "step": 57600
    },
    {
      "epoch": 0.18986508719973674,
      "grad_norm": 62.3985481262207,
      "learning_rate": 0.00024304047384007897,
      "loss": 0.9848,
      "step": 57700
    },
    {
      "epoch": 0.19019414281013491,
      "grad_norm": 0.32998138666152954,
      "learning_rate": 0.00024294175715695952,
      "loss": 0.6484,
      "step": 57800
    },
    {
      "epoch": 0.19052319842053306,
      "grad_norm": 6.975489139556885,
      "learning_rate": 0.00024284304047384004,
      "loss": 0.5817,
      "step": 57900
    },
    {
      "epoch": 0.19085225403093123,
      "grad_norm": 0.005577171221375465,
      "learning_rate": 0.00024274432379072061,
      "loss": 0.6043,
      "step": 58000
    },
    {
      "epoch": 0.19118130964132937,
      "grad_norm": 0.0010058596963062882,
      "learning_rate": 0.00024264560710760116,
      "loss": 0.6218,
      "step": 58100
    },
    {
      "epoch": 0.19151036525172754,
      "grad_norm": 0.23622116446495056,
      "learning_rate": 0.00024254689042448174,
      "loss": 0.699,
      "step": 58200
    },
    {
      "epoch": 0.1918394208621257,
      "grad_norm": 0.0018634269945323467,
      "learning_rate": 0.00024244817374136226,
      "loss": 0.483,
      "step": 58300
    },
    {
      "epoch": 0.19216847647252386,
      "grad_norm": 0.347664475440979,
      "learning_rate": 0.0002423494570582428,
      "loss": 0.9924,
      "step": 58400
    },
    {
      "epoch": 0.192497532082922,
      "grad_norm": 52.927528381347656,
      "learning_rate": 0.00024225074037512338,
      "loss": 0.7711,
      "step": 58500
    },
    {
      "epoch": 0.19282658769332017,
      "grad_norm": 49.155906677246094,
      "learning_rate": 0.00024215202369200393,
      "loss": 0.7409,
      "step": 58600
    },
    {
      "epoch": 0.19315564330371832,
      "grad_norm": 0.2358894944190979,
      "learning_rate": 0.00024205330700888446,
      "loss": 0.8518,
      "step": 58700
    },
    {
      "epoch": 0.1934846989141165,
      "grad_norm": 0.06584096699953079,
      "learning_rate": 0.00024195459032576503,
      "loss": 0.5104,
      "step": 58800
    },
    {
      "epoch": 0.19381375452451463,
      "grad_norm": 1.015148401260376,
      "learning_rate": 0.00024185587364264558,
      "loss": 0.7601,
      "step": 58900
    },
    {
      "epoch": 0.1941428101349128,
      "grad_norm": 2.270890712738037,
      "learning_rate": 0.00024175715695952616,
      "loss": 0.6379,
      "step": 59000
    },
    {
      "epoch": 0.19447186574531095,
      "grad_norm": 52.661033630371094,
      "learning_rate": 0.0002416584402764067,
      "loss": 0.7679,
      "step": 59100
    },
    {
      "epoch": 0.19480092135570912,
      "grad_norm": 15.082944869995117,
      "learning_rate": 0.00024155972359328723,
      "loss": 0.6493,
      "step": 59200
    },
    {
      "epoch": 0.19512997696610726,
      "grad_norm": 26.068723678588867,
      "learning_rate": 0.0002414610069101678,
      "loss": 0.7199,
      "step": 59300
    },
    {
      "epoch": 0.19545903257650543,
      "grad_norm": 0.0030340435914695263,
      "learning_rate": 0.00024136229022704835,
      "loss": 0.7851,
      "step": 59400
    },
    {
      "epoch": 0.19578808818690357,
      "grad_norm": 0.011214804835617542,
      "learning_rate": 0.0002412635735439289,
      "loss": 0.6965,
      "step": 59500
    },
    {
      "epoch": 0.19611714379730175,
      "grad_norm": 0.1538209617137909,
      "learning_rate": 0.00024116485686080948,
      "loss": 0.6796,
      "step": 59600
    },
    {
      "epoch": 0.1964461994076999,
      "grad_norm": 1.7961344718933105,
      "learning_rate": 0.00024106614017769,
      "loss": 0.7021,
      "step": 59700
    },
    {
      "epoch": 0.19677525501809806,
      "grad_norm": 23.54840087890625,
      "learning_rate": 0.00024096742349457057,
      "loss": 0.6588,
      "step": 59800
    },
    {
      "epoch": 0.1971043106284962,
      "grad_norm": 0.1143445074558258,
      "learning_rate": 0.00024086870681145112,
      "loss": 0.6769,
      "step": 59900
    },
    {
      "epoch": 0.19743336623889438,
      "grad_norm": 2.3927440643310547,
      "learning_rate": 0.00024076999012833167,
      "loss": 0.7221,
      "step": 60000
    },
    {
      "epoch": 0.19776242184929252,
      "grad_norm": 0.007211480289697647,
      "learning_rate": 0.00024067127344521222,
      "loss": 0.7368,
      "step": 60100
    },
    {
      "epoch": 0.1980914774596907,
      "grad_norm": 0.0038870065473020077,
      "learning_rate": 0.00024057255676209277,
      "loss": 0.8328,
      "step": 60200
    },
    {
      "epoch": 0.19842053307008883,
      "grad_norm": 0.00359005993232131,
      "learning_rate": 0.00024047384007897332,
      "loss": 0.5882,
      "step": 60300
    },
    {
      "epoch": 0.198749588680487,
      "grad_norm": 0.032627008855342865,
      "learning_rate": 0.0002403751233958539,
      "loss": 0.7522,
      "step": 60400
    },
    {
      "epoch": 0.19907864429088515,
      "grad_norm": 0.002717447467148304,
      "learning_rate": 0.00024027640671273442,
      "loss": 0.7161,
      "step": 60500
    },
    {
      "epoch": 0.19940769990128332,
      "grad_norm": 38.68769454956055,
      "learning_rate": 0.000240177690029615,
      "loss": 0.6189,
      "step": 60600
    },
    {
      "epoch": 0.19973675551168146,
      "grad_norm": 0.6563339829444885,
      "learning_rate": 0.00024007897334649554,
      "loss": 0.5933,
      "step": 60700
    },
    {
      "epoch": 0.20006581112207963,
      "grad_norm": 0.958005964756012,
      "learning_rate": 0.0002399802566633761,
      "loss": 0.7377,
      "step": 60800
    },
    {
      "epoch": 0.20039486673247778,
      "grad_norm": 0.0010314532555639744,
      "learning_rate": 0.00023988153998025666,
      "loss": 0.7459,
      "step": 60900
    },
    {
      "epoch": 0.20072392234287595,
      "grad_norm": 0.0006178172770887613,
      "learning_rate": 0.00023978282329713719,
      "loss": 0.618,
      "step": 61000
    },
    {
      "epoch": 0.2010529779532741,
      "grad_norm": 0.00870592799037695,
      "learning_rate": 0.00023968410661401774,
      "loss": 0.6907,
      "step": 61100
    },
    {
      "epoch": 0.20138203356367226,
      "grad_norm": 0.0024342972319573164,
      "learning_rate": 0.0002395853899308983,
      "loss": 0.6887,
      "step": 61200
    },
    {
      "epoch": 0.2017110891740704,
      "grad_norm": 0.043803486973047256,
      "learning_rate": 0.00023948667324777886,
      "loss": 0.6755,
      "step": 61300
    },
    {
      "epoch": 0.20204014478446858,
      "grad_norm": 0.0029591780621558428,
      "learning_rate": 0.0002393879565646594,
      "loss": 0.6531,
      "step": 61400
    },
    {
      "epoch": 0.20236920039486672,
      "grad_norm": 0.02685282938182354,
      "learning_rate": 0.00023928923988153996,
      "loss": 0.6004,
      "step": 61500
    },
    {
      "epoch": 0.2026982560052649,
      "grad_norm": 38.116580963134766,
      "learning_rate": 0.0002391905231984205,
      "loss": 0.9083,
      "step": 61600
    },
    {
      "epoch": 0.20302731161566304,
      "grad_norm": 31.484174728393555,
      "learning_rate": 0.00023909180651530108,
      "loss": 0.6319,
      "step": 61700
    },
    {
      "epoch": 0.2033563672260612,
      "grad_norm": 27.16851234436035,
      "learning_rate": 0.0002389930898321816,
      "loss": 0.5447,
      "step": 61800
    },
    {
      "epoch": 0.20368542283645935,
      "grad_norm": 0.008534800261259079,
      "learning_rate": 0.00023889437314906215,
      "loss": 0.6672,
      "step": 61900
    },
    {
      "epoch": 0.20401447844685752,
      "grad_norm": 18.48656463623047,
      "learning_rate": 0.00023879565646594273,
      "loss": 0.5904,
      "step": 62000
    },
    {
      "epoch": 0.20434353405725567,
      "grad_norm": 1.2231183052062988,
      "learning_rate": 0.00023869693978282328,
      "loss": 0.7,
      "step": 62100
    },
    {
      "epoch": 0.20467258966765384,
      "grad_norm": 0.006983628496527672,
      "learning_rate": 0.00023859822309970385,
      "loss": 0.8257,
      "step": 62200
    },
    {
      "epoch": 0.20500164527805198,
      "grad_norm": 0.07689882069826126,
      "learning_rate": 0.00023849950641658438,
      "loss": 0.5874,
      "step": 62300
    },
    {
      "epoch": 0.20533070088845015,
      "grad_norm": 0.0008905778522603214,
      "learning_rate": 0.00023840078973346492,
      "loss": 0.7969,
      "step": 62400
    },
    {
      "epoch": 0.2056597564988483,
      "grad_norm": 70.82858276367188,
      "learning_rate": 0.0002383020730503455,
      "loss": 0.8611,
      "step": 62500
    },
    {
      "epoch": 0.20598881210924647,
      "grad_norm": 0.004826595541089773,
      "learning_rate": 0.00023820335636722605,
      "loss": 0.8631,
      "step": 62600
    },
    {
      "epoch": 0.2063178677196446,
      "grad_norm": 28.2722110748291,
      "learning_rate": 0.00023810463968410657,
      "loss": 0.8948,
      "step": 62700
    },
    {
      "epoch": 0.20664692333004278,
      "grad_norm": 0.006083229556679726,
      "learning_rate": 0.00023800592300098715,
      "loss": 0.7655,
      "step": 62800
    },
    {
      "epoch": 0.20697597894044092,
      "grad_norm": 0.0032940367236733437,
      "learning_rate": 0.0002379072063178677,
      "loss": 0.6532,
      "step": 62900
    },
    {
      "epoch": 0.2073050345508391,
      "grad_norm": 0.004232441540807486,
      "learning_rate": 0.00023780848963474827,
      "loss": 0.9051,
      "step": 63000
    },
    {
      "epoch": 0.20763409016123724,
      "grad_norm": 6.633251190185547,
      "learning_rate": 0.00023770977295162882,
      "loss": 0.7621,
      "step": 63100
    },
    {
      "epoch": 0.2079631457716354,
      "grad_norm": 16.941396713256836,
      "learning_rate": 0.00023761105626850934,
      "loss": 0.454,
      "step": 63200
    },
    {
      "epoch": 0.20829220138203355,
      "grad_norm": 46.74495315551758,
      "learning_rate": 0.00023751233958538992,
      "loss": 0.619,
      "step": 63300
    },
    {
      "epoch": 0.20862125699243173,
      "grad_norm": 0.04828128591179848,
      "learning_rate": 0.00023741362290227047,
      "loss": 0.6072,
      "step": 63400
    },
    {
      "epoch": 0.20895031260282987,
      "grad_norm": 0.009163235314190388,
      "learning_rate": 0.00023731490621915101,
      "loss": 0.6524,
      "step": 63500
    },
    {
      "epoch": 0.20927936821322804,
      "grad_norm": 0.00137222686316818,
      "learning_rate": 0.00023721618953603156,
      "loss": 0.4852,
      "step": 63600
    },
    {
      "epoch": 0.20960842382362618,
      "grad_norm": 56.41707229614258,
      "learning_rate": 0.0002371174728529121,
      "loss": 0.7127,
      "step": 63700
    },
    {
      "epoch": 0.20993747943402435,
      "grad_norm": 0.001926919212564826,
      "learning_rate": 0.0002370187561697927,
      "loss": 0.8895,
      "step": 63800
    },
    {
      "epoch": 0.2102665350444225,
      "grad_norm": 0.6816232204437256,
      "learning_rate": 0.00023692003948667324,
      "loss": 0.5408,
      "step": 63900
    },
    {
      "epoch": 0.21059559065482067,
      "grad_norm": 0.7570920586585999,
      "learning_rate": 0.00023682132280355376,
      "loss": 0.5883,
      "step": 64000
    },
    {
      "epoch": 0.2109246462652188,
      "grad_norm": 0.0781511440873146,
      "learning_rate": 0.00023672260612043433,
      "loss": 0.5703,
      "step": 64100
    },
    {
      "epoch": 0.21125370187561698,
      "grad_norm": 124.85768127441406,
      "learning_rate": 0.00023662388943731488,
      "loss": 0.9857,
      "step": 64200
    },
    {
      "epoch": 0.21158275748601513,
      "grad_norm": 11.679672241210938,
      "learning_rate": 0.00023652517275419543,
      "loss": 0.7848,
      "step": 64300
    },
    {
      "epoch": 0.2119118130964133,
      "grad_norm": 39.95751953125,
      "learning_rate": 0.000236426456071076,
      "loss": 0.8593,
      "step": 64400
    },
    {
      "epoch": 0.21224086870681144,
      "grad_norm": 0.014077609404921532,
      "learning_rate": 0.00023632773938795653,
      "loss": 0.5207,
      "step": 64500
    },
    {
      "epoch": 0.2125699243172096,
      "grad_norm": 0.0021098433062434196,
      "learning_rate": 0.0002362290227048371,
      "loss": 0.7393,
      "step": 64600
    },
    {
      "epoch": 0.21289897992760776,
      "grad_norm": 0.012934955768287182,
      "learning_rate": 0.00023613030602171765,
      "loss": 0.7104,
      "step": 64700
    },
    {
      "epoch": 0.21322803553800593,
      "grad_norm": 0.1871720552444458,
      "learning_rate": 0.0002360315893385982,
      "loss": 0.7838,
      "step": 64800
    },
    {
      "epoch": 0.21355709114840407,
      "grad_norm": 102.57173919677734,
      "learning_rate": 0.00023593287265547878,
      "loss": 0.5622,
      "step": 64900
    },
    {
      "epoch": 0.21388614675880224,
      "grad_norm": 0.8004385828971863,
      "learning_rate": 0.0002358341559723593,
      "loss": 0.6911,
      "step": 65000
    },
    {
      "epoch": 0.21421520236920039,
      "grad_norm": 0.00806080736219883,
      "learning_rate": 0.00023573543928923985,
      "loss": 0.6228,
      "step": 65100
    },
    {
      "epoch": 0.21454425797959856,
      "grad_norm": 24.81534767150879,
      "learning_rate": 0.00023563672260612043,
      "loss": 0.4157,
      "step": 65200
    },
    {
      "epoch": 0.2148733135899967,
      "grad_norm": 6.057980060577393,
      "learning_rate": 0.00023553800592300095,
      "loss": 0.4787,
      "step": 65300
    },
    {
      "epoch": 0.21520236920039487,
      "grad_norm": 0.013525767251849174,
      "learning_rate": 0.00023543928923988152,
      "loss": 0.7809,
      "step": 65400
    },
    {
      "epoch": 0.21553142481079302,
      "grad_norm": 0.0705750361084938,
      "learning_rate": 0.00023534057255676207,
      "loss": 0.494,
      "step": 65500
    },
    {
      "epoch": 0.2158604804211912,
      "grad_norm": 17.49984359741211,
      "learning_rate": 0.00023524185587364262,
      "loss": 0.8909,
      "step": 65600
    },
    {
      "epoch": 0.21618953603158933,
      "grad_norm": 0.32598310708999634,
      "learning_rate": 0.0002351431391905232,
      "loss": 0.7195,
      "step": 65700
    },
    {
      "epoch": 0.2165185916419875,
      "grad_norm": 0.0013491843128576875,
      "learning_rate": 0.00023504442250740372,
      "loss": 0.6762,
      "step": 65800
    },
    {
      "epoch": 0.21684764725238564,
      "grad_norm": 27.935745239257812,
      "learning_rate": 0.00023494570582428427,
      "loss": 0.5186,
      "step": 65900
    },
    {
      "epoch": 0.21717670286278382,
      "grad_norm": 0.05191117897629738,
      "learning_rate": 0.00023484698914116484,
      "loss": 0.7513,
      "step": 66000
    },
    {
      "epoch": 0.21750575847318196,
      "grad_norm": 0.04201290011405945,
      "learning_rate": 0.0002347482724580454,
      "loss": 0.5076,
      "step": 66100
    },
    {
      "epoch": 0.21783481408358013,
      "grad_norm": 102.882080078125,
      "learning_rate": 0.00023464955577492597,
      "loss": 0.6284,
      "step": 66200
    },
    {
      "epoch": 0.21816386969397827,
      "grad_norm": 0.0261750016361475,
      "learning_rate": 0.0002345508390918065,
      "loss": 0.8792,
      "step": 66300
    },
    {
      "epoch": 0.21849292530437645,
      "grad_norm": 0.0027049537748098373,
      "learning_rate": 0.00023445212240868704,
      "loss": 0.3691,
      "step": 66400
    },
    {
      "epoch": 0.2188219809147746,
      "grad_norm": 0.02359950728714466,
      "learning_rate": 0.00023435340572556761,
      "loss": 0.7843,
      "step": 66500
    },
    {
      "epoch": 0.21915103652517276,
      "grad_norm": 10.548377990722656,
      "learning_rate": 0.00023425468904244816,
      "loss": 0.8124,
      "step": 66600
    },
    {
      "epoch": 0.2194800921355709,
      "grad_norm": 0.0009117283625528216,
      "learning_rate": 0.00023415597235932869,
      "loss": 0.7851,
      "step": 66700
    },
    {
      "epoch": 0.21980914774596907,
      "grad_norm": 0.1985744833946228,
      "learning_rate": 0.00023405725567620926,
      "loss": 0.6623,
      "step": 66800
    },
    {
      "epoch": 0.22013820335636722,
      "grad_norm": 0.10168042778968811,
      "learning_rate": 0.0002339585389930898,
      "loss": 0.7192,
      "step": 66900
    },
    {
      "epoch": 0.2204672589667654,
      "grad_norm": 0.016345953568816185,
      "learning_rate": 0.00023385982230997039,
      "loss": 0.5863,
      "step": 67000
    },
    {
      "epoch": 0.22079631457716353,
      "grad_norm": 2.7074644565582275,
      "learning_rate": 0.0002337611056268509,
      "loss": 0.7484,
      "step": 67100
    },
    {
      "epoch": 0.2211253701875617,
      "grad_norm": 0.008452235721051693,
      "learning_rate": 0.00023366238894373146,
      "loss": 0.5527,
      "step": 67200
    },
    {
      "epoch": 0.22145442579795985,
      "grad_norm": 0.012551981024444103,
      "learning_rate": 0.00023356367226061203,
      "loss": 0.3781,
      "step": 67300
    },
    {
      "epoch": 0.22178348140835802,
      "grad_norm": 1.8407012224197388,
      "learning_rate": 0.00023346495557749258,
      "loss": 0.5607,
      "step": 67400
    },
    {
      "epoch": 0.22211253701875616,
      "grad_norm": 0.46356022357940674,
      "learning_rate": 0.0002333662388943731,
      "loss": 0.7482,
      "step": 67500
    },
    {
      "epoch": 0.22244159262915433,
      "grad_norm": 0.6100084781646729,
      "learning_rate": 0.00023326752221125368,
      "loss": 0.663,
      "step": 67600
    },
    {
      "epoch": 0.22277064823955248,
      "grad_norm": 0.018251314759254456,
      "learning_rate": 0.00023316880552813423,
      "loss": 0.5675,
      "step": 67700
    },
    {
      "epoch": 0.22309970384995065,
      "grad_norm": 0.014499101787805557,
      "learning_rate": 0.0002330700888450148,
      "loss": 0.6239,
      "step": 67800
    },
    {
      "epoch": 0.2234287594603488,
      "grad_norm": 47.15958023071289,
      "learning_rate": 0.00023297137216189535,
      "loss": 0.7673,
      "step": 67900
    },
    {
      "epoch": 0.22375781507074696,
      "grad_norm": 0.01683301106095314,
      "learning_rate": 0.00023287265547877587,
      "loss": 0.7622,
      "step": 68000
    },
    {
      "epoch": 0.2240868706811451,
      "grad_norm": 0.07487796992063522,
      "learning_rate": 0.00023277393879565645,
      "loss": 0.5367,
      "step": 68100
    },
    {
      "epoch": 0.22441592629154328,
      "grad_norm": 0.023278888314962387,
      "learning_rate": 0.000232675222112537,
      "loss": 0.62,
      "step": 68200
    },
    {
      "epoch": 0.22474498190194142,
      "grad_norm": 4.309927940368652,
      "learning_rate": 0.00023257650542941755,
      "loss": 0.7057,
      "step": 68300
    },
    {
      "epoch": 0.2250740375123396,
      "grad_norm": 49.082366943359375,
      "learning_rate": 0.00023247778874629812,
      "loss": 0.721,
      "step": 68400
    },
    {
      "epoch": 0.22540309312273774,
      "grad_norm": 4.762372016906738,
      "learning_rate": 0.00023237907206317865,
      "loss": 0.631,
      "step": 68500
    },
    {
      "epoch": 0.2257321487331359,
      "grad_norm": 0.028068797662854195,
      "learning_rate": 0.00023228035538005922,
      "loss": 0.4769,
      "step": 68600
    },
    {
      "epoch": 0.22606120434353405,
      "grad_norm": 88.82042694091797,
      "learning_rate": 0.00023218163869693977,
      "loss": 0.5152,
      "step": 68700
    },
    {
      "epoch": 0.22639025995393222,
      "grad_norm": 0.10408324003219604,
      "learning_rate": 0.0002320829220138203,
      "loss": 0.5004,
      "step": 68800
    },
    {
      "epoch": 0.22671931556433036,
      "grad_norm": 14.001646041870117,
      "learning_rate": 0.00023198420533070087,
      "loss": 0.6051,
      "step": 68900
    },
    {
      "epoch": 0.22704837117472854,
      "grad_norm": 59.315311431884766,
      "learning_rate": 0.00023188548864758142,
      "loss": 0.701,
      "step": 69000
    },
    {
      "epoch": 0.22737742678512668,
      "grad_norm": 63.23471450805664,
      "learning_rate": 0.00023178677196446197,
      "loss": 0.8333,
      "step": 69100
    },
    {
      "epoch": 0.22770648239552485,
      "grad_norm": 9.092832565307617,
      "learning_rate": 0.00023168805528134254,
      "loss": 0.3973,
      "step": 69200
    },
    {
      "epoch": 0.228035538005923,
      "grad_norm": 15.534605979919434,
      "learning_rate": 0.00023158933859822306,
      "loss": 0.6144,
      "step": 69300
    },
    {
      "epoch": 0.22836459361632117,
      "grad_norm": 61.017250061035156,
      "learning_rate": 0.00023149062191510364,
      "loss": 0.7708,
      "step": 69400
    },
    {
      "epoch": 0.2286936492267193,
      "grad_norm": 5.576103210449219,
      "learning_rate": 0.0002313919052319842,
      "loss": 0.5751,
      "step": 69500
    },
    {
      "epoch": 0.22902270483711748,
      "grad_norm": 56.80398941040039,
      "learning_rate": 0.00023129318854886474,
      "loss": 0.3463,
      "step": 69600
    },
    {
      "epoch": 0.22935176044751562,
      "grad_norm": 8.741744995117188,
      "learning_rate": 0.0002311944718657453,
      "loss": 0.8567,
      "step": 69700
    },
    {
      "epoch": 0.2296808160579138,
      "grad_norm": 35.4758186340332,
      "learning_rate": 0.00023109575518262583,
      "loss": 0.5961,
      "step": 69800
    },
    {
      "epoch": 0.23000987166831194,
      "grad_norm": 0.034171540290117264,
      "learning_rate": 0.00023099703849950638,
      "loss": 0.5979,
      "step": 69900
    },
    {
      "epoch": 0.2303389272787101,
      "grad_norm": 0.12187406420707703,
      "learning_rate": 0.00023089832181638696,
      "loss": 0.6232,
      "step": 70000
    },
    {
      "epoch": 0.23066798288910825,
      "grad_norm": 0.005570475943386555,
      "learning_rate": 0.0002307996051332675,
      "loss": 0.5817,
      "step": 70100
    },
    {
      "epoch": 0.23099703849950642,
      "grad_norm": 4.637173652648926,
      "learning_rate": 0.00023070088845014806,
      "loss": 0.5486,
      "step": 70200
    },
    {
      "epoch": 0.23132609410990457,
      "grad_norm": 46.2308464050293,
      "learning_rate": 0.0002306021717670286,
      "loss": 0.6374,
      "step": 70300
    },
    {
      "epoch": 0.23165514972030274,
      "grad_norm": 0.15481923520565033,
      "learning_rate": 0.00023050345508390915,
      "loss": 0.7752,
      "step": 70400
    },
    {
      "epoch": 0.23198420533070088,
      "grad_norm": 0.02709655836224556,
      "learning_rate": 0.00023040473840078973,
      "loss": 0.6823,
      "step": 70500
    },
    {
      "epoch": 0.23231326094109905,
      "grad_norm": 0.35030585527420044,
      "learning_rate": 0.00023030602171767025,
      "loss": 0.6163,
      "step": 70600
    },
    {
      "epoch": 0.2326423165514972,
      "grad_norm": 2.956221103668213,
      "learning_rate": 0.0002302073050345508,
      "loss": 0.5773,
      "step": 70700
    },
    {
      "epoch": 0.23297137216189537,
      "grad_norm": 1.532090187072754,
      "learning_rate": 0.00023010858835143138,
      "loss": 0.4595,
      "step": 70800
    },
    {
      "epoch": 0.2333004277722935,
      "grad_norm": 0.0568501316010952,
      "learning_rate": 0.00023000987166831192,
      "loss": 0.8036,
      "step": 70900
    },
    {
      "epoch": 0.23362948338269168,
      "grad_norm": 22.942617416381836,
      "learning_rate": 0.0002299111549851925,
      "loss": 0.6767,
      "step": 71000
    },
    {
      "epoch": 0.23395853899308983,
      "grad_norm": 0.008648013696074486,
      "learning_rate": 0.00022981243830207302,
      "loss": 0.3476,
      "step": 71100
    },
    {
      "epoch": 0.234287594603488,
      "grad_norm": 0.025334889069199562,
      "learning_rate": 0.00022971372161895357,
      "loss": 0.5279,
      "step": 71200
    },
    {
      "epoch": 0.23461665021388614,
      "grad_norm": 0.004541166592389345,
      "learning_rate": 0.00022961500493583415,
      "loss": 0.784,
      "step": 71300
    },
    {
      "epoch": 0.2349457058242843,
      "grad_norm": 101.72077941894531,
      "learning_rate": 0.0002295162882527147,
      "loss": 0.6475,
      "step": 71400
    },
    {
      "epoch": 0.23527476143468246,
      "grad_norm": 0.013864047825336456,
      "learning_rate": 0.00022941757156959522,
      "loss": 0.5032,
      "step": 71500
    },
    {
      "epoch": 0.23560381704508063,
      "grad_norm": 0.3327740430831909,
      "learning_rate": 0.0002293188548864758,
      "loss": 0.6516,
      "step": 71600
    },
    {
      "epoch": 0.23593287265547877,
      "grad_norm": 5.179353713989258,
      "learning_rate": 0.00022922013820335634,
      "loss": 0.5278,
      "step": 71700
    },
    {
      "epoch": 0.23626192826587694,
      "grad_norm": 26.456058502197266,
      "learning_rate": 0.00022912142152023692,
      "loss": 0.4675,
      "step": 71800
    },
    {
      "epoch": 0.23659098387627508,
      "grad_norm": 0.000578169769141823,
      "learning_rate": 0.00022902270483711747,
      "loss": 0.6101,
      "step": 71900
    },
    {
      "epoch": 0.23692003948667326,
      "grad_norm": 119.39530181884766,
      "learning_rate": 0.000228923988153998,
      "loss": 0.5443,
      "step": 72000
    },
    {
      "epoch": 0.2372490950970714,
      "grad_norm": 0.3854423761367798,
      "learning_rate": 0.00022882527147087856,
      "loss": 0.6029,
      "step": 72100
    },
    {
      "epoch": 0.23757815070746957,
      "grad_norm": 0.0019186758436262608,
      "learning_rate": 0.00022872655478775911,
      "loss": 0.6219,
      "step": 72200
    },
    {
      "epoch": 0.23790720631786771,
      "grad_norm": 2.046349048614502,
      "learning_rate": 0.00022862783810463966,
      "loss": 0.6351,
      "step": 72300
    },
    {
      "epoch": 0.23823626192826589,
      "grad_norm": 0.005399609450250864,
      "learning_rate": 0.0002285291214215202,
      "loss": 0.5633,
      "step": 72400
    },
    {
      "epoch": 0.23856531753866403,
      "grad_norm": 0.01007457822561264,
      "learning_rate": 0.00022843040473840076,
      "loss": 0.5134,
      "step": 72500
    },
    {
      "epoch": 0.2388943731490622,
      "grad_norm": 0.04801749065518379,
      "learning_rate": 0.00022833168805528134,
      "loss": 0.6583,
      "step": 72600
    },
    {
      "epoch": 0.23922342875946034,
      "grad_norm": 0.018535727635025978,
      "learning_rate": 0.00022823297137216188,
      "loss": 0.3843,
      "step": 72700
    },
    {
      "epoch": 0.23955248436985851,
      "grad_norm": 0.007597631309181452,
      "learning_rate": 0.0002281342546890424,
      "loss": 0.4482,
      "step": 72800
    },
    {
      "epoch": 0.23988153998025666,
      "grad_norm": 0.01602916605770588,
      "learning_rate": 0.00022803553800592298,
      "loss": 0.9387,
      "step": 72900
    },
    {
      "epoch": 0.24021059559065483,
      "grad_norm": 10.962767601013184,
      "learning_rate": 0.00022793682132280353,
      "loss": 0.7269,
      "step": 73000
    },
    {
      "epoch": 0.24053965120105297,
      "grad_norm": 0.0006364302826113999,
      "learning_rate": 0.00022783810463968408,
      "loss": 0.5166,
      "step": 73100
    },
    {
      "epoch": 0.24086870681145114,
      "grad_norm": 0.4149337708950043,
      "learning_rate": 0.00022773938795656466,
      "loss": 0.5127,
      "step": 73200
    },
    {
      "epoch": 0.2411977624218493,
      "grad_norm": 74.6014404296875,
      "learning_rate": 0.00022764067127344518,
      "loss": 0.4798,
      "step": 73300
    },
    {
      "epoch": 0.24152681803224746,
      "grad_norm": 0.0012931308010593057,
      "learning_rate": 0.00022754195459032575,
      "loss": 0.4768,
      "step": 73400
    },
    {
      "epoch": 0.2418558736426456,
      "grad_norm": 0.002501116367056966,
      "learning_rate": 0.0002274432379072063,
      "loss": 0.799,
      "step": 73500
    },
    {
      "epoch": 0.24218492925304377,
      "grad_norm": 47.528629302978516,
      "learning_rate": 0.00022734452122408685,
      "loss": 0.6519,
      "step": 73600
    },
    {
      "epoch": 0.24251398486344192,
      "grad_norm": 0.0008939162944443524,
      "learning_rate": 0.0002272458045409674,
      "loss": 0.8052,
      "step": 73700
    },
    {
      "epoch": 0.2428430404738401,
      "grad_norm": 0.005516906268894672,
      "learning_rate": 0.00022714708785784795,
      "loss": 0.9078,
      "step": 73800
    },
    {
      "epoch": 0.24317209608423823,
      "grad_norm": 0.020174145698547363,
      "learning_rate": 0.0002270483711747285,
      "loss": 0.4518,
      "step": 73900
    },
    {
      "epoch": 0.2435011516946364,
      "grad_norm": 0.0015465040924027562,
      "learning_rate": 0.00022694965449160907,
      "loss": 0.8029,
      "step": 74000
    },
    {
      "epoch": 0.24383020730503455,
      "grad_norm": 17.495746612548828,
      "learning_rate": 0.0002268509378084896,
      "loss": 0.6038,
      "step": 74100
    },
    {
      "epoch": 0.24415926291543272,
      "grad_norm": 28.327362060546875,
      "learning_rate": 0.00022675222112537017,
      "loss": 0.5251,
      "step": 74200
    },
    {
      "epoch": 0.24448831852583086,
      "grad_norm": 48.602867126464844,
      "learning_rate": 0.00022665350444225072,
      "loss": 0.5935,
      "step": 74300
    },
    {
      "epoch": 0.24481737413622903,
      "grad_norm": 1.0385491847991943,
      "learning_rate": 0.00022655478775913127,
      "loss": 0.596,
      "step": 74400
    },
    {
      "epoch": 0.24514642974662718,
      "grad_norm": 0.04123082756996155,
      "learning_rate": 0.00022645607107601184,
      "loss": 0.7764,
      "step": 74500
    },
    {
      "epoch": 0.24547548535702535,
      "grad_norm": 2.4948248863220215,
      "learning_rate": 0.00022635735439289237,
      "loss": 0.4559,
      "step": 74600
    },
    {
      "epoch": 0.2458045409674235,
      "grad_norm": 44.900455474853516,
      "learning_rate": 0.00022625863770977292,
      "loss": 0.578,
      "step": 74700
    },
    {
      "epoch": 0.24613359657782166,
      "grad_norm": 0.42689642310142517,
      "learning_rate": 0.0002261599210266535,
      "loss": 0.537,
      "step": 74800
    },
    {
      "epoch": 0.2464626521882198,
      "grad_norm": 2.2242431640625,
      "learning_rate": 0.00022606120434353404,
      "loss": 0.5841,
      "step": 74900
    },
    {
      "epoch": 0.24679170779861798,
      "grad_norm": 0.5280519127845764,
      "learning_rate": 0.00022596248766041462,
      "loss": 0.5366,
      "step": 75000
    },
    {
      "epoch": 0.24712076340901612,
      "grad_norm": 0.00956711731851101,
      "learning_rate": 0.00022586377097729514,
      "loss": 0.5721,
      "step": 75100
    },
    {
      "epoch": 0.2474498190194143,
      "grad_norm": 0.19774095714092255,
      "learning_rate": 0.00022576505429417569,
      "loss": 0.4973,
      "step": 75200
    },
    {
      "epoch": 0.24777887462981243,
      "grad_norm": 22.823387145996094,
      "learning_rate": 0.00022566633761105626,
      "loss": 0.7362,
      "step": 75300
    },
    {
      "epoch": 0.2481079302402106,
      "grad_norm": 0.0011980632552877069,
      "learning_rate": 0.0002255676209279368,
      "loss": 0.5677,
      "step": 75400
    },
    {
      "epoch": 0.24843698585060875,
      "grad_norm": 5.520078047993593e-05,
      "learning_rate": 0.00022546890424481733,
      "loss": 0.6066,
      "step": 75500
    },
    {
      "epoch": 0.24876604146100692,
      "grad_norm": 44.97008514404297,
      "learning_rate": 0.0002253701875616979,
      "loss": 0.602,
      "step": 75600
    },
    {
      "epoch": 0.24909509707140506,
      "grad_norm": 67.29335021972656,
      "learning_rate": 0.00022527147087857846,
      "loss": 0.6616,
      "step": 75700
    },
    {
      "epoch": 0.24942415268180324,
      "grad_norm": 126.1983642578125,
      "learning_rate": 0.00022517275419545903,
      "loss": 0.4993,
      "step": 75800
    },
    {
      "epoch": 0.24975320829220138,
      "grad_norm": 10.659834861755371,
      "learning_rate": 0.00022507403751233956,
      "loss": 0.285,
      "step": 75900
    },
    {
      "epoch": 0.2500822639025995,
      "grad_norm": 32.83539962768555,
      "learning_rate": 0.0002249753208292201,
      "loss": 0.6793,
      "step": 76000
    },
    {
      "epoch": 0.2504113195129977,
      "grad_norm": 0.006641514599323273,
      "learning_rate": 0.00022487660414610068,
      "loss": 0.7135,
      "step": 76100
    },
    {
      "epoch": 0.25074037512339586,
      "grad_norm": 0.016474634408950806,
      "learning_rate": 0.00022477788746298123,
      "loss": 0.7562,
      "step": 76200
    },
    {
      "epoch": 0.251069430733794,
      "grad_norm": 1.0012699365615845,
      "learning_rate": 0.00022467917077986175,
      "loss": 0.6056,
      "step": 76300
    },
    {
      "epoch": 0.25139848634419215,
      "grad_norm": 0.002249537967145443,
      "learning_rate": 0.00022458045409674233,
      "loss": 0.527,
      "step": 76400
    },
    {
      "epoch": 0.25172754195459035,
      "grad_norm": 4.001668453216553,
      "learning_rate": 0.00022448173741362288,
      "loss": 0.5233,
      "step": 76500
    },
    {
      "epoch": 0.2520565975649885,
      "grad_norm": 2.2830121517181396,
      "learning_rate": 0.00022438302073050345,
      "loss": 0.4612,
      "step": 76600
    },
    {
      "epoch": 0.25238565317538664,
      "grad_norm": 0.0006810290506109595,
      "learning_rate": 0.000224284304047384,
      "loss": 0.601,
      "step": 76700
    },
    {
      "epoch": 0.2527147087857848,
      "grad_norm": 1.2933579683303833,
      "learning_rate": 0.00022418558736426452,
      "loss": 0.6692,
      "step": 76800
    },
    {
      "epoch": 0.253043764396183,
      "grad_norm": 27.219602584838867,
      "learning_rate": 0.0002240868706811451,
      "loss": 0.4712,
      "step": 76900
    },
    {
      "epoch": 0.2533728200065811,
      "grad_norm": 0.19158081710338593,
      "learning_rate": 0.00022398815399802565,
      "loss": 0.5465,
      "step": 77000
    },
    {
      "epoch": 0.25370187561697927,
      "grad_norm": 0.0007091006846167147,
      "learning_rate": 0.0002238894373149062,
      "loss": 0.5229,
      "step": 77100
    },
    {
      "epoch": 0.2540309312273774,
      "grad_norm": 54.8781623840332,
      "learning_rate": 0.00022379072063178674,
      "loss": 0.4715,
      "step": 77200
    },
    {
      "epoch": 0.2543599868377756,
      "grad_norm": 9.735198020935059,
      "learning_rate": 0.0002236920039486673,
      "loss": 0.5174,
      "step": 77300
    },
    {
      "epoch": 0.25468904244817375,
      "grad_norm": 0.030130045488476753,
      "learning_rate": 0.00022359328726554787,
      "loss": 0.4599,
      "step": 77400
    },
    {
      "epoch": 0.2550180980585719,
      "grad_norm": 26.777435302734375,
      "learning_rate": 0.00022349457058242842,
      "loss": 0.6225,
      "step": 77500
    },
    {
      "epoch": 0.25534715366897004,
      "grad_norm": 46.24578094482422,
      "learning_rate": 0.00022339585389930894,
      "loss": 0.6994,
      "step": 77600
    },
    {
      "epoch": 0.25567620927936824,
      "grad_norm": 4.2546186447143555,
      "learning_rate": 0.00022329713721618951,
      "loss": 0.586,
      "step": 77700
    },
    {
      "epoch": 0.2560052648897664,
      "grad_norm": 67.16583251953125,
      "learning_rate": 0.00022319842053307006,
      "loss": 0.6382,
      "step": 77800
    },
    {
      "epoch": 0.2563343205001645,
      "grad_norm": 0.029933836311101913,
      "learning_rate": 0.0002230997038499506,
      "loss": 0.6622,
      "step": 77900
    },
    {
      "epoch": 0.25666337611056267,
      "grad_norm": 0.1313890665769577,
      "learning_rate": 0.0002230009871668312,
      "loss": 0.6918,
      "step": 78000
    },
    {
      "epoch": 0.25699243172096087,
      "grad_norm": 86.64517211914062,
      "learning_rate": 0.0002229022704837117,
      "loss": 0.6184,
      "step": 78100
    },
    {
      "epoch": 0.257321487331359,
      "grad_norm": 0.7953072786331177,
      "learning_rate": 0.00022280355380059229,
      "loss": 0.3907,
      "step": 78200
    },
    {
      "epoch": 0.25765054294175715,
      "grad_norm": 0.004594800062477589,
      "learning_rate": 0.00022270483711747283,
      "loss": 0.6166,
      "step": 78300
    },
    {
      "epoch": 0.2579795985521553,
      "grad_norm": 0.0004156592767685652,
      "learning_rate": 0.00022260612043435338,
      "loss": 0.4924,
      "step": 78400
    },
    {
      "epoch": 0.2583086541625535,
      "grad_norm": 0.14304132759571075,
      "learning_rate": 0.00022250740375123396,
      "loss": 0.7539,
      "step": 78500
    },
    {
      "epoch": 0.25863770977295164,
      "grad_norm": 0.30881166458129883,
      "learning_rate": 0.00022240868706811448,
      "loss": 0.3589,
      "step": 78600
    },
    {
      "epoch": 0.2589667653833498,
      "grad_norm": 0.008546202443540096,
      "learning_rate": 0.00022230997038499503,
      "loss": 0.7865,
      "step": 78700
    },
    {
      "epoch": 0.2592958209937479,
      "grad_norm": 0.03263955935835838,
      "learning_rate": 0.0002222112537018756,
      "loss": 0.5767,
      "step": 78800
    },
    {
      "epoch": 0.2596248766041461,
      "grad_norm": 1.2037273645401,
      "learning_rate": 0.00022211253701875615,
      "loss": 0.4621,
      "step": 78900
    },
    {
      "epoch": 0.25995393221454427,
      "grad_norm": 0.0026505785062909126,
      "learning_rate": 0.0002220138203356367,
      "loss": 0.6137,
      "step": 79000
    },
    {
      "epoch": 0.2602829878249424,
      "grad_norm": 0.01919407770037651,
      "learning_rate": 0.00022191510365251725,
      "loss": 0.7878,
      "step": 79100
    },
    {
      "epoch": 0.26061204343534056,
      "grad_norm": 81.78001403808594,
      "learning_rate": 0.0002218163869693978,
      "loss": 0.6712,
      "step": 79200
    },
    {
      "epoch": 0.26094109904573876,
      "grad_norm": 78.96910858154297,
      "learning_rate": 0.00022171767028627838,
      "loss": 0.6041,
      "step": 79300
    },
    {
      "epoch": 0.2612701546561369,
      "grad_norm": 8.669588088989258,
      "learning_rate": 0.0002216189536031589,
      "loss": 0.5995,
      "step": 79400
    },
    {
      "epoch": 0.26159921026653504,
      "grad_norm": 0.002701257588341832,
      "learning_rate": 0.00022152023692003945,
      "loss": 0.7936,
      "step": 79500
    },
    {
      "epoch": 0.2619282658769332,
      "grad_norm": 146.4125213623047,
      "learning_rate": 0.00022142152023692002,
      "loss": 0.6227,
      "step": 79600
    },
    {
      "epoch": 0.2622573214873314,
      "grad_norm": 0.0011355929309502244,
      "learning_rate": 0.00022132280355380057,
      "loss": 0.7087,
      "step": 79700
    },
    {
      "epoch": 0.26258637709772953,
      "grad_norm": 0.009644970297813416,
      "learning_rate": 0.00022122408687068115,
      "loss": 0.5087,
      "step": 79800
    },
    {
      "epoch": 0.26291543270812767,
      "grad_norm": 12.776033401489258,
      "learning_rate": 0.00022112537018756167,
      "loss": 0.7426,
      "step": 79900
    },
    {
      "epoch": 0.2632444883185258,
      "grad_norm": 0.0016858050366863608,
      "learning_rate": 0.00022102665350444222,
      "loss": 0.6335,
      "step": 80000
    },
    {
      "epoch": 0.263573543928924,
      "grad_norm": 83.27533721923828,
      "learning_rate": 0.0002209279368213228,
      "loss": 0.9072,
      "step": 80100
    },
    {
      "epoch": 0.26390259953932216,
      "grad_norm": 0.03606269136071205,
      "learning_rate": 0.00022082922013820334,
      "loss": 0.5956,
      "step": 80200
    },
    {
      "epoch": 0.2642316551497203,
      "grad_norm": 0.005850214045494795,
      "learning_rate": 0.00022073050345508387,
      "loss": 0.4403,
      "step": 80300
    },
    {
      "epoch": 0.26456071076011844,
      "grad_norm": 0.15411987900733948,
      "learning_rate": 0.00022063178677196444,
      "loss": 0.4558,
      "step": 80400
    },
    {
      "epoch": 0.26488976637051664,
      "grad_norm": 6.89302250975743e-05,
      "learning_rate": 0.000220533070088845,
      "loss": 0.4689,
      "step": 80500
    },
    {
      "epoch": 0.2652188219809148,
      "grad_norm": 0.02421492524445057,
      "learning_rate": 0.00022043435340572557,
      "loss": 0.7529,
      "step": 80600
    },
    {
      "epoch": 0.26554787759131293,
      "grad_norm": 0.0007484175148420036,
      "learning_rate": 0.00022033563672260611,
      "loss": 0.7925,
      "step": 80700
    },
    {
      "epoch": 0.2658769332017111,
      "grad_norm": 19.20941162109375,
      "learning_rate": 0.00022023692003948664,
      "loss": 0.6116,
      "step": 80800
    },
    {
      "epoch": 0.2662059888121093,
      "grad_norm": 0.0008236254216171801,
      "learning_rate": 0.0002201382033563672,
      "loss": 0.6506,
      "step": 80900
    },
    {
      "epoch": 0.2665350444225074,
      "grad_norm": 0.013026476837694645,
      "learning_rate": 0.00022003948667324776,
      "loss": 0.5158,
      "step": 81000
    },
    {
      "epoch": 0.26686410003290556,
      "grad_norm": 71.15141296386719,
      "learning_rate": 0.00021994076999012828,
      "loss": 0.7108,
      "step": 81100
    },
    {
      "epoch": 0.2671931556433037,
      "grad_norm": 0.0013373290421441197,
      "learning_rate": 0.00021984205330700886,
      "loss": 0.5424,
      "step": 81200
    },
    {
      "epoch": 0.2675222112537019,
      "grad_norm": 0.0010913170408457518,
      "learning_rate": 0.0002197433366238894,
      "loss": 0.5224,
      "step": 81300
    },
    {
      "epoch": 0.26785126686410005,
      "grad_norm": 0.0321236290037632,
      "learning_rate": 0.00021964461994076998,
      "loss": 0.7052,
      "step": 81400
    },
    {
      "epoch": 0.2681803224744982,
      "grad_norm": 0.7187986373901367,
      "learning_rate": 0.00021954590325765053,
      "loss": 0.768,
      "step": 81500
    },
    {
      "epoch": 0.26850937808489633,
      "grad_norm": 0.00046260262024588883,
      "learning_rate": 0.00021944718657453105,
      "loss": 0.6199,
      "step": 81600
    },
    {
      "epoch": 0.26883843369529453,
      "grad_norm": 0.002640095539391041,
      "learning_rate": 0.00021934846989141163,
      "loss": 0.8827,
      "step": 81700
    },
    {
      "epoch": 0.2691674893056927,
      "grad_norm": 0.0044211335480213165,
      "learning_rate": 0.00021924975320829218,
      "loss": 0.7388,
      "step": 81800
    },
    {
      "epoch": 0.2694965449160908,
      "grad_norm": 32.391868591308594,
      "learning_rate": 0.00021915103652517273,
      "loss": 0.6875,
      "step": 81900
    },
    {
      "epoch": 0.26982560052648896,
      "grad_norm": 39.28160858154297,
      "learning_rate": 0.0002190523198420533,
      "loss": 0.4284,
      "step": 82000
    },
    {
      "epoch": 0.27015465613688716,
      "grad_norm": 16.56439781188965,
      "learning_rate": 0.00021895360315893383,
      "loss": 0.612,
      "step": 82100
    },
    {
      "epoch": 0.2704837117472853,
      "grad_norm": 1.0310163497924805,
      "learning_rate": 0.0002188548864758144,
      "loss": 0.7036,
      "step": 82200
    },
    {
      "epoch": 0.27081276735768345,
      "grad_norm": 0.021018439903855324,
      "learning_rate": 0.00021875616979269495,
      "loss": 0.5311,
      "step": 82300
    },
    {
      "epoch": 0.2711418229680816,
      "grad_norm": 0.0030178194865584373,
      "learning_rate": 0.0002186574531095755,
      "loss": 0.3734,
      "step": 82400
    },
    {
      "epoch": 0.2714708785784798,
      "grad_norm": 0.0015033308882266283,
      "learning_rate": 0.00021855873642645605,
      "loss": 0.5487,
      "step": 82500
    },
    {
      "epoch": 0.27179993418887793,
      "grad_norm": 47.59837341308594,
      "learning_rate": 0.0002184600197433366,
      "loss": 0.6322,
      "step": 82600
    },
    {
      "epoch": 0.2721289897992761,
      "grad_norm": 28.4733829498291,
      "learning_rate": 0.00021836130306021715,
      "loss": 0.4933,
      "step": 82700
    },
    {
      "epoch": 0.2724580454096742,
      "grad_norm": 0.0009115798748098314,
      "learning_rate": 0.00021826258637709772,
      "loss": 0.3541,
      "step": 82800
    },
    {
      "epoch": 0.27278710102007236,
      "grad_norm": 0.0035390895791351795,
      "learning_rate": 0.00021816386969397824,
      "loss": 0.7751,
      "step": 82900
    },
    {
      "epoch": 0.27311615663047056,
      "grad_norm": 22.120084762573242,
      "learning_rate": 0.00021806515301085882,
      "loss": 0.4407,
      "step": 83000
    },
    {
      "epoch": 0.2734452122408687,
      "grad_norm": 145.27191162109375,
      "learning_rate": 0.00021796643632773937,
      "loss": 0.5153,
      "step": 83100
    },
    {
      "epoch": 0.27377426785126685,
      "grad_norm": 0.0651223212480545,
      "learning_rate": 0.00021786771964461992,
      "loss": 0.5857,
      "step": 83200
    },
    {
      "epoch": 0.274103323461665,
      "grad_norm": 0.09298326820135117,
      "learning_rate": 0.0002177690029615005,
      "loss": 0.46,
      "step": 83300
    },
    {
      "epoch": 0.2744323790720632,
      "grad_norm": 0.0026877678465098143,
      "learning_rate": 0.00021767028627838101,
      "loss": 0.6132,
      "step": 83400
    },
    {
      "epoch": 0.27476143468246134,
      "grad_norm": 3.7105462551116943,
      "learning_rate": 0.0002175715695952616,
      "loss": 0.6089,
      "step": 83500
    },
    {
      "epoch": 0.2750904902928595,
      "grad_norm": 64.9172592163086,
      "learning_rate": 0.00021747285291214214,
      "loss": 0.8555,
      "step": 83600
    },
    {
      "epoch": 0.2754195459032576,
      "grad_norm": 35.665287017822266,
      "learning_rate": 0.0002173741362290227,
      "loss": 0.453,
      "step": 83700
    },
    {
      "epoch": 0.2757486015136558,
      "grad_norm": 0.004754967056214809,
      "learning_rate": 0.00021727541954590326,
      "loss": 0.614,
      "step": 83800
    },
    {
      "epoch": 0.27607765712405397,
      "grad_norm": 0.3298497200012207,
      "learning_rate": 0.00021717670286278379,
      "loss": 0.5667,
      "step": 83900
    },
    {
      "epoch": 0.2764067127344521,
      "grad_norm": 0.001054184976965189,
      "learning_rate": 0.00021707798617966433,
      "loss": 0.6147,
      "step": 84000
    },
    {
      "epoch": 0.27673576834485025,
      "grad_norm": 0.01222269982099533,
      "learning_rate": 0.0002169792694965449,
      "loss": 0.4838,
      "step": 84100
    },
    {
      "epoch": 0.27706482395524845,
      "grad_norm": 0.019142093136906624,
      "learning_rate": 0.00021688055281342546,
      "loss": 0.4986,
      "step": 84200
    },
    {
      "epoch": 0.2773938795656466,
      "grad_norm": 35.74216842651367,
      "learning_rate": 0.000216781836130306,
      "loss": 0.8531,
      "step": 84300
    },
    {
      "epoch": 0.27772293517604474,
      "grad_norm": 47.20038986206055,
      "learning_rate": 0.00021668311944718656,
      "loss": 0.697,
      "step": 84400
    },
    {
      "epoch": 0.2780519907864429,
      "grad_norm": 0.013044659048318863,
      "learning_rate": 0.0002165844027640671,
      "loss": 0.5329,
      "step": 84500
    },
    {
      "epoch": 0.2783810463968411,
      "grad_norm": 103.80979919433594,
      "learning_rate": 0.00021648568608094768,
      "loss": 0.6881,
      "step": 84600
    },
    {
      "epoch": 0.2787101020072392,
      "grad_norm": 0.0005890853353776038,
      "learning_rate": 0.0002163869693978282,
      "loss": 0.4974,
      "step": 84700
    },
    {
      "epoch": 0.27903915761763737,
      "grad_norm": 5.272055149078369,
      "learning_rate": 0.00021628825271470875,
      "loss": 0.6004,
      "step": 84800
    },
    {
      "epoch": 0.2793682132280355,
      "grad_norm": 1.6851849555969238,
      "learning_rate": 0.00021618953603158933,
      "loss": 0.4343,
      "step": 84900
    },
    {
      "epoch": 0.2796972688384337,
      "grad_norm": 0.02796364575624466,
      "learning_rate": 0.00021609081934846988,
      "loss": 0.5799,
      "step": 85000
    },
    {
      "epoch": 0.28002632444883185,
      "grad_norm": 20.735782623291016,
      "learning_rate": 0.00021599210266535045,
      "loss": 0.574,
      "step": 85100
    },
    {
      "epoch": 0.28035538005923,
      "grad_norm": 0.3896399438381195,
      "learning_rate": 0.00021589338598223097,
      "loss": 0.4867,
      "step": 85200
    },
    {
      "epoch": 0.28068443566962814,
      "grad_norm": 7.4319257736206055,
      "learning_rate": 0.00021579466929911152,
      "loss": 0.3418,
      "step": 85300
    },
    {
      "epoch": 0.28101349128002634,
      "grad_norm": 8.569350029574707e-05,
      "learning_rate": 0.0002156959526159921,
      "loss": 0.5291,
      "step": 85400
    },
    {
      "epoch": 0.2813425468904245,
      "grad_norm": 0.028034988790750504,
      "learning_rate": 0.00021559723593287265,
      "loss": 0.4461,
      "step": 85500
    },
    {
      "epoch": 0.2816716025008226,
      "grad_norm": 0.0002850110176950693,
      "learning_rate": 0.00021549851924975317,
      "loss": 0.3841,
      "step": 85600
    },
    {
      "epoch": 0.28200065811122077,
      "grad_norm": 1.1166937351226807,
      "learning_rate": 0.00021539980256663374,
      "loss": 0.6735,
      "step": 85700
    },
    {
      "epoch": 0.28232971372161897,
      "grad_norm": 74.8160629272461,
      "learning_rate": 0.0002153010858835143,
      "loss": 0.5089,
      "step": 85800
    },
    {
      "epoch": 0.2826587693320171,
      "grad_norm": 75.1923828125,
      "learning_rate": 0.00021520236920039487,
      "loss": 0.3836,
      "step": 85900
    },
    {
      "epoch": 0.28298782494241526,
      "grad_norm": 0.041686903685331345,
      "learning_rate": 0.0002151036525172754,
      "loss": 0.4216,
      "step": 86000
    },
    {
      "epoch": 0.2833168805528134,
      "grad_norm": 0.0016413559205830097,
      "learning_rate": 0.00021500493583415594,
      "loss": 0.7019,
      "step": 86100
    },
    {
      "epoch": 0.2836459361632116,
      "grad_norm": 0.0018764319829642773,
      "learning_rate": 0.00021490621915103652,
      "loss": 0.6756,
      "step": 86200
    },
    {
      "epoch": 0.28397499177360974,
      "grad_norm": 1.300061821937561,
      "learning_rate": 0.00021480750246791706,
      "loss": 0.6233,
      "step": 86300
    },
    {
      "epoch": 0.2843040473840079,
      "grad_norm": 8.124903678894043,
      "learning_rate": 0.0002147087857847976,
      "loss": 0.3644,
      "step": 86400
    },
    {
      "epoch": 0.28463310299440603,
      "grad_norm": 0.04870743677020073,
      "learning_rate": 0.00021461006910167816,
      "loss": 0.4347,
      "step": 86500
    },
    {
      "epoch": 0.2849621586048042,
      "grad_norm": 0.0006538702291436493,
      "learning_rate": 0.0002145113524185587,
      "loss": 0.5918,
      "step": 86600
    },
    {
      "epoch": 0.28529121421520237,
      "grad_norm": 0.12341992557048798,
      "learning_rate": 0.0002144126357354393,
      "loss": 0.4709,
      "step": 86700
    },
    {
      "epoch": 0.2856202698256005,
      "grad_norm": 0.010472333990037441,
      "learning_rate": 0.00021431391905231984,
      "loss": 0.5213,
      "step": 86800
    },
    {
      "epoch": 0.28594932543599866,
      "grad_norm": 0.00655743945389986,
      "learning_rate": 0.00021421520236920036,
      "loss": 0.7799,
      "step": 86900
    },
    {
      "epoch": 0.28627838104639686,
      "grad_norm": 0.010653787292540073,
      "learning_rate": 0.00021411648568608093,
      "loss": 0.4373,
      "step": 87000
    },
    {
      "epoch": 0.286607436656795,
      "grad_norm": 0.014299208298325539,
      "learning_rate": 0.00021401776900296148,
      "loss": 0.641,
      "step": 87100
    },
    {
      "epoch": 0.28693649226719314,
      "grad_norm": 0.02031852863729,
      "learning_rate": 0.00021391905231984203,
      "loss": 0.7726,
      "step": 87200
    },
    {
      "epoch": 0.2872655478775913,
      "grad_norm": 0.018875934183597565,
      "learning_rate": 0.0002138203356367226,
      "loss": 0.5139,
      "step": 87300
    },
    {
      "epoch": 0.2875946034879895,
      "grad_norm": 24.142799377441406,
      "learning_rate": 0.00021372161895360313,
      "loss": 0.5023,
      "step": 87400
    },
    {
      "epoch": 0.28792365909838763,
      "grad_norm": 0.0008681622566655278,
      "learning_rate": 0.0002136229022704837,
      "loss": 0.6182,
      "step": 87500
    },
    {
      "epoch": 0.2882527147087858,
      "grad_norm": 90.3680191040039,
      "learning_rate": 0.00021352418558736425,
      "loss": 0.6846,
      "step": 87600
    },
    {
      "epoch": 0.2885817703191839,
      "grad_norm": 0.005737115629017353,
      "learning_rate": 0.0002134254689042448,
      "loss": 0.5746,
      "step": 87700
    },
    {
      "epoch": 0.2889108259295821,
      "grad_norm": 0.022471021860837936,
      "learning_rate": 0.00021332675222112535,
      "loss": 0.3942,
      "step": 87800
    },
    {
      "epoch": 0.28923988153998026,
      "grad_norm": 10.383659362792969,
      "learning_rate": 0.0002132280355380059,
      "loss": 0.5066,
      "step": 87900
    },
    {
      "epoch": 0.2895689371503784,
      "grad_norm": 0.00269989762455225,
      "learning_rate": 0.00021312931885488645,
      "loss": 0.297,
      "step": 88000
    },
    {
      "epoch": 0.28989799276077655,
      "grad_norm": 0.005800505634397268,
      "learning_rate": 0.00021303060217176702,
      "loss": 0.2811,
      "step": 88100
    },
    {
      "epoch": 0.29022704837117475,
      "grad_norm": 0.06719827651977539,
      "learning_rate": 0.00021293188548864755,
      "loss": 0.6799,
      "step": 88200
    },
    {
      "epoch": 0.2905561039815729,
      "grad_norm": 0.0026594146620482206,
      "learning_rate": 0.00021283316880552812,
      "loss": 0.4053,
      "step": 88300
    },
    {
      "epoch": 0.29088515959197103,
      "grad_norm": 104.12291717529297,
      "learning_rate": 0.00021273445212240867,
      "loss": 0.6812,
      "step": 88400
    },
    {
      "epoch": 0.2912142152023692,
      "grad_norm": 34.94294357299805,
      "learning_rate": 0.00021263573543928922,
      "loss": 0.5093,
      "step": 88500
    },
    {
      "epoch": 0.2915432708127674,
      "grad_norm": 0.0032411376014351845,
      "learning_rate": 0.0002125370187561698,
      "loss": 0.5347,
      "step": 88600
    },
    {
      "epoch": 0.2918723264231655,
      "grad_norm": 0.04343508183956146,
      "learning_rate": 0.00021243830207305032,
      "loss": 0.6273,
      "step": 88700
    },
    {
      "epoch": 0.29220138203356366,
      "grad_norm": 56.78242111206055,
      "learning_rate": 0.00021233958538993087,
      "loss": 0.3743,
      "step": 88800
    },
    {
      "epoch": 0.2925304376439618,
      "grad_norm": 0.002089357003569603,
      "learning_rate": 0.00021224086870681144,
      "loss": 0.5429,
      "step": 88900
    },
    {
      "epoch": 0.29285949325436,
      "grad_norm": 42.875038146972656,
      "learning_rate": 0.000212142152023692,
      "loss": 0.6774,
      "step": 89000
    },
    {
      "epoch": 0.29318854886475815,
      "grad_norm": 0.007316200062632561,
      "learning_rate": 0.00021204343534057257,
      "loss": 0.5264,
      "step": 89100
    },
    {
      "epoch": 0.2935176044751563,
      "grad_norm": 44.06696319580078,
      "learning_rate": 0.0002119447186574531,
      "loss": 0.5689,
      "step": 89200
    },
    {
      "epoch": 0.29384666008555443,
      "grad_norm": 0.0009947126964107156,
      "learning_rate": 0.00021184600197433364,
      "loss": 0.4165,
      "step": 89300
    },
    {
      "epoch": 0.29417571569595263,
      "grad_norm": 16.62034034729004,
      "learning_rate": 0.0002117472852912142,
      "loss": 0.6728,
      "step": 89400
    },
    {
      "epoch": 0.2945047713063508,
      "grad_norm": 0.0027063486631959677,
      "learning_rate": 0.00021164856860809474,
      "loss": 0.757,
      "step": 89500
    },
    {
      "epoch": 0.2948338269167489,
      "grad_norm": 97.84764099121094,
      "learning_rate": 0.00021154985192497528,
      "loss": 0.5497,
      "step": 89600
    },
    {
      "epoch": 0.29516288252714706,
      "grad_norm": 0.000586474547162652,
      "learning_rate": 0.00021145113524185586,
      "loss": 0.5697,
      "step": 89700
    },
    {
      "epoch": 0.29549193813754526,
      "grad_norm": 0.011280793696641922,
      "learning_rate": 0.0002113524185587364,
      "loss": 0.7641,
      "step": 89800
    },
    {
      "epoch": 0.2958209937479434,
      "grad_norm": 44.78415298461914,
      "learning_rate": 0.00021125370187561698,
      "loss": 0.3429,
      "step": 89900
    },
    {
      "epoch": 0.29615004935834155,
      "grad_norm": 43.99831771850586,
      "learning_rate": 0.0002111549851924975,
      "loss": 0.6113,
      "step": 90000
    },
    {
      "epoch": 0.2964791049687397,
      "grad_norm": 1.3784428834915161,
      "learning_rate": 0.00021105626850937806,
      "loss": 0.2892,
      "step": 90100
    },
    {
      "epoch": 0.2968081605791379,
      "grad_norm": 0.13492290675640106,
      "learning_rate": 0.00021095755182625863,
      "loss": 0.7944,
      "step": 90200
    },
    {
      "epoch": 0.29713721618953604,
      "grad_norm": 0.019576068967580795,
      "learning_rate": 0.00021085883514313918,
      "loss": 0.3485,
      "step": 90300
    },
    {
      "epoch": 0.2974662717999342,
      "grad_norm": 52.59809112548828,
      "learning_rate": 0.0002107601184600197,
      "loss": 0.4601,
      "step": 90400
    },
    {
      "epoch": 0.2977953274103323,
      "grad_norm": 0.3342200517654419,
      "learning_rate": 0.00021066140177690028,
      "loss": 0.7256,
      "step": 90500
    },
    {
      "epoch": 0.2981243830207305,
      "grad_norm": 0.0019359956495463848,
      "learning_rate": 0.00021056268509378083,
      "loss": 0.4187,
      "step": 90600
    },
    {
      "epoch": 0.29845343863112866,
      "grad_norm": 1.563398003578186,
      "learning_rate": 0.0002104639684106614,
      "loss": 0.4682,
      "step": 90700
    },
    {
      "epoch": 0.2987824942415268,
      "grad_norm": 33.14975357055664,
      "learning_rate": 0.00021036525172754195,
      "loss": 0.5398,
      "step": 90800
    },
    {
      "epoch": 0.29911154985192495,
      "grad_norm": 1.1709895133972168,
      "learning_rate": 0.00021026653504442247,
      "loss": 0.6944,
      "step": 90900
    },
    {
      "epoch": 0.29944060546232315,
      "grad_norm": 0.0003849468193948269,
      "learning_rate": 0.00021016781836130305,
      "loss": 0.7201,
      "step": 91000
    },
    {
      "epoch": 0.2997696610727213,
      "grad_norm": 0.37111493945121765,
      "learning_rate": 0.0002100691016781836,
      "loss": 0.5462,
      "step": 91100
    },
    {
      "epoch": 0.30009871668311944,
      "grad_norm": 0.0018644649535417557,
      "learning_rate": 0.00020997038499506415,
      "loss": 0.4824,
      "step": 91200
    },
    {
      "epoch": 0.3004277722935176,
      "grad_norm": 0.015229739248752594,
      "learning_rate": 0.0002098716683119447,
      "loss": 0.2946,
      "step": 91300
    },
    {
      "epoch": 0.3007568279039158,
      "grad_norm": 30.568363189697266,
      "learning_rate": 0.00020977295162882524,
      "loss": 0.6763,
      "step": 91400
    },
    {
      "epoch": 0.3010858835143139,
      "grad_norm": 0.005727221257984638,
      "learning_rate": 0.00020967423494570582,
      "loss": 0.4689,
      "step": 91500
    },
    {
      "epoch": 0.30141493912471207,
      "grad_norm": 6.887782573699951,
      "learning_rate": 0.00020957551826258637,
      "loss": 0.2828,
      "step": 91600
    },
    {
      "epoch": 0.3017439947351102,
      "grad_norm": 37.540306091308594,
      "learning_rate": 0.0002094768015794669,
      "loss": 0.5839,
      "step": 91700
    },
    {
      "epoch": 0.3020730503455084,
      "grad_norm": 0.0032843733206391335,
      "learning_rate": 0.00020937808489634747,
      "loss": 0.4181,
      "step": 91800
    },
    {
      "epoch": 0.30240210595590655,
      "grad_norm": 0.009624142199754715,
      "learning_rate": 0.00020927936821322802,
      "loss": 0.6444,
      "step": 91900
    },
    {
      "epoch": 0.3027311615663047,
      "grad_norm": 1.8486875295639038,
      "learning_rate": 0.00020918065153010856,
      "loss": 0.4243,
      "step": 92000
    },
    {
      "epoch": 0.30306021717670284,
      "grad_norm": 0.00012060003791702911,
      "learning_rate": 0.00020908193484698914,
      "loss": 0.4425,
      "step": 92100
    },
    {
      "epoch": 0.30338927278710104,
      "grad_norm": 0.0001916974870255217,
      "learning_rate": 0.00020898321816386966,
      "loss": 0.4755,
      "step": 92200
    },
    {
      "epoch": 0.3037183283974992,
      "grad_norm": 0.00496677728369832,
      "learning_rate": 0.00020888450148075024,
      "loss": 0.6863,
      "step": 92300
    },
    {
      "epoch": 0.3040473840078973,
      "grad_norm": 15.488435745239258,
      "learning_rate": 0.00020878578479763079,
      "loss": 0.4645,
      "step": 92400
    },
    {
      "epoch": 0.30437643961829547,
      "grad_norm": 10.129777908325195,
      "learning_rate": 0.00020868706811451133,
      "loss": 0.63,
      "step": 92500
    },
    {
      "epoch": 0.30470549522869367,
      "grad_norm": 0.007989101111888885,
      "learning_rate": 0.0002085883514313919,
      "loss": 0.4948,
      "step": 92600
    },
    {
      "epoch": 0.3050345508390918,
      "grad_norm": 4.97844123840332,
      "learning_rate": 0.00020848963474827243,
      "loss": 0.5135,
      "step": 92700
    },
    {
      "epoch": 0.30536360644948995,
      "grad_norm": 0.0015047276392579079,
      "learning_rate": 0.00020839091806515298,
      "loss": 0.4048,
      "step": 92800
    },
    {
      "epoch": 0.3056926620598881,
      "grad_norm": 4.949897289276123,
      "learning_rate": 0.00020829220138203356,
      "loss": 0.622,
      "step": 92900
    },
    {
      "epoch": 0.3060217176702863,
      "grad_norm": 0.00028164705145172775,
      "learning_rate": 0.00020819348469891408,
      "loss": 0.7708,
      "step": 93000
    },
    {
      "epoch": 0.30635077328068444,
      "grad_norm": 0.0034968575928360224,
      "learning_rate": 0.00020809476801579465,
      "loss": 0.5868,
      "step": 93100
    },
    {
      "epoch": 0.3066798288910826,
      "grad_norm": 0.00031218011281453073,
      "learning_rate": 0.0002079960513326752,
      "loss": 0.6297,
      "step": 93200
    },
    {
      "epoch": 0.3070088845014807,
      "grad_norm": 0.002936108037829399,
      "learning_rate": 0.00020789733464955575,
      "loss": 0.4271,
      "step": 93300
    },
    {
      "epoch": 0.3073379401118789,
      "grad_norm": 108.48672485351562,
      "learning_rate": 0.00020779861796643633,
      "loss": 0.5588,
      "step": 93400
    },
    {
      "epoch": 0.30766699572227707,
      "grad_norm": 0.013766500167548656,
      "learning_rate": 0.00020769990128331685,
      "loss": 0.4452,
      "step": 93500
    },
    {
      "epoch": 0.3079960513326752,
      "grad_norm": 0.011361150071024895,
      "learning_rate": 0.0002076011846001974,
      "loss": 0.3979,
      "step": 93600
    },
    {
      "epoch": 0.30832510694307336,
      "grad_norm": 0.006849960889667273,
      "learning_rate": 0.00020750246791707797,
      "loss": 0.4171,
      "step": 93700
    },
    {
      "epoch": 0.30865416255347156,
      "grad_norm": 0.011165650561451912,
      "learning_rate": 0.00020740375123395852,
      "loss": 0.6207,
      "step": 93800
    },
    {
      "epoch": 0.3089832181638697,
      "grad_norm": 0.03393940627574921,
      "learning_rate": 0.0002073050345508391,
      "loss": 0.4698,
      "step": 93900
    },
    {
      "epoch": 0.30931227377426784,
      "grad_norm": 0.5009708404541016,
      "learning_rate": 0.00020720631786771962,
      "loss": 0.5592,
      "step": 94000
    },
    {
      "epoch": 0.309641329384666,
      "grad_norm": 26.825559616088867,
      "learning_rate": 0.00020710760118460017,
      "loss": 0.487,
      "step": 94100
    },
    {
      "epoch": 0.3099703849950642,
      "grad_norm": 16.450305938720703,
      "learning_rate": 0.00020700888450148075,
      "loss": 0.2976,
      "step": 94200
    },
    {
      "epoch": 0.31029944060546233,
      "grad_norm": 0.004851046018302441,
      "learning_rate": 0.0002069101678183613,
      "loss": 0.7058,
      "step": 94300
    },
    {
      "epoch": 0.3106284962158605,
      "grad_norm": 0.025971051305532455,
      "learning_rate": 0.00020681145113524182,
      "loss": 0.4049,
      "step": 94400
    },
    {
      "epoch": 0.3109575518262586,
      "grad_norm": 0.0007062193471938372,
      "learning_rate": 0.0002067127344521224,
      "loss": 0.4001,
      "step": 94500
    },
    {
      "epoch": 0.3112866074366568,
      "grad_norm": 0.0038928489666432142,
      "learning_rate": 0.00020661401776900294,
      "loss": 0.8178,
      "step": 94600
    },
    {
      "epoch": 0.31161566304705496,
      "grad_norm": 30.057674407958984,
      "learning_rate": 0.00020651530108588352,
      "loss": 0.4836,
      "step": 94700
    },
    {
      "epoch": 0.3119447186574531,
      "grad_norm": 1.1190605163574219,
      "learning_rate": 0.00020641658440276404,
      "loss": 0.4613,
      "step": 94800
    },
    {
      "epoch": 0.31227377426785125,
      "grad_norm": 0.12519927322864532,
      "learning_rate": 0.0002063178677196446,
      "loss": 0.4172,
      "step": 94900
    },
    {
      "epoch": 0.31260282987824944,
      "grad_norm": 2.0607519149780273,
      "learning_rate": 0.00020621915103652516,
      "loss": 0.5725,
      "step": 95000
    },
    {
      "epoch": 0.3129318854886476,
      "grad_norm": 0.055247459560632706,
      "learning_rate": 0.0002061204343534057,
      "loss": 0.6503,
      "step": 95100
    },
    {
      "epoch": 0.31326094109904573,
      "grad_norm": 33.838478088378906,
      "learning_rate": 0.00020602171767028623,
      "loss": 0.5379,
      "step": 95200
    },
    {
      "epoch": 0.3135899967094439,
      "grad_norm": 51.48393630981445,
      "learning_rate": 0.0002059230009871668,
      "loss": 0.5363,
      "step": 95300
    },
    {
      "epoch": 0.3139190523198421,
      "grad_norm": 0.0034173910971730947,
      "learning_rate": 0.00020582428430404736,
      "loss": 0.3782,
      "step": 95400
    },
    {
      "epoch": 0.3142481079302402,
      "grad_norm": 0.001013820874504745,
      "learning_rate": 0.00020572556762092793,
      "loss": 0.4114,
      "step": 95500
    },
    {
      "epoch": 0.31457716354063836,
      "grad_norm": 20.41629981994629,
      "learning_rate": 0.00020562685093780848,
      "loss": 0.6099,
      "step": 95600
    },
    {
      "epoch": 0.3149062191510365,
      "grad_norm": 5.830137252807617,
      "learning_rate": 0.000205528134254689,
      "loss": 0.5338,
      "step": 95700
    },
    {
      "epoch": 0.3152352747614347,
      "grad_norm": 0.01881679706275463,
      "learning_rate": 0.00020542941757156958,
      "loss": 0.5644,
      "step": 95800
    },
    {
      "epoch": 0.31556433037183285,
      "grad_norm": 0.009379072114825249,
      "learning_rate": 0.00020533070088845013,
      "loss": 0.4,
      "step": 95900
    },
    {
      "epoch": 0.315893385982231,
      "grad_norm": 0.21623075008392334,
      "learning_rate": 0.00020523198420533068,
      "loss": 0.4632,
      "step": 96000
    },
    {
      "epoch": 0.31622244159262913,
      "grad_norm": 6.223565578460693,
      "learning_rate": 0.00020513326752221125,
      "loss": 0.4291,
      "step": 96100
    },
    {
      "epoch": 0.31655149720302733,
      "grad_norm": 0.0019222211558371782,
      "learning_rate": 0.00020503455083909178,
      "loss": 0.4287,
      "step": 96200
    },
    {
      "epoch": 0.3168805528134255,
      "grad_norm": 0.2160130739212036,
      "learning_rate": 0.00020493583415597235,
      "loss": 0.6373,
      "step": 96300
    },
    {
      "epoch": 0.3172096084238236,
      "grad_norm": 0.0034473049454391003,
      "learning_rate": 0.0002048371174728529,
      "loss": 0.6928,
      "step": 96400
    },
    {
      "epoch": 0.31753866403422176,
      "grad_norm": 0.003858015639707446,
      "learning_rate": 0.00020473840078973345,
      "loss": 0.3959,
      "step": 96500
    },
    {
      "epoch": 0.31786771964461996,
      "grad_norm": 0.0007172875921241939,
      "learning_rate": 0.000204639684106614,
      "loss": 0.5617,
      "step": 96600
    },
    {
      "epoch": 0.3181967752550181,
      "grad_norm": 0.014386961236596107,
      "learning_rate": 0.00020454096742349455,
      "loss": 0.6903,
      "step": 96700
    },
    {
      "epoch": 0.31852583086541625,
      "grad_norm": 0.008515202440321445,
      "learning_rate": 0.0002044422507403751,
      "loss": 0.4116,
      "step": 96800
    },
    {
      "epoch": 0.3188548864758144,
      "grad_norm": 0.1541794389486313,
      "learning_rate": 0.00020434353405725567,
      "loss": 0.4615,
      "step": 96900
    },
    {
      "epoch": 0.3191839420862126,
      "grad_norm": 0.03724905103445053,
      "learning_rate": 0.0002042448173741362,
      "loss": 0.5591,
      "step": 97000
    },
    {
      "epoch": 0.31951299769661073,
      "grad_norm": 65.3162612915039,
      "learning_rate": 0.00020414610069101677,
      "loss": 0.5482,
      "step": 97100
    },
    {
      "epoch": 0.3198420533070089,
      "grad_norm": 0.0009354214416816831,
      "learning_rate": 0.00020404738400789732,
      "loss": 0.4848,
      "step": 97200
    },
    {
      "epoch": 0.320171108917407,
      "grad_norm": 20.334251403808594,
      "learning_rate": 0.00020394866732477787,
      "loss": 0.5716,
      "step": 97300
    },
    {
      "epoch": 0.3205001645278052,
      "grad_norm": 0.012986985966563225,
      "learning_rate": 0.00020384995064165844,
      "loss": 0.3985,
      "step": 97400
    },
    {
      "epoch": 0.32082922013820336,
      "grad_norm": 5.919961929321289,
      "learning_rate": 0.00020375123395853897,
      "loss": 0.4165,
      "step": 97500
    },
    {
      "epoch": 0.3211582757486015,
      "grad_norm": 0.0023211361840367317,
      "learning_rate": 0.00020365251727541951,
      "loss": 0.486,
      "step": 97600
    },
    {
      "epoch": 0.32148733135899965,
      "grad_norm": 0.005741524510085583,
      "learning_rate": 0.0002035538005923001,
      "loss": 0.5777,
      "step": 97700
    },
    {
      "epoch": 0.32181638696939785,
      "grad_norm": 0.0016041204798966646,
      "learning_rate": 0.00020345508390918064,
      "loss": 0.357,
      "step": 97800
    },
    {
      "epoch": 0.322145442579796,
      "grad_norm": 0.004659960512071848,
      "learning_rate": 0.0002033563672260612,
      "loss": 0.7112,
      "step": 97900
    },
    {
      "epoch": 0.32247449819019414,
      "grad_norm": 0.18571577966213226,
      "learning_rate": 0.00020325765054294174,
      "loss": 0.6944,
      "step": 98000
    },
    {
      "epoch": 0.3228035538005923,
      "grad_norm": 0.014361017383635044,
      "learning_rate": 0.00020315893385982229,
      "loss": 0.6417,
      "step": 98100
    },
    {
      "epoch": 0.3231326094109905,
      "grad_norm": 0.0018726505804806948,
      "learning_rate": 0.00020306021717670286,
      "loss": 0.5419,
      "step": 98200
    },
    {
      "epoch": 0.3234616650213886,
      "grad_norm": 181.3443145751953,
      "learning_rate": 0.00020296150049358338,
      "loss": 0.6915,
      "step": 98300
    },
    {
      "epoch": 0.32379072063178677,
      "grad_norm": 0.006452478468418121,
      "learning_rate": 0.00020286278381046393,
      "loss": 0.4231,
      "step": 98400
    },
    {
      "epoch": 0.3241197762421849,
      "grad_norm": 0.0048229326494038105,
      "learning_rate": 0.0002027640671273445,
      "loss": 0.4541,
      "step": 98500
    },
    {
      "epoch": 0.3244488318525831,
      "grad_norm": 1.9298462867736816,
      "learning_rate": 0.00020266535044422506,
      "loss": 0.562,
      "step": 98600
    },
    {
      "epoch": 0.32477788746298125,
      "grad_norm": 0.012775588780641556,
      "learning_rate": 0.00020256663376110563,
      "loss": 0.5575,
      "step": 98700
    },
    {
      "epoch": 0.3251069430733794,
      "grad_norm": 0.006598001811653376,
      "learning_rate": 0.00020246791707798615,
      "loss": 0.3094,
      "step": 98800
    },
    {
      "epoch": 0.32543599868377754,
      "grad_norm": 0.25279009342193604,
      "learning_rate": 0.0002023692003948667,
      "loss": 0.5806,
      "step": 98900
    },
    {
      "epoch": 0.32576505429417574,
      "grad_norm": 0.0007068893755786121,
      "learning_rate": 0.00020227048371174728,
      "loss": 0.5885,
      "step": 99000
    },
    {
      "epoch": 0.3260941099045739,
      "grad_norm": 0.010680392384529114,
      "learning_rate": 0.00020217176702862783,
      "loss": 0.5478,
      "step": 99100
    },
    {
      "epoch": 0.326423165514972,
      "grad_norm": 15.9668550491333,
      "learning_rate": 0.00020207305034550835,
      "loss": 0.7221,
      "step": 99200
    },
    {
      "epoch": 0.32675222112537017,
      "grad_norm": 25.80950927734375,
      "learning_rate": 0.00020197433366238892,
      "loss": 0.5283,
      "step": 99300
    },
    {
      "epoch": 0.32708127673576837,
      "grad_norm": 0.23343278467655182,
      "learning_rate": 0.00020187561697926947,
      "loss": 0.5353,
      "step": 99400
    },
    {
      "epoch": 0.3274103323461665,
      "grad_norm": 0.0037577273324131966,
      "learning_rate": 0.00020177690029615005,
      "loss": 0.5885,
      "step": 99500
    },
    {
      "epoch": 0.32773938795656465,
      "grad_norm": 6.63934915792197e-05,
      "learning_rate": 0.0002016781836130306,
      "loss": 0.6608,
      "step": 99600
    },
    {
      "epoch": 0.3280684435669628,
      "grad_norm": 0.00036143811303190887,
      "learning_rate": 0.00020157946692991112,
      "loss": 0.4882,
      "step": 99700
    },
    {
      "epoch": 0.328397499177361,
      "grad_norm": 0.0019744790624827147,
      "learning_rate": 0.0002014807502467917,
      "loss": 0.3229,
      "step": 99800
    },
    {
      "epoch": 0.32872655478775914,
      "grad_norm": 6.6544599533081055,
      "learning_rate": 0.00020138203356367224,
      "loss": 0.6308,
      "step": 99900
    },
    {
      "epoch": 0.3290556103981573,
      "grad_norm": 0.00026071714819408953,
      "learning_rate": 0.0002012833168805528,
      "loss": 0.578,
      "step": 100000
    },
    {
      "epoch": 0.3293846660085554,
      "grad_norm": 0.0024949265643954277,
      "learning_rate": 0.00020118460019743334,
      "loss": 0.3893,
      "step": 100100
    },
    {
      "epoch": 0.3297137216189536,
      "grad_norm": 0.001997386571019888,
      "learning_rate": 0.0002010858835143139,
      "loss": 0.3897,
      "step": 100200
    },
    {
      "epoch": 0.33004277722935177,
      "grad_norm": 121.802490234375,
      "learning_rate": 0.00020098716683119447,
      "loss": 0.3801,
      "step": 100300
    },
    {
      "epoch": 0.3303718328397499,
      "grad_norm": 0.0024165210779756308,
      "learning_rate": 0.00020088845014807502,
      "loss": 0.6498,
      "step": 100400
    },
    {
      "epoch": 0.33070088845014806,
      "grad_norm": 0.00337209552526474,
      "learning_rate": 0.00020078973346495554,
      "loss": 0.3508,
      "step": 100500
    },
    {
      "epoch": 0.33102994406054626,
      "grad_norm": 0.0019842940382659435,
      "learning_rate": 0.00020069101678183611,
      "loss": 0.3769,
      "step": 100600
    },
    {
      "epoch": 0.3313589996709444,
      "grad_norm": 0.0007084830431267619,
      "learning_rate": 0.00020059230009871666,
      "loss": 0.5792,
      "step": 100700
    },
    {
      "epoch": 0.33168805528134254,
      "grad_norm": 7.9976582527160645,
      "learning_rate": 0.0002004935834155972,
      "loss": 0.4461,
      "step": 100800
    },
    {
      "epoch": 0.3320171108917407,
      "grad_norm": 53.77599334716797,
      "learning_rate": 0.0002003948667324778,
      "loss": 0.5086,
      "step": 100900
    },
    {
      "epoch": 0.3323461665021389,
      "grad_norm": 15.097050666809082,
      "learning_rate": 0.0002002961500493583,
      "loss": 0.6466,
      "step": 101000
    },
    {
      "epoch": 0.33267522211253703,
      "grad_norm": 28.153963088989258,
      "learning_rate": 0.00020019743336623888,
      "loss": 0.4573,
      "step": 101100
    },
    {
      "epoch": 0.33300427772293517,
      "grad_norm": 0.001567587023600936,
      "learning_rate": 0.00020009871668311943,
      "loss": 0.4127,
      "step": 101200
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 35.346092224121094,
      "learning_rate": 0.00019999999999999998,
      "loss": 0.6403,
      "step": 101300
    },
    {
      "epoch": 0.3336623889437315,
      "grad_norm": 2.5436089038848877,
      "learning_rate": 0.00019990128331688056,
      "loss": 0.6516,
      "step": 101400
    },
    {
      "epoch": 0.33399144455412966,
      "grad_norm": 0.0032068369910120964,
      "learning_rate": 0.00019980256663376108,
      "loss": 0.3881,
      "step": 101500
    },
    {
      "epoch": 0.3343205001645278,
      "grad_norm": 10.599700927734375,
      "learning_rate": 0.00019970384995064163,
      "loss": 0.7889,
      "step": 101600
    },
    {
      "epoch": 0.33464955577492594,
      "grad_norm": 0.0014779827324673533,
      "learning_rate": 0.0001996051332675222,
      "loss": 0.4719,
      "step": 101700
    },
    {
      "epoch": 0.33497861138532414,
      "grad_norm": 0.005959032569080591,
      "learning_rate": 0.00019950641658440273,
      "loss": 0.5847,
      "step": 101800
    },
    {
      "epoch": 0.3353076669957223,
      "grad_norm": 0.003173783654347062,
      "learning_rate": 0.0001994076999012833,
      "loss": 0.6016,
      "step": 101900
    },
    {
      "epoch": 0.33563672260612043,
      "grad_norm": 0.28886649012565613,
      "learning_rate": 0.00019930898321816385,
      "loss": 0.6544,
      "step": 102000
    },
    {
      "epoch": 0.3359657782165186,
      "grad_norm": 0.006830031517893076,
      "learning_rate": 0.0001992102665350444,
      "loss": 0.4903,
      "step": 102100
    },
    {
      "epoch": 0.3362948338269168,
      "grad_norm": 0.4115866720676422,
      "learning_rate": 0.00019911154985192498,
      "loss": 0.5192,
      "step": 102200
    },
    {
      "epoch": 0.3366238894373149,
      "grad_norm": 0.001973491394892335,
      "learning_rate": 0.0001990128331688055,
      "loss": 0.5365,
      "step": 102300
    },
    {
      "epoch": 0.33695294504771306,
      "grad_norm": 1.5384601354599,
      "learning_rate": 0.00019891411648568605,
      "loss": 0.3835,
      "step": 102400
    },
    {
      "epoch": 0.3372820006581112,
      "grad_norm": 14.371572494506836,
      "learning_rate": 0.00019881539980256662,
      "loss": 0.4711,
      "step": 102500
    },
    {
      "epoch": 0.3376110562685094,
      "grad_norm": 1.0019350051879883,
      "learning_rate": 0.00019871668311944717,
      "loss": 0.4051,
      "step": 102600
    },
    {
      "epoch": 0.33794011187890755,
      "grad_norm": 0.0003995105507783592,
      "learning_rate": 0.00019861796643632775,
      "loss": 0.5961,
      "step": 102700
    },
    {
      "epoch": 0.3382691674893057,
      "grad_norm": 0.0021832550410181284,
      "learning_rate": 0.00019851924975320827,
      "loss": 0.5845,
      "step": 102800
    },
    {
      "epoch": 0.33859822309970383,
      "grad_norm": 0.07024060189723969,
      "learning_rate": 0.00019842053307008882,
      "loss": 0.3591,
      "step": 102900
    },
    {
      "epoch": 0.33892727871010203,
      "grad_norm": 0.21583323180675507,
      "learning_rate": 0.0001983218163869694,
      "loss": 0.4787,
      "step": 103000
    },
    {
      "epoch": 0.3392563343205002,
      "grad_norm": 15.13228988647461,
      "learning_rate": 0.00019822309970384994,
      "loss": 0.5751,
      "step": 103100
    },
    {
      "epoch": 0.3395853899308983,
      "grad_norm": 0.025104427710175514,
      "learning_rate": 0.00019812438302073046,
      "loss": 0.5259,
      "step": 103200
    },
    {
      "epoch": 0.33991444554129646,
      "grad_norm": 0.0014268835075199604,
      "learning_rate": 0.00019802566633761104,
      "loss": 0.4555,
      "step": 103300
    },
    {
      "epoch": 0.34024350115169466,
      "grad_norm": 77.652587890625,
      "learning_rate": 0.0001979269496544916,
      "loss": 0.5191,
      "step": 103400
    },
    {
      "epoch": 0.3405725567620928,
      "grad_norm": 0.008711108937859535,
      "learning_rate": 0.00019782823297137216,
      "loss": 0.5704,
      "step": 103500
    },
    {
      "epoch": 0.34090161237249095,
      "grad_norm": 22.8581600189209,
      "learning_rate": 0.00019772951628825269,
      "loss": 0.417,
      "step": 103600
    },
    {
      "epoch": 0.3412306679828891,
      "grad_norm": 0.00010468631808180362,
      "learning_rate": 0.00019763079960513324,
      "loss": 0.3471,
      "step": 103700
    },
    {
      "epoch": 0.3415597235932873,
      "grad_norm": 0.03620080277323723,
      "learning_rate": 0.0001975320829220138,
      "loss": 0.4782,
      "step": 103800
    },
    {
      "epoch": 0.34188877920368543,
      "grad_norm": 0.06386439502239227,
      "learning_rate": 0.00019743336623889436,
      "loss": 0.376,
      "step": 103900
    },
    {
      "epoch": 0.3422178348140836,
      "grad_norm": 1.286892294883728,
      "learning_rate": 0.00019733464955577488,
      "loss": 0.6311,
      "step": 104000
    },
    {
      "epoch": 0.3425468904244817,
      "grad_norm": 21.479616165161133,
      "learning_rate": 0.00019723593287265546,
      "loss": 0.7445,
      "step": 104100
    },
    {
      "epoch": 0.3428759460348799,
      "grad_norm": 0.0032940709497779608,
      "learning_rate": 0.000197137216189536,
      "loss": 0.7261,
      "step": 104200
    },
    {
      "epoch": 0.34320500164527806,
      "grad_norm": 0.0033189968671649694,
      "learning_rate": 0.00019703849950641658,
      "loss": 0.4912,
      "step": 104300
    },
    {
      "epoch": 0.3435340572556762,
      "grad_norm": 90.63796997070312,
      "learning_rate": 0.00019693978282329713,
      "loss": 0.4717,
      "step": 104400
    },
    {
      "epoch": 0.34386311286607435,
      "grad_norm": 0.011120985262095928,
      "learning_rate": 0.00019684106614017765,
      "loss": 0.6129,
      "step": 104500
    },
    {
      "epoch": 0.34419216847647255,
      "grad_norm": 27.113256454467773,
      "learning_rate": 0.00019674234945705823,
      "loss": 0.4151,
      "step": 104600
    },
    {
      "epoch": 0.3445212240868707,
      "grad_norm": 0.0003463670436758548,
      "learning_rate": 0.00019664363277393878,
      "loss": 0.5491,
      "step": 104700
    },
    {
      "epoch": 0.34485027969726884,
      "grad_norm": 55.86672592163086,
      "learning_rate": 0.00019654491609081933,
      "loss": 0.4924,
      "step": 104800
    },
    {
      "epoch": 0.345179335307667,
      "grad_norm": 0.19068235158920288,
      "learning_rate": 0.0001964461994076999,
      "loss": 0.3362,
      "step": 104900
    },
    {
      "epoch": 0.3455083909180652,
      "grad_norm": 68.59761810302734,
      "learning_rate": 0.00019634748272458042,
      "loss": 0.541,
      "step": 105000
    },
    {
      "epoch": 0.3458374465284633,
      "grad_norm": 8.506424903869629,
      "learning_rate": 0.000196248766041461,
      "loss": 0.7949,
      "step": 105100
    },
    {
      "epoch": 0.34616650213886146,
      "grad_norm": 0.2024705559015274,
      "learning_rate": 0.00019615004935834155,
      "loss": 0.3771,
      "step": 105200
    },
    {
      "epoch": 0.3464955577492596,
      "grad_norm": 0.0006083971238695085,
      "learning_rate": 0.00019605133267522207,
      "loss": 0.5871,
      "step": 105300
    },
    {
      "epoch": 0.3468246133596578,
      "grad_norm": 12.314093589782715,
      "learning_rate": 0.00019595261599210265,
      "loss": 0.5392,
      "step": 105400
    },
    {
      "epoch": 0.34715366897005595,
      "grad_norm": 0.005514339543879032,
      "learning_rate": 0.0001958538993089832,
      "loss": 0.4266,
      "step": 105500
    },
    {
      "epoch": 0.3474827245804541,
      "grad_norm": 0.001985375303775072,
      "learning_rate": 0.00019575518262586374,
      "loss": 0.4816,
      "step": 105600
    },
    {
      "epoch": 0.34781178019085224,
      "grad_norm": 0.6996004581451416,
      "learning_rate": 0.00019565646594274432,
      "loss": 0.637,
      "step": 105700
    },
    {
      "epoch": 0.34814083580125044,
      "grad_norm": 89.5511703491211,
      "learning_rate": 0.00019555774925962484,
      "loss": 0.2731,
      "step": 105800
    },
    {
      "epoch": 0.3484698914116486,
      "grad_norm": 0.0846465677022934,
      "learning_rate": 0.00019545903257650542,
      "loss": 0.6277,
      "step": 105900
    },
    {
      "epoch": 0.3487989470220467,
      "grad_norm": 0.01429731398820877,
      "learning_rate": 0.00019536031589338597,
      "loss": 0.358,
      "step": 106000
    },
    {
      "epoch": 0.34912800263244487,
      "grad_norm": 0.00031812372617423534,
      "learning_rate": 0.00019526159921026652,
      "loss": 0.395,
      "step": 106100
    },
    {
      "epoch": 0.34945705824284307,
      "grad_norm": 18.728439331054688,
      "learning_rate": 0.0001951628825271471,
      "loss": 0.4561,
      "step": 106200
    },
    {
      "epoch": 0.3497861138532412,
      "grad_norm": 0.005220524035394192,
      "learning_rate": 0.0001950641658440276,
      "loss": 0.3382,
      "step": 106300
    },
    {
      "epoch": 0.35011516946363935,
      "grad_norm": 125.72539520263672,
      "learning_rate": 0.00019496544916090816,
      "loss": 0.3391,
      "step": 106400
    },
    {
      "epoch": 0.3504442250740375,
      "grad_norm": 92.81832885742188,
      "learning_rate": 0.00019486673247778874,
      "loss": 0.5171,
      "step": 106500
    },
    {
      "epoch": 0.3507732806844357,
      "grad_norm": 10.507091522216797,
      "learning_rate": 0.00019476801579466929,
      "loss": 0.4467,
      "step": 106600
    },
    {
      "epoch": 0.35110233629483384,
      "grad_norm": 0.06412504613399506,
      "learning_rate": 0.00019466929911154983,
      "loss": 0.3477,
      "step": 106700
    },
    {
      "epoch": 0.351431391905232,
      "grad_norm": 0.0006463851314038038,
      "learning_rate": 0.00019457058242843038,
      "loss": 0.3492,
      "step": 106800
    },
    {
      "epoch": 0.3517604475156301,
      "grad_norm": 0.0014028593432158232,
      "learning_rate": 0.00019447186574531093,
      "loss": 0.4919,
      "step": 106900
    },
    {
      "epoch": 0.3520895031260283,
      "grad_norm": 0.016028279438614845,
      "learning_rate": 0.0001943731490621915,
      "loss": 0.5222,
      "step": 107000
    },
    {
      "epoch": 0.35241855873642647,
      "grad_norm": 0.005397400818765163,
      "learning_rate": 0.00019427443237907203,
      "loss": 0.4558,
      "step": 107100
    },
    {
      "epoch": 0.3527476143468246,
      "grad_norm": 4.750965118408203,
      "learning_rate": 0.00019417571569595258,
      "loss": 0.3708,
      "step": 107200
    },
    {
      "epoch": 0.35307666995722276,
      "grad_norm": 72.08436584472656,
      "learning_rate": 0.00019407699901283315,
      "loss": 0.6356,
      "step": 107300
    },
    {
      "epoch": 0.35340572556762095,
      "grad_norm": 0.18683438003063202,
      "learning_rate": 0.0001939782823297137,
      "loss": 0.532,
      "step": 107400
    },
    {
      "epoch": 0.3537347811780191,
      "grad_norm": 92.49107360839844,
      "learning_rate": 0.00019387956564659428,
      "loss": 0.2822,
      "step": 107500
    },
    {
      "epoch": 0.35406383678841724,
      "grad_norm": 0.40421876311302185,
      "learning_rate": 0.0001937808489634748,
      "loss": 0.5286,
      "step": 107600
    },
    {
      "epoch": 0.3543928923988154,
      "grad_norm": 18.28127098083496,
      "learning_rate": 0.00019368213228035535,
      "loss": 0.4937,
      "step": 107700
    },
    {
      "epoch": 0.3547219480092136,
      "grad_norm": 0.003927238751202822,
      "learning_rate": 0.00019358341559723593,
      "loss": 0.3005,
      "step": 107800
    },
    {
      "epoch": 0.3550510036196117,
      "grad_norm": 15.478598594665527,
      "learning_rate": 0.00019348469891411647,
      "loss": 0.5773,
      "step": 107900
    },
    {
      "epoch": 0.35538005923000987,
      "grad_norm": 1.8275145292282104,
      "learning_rate": 0.000193385982230997,
      "loss": 0.4319,
      "step": 108000
    },
    {
      "epoch": 0.355709114840408,
      "grad_norm": 0.004794261418282986,
      "learning_rate": 0.00019328726554787757,
      "loss": 0.6515,
      "step": 108100
    },
    {
      "epoch": 0.3560381704508062,
      "grad_norm": 0.0008977841353043914,
      "learning_rate": 0.00019318854886475812,
      "loss": 0.5313,
      "step": 108200
    },
    {
      "epoch": 0.35636722606120436,
      "grad_norm": 0.0015021699946373701,
      "learning_rate": 0.0001930898321816387,
      "loss": 0.3096,
      "step": 108300
    },
    {
      "epoch": 0.3566962816716025,
      "grad_norm": 0.01566164195537567,
      "learning_rate": 0.00019299111549851925,
      "loss": 0.3902,
      "step": 108400
    },
    {
      "epoch": 0.35702533728200064,
      "grad_norm": 0.0003274418704677373,
      "learning_rate": 0.00019289239881539977,
      "loss": 0.3753,
      "step": 108500
    },
    {
      "epoch": 0.35735439289239884,
      "grad_norm": 1.719534158706665,
      "learning_rate": 0.00019279368213228034,
      "loss": 0.4143,
      "step": 108600
    },
    {
      "epoch": 0.357683448502797,
      "grad_norm": 0.9751313924789429,
      "learning_rate": 0.0001926949654491609,
      "loss": 0.7087,
      "step": 108700
    },
    {
      "epoch": 0.35801250411319513,
      "grad_norm": 0.13425719738006592,
      "learning_rate": 0.00019259624876604144,
      "loss": 0.5417,
      "step": 108800
    },
    {
      "epoch": 0.3583415597235933,
      "grad_norm": 0.0008100788109004498,
      "learning_rate": 0.000192497532082922,
      "loss": 0.6004,
      "step": 108900
    },
    {
      "epoch": 0.35867061533399147,
      "grad_norm": 0.0009706580312922597,
      "learning_rate": 0.00019239881539980254,
      "loss": 0.3837,
      "step": 109000
    },
    {
      "epoch": 0.3589996709443896,
      "grad_norm": 0.01273546926677227,
      "learning_rate": 0.00019230009871668311,
      "loss": 0.376,
      "step": 109100
    },
    {
      "epoch": 0.35932872655478776,
      "grad_norm": 0.009590758942067623,
      "learning_rate": 0.00019220138203356366,
      "loss": 0.5265,
      "step": 109200
    },
    {
      "epoch": 0.3596577821651859,
      "grad_norm": 0.0832555890083313,
      "learning_rate": 0.00019210266535044419,
      "loss": 0.5283,
      "step": 109300
    },
    {
      "epoch": 0.3599868377755841,
      "grad_norm": 0.00024135208514053375,
      "learning_rate": 0.00019200394866732476,
      "loss": 0.461,
      "step": 109400
    },
    {
      "epoch": 0.36031589338598224,
      "grad_norm": 0.0035348739475011826,
      "learning_rate": 0.0001919052319842053,
      "loss": 0.3231,
      "step": 109500
    },
    {
      "epoch": 0.3606449489963804,
      "grad_norm": 0.09338916093111038,
      "learning_rate": 0.00019180651530108586,
      "loss": 0.3594,
      "step": 109600
    },
    {
      "epoch": 0.36097400460677853,
      "grad_norm": 29.78997230529785,
      "learning_rate": 0.00019170779861796643,
      "loss": 0.1536,
      "step": 109700
    },
    {
      "epoch": 0.36130306021717673,
      "grad_norm": 0.0028398267459124327,
      "learning_rate": 0.00019160908193484696,
      "loss": 0.5275,
      "step": 109800
    },
    {
      "epoch": 0.3616321158275749,
      "grad_norm": 27.420278549194336,
      "learning_rate": 0.00019151036525172753,
      "loss": 0.7022,
      "step": 109900
    },
    {
      "epoch": 0.361961171437973,
      "grad_norm": 11.935199737548828,
      "learning_rate": 0.00019141164856860808,
      "loss": 0.5481,
      "step": 110000
    },
    {
      "epoch": 0.36229022704837116,
      "grad_norm": 0.8505810499191284,
      "learning_rate": 0.00019131293188548863,
      "loss": 0.4304,
      "step": 110100
    },
    {
      "epoch": 0.36261928265876936,
      "grad_norm": 1.0225340127944946,
      "learning_rate": 0.00019121421520236918,
      "loss": 0.5366,
      "step": 110200
    },
    {
      "epoch": 0.3629483382691675,
      "grad_norm": 0.0008048995514400303,
      "learning_rate": 0.00019111549851924973,
      "loss": 0.6012,
      "step": 110300
    },
    {
      "epoch": 0.36327739387956565,
      "grad_norm": 0.0014250223757699132,
      "learning_rate": 0.00019101678183613028,
      "loss": 0.4923,
      "step": 110400
    },
    {
      "epoch": 0.3636064494899638,
      "grad_norm": 0.1734772026538849,
      "learning_rate": 0.00019091806515301085,
      "loss": 0.3553,
      "step": 110500
    },
    {
      "epoch": 0.36393550510036193,
      "grad_norm": 0.019699303433299065,
      "learning_rate": 0.00019081934846989137,
      "loss": 0.4073,
      "step": 110600
    },
    {
      "epoch": 0.36426456071076013,
      "grad_norm": 22.524831771850586,
      "learning_rate": 0.00019072063178677195,
      "loss": 0.4439,
      "step": 110700
    },
    {
      "epoch": 0.3645936163211583,
      "grad_norm": 7.412065029144287,
      "learning_rate": 0.0001906219151036525,
      "loss": 0.3655,
      "step": 110800
    },
    {
      "epoch": 0.3649226719315564,
      "grad_norm": 30.83083724975586,
      "learning_rate": 0.00019052319842053305,
      "loss": 0.6536,
      "step": 110900
    },
    {
      "epoch": 0.36525172754195456,
      "grad_norm": 0.14914055168628693,
      "learning_rate": 0.00019042448173741362,
      "loss": 0.5301,
      "step": 111000
    },
    {
      "epoch": 0.36558078315235276,
      "grad_norm": 1.7247898578643799,
      "learning_rate": 0.00019032576505429415,
      "loss": 0.5649,
      "step": 111100
    },
    {
      "epoch": 0.3659098387627509,
      "grad_norm": 0.0014749475521966815,
      "learning_rate": 0.00019022704837117472,
      "loss": 0.4336,
      "step": 111200
    },
    {
      "epoch": 0.36623889437314905,
      "grad_norm": 0.0036058053374290466,
      "learning_rate": 0.00019012833168805527,
      "loss": 0.3327,
      "step": 111300
    },
    {
      "epoch": 0.3665679499835472,
      "grad_norm": 0.8973181247711182,
      "learning_rate": 0.00019002961500493582,
      "loss": 0.3274,
      "step": 111400
    },
    {
      "epoch": 0.3668970055939454,
      "grad_norm": 5.815202713012695,
      "learning_rate": 0.0001899308983218164,
      "loss": 0.4406,
      "step": 111500
    },
    {
      "epoch": 0.36722606120434353,
      "grad_norm": 0.0014057952212169766,
      "learning_rate": 0.00018983218163869692,
      "loss": 0.3889,
      "step": 111600
    },
    {
      "epoch": 0.3675551168147417,
      "grad_norm": 13.900047302246094,
      "learning_rate": 0.00018973346495557747,
      "loss": 0.4412,
      "step": 111700
    },
    {
      "epoch": 0.3678841724251398,
      "grad_norm": 20.457754135131836,
      "learning_rate": 0.00018963474827245804,
      "loss": 0.4674,
      "step": 111800
    },
    {
      "epoch": 0.368213228035538,
      "grad_norm": 1.0771352052688599,
      "learning_rate": 0.0001895360315893386,
      "loss": 0.7669,
      "step": 111900
    },
    {
      "epoch": 0.36854228364593616,
      "grad_norm": 0.02306484617292881,
      "learning_rate": 0.00018943731490621914,
      "loss": 0.4613,
      "step": 112000
    },
    {
      "epoch": 0.3688713392563343,
      "grad_norm": 24.10323715209961,
      "learning_rate": 0.0001893385982230997,
      "loss": 0.5006,
      "step": 112100
    },
    {
      "epoch": 0.36920039486673245,
      "grad_norm": 0.0005951251951046288,
      "learning_rate": 0.00018923988153998024,
      "loss": 0.4661,
      "step": 112200
    },
    {
      "epoch": 0.36952945047713065,
      "grad_norm": 63.098323822021484,
      "learning_rate": 0.0001891411648568608,
      "loss": 0.4926,
      "step": 112300
    },
    {
      "epoch": 0.3698585060875288,
      "grad_norm": 0.1578739583492279,
      "learning_rate": 0.00018904244817374133,
      "loss": 0.451,
      "step": 112400
    },
    {
      "epoch": 0.37018756169792694,
      "grad_norm": 0.0205181036144495,
      "learning_rate": 0.00018894373149062188,
      "loss": 0.575,
      "step": 112500
    },
    {
      "epoch": 0.3705166173083251,
      "grad_norm": 0.009385291486978531,
      "learning_rate": 0.00018884501480750246,
      "loss": 0.551,
      "step": 112600
    },
    {
      "epoch": 0.3708456729187233,
      "grad_norm": 106.29227447509766,
      "learning_rate": 0.000188746298124383,
      "loss": 0.4519,
      "step": 112700
    },
    {
      "epoch": 0.3711747285291214,
      "grad_norm": 0.0021906497422605753,
      "learning_rate": 0.00018864758144126358,
      "loss": 0.8325,
      "step": 112800
    },
    {
      "epoch": 0.37150378413951957,
      "grad_norm": 0.0007029997068457305,
      "learning_rate": 0.0001885488647581441,
      "loss": 0.4249,
      "step": 112900
    },
    {
      "epoch": 0.3718328397499177,
      "grad_norm": 0.003777001518756151,
      "learning_rate": 0.00018845014807502465,
      "loss": 0.5656,
      "step": 113000
    },
    {
      "epoch": 0.3721618953603159,
      "grad_norm": 0.0012800338445231318,
      "learning_rate": 0.00018835143139190523,
      "loss": 0.4853,
      "step": 113100
    },
    {
      "epoch": 0.37249095097071405,
      "grad_norm": 2.957897424697876,
      "learning_rate": 0.00018825271470878578,
      "loss": 0.5855,
      "step": 113200
    },
    {
      "epoch": 0.3728200065811122,
      "grad_norm": 0.3371073305606842,
      "learning_rate": 0.0001881539980256663,
      "loss": 0.4381,
      "step": 113300
    },
    {
      "epoch": 0.37314906219151034,
      "grad_norm": 0.007008225657045841,
      "learning_rate": 0.00018805528134254688,
      "loss": 0.5092,
      "step": 113400
    },
    {
      "epoch": 0.37347811780190854,
      "grad_norm": 0.035571835935115814,
      "learning_rate": 0.00018795656465942743,
      "loss": 0.4299,
      "step": 113500
    },
    {
      "epoch": 0.3738071734123067,
      "grad_norm": 0.2561078369617462,
      "learning_rate": 0.000187857847976308,
      "loss": 0.3333,
      "step": 113600
    },
    {
      "epoch": 0.3741362290227048,
      "grad_norm": 0.2749519944190979,
      "learning_rate": 0.00018775913129318852,
      "loss": 0.312,
      "step": 113700
    },
    {
      "epoch": 0.37446528463310297,
      "grad_norm": 0.04737216234207153,
      "learning_rate": 0.00018766041461006907,
      "loss": 0.3938,
      "step": 113800
    },
    {
      "epoch": 0.37479434024350117,
      "grad_norm": 0.003549726912751794,
      "learning_rate": 0.00018756169792694965,
      "loss": 0.3914,
      "step": 113900
    },
    {
      "epoch": 0.3751233958538993,
      "grad_norm": 0.00873915571719408,
      "learning_rate": 0.0001874629812438302,
      "loss": 0.2679,
      "step": 114000
    },
    {
      "epoch": 0.37545245146429745,
      "grad_norm": 0.6498008966445923,
      "learning_rate": 0.00018736426456071072,
      "loss": 0.572,
      "step": 114100
    },
    {
      "epoch": 0.3757815070746956,
      "grad_norm": 0.0010582987451925874,
      "learning_rate": 0.0001872655478775913,
      "loss": 0.4911,
      "step": 114200
    },
    {
      "epoch": 0.3761105626850938,
      "grad_norm": 0.0014342482900246978,
      "learning_rate": 0.00018716683119447184,
      "loss": 0.5018,
      "step": 114300
    },
    {
      "epoch": 0.37643961829549194,
      "grad_norm": 0.00650798249989748,
      "learning_rate": 0.00018706811451135242,
      "loss": 0.5691,
      "step": 114400
    },
    {
      "epoch": 0.3767686739058901,
      "grad_norm": 0.0017500469693914056,
      "learning_rate": 0.00018696939782823297,
      "loss": 0.4667,
      "step": 114500
    },
    {
      "epoch": 0.3770977295162882,
      "grad_norm": 0.023219767957925797,
      "learning_rate": 0.0001868706811451135,
      "loss": 0.9214,
      "step": 114600
    },
    {
      "epoch": 0.3774267851266864,
      "grad_norm": 34.70756149291992,
      "learning_rate": 0.00018677196446199406,
      "loss": 0.4081,
      "step": 114700
    },
    {
      "epoch": 0.37775584073708457,
      "grad_norm": 0.002570142038166523,
      "learning_rate": 0.00018667324777887461,
      "loss": 0.4961,
      "step": 114800
    },
    {
      "epoch": 0.3780848963474827,
      "grad_norm": 0.0008767634280957282,
      "learning_rate": 0.00018657453109575516,
      "loss": 0.4075,
      "step": 114900
    },
    {
      "epoch": 0.37841395195788086,
      "grad_norm": 0.00045863536070100963,
      "learning_rate": 0.00018647581441263574,
      "loss": 0.2966,
      "step": 115000
    },
    {
      "epoch": 0.37874300756827906,
      "grad_norm": 13.567935943603516,
      "learning_rate": 0.00018637709772951626,
      "loss": 0.4188,
      "step": 115100
    },
    {
      "epoch": 0.3790720631786772,
      "grad_norm": 0.0029995350632816553,
      "learning_rate": 0.00018627838104639684,
      "loss": 0.3966,
      "step": 115200
    },
    {
      "epoch": 0.37940111878907534,
      "grad_norm": 23.563419342041016,
      "learning_rate": 0.00018617966436327738,
      "loss": 0.3997,
      "step": 115300
    },
    {
      "epoch": 0.3797301743994735,
      "grad_norm": 8.218055725097656,
      "learning_rate": 0.00018608094768015793,
      "loss": 0.4347,
      "step": 115400
    },
    {
      "epoch": 0.3800592300098717,
      "grad_norm": 0.00042145283077843487,
      "learning_rate": 0.00018598223099703848,
      "loss": 0.5989,
      "step": 115500
    },
    {
      "epoch": 0.38038828562026983,
      "grad_norm": 12.824333190917969,
      "learning_rate": 0.00018588351431391903,
      "loss": 0.5227,
      "step": 115600
    },
    {
      "epoch": 0.38071734123066797,
      "grad_norm": 0.02394692227244377,
      "learning_rate": 0.00018578479763079958,
      "loss": 0.7009,
      "step": 115700
    },
    {
      "epoch": 0.3810463968410661,
      "grad_norm": 0.007516931276768446,
      "learning_rate": 0.00018568608094768016,
      "loss": 0.4644,
      "step": 115800
    },
    {
      "epoch": 0.3813754524514643,
      "grad_norm": 0.8922999501228333,
      "learning_rate": 0.00018558736426456068,
      "loss": 0.4652,
      "step": 115900
    },
    {
      "epoch": 0.38170450806186246,
      "grad_norm": 0.001938291359692812,
      "learning_rate": 0.00018548864758144125,
      "loss": 0.3532,
      "step": 116000
    },
    {
      "epoch": 0.3820335636722606,
      "grad_norm": 0.02474384196102619,
      "learning_rate": 0.0001853899308983218,
      "loss": 0.4154,
      "step": 116100
    },
    {
      "epoch": 0.38236261928265874,
      "grad_norm": 54.68915939331055,
      "learning_rate": 0.00018529121421520235,
      "loss": 0.8389,
      "step": 116200
    },
    {
      "epoch": 0.38269167489305694,
      "grad_norm": 8.536160469055176,
      "learning_rate": 0.00018519249753208293,
      "loss": 0.3962,
      "step": 116300
    },
    {
      "epoch": 0.3830207305034551,
      "grad_norm": 1.0768219232559204,
      "learning_rate": 0.00018509378084896345,
      "loss": 0.3792,
      "step": 116400
    },
    {
      "epoch": 0.38334978611385323,
      "grad_norm": 0.027230141684412956,
      "learning_rate": 0.000184995064165844,
      "loss": 0.3294,
      "step": 116500
    },
    {
      "epoch": 0.3836788417242514,
      "grad_norm": 93.34349060058594,
      "learning_rate": 0.00018489634748272457,
      "loss": 0.5804,
      "step": 116600
    },
    {
      "epoch": 0.3840078973346496,
      "grad_norm": 0.0009461053414270282,
      "learning_rate": 0.00018479763079960512,
      "loss": 0.5055,
      "step": 116700
    },
    {
      "epoch": 0.3843369529450477,
      "grad_norm": 0.09819014370441437,
      "learning_rate": 0.0001846989141164857,
      "loss": 0.3229,
      "step": 116800
    },
    {
      "epoch": 0.38466600855544586,
      "grad_norm": 2.3560855388641357,
      "learning_rate": 0.00018460019743336622,
      "loss": 0.4569,
      "step": 116900
    },
    {
      "epoch": 0.384995064165844,
      "grad_norm": 0.6526496410369873,
      "learning_rate": 0.00018450148075024677,
      "loss": 0.5511,
      "step": 117000
    },
    {
      "epoch": 0.3853241197762422,
      "grad_norm": 0.008358011953532696,
      "learning_rate": 0.00018440276406712734,
      "loss": 0.575,
      "step": 117100
    },
    {
      "epoch": 0.38565317538664035,
      "grad_norm": 1.5876165628433228,
      "learning_rate": 0.0001843040473840079,
      "loss": 0.3946,
      "step": 117200
    },
    {
      "epoch": 0.3859822309970385,
      "grad_norm": 0.006348178721964359,
      "learning_rate": 0.00018420533070088842,
      "loss": 0.3987,
      "step": 117300
    },
    {
      "epoch": 0.38631128660743663,
      "grad_norm": 43.32343673706055,
      "learning_rate": 0.000184106614017769,
      "loss": 0.3534,
      "step": 117400
    },
    {
      "epoch": 0.38664034221783483,
      "grad_norm": 12.569610595703125,
      "learning_rate": 0.00018400789733464954,
      "loss": 0.3784,
      "step": 117500
    },
    {
      "epoch": 0.386969397828233,
      "grad_norm": 0.013955704867839813,
      "learning_rate": 0.00018390918065153012,
      "loss": 0.6037,
      "step": 117600
    },
    {
      "epoch": 0.3872984534386311,
      "grad_norm": 59.38835525512695,
      "learning_rate": 0.00018381046396841064,
      "loss": 0.5467,
      "step": 117700
    },
    {
      "epoch": 0.38762750904902926,
      "grad_norm": 1.4244861602783203,
      "learning_rate": 0.00018371174728529119,
      "loss": 0.4493,
      "step": 117800
    },
    {
      "epoch": 0.38795656465942746,
      "grad_norm": 0.003910049330443144,
      "learning_rate": 0.00018361303060217176,
      "loss": 0.4211,
      "step": 117900
    },
    {
      "epoch": 0.3882856202698256,
      "grad_norm": 0.020166264846920967,
      "learning_rate": 0.0001835143139190523,
      "loss": 0.6536,
      "step": 118000
    },
    {
      "epoch": 0.38861467588022375,
      "grad_norm": 2.4149906635284424,
      "learning_rate": 0.00018341559723593283,
      "loss": 0.3683,
      "step": 118100
    },
    {
      "epoch": 0.3889437314906219,
      "grad_norm": 11.543169975280762,
      "learning_rate": 0.0001833168805528134,
      "loss": 0.2845,
      "step": 118200
    },
    {
      "epoch": 0.3892727871010201,
      "grad_norm": 0.002538189524784684,
      "learning_rate": 0.00018321816386969396,
      "loss": 0.4666,
      "step": 118300
    },
    {
      "epoch": 0.38960184271141823,
      "grad_norm": 1.7891618013381958,
      "learning_rate": 0.00018311944718657453,
      "loss": 0.6028,
      "step": 118400
    },
    {
      "epoch": 0.3899308983218164,
      "grad_norm": 0.0008753195870667696,
      "learning_rate": 0.00018302073050345508,
      "loss": 0.3104,
      "step": 118500
    },
    {
      "epoch": 0.3902599539322145,
      "grad_norm": 117.27835845947266,
      "learning_rate": 0.0001829220138203356,
      "loss": 0.339,
      "step": 118600
    },
    {
      "epoch": 0.3905890095426127,
      "grad_norm": 0.0014283363707363605,
      "learning_rate": 0.00018282329713721618,
      "loss": 0.1983,
      "step": 118700
    },
    {
      "epoch": 0.39091806515301086,
      "grad_norm": 0.0003354330256115645,
      "learning_rate": 0.00018272458045409673,
      "loss": 0.3395,
      "step": 118800
    },
    {
      "epoch": 0.391247120763409,
      "grad_norm": 32.398529052734375,
      "learning_rate": 0.00018262586377097728,
      "loss": 0.5071,
      "step": 118900
    },
    {
      "epoch": 0.39157617637380715,
      "grad_norm": 0.0031535360030829906,
      "learning_rate": 0.00018252714708785783,
      "loss": 0.3651,
      "step": 119000
    },
    {
      "epoch": 0.39190523198420535,
      "grad_norm": 33.7746696472168,
      "learning_rate": 0.00018242843040473838,
      "loss": 0.5108,
      "step": 119100
    },
    {
      "epoch": 0.3922342875946035,
      "grad_norm": 0.0001627299061510712,
      "learning_rate": 0.00018232971372161895,
      "loss": 0.2541,
      "step": 119200
    },
    {
      "epoch": 0.39256334320500164,
      "grad_norm": 0.005195939913392067,
      "learning_rate": 0.0001822309970384995,
      "loss": 0.5168,
      "step": 119300
    },
    {
      "epoch": 0.3928923988153998,
      "grad_norm": 0.5604223608970642,
      "learning_rate": 0.00018213228035538002,
      "loss": 0.3923,
      "step": 119400
    },
    {
      "epoch": 0.393221454425798,
      "grad_norm": 38.621009826660156,
      "learning_rate": 0.0001820335636722606,
      "loss": 0.3972,
      "step": 119500
    },
    {
      "epoch": 0.3935505100361961,
      "grad_norm": 0.009072385728359222,
      "learning_rate": 0.00018193484698914115,
      "loss": 0.6091,
      "step": 119600
    },
    {
      "epoch": 0.39387956564659427,
      "grad_norm": 13.254104614257812,
      "learning_rate": 0.0001818361303060217,
      "loss": 0.5042,
      "step": 119700
    },
    {
      "epoch": 0.3942086212569924,
      "grad_norm": 0.00026041202363558114,
      "learning_rate": 0.00018173741362290227,
      "loss": 0.4306,
      "step": 119800
    },
    {
      "epoch": 0.3945376768673906,
      "grad_norm": 0.007022147066891193,
      "learning_rate": 0.0001816386969397828,
      "loss": 0.2471,
      "step": 119900
    },
    {
      "epoch": 0.39486673247778875,
      "grad_norm": 0.48098355531692505,
      "learning_rate": 0.00018153998025666337,
      "loss": 0.5303,
      "step": 120000
    },
    {
      "epoch": 0.3951957880881869,
      "grad_norm": 0.010447882115840912,
      "learning_rate": 0.00018144126357354392,
      "loss": 0.2953,
      "step": 120100
    },
    {
      "epoch": 0.39552484369858504,
      "grad_norm": 90.1509017944336,
      "learning_rate": 0.00018134254689042447,
      "loss": 0.3828,
      "step": 120200
    },
    {
      "epoch": 0.39585389930898324,
      "grad_norm": 0.003175565740093589,
      "learning_rate": 0.00018124383020730504,
      "loss": 0.4044,
      "step": 120300
    },
    {
      "epoch": 0.3961829549193814,
      "grad_norm": 0.13513235747814178,
      "learning_rate": 0.00018114511352418556,
      "loss": 0.2785,
      "step": 120400
    },
    {
      "epoch": 0.3965120105297795,
      "grad_norm": 77.2789306640625,
      "learning_rate": 0.0001810463968410661,
      "loss": 0.3906,
      "step": 120500
    },
    {
      "epoch": 0.39684106614017767,
      "grad_norm": 0.004456561524420977,
      "learning_rate": 0.0001809476801579467,
      "loss": 0.4543,
      "step": 120600
    },
    {
      "epoch": 0.39717012175057587,
      "grad_norm": 0.001384166651405394,
      "learning_rate": 0.00018084896347482724,
      "loss": 0.552,
      "step": 120700
    },
    {
      "epoch": 0.397499177360974,
      "grad_norm": 35.7476692199707,
      "learning_rate": 0.00018075024679170779,
      "loss": 0.502,
      "step": 120800
    },
    {
      "epoch": 0.39782823297137215,
      "grad_norm": 59.219940185546875,
      "learning_rate": 0.00018065153010858833,
      "loss": 0.4739,
      "step": 120900
    },
    {
      "epoch": 0.3981572885817703,
      "grad_norm": 12.616385459899902,
      "learning_rate": 0.00018055281342546888,
      "loss": 0.334,
      "step": 121000
    },
    {
      "epoch": 0.3984863441921685,
      "grad_norm": 0.006809168495237827,
      "learning_rate": 0.00018045409674234946,
      "loss": 0.5853,
      "step": 121100
    },
    {
      "epoch": 0.39881539980256664,
      "grad_norm": 31.361270904541016,
      "learning_rate": 0.00018035538005922998,
      "loss": 0.2824,
      "step": 121200
    },
    {
      "epoch": 0.3991444554129648,
      "grad_norm": 0.0037116827443242073,
      "learning_rate": 0.00018025666337611053,
      "loss": 0.3312,
      "step": 121300
    },
    {
      "epoch": 0.3994735110233629,
      "grad_norm": 32.915794372558594,
      "learning_rate": 0.0001801579466929911,
      "loss": 0.3196,
      "step": 121400
    },
    {
      "epoch": 0.3998025666337611,
      "grad_norm": 0.005943070165812969,
      "learning_rate": 0.00018005923000987165,
      "loss": 0.428,
      "step": 121500
    },
    {
      "epoch": 0.40013162224415927,
      "grad_norm": 0.05533874034881592,
      "learning_rate": 0.00017996051332675223,
      "loss": 0.4149,
      "step": 121600
    },
    {
      "epoch": 0.4004606778545574,
      "grad_norm": 0.0008526621968485415,
      "learning_rate": 0.00017986179664363275,
      "loss": 0.3138,
      "step": 121700
    },
    {
      "epoch": 0.40078973346495556,
      "grad_norm": 0.0061052218079566956,
      "learning_rate": 0.0001797630799605133,
      "loss": 0.342,
      "step": 121800
    },
    {
      "epoch": 0.40111878907535375,
      "grad_norm": 0.0012712458847090602,
      "learning_rate": 0.00017966436327739388,
      "loss": 0.4428,
      "step": 121900
    },
    {
      "epoch": 0.4014478446857519,
      "grad_norm": 0.040349069982767105,
      "learning_rate": 0.00017956564659427443,
      "loss": 0.5386,
      "step": 122000
    },
    {
      "epoch": 0.40177690029615004,
      "grad_norm": 0.0003460340667515993,
      "learning_rate": 0.00017946692991115495,
      "loss": 0.4905,
      "step": 122100
    },
    {
      "epoch": 0.4021059559065482,
      "grad_norm": 33.791019439697266,
      "learning_rate": 0.00017936821322803552,
      "loss": 0.3112,
      "step": 122200
    },
    {
      "epoch": 0.4024350115169464,
      "grad_norm": 0.00014312833081930876,
      "learning_rate": 0.00017926949654491607,
      "loss": 0.5617,
      "step": 122300
    },
    {
      "epoch": 0.4027640671273445,
      "grad_norm": 117.03516387939453,
      "learning_rate": 0.00017917077986179665,
      "loss": 0.3566,
      "step": 122400
    },
    {
      "epoch": 0.40309312273774267,
      "grad_norm": 0.0009899367578327656,
      "learning_rate": 0.00017907206317867717,
      "loss": 0.3879,
      "step": 122500
    },
    {
      "epoch": 0.4034221783481408,
      "grad_norm": 0.07387273013591766,
      "learning_rate": 0.00017897334649555772,
      "loss": 0.5375,
      "step": 122600
    },
    {
      "epoch": 0.403751233958539,
      "grad_norm": 0.003730973694473505,
      "learning_rate": 0.0001788746298124383,
      "loss": 0.3616,
      "step": 122700
    },
    {
      "epoch": 0.40408028956893716,
      "grad_norm": 0.0068080248311161995,
      "learning_rate": 0.00017877591312931884,
      "loss": 0.3587,
      "step": 122800
    },
    {
      "epoch": 0.4044093451793353,
      "grad_norm": 112.87165069580078,
      "learning_rate": 0.00017867719644619937,
      "loss": 0.5326,
      "step": 122900
    },
    {
      "epoch": 0.40473840078973344,
      "grad_norm": 0.0034354340750724077,
      "learning_rate": 0.00017857847976307994,
      "loss": 0.3353,
      "step": 123000
    },
    {
      "epoch": 0.40506745640013164,
      "grad_norm": 0.025407874956727028,
      "learning_rate": 0.0001784797630799605,
      "loss": 0.3638,
      "step": 123100
    },
    {
      "epoch": 0.4053965120105298,
      "grad_norm": 0.2681002914905548,
      "learning_rate": 0.00017838104639684107,
      "loss": 0.5525,
      "step": 123200
    },
    {
      "epoch": 0.40572556762092793,
      "grad_norm": 64.46965026855469,
      "learning_rate": 0.00017828232971372161,
      "loss": 0.3965,
      "step": 123300
    },
    {
      "epoch": 0.4060546232313261,
      "grad_norm": 0.0005120302666909993,
      "learning_rate": 0.00017818361303060214,
      "loss": 0.366,
      "step": 123400
    },
    {
      "epoch": 0.40638367884172427,
      "grad_norm": 27.40654754638672,
      "learning_rate": 0.0001780848963474827,
      "loss": 0.5332,
      "step": 123500
    },
    {
      "epoch": 0.4067127344521224,
      "grad_norm": 8.781760215759277,
      "learning_rate": 0.00017798617966436326,
      "loss": 0.2894,
      "step": 123600
    },
    {
      "epoch": 0.40704179006252056,
      "grad_norm": 0.0014538521645590663,
      "learning_rate": 0.0001778874629812438,
      "loss": 0.4626,
      "step": 123700
    },
    {
      "epoch": 0.4073708456729187,
      "grad_norm": 0.0006411577924154699,
      "learning_rate": 0.00017778874629812439,
      "loss": 0.4153,
      "step": 123800
    },
    {
      "epoch": 0.4076999012833169,
      "grad_norm": 0.0017601591534912586,
      "learning_rate": 0.0001776900296150049,
      "loss": 0.3231,
      "step": 123900
    },
    {
      "epoch": 0.40802895689371504,
      "grad_norm": 0.0009488786454312503,
      "learning_rate": 0.00017759131293188548,
      "loss": 0.356,
      "step": 124000
    },
    {
      "epoch": 0.4083580125041132,
      "grad_norm": 0.3884289562702179,
      "learning_rate": 0.00017749259624876603,
      "loss": 0.5279,
      "step": 124100
    },
    {
      "epoch": 0.40868706811451133,
      "grad_norm": 0.2691427767276764,
      "learning_rate": 0.00017739387956564658,
      "loss": 0.2935,
      "step": 124200
    },
    {
      "epoch": 0.40901612372490953,
      "grad_norm": 0.0036091944202780724,
      "learning_rate": 0.00017729516288252713,
      "loss": 0.5356,
      "step": 124300
    },
    {
      "epoch": 0.4093451793353077,
      "grad_norm": 0.09837900102138519,
      "learning_rate": 0.00017719644619940768,
      "loss": 0.3,
      "step": 124400
    },
    {
      "epoch": 0.4096742349457058,
      "grad_norm": 0.011206164956092834,
      "learning_rate": 0.00017709772951628823,
      "loss": 0.5292,
      "step": 124500
    },
    {
      "epoch": 0.41000329055610396,
      "grad_norm": 23.72881317138672,
      "learning_rate": 0.0001769990128331688,
      "loss": 0.647,
      "step": 124600
    },
    {
      "epoch": 0.41033234616650216,
      "grad_norm": 31.969261169433594,
      "learning_rate": 0.00017690029615004933,
      "loss": 0.5185,
      "step": 124700
    },
    {
      "epoch": 0.4106614017769003,
      "grad_norm": 0.0052431803196668625,
      "learning_rate": 0.0001768015794669299,
      "loss": 0.865,
      "step": 124800
    },
    {
      "epoch": 0.41099045738729845,
      "grad_norm": 0.006872874218970537,
      "learning_rate": 0.00017670286278381045,
      "loss": 0.392,
      "step": 124900
    },
    {
      "epoch": 0.4113195129976966,
      "grad_norm": 50.1006965637207,
      "learning_rate": 0.000176604146100691,
      "loss": 0.1921,
      "step": 125000
    },
    {
      "epoch": 0.4116485686080948,
      "grad_norm": 0.0009498193976469338,
      "learning_rate": 0.00017650542941757157,
      "loss": 0.3309,
      "step": 125100
    },
    {
      "epoch": 0.41197762421849293,
      "grad_norm": 6.824641227722168,
      "learning_rate": 0.0001764067127344521,
      "loss": 0.4524,
      "step": 125200
    },
    {
      "epoch": 0.4123066798288911,
      "grad_norm": 27.42283821105957,
      "learning_rate": 0.00017630799605133265,
      "loss": 0.3723,
      "step": 125300
    },
    {
      "epoch": 0.4126357354392892,
      "grad_norm": 1.331740140914917,
      "learning_rate": 0.00017620927936821322,
      "loss": 0.3027,
      "step": 125400
    },
    {
      "epoch": 0.4129647910496874,
      "grad_norm": 18.09503173828125,
      "learning_rate": 0.00017611056268509377,
      "loss": 0.3315,
      "step": 125500
    },
    {
      "epoch": 0.41329384666008556,
      "grad_norm": 0.01010842528194189,
      "learning_rate": 0.00017601184600197435,
      "loss": 0.4626,
      "step": 125600
    },
    {
      "epoch": 0.4136229022704837,
      "grad_norm": 0.003541446989402175,
      "learning_rate": 0.00017591312931885487,
      "loss": 0.3887,
      "step": 125700
    },
    {
      "epoch": 0.41395195788088185,
      "grad_norm": 0.001477880054153502,
      "learning_rate": 0.00017581441263573542,
      "loss": 0.3685,
      "step": 125800
    },
    {
      "epoch": 0.41428101349128005,
      "grad_norm": 0.004963495768606663,
      "learning_rate": 0.000175715695952616,
      "loss": 0.5543,
      "step": 125900
    },
    {
      "epoch": 0.4146100691016782,
      "grad_norm": 0.01500095333904028,
      "learning_rate": 0.00017561697926949651,
      "loss": 0.4727,
      "step": 126000
    },
    {
      "epoch": 0.41493912471207633,
      "grad_norm": 26.515493392944336,
      "learning_rate": 0.00017551826258637706,
      "loss": 0.6225,
      "step": 126100
    },
    {
      "epoch": 0.4152681803224745,
      "grad_norm": 0.21356907486915588,
      "learning_rate": 0.00017541954590325764,
      "loss": 0.4863,
      "step": 126200
    },
    {
      "epoch": 0.4155972359328727,
      "grad_norm": 0.001750395749695599,
      "learning_rate": 0.0001753208292201382,
      "loss": 0.367,
      "step": 126300
    },
    {
      "epoch": 0.4159262915432708,
      "grad_norm": 0.0028742204885929823,
      "learning_rate": 0.00017522211253701876,
      "loss": 0.3591,
      "step": 126400
    },
    {
      "epoch": 0.41625534715366896,
      "grad_norm": 0.013457180932164192,
      "learning_rate": 0.00017512339585389929,
      "loss": 0.4569,
      "step": 126500
    },
    {
      "epoch": 0.4165844027640671,
      "grad_norm": 53.12196350097656,
      "learning_rate": 0.00017502467917077983,
      "loss": 0.4764,
      "step": 126600
    },
    {
      "epoch": 0.4169134583744653,
      "grad_norm": 0.005722860340029001,
      "learning_rate": 0.0001749259624876604,
      "loss": 0.3998,
      "step": 126700
    },
    {
      "epoch": 0.41724251398486345,
      "grad_norm": 2.7041831016540527,
      "learning_rate": 0.00017482724580454096,
      "loss": 0.7445,
      "step": 126800
    },
    {
      "epoch": 0.4175715695952616,
      "grad_norm": 10.683917045593262,
      "learning_rate": 0.00017472852912142148,
      "loss": 0.441,
      "step": 126900
    },
    {
      "epoch": 0.41790062520565974,
      "grad_norm": 90.03853607177734,
      "learning_rate": 0.00017462981243830206,
      "loss": 0.4334,
      "step": 127000
    },
    {
      "epoch": 0.41822968081605794,
      "grad_norm": 0.0012636893661692739,
      "learning_rate": 0.0001745310957551826,
      "loss": 0.5547,
      "step": 127100
    },
    {
      "epoch": 0.4185587364264561,
      "grad_norm": 0.03580997511744499,
      "learning_rate": 0.00017443237907206318,
      "loss": 0.4528,
      "step": 127200
    },
    {
      "epoch": 0.4188877920368542,
      "grad_norm": 0.0019707861356437206,
      "learning_rate": 0.00017433366238894373,
      "loss": 0.3995,
      "step": 127300
    },
    {
      "epoch": 0.41921684764725237,
      "grad_norm": 0.01394472736865282,
      "learning_rate": 0.00017423494570582425,
      "loss": 0.4699,
      "step": 127400
    },
    {
      "epoch": 0.41954590325765057,
      "grad_norm": 0.007445711176842451,
      "learning_rate": 0.00017413622902270483,
      "loss": 0.4454,
      "step": 127500
    },
    {
      "epoch": 0.4198749588680487,
      "grad_norm": 0.0015411375788971782,
      "learning_rate": 0.00017403751233958538,
      "loss": 0.4797,
      "step": 127600
    },
    {
      "epoch": 0.42020401447844685,
      "grad_norm": 0.00010545094846747816,
      "learning_rate": 0.00017393879565646593,
      "loss": 0.5512,
      "step": 127700
    },
    {
      "epoch": 0.420533070088845,
      "grad_norm": 7.948595157358795e-05,
      "learning_rate": 0.00017384007897334647,
      "loss": 0.4532,
      "step": 127800
    },
    {
      "epoch": 0.4208621256992432,
      "grad_norm": 0.14588390290737152,
      "learning_rate": 0.00017374136229022702,
      "loss": 0.4622,
      "step": 127900
    },
    {
      "epoch": 0.42119118130964134,
      "grad_norm": 0.0014029734302312136,
      "learning_rate": 0.0001736426456071076,
      "loss": 0.4779,
      "step": 128000
    },
    {
      "epoch": 0.4215202369200395,
      "grad_norm": 0.00013170238526072353,
      "learning_rate": 0.00017354392892398815,
      "loss": 0.2859,
      "step": 128100
    },
    {
      "epoch": 0.4218492925304376,
      "grad_norm": 74.70210266113281,
      "learning_rate": 0.00017344521224086867,
      "loss": 0.3411,
      "step": 128200
    },
    {
      "epoch": 0.4221783481408358,
      "grad_norm": 0.0026523880660533905,
      "learning_rate": 0.00017334649555774924,
      "loss": 0.2375,
      "step": 128300
    },
    {
      "epoch": 0.42250740375123397,
      "grad_norm": 0.4804862439632416,
      "learning_rate": 0.0001732477788746298,
      "loss": 0.3745,
      "step": 128400
    },
    {
      "epoch": 0.4228364593616321,
      "grad_norm": 0.0007724951137788594,
      "learning_rate": 0.00017314906219151034,
      "loss": 0.383,
      "step": 128500
    },
    {
      "epoch": 0.42316551497203025,
      "grad_norm": 42.58744812011719,
      "learning_rate": 0.00017305034550839092,
      "loss": 0.4457,
      "step": 128600
    },
    {
      "epoch": 0.42349457058242845,
      "grad_norm": 0.012415649369359016,
      "learning_rate": 0.00017295162882527144,
      "loss": 0.3532,
      "step": 128700
    },
    {
      "epoch": 0.4238236261928266,
      "grad_norm": 0.002068514935672283,
      "learning_rate": 0.00017285291214215202,
      "loss": 0.2796,
      "step": 128800
    },
    {
      "epoch": 0.42415268180322474,
      "grad_norm": 0.0021479972638189793,
      "learning_rate": 0.00017275419545903256,
      "loss": 0.43,
      "step": 128900
    },
    {
      "epoch": 0.4244817374136229,
      "grad_norm": 0.016676349565386772,
      "learning_rate": 0.00017265547877591311,
      "loss": 0.3814,
      "step": 129000
    },
    {
      "epoch": 0.4248107930240211,
      "grad_norm": 49.82918167114258,
      "learning_rate": 0.0001725567620927937,
      "loss": 0.6048,
      "step": 129100
    },
    {
      "epoch": 0.4251398486344192,
      "grad_norm": 0.034168705344200134,
      "learning_rate": 0.0001724580454096742,
      "loss": 0.3069,
      "step": 129200
    },
    {
      "epoch": 0.42546890424481737,
      "grad_norm": 6.234167813090608e-05,
      "learning_rate": 0.00017235932872655476,
      "loss": 0.3849,
      "step": 129300
    },
    {
      "epoch": 0.4257979598552155,
      "grad_norm": 0.04395768791437149,
      "learning_rate": 0.00017226061204343534,
      "loss": 0.164,
      "step": 129400
    },
    {
      "epoch": 0.4261270154656137,
      "grad_norm": 0.00836766418069601,
      "learning_rate": 0.00017216189536031586,
      "loss": 0.3332,
      "step": 129500
    },
    {
      "epoch": 0.42645607107601186,
      "grad_norm": 0.6878737211227417,
      "learning_rate": 0.00017206317867719643,
      "loss": 0.4298,
      "step": 129600
    },
    {
      "epoch": 0.42678512668641,
      "grad_norm": 0.19440516829490662,
      "learning_rate": 0.00017196446199407698,
      "loss": 0.1963,
      "step": 129700
    },
    {
      "epoch": 0.42711418229680814,
      "grad_norm": 0.0004608113958965987,
      "learning_rate": 0.00017186574531095753,
      "loss": 0.3964,
      "step": 129800
    },
    {
      "epoch": 0.42744323790720634,
      "grad_norm": 0.0007569354493170977,
      "learning_rate": 0.0001717670286278381,
      "loss": 0.6009,
      "step": 129900
    },
    {
      "epoch": 0.4277722935176045,
      "grad_norm": 0.0017648214707151055,
      "learning_rate": 0.00017166831194471863,
      "loss": 0.4018,
      "step": 130000
    },
    {
      "epoch": 0.42810134912800263,
      "grad_norm": 0.003301860298961401,
      "learning_rate": 0.00017156959526159918,
      "loss": 0.3219,
      "step": 130100
    },
    {
      "epoch": 0.42843040473840077,
      "grad_norm": 3.7374794483184814,
      "learning_rate": 0.00017147087857847975,
      "loss": 0.3752,
      "step": 130200
    },
    {
      "epoch": 0.42875946034879897,
      "grad_norm": 0.002397306729108095,
      "learning_rate": 0.0001713721618953603,
      "loss": 0.2682,
      "step": 130300
    },
    {
      "epoch": 0.4290885159591971,
      "grad_norm": 61.8807258605957,
      "learning_rate": 0.00017127344521224088,
      "loss": 0.3144,
      "step": 130400
    },
    {
      "epoch": 0.42941757156959526,
      "grad_norm": 2.628133773803711,
      "learning_rate": 0.0001711747285291214,
      "loss": 0.4298,
      "step": 130500
    },
    {
      "epoch": 0.4297466271799934,
      "grad_norm": 0.0059962491504848,
      "learning_rate": 0.00017107601184600195,
      "loss": 0.3906,
      "step": 130600
    },
    {
      "epoch": 0.4300756827903916,
      "grad_norm": 45.85319137573242,
      "learning_rate": 0.00017097729516288252,
      "loss": 0.3379,
      "step": 130700
    },
    {
      "epoch": 0.43040473840078974,
      "grad_norm": 21.54109001159668,
      "learning_rate": 0.00017087857847976307,
      "loss": 0.4919,
      "step": 130800
    },
    {
      "epoch": 0.4307337940111879,
      "grad_norm": 74.61095428466797,
      "learning_rate": 0.0001707798617966436,
      "loss": 0.3732,
      "step": 130900
    },
    {
      "epoch": 0.43106284962158603,
      "grad_norm": 0.051036536693573,
      "learning_rate": 0.00017068114511352417,
      "loss": 0.4885,
      "step": 131000
    },
    {
      "epoch": 0.43139190523198423,
      "grad_norm": 0.00927018653601408,
      "learning_rate": 0.00017058242843040472,
      "loss": 0.5472,
      "step": 131100
    },
    {
      "epoch": 0.4317209608423824,
      "grad_norm": 0.0009191292338073254,
      "learning_rate": 0.0001704837117472853,
      "loss": 0.4485,
      "step": 131200
    },
    {
      "epoch": 0.4320500164527805,
      "grad_norm": 11.544031143188477,
      "learning_rate": 0.00017038499506416582,
      "loss": 0.3522,
      "step": 131300
    },
    {
      "epoch": 0.43237907206317866,
      "grad_norm": 0.0008273971034213901,
      "learning_rate": 0.00017028627838104637,
      "loss": 0.4467,
      "step": 131400
    },
    {
      "epoch": 0.43270812767357686,
      "grad_norm": 0.16376136243343353,
      "learning_rate": 0.00017018756169792694,
      "loss": 0.3172,
      "step": 131500
    },
    {
      "epoch": 0.433037183283975,
      "grad_norm": 0.0088034113869071,
      "learning_rate": 0.0001700888450148075,
      "loss": 0.4092,
      "step": 131600
    },
    {
      "epoch": 0.43336623889437315,
      "grad_norm": 120.46595764160156,
      "learning_rate": 0.000169990128331688,
      "loss": 0.4132,
      "step": 131700
    },
    {
      "epoch": 0.4336952945047713,
      "grad_norm": 0.0008222867036238313,
      "learning_rate": 0.0001698914116485686,
      "loss": 0.3643,
      "step": 131800
    },
    {
      "epoch": 0.4340243501151695,
      "grad_norm": 0.023870915174484253,
      "learning_rate": 0.00016979269496544914,
      "loss": 0.4515,
      "step": 131900
    },
    {
      "epoch": 0.43435340572556763,
      "grad_norm": 0.0009508182993158698,
      "learning_rate": 0.0001696939782823297,
      "loss": 0.3496,
      "step": 132000
    },
    {
      "epoch": 0.4346824613359658,
      "grad_norm": 0.007801539730280638,
      "learning_rate": 0.00016959526159921026,
      "loss": 0.3599,
      "step": 132100
    },
    {
      "epoch": 0.4350115169463639,
      "grad_norm": 0.000441415817476809,
      "learning_rate": 0.00016949654491609078,
      "loss": 0.6203,
      "step": 132200
    },
    {
      "epoch": 0.4353405725567621,
      "grad_norm": 0.1171034574508667,
      "learning_rate": 0.00016939782823297136,
      "loss": 0.5413,
      "step": 132300
    },
    {
      "epoch": 0.43566962816716026,
      "grad_norm": 0.08958715945482254,
      "learning_rate": 0.0001692991115498519,
      "loss": 0.5516,
      "step": 132400
    },
    {
      "epoch": 0.4359986837775584,
      "grad_norm": 0.07189135998487473,
      "learning_rate": 0.00016920039486673246,
      "loss": 0.29,
      "step": 132500
    },
    {
      "epoch": 0.43632773938795655,
      "grad_norm": 0.00032446178374812007,
      "learning_rate": 0.00016910167818361303,
      "loss": 0.4945,
      "step": 132600
    },
    {
      "epoch": 0.43665679499835475,
      "grad_norm": 0.011939029209315777,
      "learning_rate": 0.00016900296150049356,
      "loss": 0.3525,
      "step": 132700
    },
    {
      "epoch": 0.4369858506087529,
      "grad_norm": 0.0035298506263643503,
      "learning_rate": 0.00016890424481737413,
      "loss": 0.2995,
      "step": 132800
    },
    {
      "epoch": 0.43731490621915103,
      "grad_norm": 0.012781086377799511,
      "learning_rate": 0.00016880552813425468,
      "loss": 0.452,
      "step": 132900
    },
    {
      "epoch": 0.4376439618295492,
      "grad_norm": 0.07130392640829086,
      "learning_rate": 0.00016870681145113523,
      "loss": 0.3118,
      "step": 133000
    },
    {
      "epoch": 0.4379730174399474,
      "grad_norm": 55.198299407958984,
      "learning_rate": 0.00016860809476801578,
      "loss": 0.5943,
      "step": 133100
    },
    {
      "epoch": 0.4383020730503455,
      "grad_norm": 17.375198364257812,
      "learning_rate": 0.00016850937808489633,
      "loss": 0.4157,
      "step": 133200
    },
    {
      "epoch": 0.43863112866074366,
      "grad_norm": 0.014103434979915619,
      "learning_rate": 0.00016841066140177688,
      "loss": 0.4805,
      "step": 133300
    },
    {
      "epoch": 0.4389601842711418,
      "grad_norm": 0.14474308490753174,
      "learning_rate": 0.00016831194471865745,
      "loss": 0.5683,
      "step": 133400
    },
    {
      "epoch": 0.43928923988154,
      "grad_norm": 0.15553218126296997,
      "learning_rate": 0.00016821322803553797,
      "loss": 0.5431,
      "step": 133500
    },
    {
      "epoch": 0.43961829549193815,
      "grad_norm": 0.06779134273529053,
      "learning_rate": 0.00016811451135241855,
      "loss": 0.3819,
      "step": 133600
    },
    {
      "epoch": 0.4399473511023363,
      "grad_norm": 44.17552185058594,
      "learning_rate": 0.0001680157946692991,
      "loss": 0.4964,
      "step": 133700
    },
    {
      "epoch": 0.44027640671273444,
      "grad_norm": 0.01480557955801487,
      "learning_rate": 0.00016791707798617965,
      "loss": 0.4604,
      "step": 133800
    },
    {
      "epoch": 0.44060546232313264,
      "grad_norm": 0.0007697154069319367,
      "learning_rate": 0.00016781836130306022,
      "loss": 0.4355,
      "step": 133900
    },
    {
      "epoch": 0.4409345179335308,
      "grad_norm": 0.04187369346618652,
      "learning_rate": 0.00016771964461994074,
      "loss": 0.3286,
      "step": 134000
    },
    {
      "epoch": 0.4412635735439289,
      "grad_norm": 0.007851669564843178,
      "learning_rate": 0.0001676209279368213,
      "loss": 0.4505,
      "step": 134100
    },
    {
      "epoch": 0.44159262915432707,
      "grad_norm": 0.04949008300900459,
      "learning_rate": 0.00016752221125370187,
      "loss": 0.5217,
      "step": 134200
    },
    {
      "epoch": 0.44192168476472526,
      "grad_norm": 51.844852447509766,
      "learning_rate": 0.00016742349457058242,
      "loss": 0.4557,
      "step": 134300
    },
    {
      "epoch": 0.4422507403751234,
      "grad_norm": 0.007875774055719376,
      "learning_rate": 0.00016732477788746297,
      "loss": 0.2665,
      "step": 134400
    },
    {
      "epoch": 0.44257979598552155,
      "grad_norm": 0.002048343187198043,
      "learning_rate": 0.00016722606120434352,
      "loss": 0.3255,
      "step": 134500
    },
    {
      "epoch": 0.4429088515959197,
      "grad_norm": 0.01183608639985323,
      "learning_rate": 0.00016712734452122406,
      "loss": 0.1245,
      "step": 134600
    },
    {
      "epoch": 0.4432379072063179,
      "grad_norm": 0.0006767353625036776,
      "learning_rate": 0.00016702862783810464,
      "loss": 0.3465,
      "step": 134700
    },
    {
      "epoch": 0.44356696281671604,
      "grad_norm": 0.8124033212661743,
      "learning_rate": 0.00016692991115498516,
      "loss": 0.4461,
      "step": 134800
    },
    {
      "epoch": 0.4438960184271142,
      "grad_norm": 0.012416703626513481,
      "learning_rate": 0.0001668311944718657,
      "loss": 0.3653,
      "step": 134900
    },
    {
      "epoch": 0.4442250740375123,
      "grad_norm": 0.013509776443243027,
      "learning_rate": 0.00016673247778874629,
      "loss": 0.3923,
      "step": 135000
    },
    {
      "epoch": 0.4445541296479105,
      "grad_norm": 0.00018826447194442153,
      "learning_rate": 0.00016663376110562684,
      "loss": 0.1963,
      "step": 135100
    },
    {
      "epoch": 0.44488318525830867,
      "grad_norm": 0.005682181101292372,
      "learning_rate": 0.0001665350444225074,
      "loss": 0.4797,
      "step": 135200
    },
    {
      "epoch": 0.4452122408687068,
      "grad_norm": 0.0013776007108390331,
      "learning_rate": 0.00016643632773938793,
      "loss": 0.314,
      "step": 135300
    },
    {
      "epoch": 0.44554129647910495,
      "grad_norm": 0.0005974576924927533,
      "learning_rate": 0.00016633761105626848,
      "loss": 0.3649,
      "step": 135400
    },
    {
      "epoch": 0.44587035208950315,
      "grad_norm": 0.000249422068009153,
      "learning_rate": 0.00016623889437314906,
      "loss": 0.2572,
      "step": 135500
    },
    {
      "epoch": 0.4461994076999013,
      "grad_norm": 0.34812912344932556,
      "learning_rate": 0.0001661401776900296,
      "loss": 0.4874,
      "step": 135600
    },
    {
      "epoch": 0.44652846331029944,
      "grad_norm": 104.86206817626953,
      "learning_rate": 0.00016604146100691013,
      "loss": 0.1962,
      "step": 135700
    },
    {
      "epoch": 0.4468575189206976,
      "grad_norm": 0.10751090943813324,
      "learning_rate": 0.0001659427443237907,
      "loss": 0.3111,
      "step": 135800
    },
    {
      "epoch": 0.4471865745310958,
      "grad_norm": 0.0018611688865348697,
      "learning_rate": 0.00016584402764067125,
      "loss": 0.3851,
      "step": 135900
    },
    {
      "epoch": 0.4475156301414939,
      "grad_norm": 0.4561006724834442,
      "learning_rate": 0.00016574531095755183,
      "loss": 0.4214,
      "step": 136000
    },
    {
      "epoch": 0.44784468575189207,
      "grad_norm": 0.0033099809661507607,
      "learning_rate": 0.00016564659427443238,
      "loss": 0.2951,
      "step": 136100
    },
    {
      "epoch": 0.4481737413622902,
      "grad_norm": 329.8948669433594,
      "learning_rate": 0.0001655478775913129,
      "loss": 0.3373,
      "step": 136200
    },
    {
      "epoch": 0.4485027969726884,
      "grad_norm": 0.0007688240730203688,
      "learning_rate": 0.00016544916090819347,
      "loss": 0.3626,
      "step": 136300
    },
    {
      "epoch": 0.44883185258308655,
      "grad_norm": 0.0031104297377169132,
      "learning_rate": 0.00016535044422507402,
      "loss": 0.3588,
      "step": 136400
    },
    {
      "epoch": 0.4491609081934847,
      "grad_norm": 0.000701415236108005,
      "learning_rate": 0.00016525172754195457,
      "loss": 0.5019,
      "step": 136500
    },
    {
      "epoch": 0.44948996380388284,
      "grad_norm": 51.39155197143555,
      "learning_rate": 0.00016515301085883512,
      "loss": 0.4941,
      "step": 136600
    },
    {
      "epoch": 0.44981901941428104,
      "grad_norm": 0.0005023189587518573,
      "learning_rate": 0.00016505429417571567,
      "loss": 0.2688,
      "step": 136700
    },
    {
      "epoch": 0.4501480750246792,
      "grad_norm": 0.014743294566869736,
      "learning_rate": 0.00016495557749259625,
      "loss": 0.4057,
      "step": 136800
    },
    {
      "epoch": 0.4504771306350773,
      "grad_norm": 0.01214805617928505,
      "learning_rate": 0.0001648568608094768,
      "loss": 0.5403,
      "step": 136900
    },
    {
      "epoch": 0.45080618624547547,
      "grad_norm": 0.0469280444085598,
      "learning_rate": 0.00016475814412635732,
      "loss": 0.3952,
      "step": 137000
    },
    {
      "epoch": 0.45113524185587367,
      "grad_norm": 0.0016981775406748056,
      "learning_rate": 0.0001646594274432379,
      "loss": 0.1801,
      "step": 137100
    },
    {
      "epoch": 0.4514642974662718,
      "grad_norm": 96.74188995361328,
      "learning_rate": 0.00016456071076011844,
      "loss": 0.2507,
      "step": 137200
    },
    {
      "epoch": 0.45179335307666996,
      "grad_norm": 51.224491119384766,
      "learning_rate": 0.000164461994076999,
      "loss": 0.2827,
      "step": 137300
    },
    {
      "epoch": 0.4521224086870681,
      "grad_norm": 39.86111831665039,
      "learning_rate": 0.00016436327739387957,
      "loss": 0.3135,
      "step": 137400
    },
    {
      "epoch": 0.4524514642974663,
      "grad_norm": 0.0007466870010830462,
      "learning_rate": 0.0001642645607107601,
      "loss": 0.2011,
      "step": 137500
    },
    {
      "epoch": 0.45278051990786444,
      "grad_norm": 84.54378509521484,
      "learning_rate": 0.00016416584402764066,
      "loss": 0.3164,
      "step": 137600
    },
    {
      "epoch": 0.4531095755182626,
      "grad_norm": 0.0052158948965370655,
      "learning_rate": 0.0001640671273445212,
      "loss": 0.2285,
      "step": 137700
    },
    {
      "epoch": 0.45343863112866073,
      "grad_norm": 35.023067474365234,
      "learning_rate": 0.00016396841066140176,
      "loss": 0.4744,
      "step": 137800
    },
    {
      "epoch": 0.45376768673905893,
      "grad_norm": 112.6611099243164,
      "learning_rate": 0.00016386969397828234,
      "loss": 0.5553,
      "step": 137900
    },
    {
      "epoch": 0.45409674234945707,
      "grad_norm": 0.005711737088859081,
      "learning_rate": 0.00016377097729516286,
      "loss": 0.4617,
      "step": 138000
    },
    {
      "epoch": 0.4544257979598552,
      "grad_norm": 42.60578536987305,
      "learning_rate": 0.0001636722606120434,
      "loss": 0.3754,
      "step": 138100
    },
    {
      "epoch": 0.45475485357025336,
      "grad_norm": 0.0014165352331474423,
      "learning_rate": 0.00016357354392892398,
      "loss": 0.2974,
      "step": 138200
    },
    {
      "epoch": 0.4550839091806515,
      "grad_norm": 0.21175359189510345,
      "learning_rate": 0.0001634748272458045,
      "loss": 0.3126,
      "step": 138300
    },
    {
      "epoch": 0.4554129647910497,
      "grad_norm": 0.0012725088745355606,
      "learning_rate": 0.00016337611056268508,
      "loss": 0.5805,
      "step": 138400
    },
    {
      "epoch": 0.45574202040144784,
      "grad_norm": 0.0018027365440502763,
      "learning_rate": 0.00016327739387956563,
      "loss": 0.5201,
      "step": 138500
    },
    {
      "epoch": 0.456071076011846,
      "grad_norm": 70.64876556396484,
      "learning_rate": 0.00016317867719644618,
      "loss": 0.5022,
      "step": 138600
    },
    {
      "epoch": 0.45640013162224413,
      "grad_norm": 0.0012707337737083435,
      "learning_rate": 0.00016307996051332675,
      "loss": 0.4011,
      "step": 138700
    },
    {
      "epoch": 0.45672918723264233,
      "grad_norm": 0.01021080743521452,
      "learning_rate": 0.00016298124383020728,
      "loss": 0.2615,
      "step": 138800
    },
    {
      "epoch": 0.4570582428430405,
      "grad_norm": 0.014765442349016666,
      "learning_rate": 0.00016288252714708785,
      "loss": 0.3959,
      "step": 138900
    },
    {
      "epoch": 0.4573872984534386,
      "grad_norm": 0.00035640501300804317,
      "learning_rate": 0.0001627838104639684,
      "loss": 0.4473,
      "step": 139000
    },
    {
      "epoch": 0.45771635406383676,
      "grad_norm": 74.23729705810547,
      "learning_rate": 0.00016268509378084895,
      "loss": 0.4998,
      "step": 139100
    },
    {
      "epoch": 0.45804540967423496,
      "grad_norm": 0.015322646126151085,
      "learning_rate": 0.00016258637709772953,
      "loss": 0.497,
      "step": 139200
    },
    {
      "epoch": 0.4583744652846331,
      "grad_norm": 0.002997491741552949,
      "learning_rate": 0.00016248766041461005,
      "loss": 0.5287,
      "step": 139300
    },
    {
      "epoch": 0.45870352089503125,
      "grad_norm": 0.0865148976445198,
      "learning_rate": 0.0001623889437314906,
      "loss": 0.4,
      "step": 139400
    },
    {
      "epoch": 0.4590325765054294,
      "grad_norm": 3.421888828277588,
      "learning_rate": 0.00016229022704837117,
      "loss": 0.383,
      "step": 139500
    },
    {
      "epoch": 0.4593616321158276,
      "grad_norm": 95.68315887451172,
      "learning_rate": 0.00016219151036525172,
      "loss": 0.5957,
      "step": 139600
    },
    {
      "epoch": 0.45969068772622573,
      "grad_norm": 0.014725444838404655,
      "learning_rate": 0.00016209279368213227,
      "loss": 0.5748,
      "step": 139700
    },
    {
      "epoch": 0.4600197433366239,
      "grad_norm": 0.03712473064661026,
      "learning_rate": 0.00016199407699901282,
      "loss": 0.3889,
      "step": 139800
    },
    {
      "epoch": 0.460348798947022,
      "grad_norm": 0.00032527244184166193,
      "learning_rate": 0.00016189536031589337,
      "loss": 0.3257,
      "step": 139900
    },
    {
      "epoch": 0.4606778545574202,
      "grad_norm": 85.33317565917969,
      "learning_rate": 0.00016179664363277394,
      "loss": 0.7294,
      "step": 140000
    },
    {
      "epoch": 0.46100691016781836,
      "grad_norm": 0.0004606653528753668,
      "learning_rate": 0.00016169792694965447,
      "loss": 0.3576,
      "step": 140100
    },
    {
      "epoch": 0.4613359657782165,
      "grad_norm": 0.016333645209670067,
      "learning_rate": 0.00016159921026653501,
      "loss": 0.3479,
      "step": 140200
    },
    {
      "epoch": 0.46166502138861465,
      "grad_norm": 0.10469527542591095,
      "learning_rate": 0.0001615004935834156,
      "loss": 0.2899,
      "step": 140300
    },
    {
      "epoch": 0.46199407699901285,
      "grad_norm": 58.60049057006836,
      "learning_rate": 0.00016140177690029614,
      "loss": 0.5249,
      "step": 140400
    },
    {
      "epoch": 0.462323132609411,
      "grad_norm": 17.829092025756836,
      "learning_rate": 0.00016130306021717671,
      "loss": 0.3009,
      "step": 140500
    },
    {
      "epoch": 0.46265218821980914,
      "grad_norm": 0.0003661478403955698,
      "learning_rate": 0.00016120434353405724,
      "loss": 0.339,
      "step": 140600
    },
    {
      "epoch": 0.4629812438302073,
      "grad_norm": 20.309972763061523,
      "learning_rate": 0.00016110562685093779,
      "loss": 0.5701,
      "step": 140700
    },
    {
      "epoch": 0.4633102994406055,
      "grad_norm": 1.4800019264221191,
      "learning_rate": 0.00016100691016781836,
      "loss": 0.3982,
      "step": 140800
    },
    {
      "epoch": 0.4636393550510036,
      "grad_norm": 0.00011340540368109941,
      "learning_rate": 0.0001609081934846989,
      "loss": 0.1585,
      "step": 140900
    },
    {
      "epoch": 0.46396841066140176,
      "grad_norm": 0.004111854359507561,
      "learning_rate": 0.00016080947680157943,
      "loss": 0.4137,
      "step": 141000
    },
    {
      "epoch": 0.4642974662717999,
      "grad_norm": 0.001166190137155354,
      "learning_rate": 0.00016071076011846,
      "loss": 0.3406,
      "step": 141100
    },
    {
      "epoch": 0.4646265218821981,
      "grad_norm": 0.0023218090645968914,
      "learning_rate": 0.00016061204343534056,
      "loss": 0.3295,
      "step": 141200
    },
    {
      "epoch": 0.46495557749259625,
      "grad_norm": 0.002302875043824315,
      "learning_rate": 0.00016051332675222113,
      "loss": 0.4075,
      "step": 141300
    },
    {
      "epoch": 0.4652846331029944,
      "grad_norm": 0.0004196734807919711,
      "learning_rate": 0.00016041461006910168,
      "loss": 0.3509,
      "step": 141400
    },
    {
      "epoch": 0.46561368871339254,
      "grad_norm": 0.00020598674018401653,
      "learning_rate": 0.0001603158933859822,
      "loss": 0.3076,
      "step": 141500
    },
    {
      "epoch": 0.46594274432379074,
      "grad_norm": 0.001369541510939598,
      "learning_rate": 0.00016021717670286278,
      "loss": 0.3073,
      "step": 141600
    },
    {
      "epoch": 0.4662717999341889,
      "grad_norm": 50.72365951538086,
      "learning_rate": 0.00016011846001974333,
      "loss": 0.3759,
      "step": 141700
    },
    {
      "epoch": 0.466600855544587,
      "grad_norm": 0.0068382378667593,
      "learning_rate": 0.00016001974333662385,
      "loss": 0.6167,
      "step": 141800
    },
    {
      "epoch": 0.46692991115498517,
      "grad_norm": 0.000783005787525326,
      "learning_rate": 0.00015992102665350443,
      "loss": 0.3573,
      "step": 141900
    },
    {
      "epoch": 0.46725896676538337,
      "grad_norm": 0.09851758927106857,
      "learning_rate": 0.00015982230997038497,
      "loss": 0.3536,
      "step": 142000
    },
    {
      "epoch": 0.4675880223757815,
      "grad_norm": 1.4142292737960815,
      "learning_rate": 0.00015972359328726555,
      "loss": 0.3901,
      "step": 142100
    },
    {
      "epoch": 0.46791707798617965,
      "grad_norm": 0.0028078979812562466,
      "learning_rate": 0.0001596248766041461,
      "loss": 0.4226,
      "step": 142200
    },
    {
      "epoch": 0.4682461335965778,
      "grad_norm": 0.1557462513446808,
      "learning_rate": 0.00015952615992102662,
      "loss": 0.2278,
      "step": 142300
    },
    {
      "epoch": 0.468575189206976,
      "grad_norm": 0.10249992460012436,
      "learning_rate": 0.0001594274432379072,
      "loss": 0.2851,
      "step": 142400
    },
    {
      "epoch": 0.46890424481737414,
      "grad_norm": 21.293394088745117,
      "learning_rate": 0.00015932872655478774,
      "loss": 0.5648,
      "step": 142500
    },
    {
      "epoch": 0.4692333004277723,
      "grad_norm": 0.006058892235159874,
      "learning_rate": 0.0001592300098716683,
      "loss": 0.4221,
      "step": 142600
    },
    {
      "epoch": 0.4695623560381704,
      "grad_norm": 0.004501409363001585,
      "learning_rate": 0.00015913129318854887,
      "loss": 0.2769,
      "step": 142700
    },
    {
      "epoch": 0.4698914116485686,
      "grad_norm": 6.699029472656548e-05,
      "learning_rate": 0.0001590325765054294,
      "loss": 0.305,
      "step": 142800
    },
    {
      "epoch": 0.47022046725896677,
      "grad_norm": 0.043771062046289444,
      "learning_rate": 0.00015893385982230997,
      "loss": 0.1455,
      "step": 142900
    },
    {
      "epoch": 0.4705495228693649,
      "grad_norm": 0.0011181762674823403,
      "learning_rate": 0.00015883514313919052,
      "loss": 0.3265,
      "step": 143000
    },
    {
      "epoch": 0.47087857847976305,
      "grad_norm": 0.0019968694541603327,
      "learning_rate": 0.00015873642645607106,
      "loss": 0.3434,
      "step": 143100
    },
    {
      "epoch": 0.47120763409016125,
      "grad_norm": 0.0053990981541574,
      "learning_rate": 0.00015863770977295161,
      "loss": 0.3351,
      "step": 143200
    },
    {
      "epoch": 0.4715366897005594,
      "grad_norm": 0.0025014004204422235,
      "learning_rate": 0.00015853899308983216,
      "loss": 0.3502,
      "step": 143300
    },
    {
      "epoch": 0.47186574531095754,
      "grad_norm": 0.0007507014088332653,
      "learning_rate": 0.0001584402764067127,
      "loss": 0.1988,
      "step": 143400
    },
    {
      "epoch": 0.4721948009213557,
      "grad_norm": 0.0027968878857791424,
      "learning_rate": 0.0001583415597235933,
      "loss": 0.3888,
      "step": 143500
    },
    {
      "epoch": 0.4725238565317539,
      "grad_norm": 0.00044232371146790683,
      "learning_rate": 0.0001582428430404738,
      "loss": 0.4675,
      "step": 143600
    },
    {
      "epoch": 0.472852912142152,
      "grad_norm": 0.003176232101395726,
      "learning_rate": 0.00015814412635735438,
      "loss": 0.3396,
      "step": 143700
    },
    {
      "epoch": 0.47318196775255017,
      "grad_norm": 0.01788898929953575,
      "learning_rate": 0.00015804540967423493,
      "loss": 0.3912,
      "step": 143800
    },
    {
      "epoch": 0.4735110233629483,
      "grad_norm": 29.117733001708984,
      "learning_rate": 0.00015794669299111548,
      "loss": 0.4236,
      "step": 143900
    },
    {
      "epoch": 0.4738400789733465,
      "grad_norm": 22.459522247314453,
      "learning_rate": 0.00015784797630799606,
      "loss": 0.4815,
      "step": 144000
    },
    {
      "epoch": 0.47416913458374466,
      "grad_norm": 0.0006187637336552143,
      "learning_rate": 0.00015774925962487658,
      "loss": 0.3914,
      "step": 144100
    },
    {
      "epoch": 0.4744981901941428,
      "grad_norm": 1.4427818059921265,
      "learning_rate": 0.00015765054294175713,
      "loss": 0.4744,
      "step": 144200
    },
    {
      "epoch": 0.47482724580454094,
      "grad_norm": 0.004015710670500994,
      "learning_rate": 0.0001575518262586377,
      "loss": 0.2812,
      "step": 144300
    },
    {
      "epoch": 0.47515630141493914,
      "grad_norm": 0.0089865205809474,
      "learning_rate": 0.00015745310957551825,
      "loss": 0.5331,
      "step": 144400
    },
    {
      "epoch": 0.4754853570253373,
      "grad_norm": 2.9187779426574707,
      "learning_rate": 0.00015735439289239883,
      "loss": 0.2785,
      "step": 144500
    },
    {
      "epoch": 0.47581441263573543,
      "grad_norm": 0.0006949288654141128,
      "learning_rate": 0.00015725567620927935,
      "loss": 0.2095,
      "step": 144600
    },
    {
      "epoch": 0.47614346824613357,
      "grad_norm": 0.0036312201991677284,
      "learning_rate": 0.0001571569595261599,
      "loss": 0.4332,
      "step": 144700
    },
    {
      "epoch": 0.47647252385653177,
      "grad_norm": 0.0006841009017080069,
      "learning_rate": 0.00015705824284304048,
      "loss": 0.4538,
      "step": 144800
    },
    {
      "epoch": 0.4768015794669299,
      "grad_norm": 0.0011492552002891898,
      "learning_rate": 0.00015695952615992102,
      "loss": 0.2089,
      "step": 144900
    },
    {
      "epoch": 0.47713063507732806,
      "grad_norm": 0.008370350115001202,
      "learning_rate": 0.00015686080947680155,
      "loss": 0.5052,
      "step": 145000
    },
    {
      "epoch": 0.4774596906877262,
      "grad_norm": 0.038287896662950516,
      "learning_rate": 0.00015676209279368212,
      "loss": 0.3408,
      "step": 145100
    },
    {
      "epoch": 0.4777887462981244,
      "grad_norm": 44.491451263427734,
      "learning_rate": 0.00015666337611056267,
      "loss": 0.3533,
      "step": 145200
    },
    {
      "epoch": 0.47811780190852254,
      "grad_norm": 0.0019561336375772953,
      "learning_rate": 0.00015656465942744325,
      "loss": 0.4234,
      "step": 145300
    },
    {
      "epoch": 0.4784468575189207,
      "grad_norm": 0.005232132505625486,
      "learning_rate": 0.00015646594274432377,
      "loss": 0.3716,
      "step": 145400
    },
    {
      "epoch": 0.47877591312931883,
      "grad_norm": 79.71553802490234,
      "learning_rate": 0.00015636722606120432,
      "loss": 0.3022,
      "step": 145500
    },
    {
      "epoch": 0.47910496873971703,
      "grad_norm": 0.004106807988137007,
      "learning_rate": 0.0001562685093780849,
      "loss": 0.395,
      "step": 145600
    },
    {
      "epoch": 0.4794340243501152,
      "grad_norm": 26.222118377685547,
      "learning_rate": 0.00015616979269496544,
      "loss": 0.3917,
      "step": 145700
    },
    {
      "epoch": 0.4797630799605133,
      "grad_norm": 0.00342660048045218,
      "learning_rate": 0.00015607107601184596,
      "loss": 0.2215,
      "step": 145800
    },
    {
      "epoch": 0.48009213557091146,
      "grad_norm": 11.986764907836914,
      "learning_rate": 0.00015597235932872654,
      "loss": 0.3099,
      "step": 145900
    },
    {
      "epoch": 0.48042119118130966,
      "grad_norm": 0.004549537785351276,
      "learning_rate": 0.0001558736426456071,
      "loss": 0.4091,
      "step": 146000
    },
    {
      "epoch": 0.4807502467917078,
      "grad_norm": 0.0005069469334557652,
      "learning_rate": 0.00015577492596248766,
      "loss": 0.358,
      "step": 146100
    },
    {
      "epoch": 0.48107930240210595,
      "grad_norm": 0.00232485868036747,
      "learning_rate": 0.0001556762092793682,
      "loss": 0.363,
      "step": 146200
    },
    {
      "epoch": 0.4814083580125041,
      "grad_norm": 0.004238579422235489,
      "learning_rate": 0.00015557749259624874,
      "loss": 0.3212,
      "step": 146300
    },
    {
      "epoch": 0.4817374136229023,
      "grad_norm": 0.0035182505380362272,
      "learning_rate": 0.0001554787759131293,
      "loss": 0.4541,
      "step": 146400
    },
    {
      "epoch": 0.48206646923330043,
      "grad_norm": 0.09874893724918365,
      "learning_rate": 0.00015538005923000986,
      "loss": 0.4403,
      "step": 146500
    },
    {
      "epoch": 0.4823955248436986,
      "grad_norm": 0.0040518357418477535,
      "learning_rate": 0.0001552813425468904,
      "loss": 0.4716,
      "step": 146600
    },
    {
      "epoch": 0.4827245804540967,
      "grad_norm": 0.0005453635239973664,
      "learning_rate": 0.00015518262586377096,
      "loss": 0.2388,
      "step": 146700
    },
    {
      "epoch": 0.4830536360644949,
      "grad_norm": 0.014679665677249432,
      "learning_rate": 0.0001550839091806515,
      "loss": 0.3772,
      "step": 146800
    },
    {
      "epoch": 0.48338269167489306,
      "grad_norm": 0.12330850958824158,
      "learning_rate": 0.00015498519249753208,
      "loss": 0.3725,
      "step": 146900
    },
    {
      "epoch": 0.4837117472852912,
      "grad_norm": 0.0015722585376352072,
      "learning_rate": 0.00015488647581441263,
      "loss": 0.4935,
      "step": 147000
    },
    {
      "epoch": 0.48404080289568935,
      "grad_norm": 0.013962854631245136,
      "learning_rate": 0.00015478775913129315,
      "loss": 0.5964,
      "step": 147100
    },
    {
      "epoch": 0.48436985850608755,
      "grad_norm": 0.005219002719968557,
      "learning_rate": 0.00015468904244817373,
      "loss": 0.2623,
      "step": 147200
    },
    {
      "epoch": 0.4846989141164857,
      "grad_norm": 0.00011851126328110695,
      "learning_rate": 0.00015459032576505428,
      "loss": 0.3576,
      "step": 147300
    },
    {
      "epoch": 0.48502796972688383,
      "grad_norm": 17.276811599731445,
      "learning_rate": 0.00015449160908193483,
      "loss": 0.5488,
      "step": 147400
    },
    {
      "epoch": 0.485357025337282,
      "grad_norm": 0.0015707192942500114,
      "learning_rate": 0.0001543928923988154,
      "loss": 0.3134,
      "step": 147500
    },
    {
      "epoch": 0.4856860809476802,
      "grad_norm": 0.024346400052309036,
      "learning_rate": 0.00015429417571569592,
      "loss": 0.4178,
      "step": 147600
    },
    {
      "epoch": 0.4860151365580783,
      "grad_norm": 0.5661478042602539,
      "learning_rate": 0.0001541954590325765,
      "loss": 0.4423,
      "step": 147700
    },
    {
      "epoch": 0.48634419216847646,
      "grad_norm": 0.0023266742937266827,
      "learning_rate": 0.00015409674234945705,
      "loss": 0.3543,
      "step": 147800
    },
    {
      "epoch": 0.4866732477788746,
      "grad_norm": 0.20739495754241943,
      "learning_rate": 0.0001539980256663376,
      "loss": 0.3648,
      "step": 147900
    },
    {
      "epoch": 0.4870023033892728,
      "grad_norm": 0.0005791878211311996,
      "learning_rate": 0.00015389930898321817,
      "loss": 0.3593,
      "step": 148000
    },
    {
      "epoch": 0.48733135899967095,
      "grad_norm": 66.59235382080078,
      "learning_rate": 0.0001538005923000987,
      "loss": 0.3838,
      "step": 148100
    },
    {
      "epoch": 0.4876604146100691,
      "grad_norm": 27.479040145874023,
      "learning_rate": 0.00015370187561697924,
      "loss": 0.3228,
      "step": 148200
    },
    {
      "epoch": 0.48798947022046724,
      "grad_norm": 0.038657717406749725,
      "learning_rate": 0.00015360315893385982,
      "loss": 0.511,
      "step": 148300
    },
    {
      "epoch": 0.48831852583086544,
      "grad_norm": 0.0019366464111953974,
      "learning_rate": 0.00015350444225074037,
      "loss": 0.3006,
      "step": 148400
    },
    {
      "epoch": 0.4886475814412636,
      "grad_norm": 0.27991512417793274,
      "learning_rate": 0.00015340572556762092,
      "loss": 0.6845,
      "step": 148500
    },
    {
      "epoch": 0.4889766370516617,
      "grad_norm": 2.7755329608917236,
      "learning_rate": 0.00015330700888450147,
      "loss": 0.3021,
      "step": 148600
    },
    {
      "epoch": 0.48930569266205987,
      "grad_norm": 45.29449462890625,
      "learning_rate": 0.00015320829220138202,
      "loss": 0.3367,
      "step": 148700
    },
    {
      "epoch": 0.48963474827245806,
      "grad_norm": 87.08096313476562,
      "learning_rate": 0.0001531095755182626,
      "loss": 0.467,
      "step": 148800
    },
    {
      "epoch": 0.4899638038828562,
      "grad_norm": 0.2636204957962036,
      "learning_rate": 0.0001530108588351431,
      "loss": 0.2957,
      "step": 148900
    },
    {
      "epoch": 0.49029285949325435,
      "grad_norm": 0.0005047554150223732,
      "learning_rate": 0.00015291214215202366,
      "loss": 0.3812,
      "step": 149000
    },
    {
      "epoch": 0.4906219151036525,
      "grad_norm": 0.001541490899398923,
      "learning_rate": 0.00015281342546890424,
      "loss": 0.2607,
      "step": 149100
    },
    {
      "epoch": 0.4909509707140507,
      "grad_norm": 0.0001671128993621096,
      "learning_rate": 0.00015271470878578479,
      "loss": 0.2246,
      "step": 149200
    },
    {
      "epoch": 0.49128002632444884,
      "grad_norm": 0.00013689968909602612,
      "learning_rate": 0.00015261599210266536,
      "loss": 0.3163,
      "step": 149300
    },
    {
      "epoch": 0.491609081934847,
      "grad_norm": 0.003024329664185643,
      "learning_rate": 0.00015251727541954588,
      "loss": 0.2055,
      "step": 149400
    },
    {
      "epoch": 0.4919381375452451,
      "grad_norm": 0.004921449813991785,
      "learning_rate": 0.00015241855873642643,
      "loss": 0.3916,
      "step": 149500
    },
    {
      "epoch": 0.4922671931556433,
      "grad_norm": 0.00026016178890131414,
      "learning_rate": 0.000152319842053307,
      "loss": 0.3393,
      "step": 149600
    },
    {
      "epoch": 0.49259624876604147,
      "grad_norm": 0.5562781691551208,
      "learning_rate": 0.00015222112537018756,
      "loss": 0.393,
      "step": 149700
    },
    {
      "epoch": 0.4929253043764396,
      "grad_norm": 0.008266438730061054,
      "learning_rate": 0.00015212240868706808,
      "loss": 0.3806,
      "step": 149800
    },
    {
      "epoch": 0.49325435998683775,
      "grad_norm": 0.30224716663360596,
      "learning_rate": 0.00015202369200394865,
      "loss": 0.3903,
      "step": 149900
    },
    {
      "epoch": 0.49358341559723595,
      "grad_norm": 0.0037319560069590807,
      "learning_rate": 0.0001519249753208292,
      "loss": 0.389,
      "step": 150000
    },
    {
      "epoch": 0.4939124712076341,
      "grad_norm": 0.0033101094886660576,
      "learning_rate": 0.00015182625863770978,
      "loss": 0.3348,
      "step": 150100
    },
    {
      "epoch": 0.49424152681803224,
      "grad_norm": 13.588253021240234,
      "learning_rate": 0.0001517275419545903,
      "loss": 0.4053,
      "step": 150200
    },
    {
      "epoch": 0.4945705824284304,
      "grad_norm": 25.001985549926758,
      "learning_rate": 0.00015162882527147085,
      "loss": 0.5675,
      "step": 150300
    },
    {
      "epoch": 0.4948996380388286,
      "grad_norm": 0.0030197016894817352,
      "learning_rate": 0.00015153010858835143,
      "loss": 0.3313,
      "step": 150400
    },
    {
      "epoch": 0.4952286936492267,
      "grad_norm": 20.547340393066406,
      "learning_rate": 0.00015143139190523197,
      "loss": 0.3999,
      "step": 150500
    },
    {
      "epoch": 0.49555774925962487,
      "grad_norm": 0.005906510166823864,
      "learning_rate": 0.0001513326752221125,
      "loss": 0.4331,
      "step": 150600
    },
    {
      "epoch": 0.495886804870023,
      "grad_norm": 0.0004290490469429642,
      "learning_rate": 0.00015123395853899307,
      "loss": 0.4084,
      "step": 150700
    },
    {
      "epoch": 0.4962158604804212,
      "grad_norm": 0.0002143435995094478,
      "learning_rate": 0.00015113524185587362,
      "loss": 0.2944,
      "step": 150800
    },
    {
      "epoch": 0.49654491609081935,
      "grad_norm": 0.001017218572087586,
      "learning_rate": 0.0001510365251727542,
      "loss": 0.5593,
      "step": 150900
    },
    {
      "epoch": 0.4968739717012175,
      "grad_norm": 0.0005384790129028261,
      "learning_rate": 0.00015093780848963475,
      "loss": 0.4176,
      "step": 151000
    },
    {
      "epoch": 0.49720302731161564,
      "grad_norm": 9.376032829284668,
      "learning_rate": 0.00015083909180651527,
      "loss": 0.5289,
      "step": 151100
    },
    {
      "epoch": 0.49753208292201384,
      "grad_norm": 0.00030558957951143384,
      "learning_rate": 0.00015074037512339584,
      "loss": 0.4491,
      "step": 151200
    },
    {
      "epoch": 0.497861138532412,
      "grad_norm": 0.007918916642665863,
      "learning_rate": 0.0001506416584402764,
      "loss": 0.4137,
      "step": 151300
    },
    {
      "epoch": 0.4981901941428101,
      "grad_norm": 5.051910400390625,
      "learning_rate": 0.00015054294175715694,
      "loss": 0.3251,
      "step": 151400
    },
    {
      "epoch": 0.49851924975320827,
      "grad_norm": 0.0003857293340843171,
      "learning_rate": 0.00015044422507403752,
      "loss": 0.3841,
      "step": 151500
    },
    {
      "epoch": 0.49884830536360647,
      "grad_norm": 0.0049137636087834835,
      "learning_rate": 0.00015034550839091804,
      "loss": 0.3818,
      "step": 151600
    },
    {
      "epoch": 0.4991773609740046,
      "grad_norm": 0.0034558738116174936,
      "learning_rate": 0.00015024679170779861,
      "loss": 0.3102,
      "step": 151700
    },
    {
      "epoch": 0.49950641658440276,
      "grad_norm": 1.752579689025879,
      "learning_rate": 0.00015014807502467916,
      "loss": 0.4613,
      "step": 151800
    },
    {
      "epoch": 0.4998354721948009,
      "grad_norm": 1.8973997831344604,
      "learning_rate": 0.0001500493583415597,
      "loss": 0.2219,
      "step": 151900
    },
    {
      "epoch": 0.500164527805199,
      "grad_norm": 0.26347577571868896,
      "learning_rate": 0.00014995064165844026,
      "loss": 0.2278,
      "step": 152000
    },
    {
      "epoch": 0.5004935834155972,
      "grad_norm": 0.0008685244247317314,
      "learning_rate": 0.0001498519249753208,
      "loss": 0.2278,
      "step": 152100
    },
    {
      "epoch": 0.5008226390259954,
      "grad_norm": 0.0002027118025580421,
      "learning_rate": 0.00014975320829220136,
      "loss": 0.4778,
      "step": 152200
    },
    {
      "epoch": 0.5011516946363935,
      "grad_norm": 0.003627261845394969,
      "learning_rate": 0.00014965449160908193,
      "loss": 0.3206,
      "step": 152300
    },
    {
      "epoch": 0.5014807502467917,
      "grad_norm": 0.004610355477780104,
      "learning_rate": 0.00014955577492596246,
      "loss": 0.2569,
      "step": 152400
    },
    {
      "epoch": 0.5018098058571898,
      "grad_norm": 0.01804984174668789,
      "learning_rate": 0.00014945705824284303,
      "loss": 0.5843,
      "step": 152500
    },
    {
      "epoch": 0.502138861467588,
      "grad_norm": 0.001966441050171852,
      "learning_rate": 0.00014935834155972358,
      "loss": 0.4468,
      "step": 152600
    },
    {
      "epoch": 0.5024679170779862,
      "grad_norm": 0.0062677208334207535,
      "learning_rate": 0.00014925962487660413,
      "loss": 0.277,
      "step": 152700
    },
    {
      "epoch": 0.5027969726883843,
      "grad_norm": 0.031009148806333542,
      "learning_rate": 0.00014916090819348468,
      "loss": 0.4613,
      "step": 152800
    },
    {
      "epoch": 0.5031260282987825,
      "grad_norm": 0.0858934298157692,
      "learning_rate": 0.00014906219151036523,
      "loss": 0.2089,
      "step": 152900
    },
    {
      "epoch": 0.5034550839091807,
      "grad_norm": 0.004081480670720339,
      "learning_rate": 0.0001489634748272458,
      "loss": 0.4368,
      "step": 153000
    },
    {
      "epoch": 0.5037841395195788,
      "grad_norm": 26.413251876831055,
      "learning_rate": 0.00014886475814412635,
      "loss": 0.1996,
      "step": 153100
    },
    {
      "epoch": 0.504113195129977,
      "grad_norm": 0.047571662813425064,
      "learning_rate": 0.0001487660414610069,
      "loss": 0.0923,
      "step": 153200
    },
    {
      "epoch": 0.5044422507403751,
      "grad_norm": 0.000586540496442467,
      "learning_rate": 0.00014866732477788745,
      "loss": 0.3115,
      "step": 153300
    },
    {
      "epoch": 0.5047713063507733,
      "grad_norm": 18.772891998291016,
      "learning_rate": 0.000148568608094768,
      "loss": 0.2601,
      "step": 153400
    },
    {
      "epoch": 0.5051003619611715,
      "grad_norm": 0.02022741176187992,
      "learning_rate": 0.00014846989141164857,
      "loss": 0.3488,
      "step": 153500
    },
    {
      "epoch": 0.5054294175715696,
      "grad_norm": 8.257302284240723,
      "learning_rate": 0.0001483711747285291,
      "loss": 0.3136,
      "step": 153600
    },
    {
      "epoch": 0.5057584731819678,
      "grad_norm": 0.22691845893859863,
      "learning_rate": 0.00014827245804540967,
      "loss": 0.3843,
      "step": 153700
    },
    {
      "epoch": 0.506087528792366,
      "grad_norm": 73.62173461914062,
      "learning_rate": 0.00014817374136229022,
      "loss": 0.3214,
      "step": 153800
    },
    {
      "epoch": 0.506416584402764,
      "grad_norm": 7.484738349914551,
      "learning_rate": 0.00014807502467917077,
      "loss": 0.3623,
      "step": 153900
    },
    {
      "epoch": 0.5067456400131622,
      "grad_norm": 0.0005642786272801459,
      "learning_rate": 0.00014797630799605132,
      "loss": 0.409,
      "step": 154000
    },
    {
      "epoch": 0.5070746956235603,
      "grad_norm": 0.0004392610862851143,
      "learning_rate": 0.00014787759131293187,
      "loss": 0.2771,
      "step": 154100
    },
    {
      "epoch": 0.5074037512339585,
      "grad_norm": 119.98133850097656,
      "learning_rate": 0.00014777887462981242,
      "loss": 0.4358,
      "step": 154200
    },
    {
      "epoch": 0.5077328068443567,
      "grad_norm": 0.07179025560617447,
      "learning_rate": 0.000147680157946693,
      "loss": 0.2146,
      "step": 154300
    },
    {
      "epoch": 0.5080618624547548,
      "grad_norm": 29.020740509033203,
      "learning_rate": 0.00014758144126357351,
      "loss": 0.6014,
      "step": 154400
    },
    {
      "epoch": 0.508390918065153,
      "grad_norm": 0.19742636382579803,
      "learning_rate": 0.0001474827245804541,
      "loss": 0.2854,
      "step": 154500
    },
    {
      "epoch": 0.5087199736755512,
      "grad_norm": 0.001358131179586053,
      "learning_rate": 0.00014738400789733464,
      "loss": 0.2976,
      "step": 154600
    },
    {
      "epoch": 0.5090490292859493,
      "grad_norm": 0.0005774489254690707,
      "learning_rate": 0.0001472852912142152,
      "loss": 0.2399,
      "step": 154700
    },
    {
      "epoch": 0.5093780848963475,
      "grad_norm": 0.007537656929343939,
      "learning_rate": 0.00014718657453109574,
      "loss": 0.2107,
      "step": 154800
    },
    {
      "epoch": 0.5097071405067456,
      "grad_norm": 0.0018465069588273764,
      "learning_rate": 0.00014708785784797629,
      "loss": 0.3181,
      "step": 154900
    },
    {
      "epoch": 0.5100361961171438,
      "grad_norm": 0.0020156868267804384,
      "learning_rate": 0.00014698914116485686,
      "loss": 0.2326,
      "step": 155000
    },
    {
      "epoch": 0.510365251727542,
      "grad_norm": 0.003313617082312703,
      "learning_rate": 0.0001468904244817374,
      "loss": 0.2123,
      "step": 155100
    },
    {
      "epoch": 0.5106943073379401,
      "grad_norm": 0.0018301171949133277,
      "learning_rate": 0.00014679170779861796,
      "loss": 0.3983,
      "step": 155200
    },
    {
      "epoch": 0.5110233629483383,
      "grad_norm": 1.5892537832260132,
      "learning_rate": 0.0001466929911154985,
      "loss": 0.3164,
      "step": 155300
    },
    {
      "epoch": 0.5113524185587365,
      "grad_norm": 82.96508026123047,
      "learning_rate": 0.00014659427443237906,
      "loss": 0.5649,
      "step": 155400
    },
    {
      "epoch": 0.5116814741691346,
      "grad_norm": 0.31125637888908386,
      "learning_rate": 0.0001464955577492596,
      "loss": 0.3111,
      "step": 155500
    },
    {
      "epoch": 0.5120105297795328,
      "grad_norm": 0.25063827633857727,
      "learning_rate": 0.00014639684106614015,
      "loss": 0.5211,
      "step": 155600
    },
    {
      "epoch": 0.5123395853899309,
      "grad_norm": 2.2795965671539307,
      "learning_rate": 0.0001462981243830207,
      "loss": 0.5488,
      "step": 155700
    },
    {
      "epoch": 0.512668641000329,
      "grad_norm": 0.002792427083477378,
      "learning_rate": 0.00014619940769990128,
      "loss": 0.371,
      "step": 155800
    },
    {
      "epoch": 0.5129976966107272,
      "grad_norm": 0.007210369687527418,
      "learning_rate": 0.00014610069101678183,
      "loss": 0.3373,
      "step": 155900
    },
    {
      "epoch": 0.5133267522211253,
      "grad_norm": 0.005406280979514122,
      "learning_rate": 0.00014600197433366238,
      "loss": 0.302,
      "step": 156000
    },
    {
      "epoch": 0.5136558078315235,
      "grad_norm": 0.001763383042998612,
      "learning_rate": 0.00014590325765054293,
      "loss": 0.3735,
      "step": 156100
    },
    {
      "epoch": 0.5139848634419217,
      "grad_norm": 8.121158316498622e-05,
      "learning_rate": 0.00014580454096742347,
      "loss": 0.2407,
      "step": 156200
    },
    {
      "epoch": 0.5143139190523198,
      "grad_norm": 0.23231235146522522,
      "learning_rate": 0.00014570582428430405,
      "loss": 0.3903,
      "step": 156300
    },
    {
      "epoch": 0.514642974662718,
      "grad_norm": 0.25707781314849854,
      "learning_rate": 0.00014560710760118457,
      "loss": 0.2523,
      "step": 156400
    },
    {
      "epoch": 0.5149720302731161,
      "grad_norm": 51.42448806762695,
      "learning_rate": 0.00014550839091806515,
      "loss": 0.4299,
      "step": 156500
    },
    {
      "epoch": 0.5153010858835143,
      "grad_norm": 0.052162054926157,
      "learning_rate": 0.0001454096742349457,
      "loss": 0.5027,
      "step": 156600
    },
    {
      "epoch": 0.5156301414939125,
      "grad_norm": 0.002508359495550394,
      "learning_rate": 0.00014531095755182624,
      "loss": 0.6147,
      "step": 156700
    },
    {
      "epoch": 0.5159591971043106,
      "grad_norm": 0.004556348081678152,
      "learning_rate": 0.0001452122408687068,
      "loss": 0.2679,
      "step": 156800
    },
    {
      "epoch": 0.5162882527147088,
      "grad_norm": 0.13064448535442352,
      "learning_rate": 0.00014511352418558734,
      "loss": 0.5991,
      "step": 156900
    },
    {
      "epoch": 0.516617308325107,
      "grad_norm": 37.28453063964844,
      "learning_rate": 0.00014501480750246792,
      "loss": 0.3784,
      "step": 157000
    },
    {
      "epoch": 0.5169463639355051,
      "grad_norm": 0.0011618216522037983,
      "learning_rate": 0.00014491609081934847,
      "loss": 0.3732,
      "step": 157100
    },
    {
      "epoch": 0.5172754195459033,
      "grad_norm": 21.97431182861328,
      "learning_rate": 0.00014481737413622902,
      "loss": 0.4383,
      "step": 157200
    },
    {
      "epoch": 0.5176044751563014,
      "grad_norm": 0.4518384337425232,
      "learning_rate": 0.00014471865745310956,
      "loss": 0.4385,
      "step": 157300
    },
    {
      "epoch": 0.5179335307666996,
      "grad_norm": 125.35173797607422,
      "learning_rate": 0.00014461994076999011,
      "loss": 0.4225,
      "step": 157400
    },
    {
      "epoch": 0.5182625863770978,
      "grad_norm": 0.004859753884375095,
      "learning_rate": 0.00014452122408687066,
      "loss": 0.3678,
      "step": 157500
    },
    {
      "epoch": 0.5185916419874959,
      "grad_norm": 0.005690673366189003,
      "learning_rate": 0.0001444225074037512,
      "loss": 0.3784,
      "step": 157600
    },
    {
      "epoch": 0.518920697597894,
      "grad_norm": 0.019698698073625565,
      "learning_rate": 0.00014432379072063176,
      "loss": 0.2746,
      "step": 157700
    },
    {
      "epoch": 0.5192497532082923,
      "grad_norm": 0.0037856455892324448,
      "learning_rate": 0.00014422507403751234,
      "loss": 0.3052,
      "step": 157800
    },
    {
      "epoch": 0.5195788088186903,
      "grad_norm": 0.0028466812800616026,
      "learning_rate": 0.00014412635735439288,
      "loss": 0.4205,
      "step": 157900
    },
    {
      "epoch": 0.5199078644290885,
      "grad_norm": 51.445316314697266,
      "learning_rate": 0.00014402764067127343,
      "loss": 0.3069,
      "step": 158000
    },
    {
      "epoch": 0.5202369200394866,
      "grad_norm": 0.00010328165080863982,
      "learning_rate": 0.00014392892398815398,
      "loss": 0.2126,
      "step": 158100
    },
    {
      "epoch": 0.5205659756498848,
      "grad_norm": 18.59063148498535,
      "learning_rate": 0.00014383020730503453,
      "loss": 0.4433,
      "step": 158200
    },
    {
      "epoch": 0.520895031260283,
      "grad_norm": 80.25180053710938,
      "learning_rate": 0.0001437314906219151,
      "loss": 0.1627,
      "step": 158300
    },
    {
      "epoch": 0.5212240868706811,
      "grad_norm": 12.943034172058105,
      "learning_rate": 0.00014363277393879563,
      "loss": 0.4638,
      "step": 158400
    },
    {
      "epoch": 0.5215531424810793,
      "grad_norm": 0.001287979306653142,
      "learning_rate": 0.0001435340572556762,
      "loss": 0.3571,
      "step": 158500
    },
    {
      "epoch": 0.5218821980914775,
      "grad_norm": 15.933721542358398,
      "learning_rate": 0.00014343534057255675,
      "loss": 0.3132,
      "step": 158600
    },
    {
      "epoch": 0.5222112537018756,
      "grad_norm": 0.12817534804344177,
      "learning_rate": 0.0001433366238894373,
      "loss": 0.4058,
      "step": 158700
    },
    {
      "epoch": 0.5225403093122738,
      "grad_norm": 0.00014339077461045235,
      "learning_rate": 0.00014323790720631785,
      "loss": 0.3835,
      "step": 158800
    },
    {
      "epoch": 0.5228693649226719,
      "grad_norm": 0.13800625503063202,
      "learning_rate": 0.0001431391905231984,
      "loss": 0.3207,
      "step": 158900
    },
    {
      "epoch": 0.5231984205330701,
      "grad_norm": 0.005902192089706659,
      "learning_rate": 0.00014304047384007895,
      "loss": 0.3964,
      "step": 159000
    },
    {
      "epoch": 0.5235274761434683,
      "grad_norm": 17.767629623413086,
      "learning_rate": 0.00014294175715695952,
      "loss": 0.2938,
      "step": 159100
    },
    {
      "epoch": 0.5238565317538664,
      "grad_norm": 0.017736384645104408,
      "learning_rate": 0.00014284304047384005,
      "loss": 0.4026,
      "step": 159200
    },
    {
      "epoch": 0.5241855873642646,
      "grad_norm": 0.0013133506290614605,
      "learning_rate": 0.00014274432379072062,
      "loss": 0.2518,
      "step": 159300
    },
    {
      "epoch": 0.5245146429746628,
      "grad_norm": 0.0004847794771194458,
      "learning_rate": 0.00014264560710760117,
      "loss": 0.2649,
      "step": 159400
    },
    {
      "epoch": 0.5248436985850609,
      "grad_norm": 0.0017772904830053449,
      "learning_rate": 0.00014254689042448172,
      "loss": 0.4652,
      "step": 159500
    },
    {
      "epoch": 0.5251727541954591,
      "grad_norm": 0.03382714465260506,
      "learning_rate": 0.00014244817374136227,
      "loss": 0.3093,
      "step": 159600
    },
    {
      "epoch": 0.5255018098058571,
      "grad_norm": 0.000888168637175113,
      "learning_rate": 0.00014234945705824282,
      "loss": 0.2895,
      "step": 159700
    },
    {
      "epoch": 0.5258308654162553,
      "grad_norm": 2.7945801775786094e-05,
      "learning_rate": 0.0001422507403751234,
      "loss": 0.133,
      "step": 159800
    },
    {
      "epoch": 0.5261599210266535,
      "grad_norm": 2.080683946609497,
      "learning_rate": 0.00014215202369200394,
      "loss": 0.3358,
      "step": 159900
    },
    {
      "epoch": 0.5264889766370516,
      "grad_norm": 132.51425170898438,
      "learning_rate": 0.0001420533070088845,
      "loss": 0.3663,
      "step": 160000
    },
    {
      "epoch": 0.5268180322474498,
      "grad_norm": 0.0006174416048452258,
      "learning_rate": 0.00014195459032576504,
      "loss": 0.3062,
      "step": 160100
    },
    {
      "epoch": 0.527147087857848,
      "grad_norm": 0.00103734468575567,
      "learning_rate": 0.0001418558736426456,
      "loss": 0.214,
      "step": 160200
    },
    {
      "epoch": 0.5274761434682461,
      "grad_norm": 0.0007650449988432229,
      "learning_rate": 0.00014175715695952616,
      "loss": 0.3162,
      "step": 160300
    },
    {
      "epoch": 0.5278051990786443,
      "grad_norm": 0.0007594489143230021,
      "learning_rate": 0.00014165844027640669,
      "loss": 0.26,
      "step": 160400
    },
    {
      "epoch": 0.5281342546890424,
      "grad_norm": 0.003175479592755437,
      "learning_rate": 0.00014155972359328726,
      "loss": 0.3813,
      "step": 160500
    },
    {
      "epoch": 0.5284633102994406,
      "grad_norm": 0.18611730635166168,
      "learning_rate": 0.0001414610069101678,
      "loss": 0.385,
      "step": 160600
    },
    {
      "epoch": 0.5287923659098388,
      "grad_norm": 0.00024212502466980368,
      "learning_rate": 0.00014136229022704836,
      "loss": 0.4443,
      "step": 160700
    },
    {
      "epoch": 0.5291214215202369,
      "grad_norm": 0.000662866747006774,
      "learning_rate": 0.0001412635735439289,
      "loss": 0.3861,
      "step": 160800
    },
    {
      "epoch": 0.5294504771306351,
      "grad_norm": 0.0008330155978910625,
      "learning_rate": 0.00014116485686080946,
      "loss": 0.4065,
      "step": 160900
    },
    {
      "epoch": 0.5297795327410333,
      "grad_norm": 30.29620361328125,
      "learning_rate": 0.00014106614017769,
      "loss": 0.3846,
      "step": 161000
    },
    {
      "epoch": 0.5301085883514314,
      "grad_norm": 27.017717361450195,
      "learning_rate": 0.00014096742349457058,
      "loss": 0.526,
      "step": 161100
    },
    {
      "epoch": 0.5304376439618296,
      "grad_norm": 0.017093196511268616,
      "learning_rate": 0.0001408687068114511,
      "loss": 0.4739,
      "step": 161200
    },
    {
      "epoch": 0.5307666995722277,
      "grad_norm": 45.39278793334961,
      "learning_rate": 0.00014076999012833168,
      "loss": 0.3199,
      "step": 161300
    },
    {
      "epoch": 0.5310957551826259,
      "grad_norm": 0.002145737875252962,
      "learning_rate": 0.00014067127344521223,
      "loss": 0.1474,
      "step": 161400
    },
    {
      "epoch": 0.5314248107930241,
      "grad_norm": 0.9706538915634155,
      "learning_rate": 0.00014057255676209278,
      "loss": 0.1707,
      "step": 161500
    },
    {
      "epoch": 0.5317538664034221,
      "grad_norm": 0.0069764843210577965,
      "learning_rate": 0.00014047384007897333,
      "loss": 0.2569,
      "step": 161600
    },
    {
      "epoch": 0.5320829220138203,
      "grad_norm": 481.4244079589844,
      "learning_rate": 0.00014037512339585388,
      "loss": 0.3186,
      "step": 161700
    },
    {
      "epoch": 0.5324119776242185,
      "grad_norm": 0.0015875824028626084,
      "learning_rate": 0.00014027640671273445,
      "loss": 0.3392,
      "step": 161800
    },
    {
      "epoch": 0.5327410332346166,
      "grad_norm": 0.0018166359513998032,
      "learning_rate": 0.000140177690029615,
      "loss": 0.3364,
      "step": 161900
    },
    {
      "epoch": 0.5330700888450148,
      "grad_norm": 63.380889892578125,
      "learning_rate": 0.00014007897334649555,
      "loss": 0.2723,
      "step": 162000
    },
    {
      "epoch": 0.5333991444554129,
      "grad_norm": 0.0002651155518833548,
      "learning_rate": 0.0001399802566633761,
      "loss": 0.2732,
      "step": 162100
    },
    {
      "epoch": 0.5337282000658111,
      "grad_norm": 0.0003566796949598938,
      "learning_rate": 0.00013988153998025665,
      "loss": 0.2865,
      "step": 162200
    },
    {
      "epoch": 0.5340572556762093,
      "grad_norm": 0.4673977196216583,
      "learning_rate": 0.0001397828232971372,
      "loss": 0.4247,
      "step": 162300
    },
    {
      "epoch": 0.5343863112866074,
      "grad_norm": 0.0005499051185324788,
      "learning_rate": 0.00013968410661401774,
      "loss": 0.2312,
      "step": 162400
    },
    {
      "epoch": 0.5347153668970056,
      "grad_norm": 0.0004068658745381981,
      "learning_rate": 0.0001395853899308983,
      "loss": 0.3631,
      "step": 162500
    },
    {
      "epoch": 0.5350444225074038,
      "grad_norm": 15.906746864318848,
      "learning_rate": 0.00013948667324777887,
      "loss": 0.2994,
      "step": 162600
    },
    {
      "epoch": 0.5353734781178019,
      "grad_norm": 2.318594455718994,
      "learning_rate": 0.00013938795656465942,
      "loss": 0.2912,
      "step": 162700
    },
    {
      "epoch": 0.5357025337282001,
      "grad_norm": 12.01951789855957,
      "learning_rate": 0.00013928923988153997,
      "loss": 0.3797,
      "step": 162800
    },
    {
      "epoch": 0.5360315893385982,
      "grad_norm": 0.000365124229574576,
      "learning_rate": 0.00013919052319842052,
      "loss": 0.4064,
      "step": 162900
    },
    {
      "epoch": 0.5363606449489964,
      "grad_norm": 0.00026757968589663506,
      "learning_rate": 0.00013909180651530106,
      "loss": 0.2818,
      "step": 163000
    },
    {
      "epoch": 0.5366897005593946,
      "grad_norm": 36.61848831176758,
      "learning_rate": 0.00013899308983218164,
      "loss": 0.4515,
      "step": 163100
    },
    {
      "epoch": 0.5370187561697927,
      "grad_norm": 0.00045555413817055523,
      "learning_rate": 0.00013889437314906216,
      "loss": 0.5964,
      "step": 163200
    },
    {
      "epoch": 0.5373478117801909,
      "grad_norm": 0.0001680304267210886,
      "learning_rate": 0.00013879565646594274,
      "loss": 0.3649,
      "step": 163300
    },
    {
      "epoch": 0.5376768673905891,
      "grad_norm": 22.897926330566406,
      "learning_rate": 0.00013869693978282329,
      "loss": 0.3142,
      "step": 163400
    },
    {
      "epoch": 0.5380059230009872,
      "grad_norm": 1.857029676437378,
      "learning_rate": 0.00013859822309970384,
      "loss": 0.3648,
      "step": 163500
    },
    {
      "epoch": 0.5383349786113854,
      "grad_norm": 0.06875156611204147,
      "learning_rate": 0.00013849950641658438,
      "loss": 0.2964,
      "step": 163600
    },
    {
      "epoch": 0.5386640342217834,
      "grad_norm": 1.3623969554901123,
      "learning_rate": 0.00013840078973346493,
      "loss": 0.3694,
      "step": 163700
    },
    {
      "epoch": 0.5389930898321816,
      "grad_norm": 0.0008803266100585461,
      "learning_rate": 0.0001383020730503455,
      "loss": 0.4574,
      "step": 163800
    },
    {
      "epoch": 0.5393221454425798,
      "grad_norm": 0.004824500065296888,
      "learning_rate": 0.00013820335636722606,
      "loss": 0.2636,
      "step": 163900
    },
    {
      "epoch": 0.5396512010529779,
      "grad_norm": 0.004433961119502783,
      "learning_rate": 0.0001381046396841066,
      "loss": 0.182,
      "step": 164000
    },
    {
      "epoch": 0.5399802566633761,
      "grad_norm": 0.0006658858619630337,
      "learning_rate": 0.00013800592300098715,
      "loss": 0.3534,
      "step": 164100
    },
    {
      "epoch": 0.5403093122737743,
      "grad_norm": 0.0004783332988154143,
      "learning_rate": 0.0001379072063178677,
      "loss": 0.2882,
      "step": 164200
    },
    {
      "epoch": 0.5406383678841724,
      "grad_norm": 0.0008588390192016959,
      "learning_rate": 0.00013780848963474825,
      "loss": 0.4568,
      "step": 164300
    },
    {
      "epoch": 0.5409674234945706,
      "grad_norm": 0.0006955803255550563,
      "learning_rate": 0.0001377097729516288,
      "loss": 0.4104,
      "step": 164400
    },
    {
      "epoch": 0.5412964791049687,
      "grad_norm": 43.536834716796875,
      "learning_rate": 0.00013761105626850935,
      "loss": 0.3903,
      "step": 164500
    },
    {
      "epoch": 0.5416255347153669,
      "grad_norm": 0.001494249445386231,
      "learning_rate": 0.00013751233958538993,
      "loss": 0.4839,
      "step": 164600
    },
    {
      "epoch": 0.5419545903257651,
      "grad_norm": 0.00307015934959054,
      "learning_rate": 0.00013741362290227047,
      "loss": 0.4535,
      "step": 164700
    },
    {
      "epoch": 0.5422836459361632,
      "grad_norm": 9.387218597112224e-05,
      "learning_rate": 0.00013731490621915102,
      "loss": 0.5663,
      "step": 164800
    },
    {
      "epoch": 0.5426127015465614,
      "grad_norm": 0.009470880962908268,
      "learning_rate": 0.00013721618953603157,
      "loss": 0.3121,
      "step": 164900
    },
    {
      "epoch": 0.5429417571569596,
      "grad_norm": 0.0009212239529006183,
      "learning_rate": 0.00013711747285291212,
      "loss": 0.3163,
      "step": 165000
    },
    {
      "epoch": 0.5432708127673577,
      "grad_norm": 0.004095018375664949,
      "learning_rate": 0.0001370187561697927,
      "loss": 0.1902,
      "step": 165100
    },
    {
      "epoch": 0.5435998683777559,
      "grad_norm": 0.00023777158639859408,
      "learning_rate": 0.00013692003948667322,
      "loss": 0.3291,
      "step": 165200
    },
    {
      "epoch": 0.543928923988154,
      "grad_norm": 13.731278419494629,
      "learning_rate": 0.0001368213228035538,
      "loss": 0.3909,
      "step": 165300
    },
    {
      "epoch": 0.5442579795985522,
      "grad_norm": 0.030176427215337753,
      "learning_rate": 0.00013672260612043434,
      "loss": 0.2287,
      "step": 165400
    },
    {
      "epoch": 0.5445870352089504,
      "grad_norm": 0.11041589081287384,
      "learning_rate": 0.0001366238894373149,
      "loss": 0.3061,
      "step": 165500
    },
    {
      "epoch": 0.5449160908193484,
      "grad_norm": 0.062030743807554245,
      "learning_rate": 0.00013652517275419544,
      "loss": 0.5217,
      "step": 165600
    },
    {
      "epoch": 0.5452451464297466,
      "grad_norm": 37.19867706298828,
      "learning_rate": 0.000136426456071076,
      "loss": 0.3516,
      "step": 165700
    },
    {
      "epoch": 0.5455742020401447,
      "grad_norm": 0.0012380954576656222,
      "learning_rate": 0.00013632773938795657,
      "loss": 0.3222,
      "step": 165800
    },
    {
      "epoch": 0.5459032576505429,
      "grad_norm": 122.86498260498047,
      "learning_rate": 0.00013622902270483711,
      "loss": 0.3324,
      "step": 165900
    },
    {
      "epoch": 0.5462323132609411,
      "grad_norm": 0.0006504020420834422,
      "learning_rate": 0.00013613030602171766,
      "loss": 0.4424,
      "step": 166000
    },
    {
      "epoch": 0.5465613688713392,
      "grad_norm": 0.0006755213253200054,
      "learning_rate": 0.0001360315893385982,
      "loss": 0.5119,
      "step": 166100
    },
    {
      "epoch": 0.5468904244817374,
      "grad_norm": 59.692527770996094,
      "learning_rate": 0.00013593287265547876,
      "loss": 0.3966,
      "step": 166200
    },
    {
      "epoch": 0.5472194800921356,
      "grad_norm": 0.0025657927617430687,
      "learning_rate": 0.0001358341559723593,
      "loss": 0.3553,
      "step": 166300
    },
    {
      "epoch": 0.5475485357025337,
      "grad_norm": 15.264124870300293,
      "learning_rate": 0.00013573543928923989,
      "loss": 0.3227,
      "step": 166400
    },
    {
      "epoch": 0.5478775913129319,
      "grad_norm": 125.59123229980469,
      "learning_rate": 0.0001356367226061204,
      "loss": 0.3375,
      "step": 166500
    },
    {
      "epoch": 0.54820664692333,
      "grad_norm": 0.02010219730436802,
      "learning_rate": 0.00013553800592300098,
      "loss": 0.4378,
      "step": 166600
    },
    {
      "epoch": 0.5485357025337282,
      "grad_norm": 0.20216745138168335,
      "learning_rate": 0.00013543928923988153,
      "loss": 0.4784,
      "step": 166700
    },
    {
      "epoch": 0.5488647581441264,
      "grad_norm": 0.13161605596542358,
      "learning_rate": 0.00013534057255676208,
      "loss": 0.264,
      "step": 166800
    },
    {
      "epoch": 0.5491938137545245,
      "grad_norm": 0.000633533694781363,
      "learning_rate": 0.00013524185587364263,
      "loss": 0.2125,
      "step": 166900
    },
    {
      "epoch": 0.5495228693649227,
      "grad_norm": 0.0007056702743284404,
      "learning_rate": 0.00013514313919052318,
      "loss": 0.3476,
      "step": 167000
    },
    {
      "epoch": 0.5498519249753209,
      "grad_norm": 0.0028238731902092695,
      "learning_rate": 0.00013504442250740375,
      "loss": 0.4478,
      "step": 167100
    },
    {
      "epoch": 0.550180980585719,
      "grad_norm": 12.417311668395996,
      "learning_rate": 0.0001349457058242843,
      "loss": 0.2692,
      "step": 167200
    },
    {
      "epoch": 0.5505100361961172,
      "grad_norm": 0.001350112957879901,
      "learning_rate": 0.00013484698914116485,
      "loss": 0.3495,
      "step": 167300
    },
    {
      "epoch": 0.5508390918065152,
      "grad_norm": 0.00021318045037332922,
      "learning_rate": 0.0001347482724580454,
      "loss": 0.2588,
      "step": 167400
    },
    {
      "epoch": 0.5511681474169134,
      "grad_norm": 0.0010621119290590286,
      "learning_rate": 0.00013464955577492595,
      "loss": 0.2032,
      "step": 167500
    },
    {
      "epoch": 0.5514972030273116,
      "grad_norm": 0.05127539113163948,
      "learning_rate": 0.0001345508390918065,
      "loss": 0.297,
      "step": 167600
    },
    {
      "epoch": 0.5518262586377097,
      "grad_norm": 0.001446200767531991,
      "learning_rate": 0.00013445212240868705,
      "loss": 0.2984,
      "step": 167700
    },
    {
      "epoch": 0.5521553142481079,
      "grad_norm": 0.0019658608362078667,
      "learning_rate": 0.0001343534057255676,
      "loss": 0.3314,
      "step": 167800
    },
    {
      "epoch": 0.5524843698585061,
      "grad_norm": 0.007086275145411491,
      "learning_rate": 0.00013425468904244817,
      "loss": 0.538,
      "step": 167900
    },
    {
      "epoch": 0.5528134254689042,
      "grad_norm": 112.820068359375,
      "learning_rate": 0.00013415597235932872,
      "loss": 0.477,
      "step": 168000
    },
    {
      "epoch": 0.5531424810793024,
      "grad_norm": 0.00016515862080268562,
      "learning_rate": 0.00013405725567620927,
      "loss": 0.3201,
      "step": 168100
    },
    {
      "epoch": 0.5534715366897005,
      "grad_norm": 13.49088191986084,
      "learning_rate": 0.00013395853899308982,
      "loss": 0.267,
      "step": 168200
    },
    {
      "epoch": 0.5538005923000987,
      "grad_norm": 0.3079906702041626,
      "learning_rate": 0.00013385982230997037,
      "loss": 0.3637,
      "step": 168300
    },
    {
      "epoch": 0.5541296479104969,
      "grad_norm": 89.60987854003906,
      "learning_rate": 0.00013376110562685094,
      "loss": 0.3798,
      "step": 168400
    },
    {
      "epoch": 0.554458703520895,
      "grad_norm": 0.34787634015083313,
      "learning_rate": 0.00013366238894373147,
      "loss": 0.2139,
      "step": 168500
    },
    {
      "epoch": 0.5547877591312932,
      "grad_norm": 0.0004375796706881374,
      "learning_rate": 0.00013356367226061204,
      "loss": 0.3227,
      "step": 168600
    },
    {
      "epoch": 0.5551168147416914,
      "grad_norm": 0.006164475344121456,
      "learning_rate": 0.0001334649555774926,
      "loss": 0.5556,
      "step": 168700
    },
    {
      "epoch": 0.5554458703520895,
      "grad_norm": 0.0010853571584448218,
      "learning_rate": 0.00013336623889437314,
      "loss": 0.3374,
      "step": 168800
    },
    {
      "epoch": 0.5557749259624877,
      "grad_norm": 0.0025686079170554876,
      "learning_rate": 0.0001332675222112537,
      "loss": 0.2537,
      "step": 168900
    },
    {
      "epoch": 0.5561039815728858,
      "grad_norm": 91.92912292480469,
      "learning_rate": 0.00013316880552813424,
      "loss": 0.2851,
      "step": 169000
    },
    {
      "epoch": 0.556433037183284,
      "grad_norm": 0.0021402102429419756,
      "learning_rate": 0.0001330700888450148,
      "loss": 0.1308,
      "step": 169100
    },
    {
      "epoch": 0.5567620927936822,
      "grad_norm": 0.687972903251648,
      "learning_rate": 0.00013297137216189536,
      "loss": 0.4426,
      "step": 169200
    },
    {
      "epoch": 0.5570911484040802,
      "grad_norm": 20.26181411743164,
      "learning_rate": 0.0001328726554787759,
      "loss": 0.4809,
      "step": 169300
    },
    {
      "epoch": 0.5574202040144784,
      "grad_norm": 6.230592250823975,
      "learning_rate": 0.00013277393879565646,
      "loss": 0.4554,
      "step": 169400
    },
    {
      "epoch": 0.5577492596248766,
      "grad_norm": 60.078712463378906,
      "learning_rate": 0.000132675222112537,
      "loss": 0.5159,
      "step": 169500
    },
    {
      "epoch": 0.5580783152352747,
      "grad_norm": 0.02038111910223961,
      "learning_rate": 0.00013257650542941756,
      "loss": 0.3701,
      "step": 169600
    },
    {
      "epoch": 0.5584073708456729,
      "grad_norm": 0.0008808726561255753,
      "learning_rate": 0.0001324777887462981,
      "loss": 0.173,
      "step": 169700
    },
    {
      "epoch": 0.558736426456071,
      "grad_norm": 0.0014561336720362306,
      "learning_rate": 0.00013237907206317865,
      "loss": 0.2938,
      "step": 169800
    },
    {
      "epoch": 0.5590654820664692,
      "grad_norm": 0.0002842359826900065,
      "learning_rate": 0.00013228035538005923,
      "loss": 0.3966,
      "step": 169900
    },
    {
      "epoch": 0.5593945376768674,
      "grad_norm": 5.214775228523649e-06,
      "learning_rate": 0.00013218163869693978,
      "loss": 0.2466,
      "step": 170000
    },
    {
      "epoch": 0.5597235932872655,
      "grad_norm": 2.330912320758216e-05,
      "learning_rate": 0.00013208292201382033,
      "loss": 0.482,
      "step": 170100
    },
    {
      "epoch": 0.5600526488976637,
      "grad_norm": 0.0020572985522449017,
      "learning_rate": 0.00013198420533070088,
      "loss": 0.328,
      "step": 170200
    },
    {
      "epoch": 0.5603817045080619,
      "grad_norm": 9.2903028416913e-05,
      "learning_rate": 0.00013188548864758143,
      "loss": 0.3403,
      "step": 170300
    },
    {
      "epoch": 0.56071076011846,
      "grad_norm": 1.8031803369522095,
      "learning_rate": 0.000131786771964462,
      "loss": 0.3129,
      "step": 170400
    },
    {
      "epoch": 0.5610398157288582,
      "grad_norm": 0.0010111398296430707,
      "learning_rate": 0.00013168805528134252,
      "loss": 0.3011,
      "step": 170500
    },
    {
      "epoch": 0.5613688713392563,
      "grad_norm": 0.008671466261148453,
      "learning_rate": 0.0001315893385982231,
      "loss": 0.3652,
      "step": 170600
    },
    {
      "epoch": 0.5616979269496545,
      "grad_norm": 0.0041673495434224606,
      "learning_rate": 0.00013149062191510365,
      "loss": 0.2939,
      "step": 170700
    },
    {
      "epoch": 0.5620269825600527,
      "grad_norm": 0.014837919734418392,
      "learning_rate": 0.0001313919052319842,
      "loss": 0.2552,
      "step": 170800
    },
    {
      "epoch": 0.5623560381704508,
      "grad_norm": 0.0010812965920194983,
      "learning_rate": 0.00013129318854886475,
      "loss": 0.4351,
      "step": 170900
    },
    {
      "epoch": 0.562685093780849,
      "grad_norm": 29.47694206237793,
      "learning_rate": 0.0001311944718657453,
      "loss": 0.3328,
      "step": 171000
    },
    {
      "epoch": 0.5630141493912472,
      "grad_norm": 0.001483887666836381,
      "learning_rate": 0.00013109575518262584,
      "loss": 0.3135,
      "step": 171100
    },
    {
      "epoch": 0.5633432050016453,
      "grad_norm": 0.0021385615691542625,
      "learning_rate": 0.00013099703849950642,
      "loss": 0.2618,
      "step": 171200
    },
    {
      "epoch": 0.5636722606120435,
      "grad_norm": 0.00048380717635154724,
      "learning_rate": 0.00013089832181638694,
      "loss": 0.1665,
      "step": 171300
    },
    {
      "epoch": 0.5640013162224415,
      "grad_norm": 76.88893127441406,
      "learning_rate": 0.00013079960513326752,
      "loss": 0.3649,
      "step": 171400
    },
    {
      "epoch": 0.5643303718328397,
      "grad_norm": 0.002410578541457653,
      "learning_rate": 0.00013070088845014806,
      "loss": 0.3497,
      "step": 171500
    },
    {
      "epoch": 0.5646594274432379,
      "grad_norm": 0.008875289000570774,
      "learning_rate": 0.00013060217176702861,
      "loss": 0.3659,
      "step": 171600
    },
    {
      "epoch": 0.564988483053636,
      "grad_norm": 0.00028739668778143823,
      "learning_rate": 0.00013050345508390916,
      "loss": 0.1949,
      "step": 171700
    },
    {
      "epoch": 0.5653175386640342,
      "grad_norm": 0.007728511933237314,
      "learning_rate": 0.0001304047384007897,
      "loss": 0.2488,
      "step": 171800
    },
    {
      "epoch": 0.5656465942744324,
      "grad_norm": 0.040750134736299515,
      "learning_rate": 0.0001303060217176703,
      "loss": 0.4859,
      "step": 171900
    },
    {
      "epoch": 0.5659756498848305,
      "grad_norm": 0.009083283133804798,
      "learning_rate": 0.00013020730503455084,
      "loss": 0.3315,
      "step": 172000
    },
    {
      "epoch": 0.5663047054952287,
      "grad_norm": 27.83441734313965,
      "learning_rate": 0.00013010858835143138,
      "loss": 0.4204,
      "step": 172100
    },
    {
      "epoch": 0.5666337611056268,
      "grad_norm": 0.030015315860509872,
      "learning_rate": 0.00013000987166831193,
      "loss": 0.1921,
      "step": 172200
    },
    {
      "epoch": 0.566962816716025,
      "grad_norm": 0.00235529406927526,
      "learning_rate": 0.00012991115498519248,
      "loss": 0.3122,
      "step": 172300
    },
    {
      "epoch": 0.5672918723264232,
      "grad_norm": 0.006960020866245031,
      "learning_rate": 0.00012981243830207306,
      "loss": 0.3163,
      "step": 172400
    },
    {
      "epoch": 0.5676209279368213,
      "grad_norm": 0.00044812055421061814,
      "learning_rate": 0.00012971372161895358,
      "loss": 0.3468,
      "step": 172500
    },
    {
      "epoch": 0.5679499835472195,
      "grad_norm": 0.007199075538665056,
      "learning_rate": 0.00012961500493583416,
      "loss": 0.2912,
      "step": 172600
    },
    {
      "epoch": 0.5682790391576177,
      "grad_norm": 8.62793531268835e-05,
      "learning_rate": 0.0001295162882527147,
      "loss": 0.3186,
      "step": 172700
    },
    {
      "epoch": 0.5686080947680158,
      "grad_norm": 0.00012672743469011039,
      "learning_rate": 0.00012941757156959525,
      "loss": 0.1835,
      "step": 172800
    },
    {
      "epoch": 0.568937150378414,
      "grad_norm": 0.010715585201978683,
      "learning_rate": 0.0001293188548864758,
      "loss": 0.2942,
      "step": 172900
    },
    {
      "epoch": 0.5692662059888121,
      "grad_norm": 0.0002320933563169092,
      "learning_rate": 0.00012922013820335635,
      "loss": 0.3194,
      "step": 173000
    },
    {
      "epoch": 0.5695952615992103,
      "grad_norm": 0.0003646288823802024,
      "learning_rate": 0.0001291214215202369,
      "loss": 0.2541,
      "step": 173100
    },
    {
      "epoch": 0.5699243172096085,
      "grad_norm": 0.0004978598444722593,
      "learning_rate": 0.00012902270483711748,
      "loss": 0.3335,
      "step": 173200
    },
    {
      "epoch": 0.5702533728200065,
      "grad_norm": 0.003996431827545166,
      "learning_rate": 0.000128923988153998,
      "loss": 0.1265,
      "step": 173300
    },
    {
      "epoch": 0.5705824284304047,
      "grad_norm": 0.007535136770457029,
      "learning_rate": 0.00012882527147087857,
      "loss": 0.1575,
      "step": 173400
    },
    {
      "epoch": 0.5709114840408029,
      "grad_norm": 70.00090789794922,
      "learning_rate": 0.00012872655478775912,
      "loss": 0.2996,
      "step": 173500
    },
    {
      "epoch": 0.571240539651201,
      "grad_norm": 0.0031802637968212366,
      "learning_rate": 0.00012862783810463967,
      "loss": 0.281,
      "step": 173600
    },
    {
      "epoch": 0.5715695952615992,
      "grad_norm": 0.001032735570333898,
      "learning_rate": 0.00012852912142152022,
      "loss": 0.4765,
      "step": 173700
    },
    {
      "epoch": 0.5718986508719973,
      "grad_norm": 0.0017397527117282152,
      "learning_rate": 0.00012843040473840077,
      "loss": 0.3604,
      "step": 173800
    },
    {
      "epoch": 0.5722277064823955,
      "grad_norm": 0.0003918526053894311,
      "learning_rate": 0.00012833168805528134,
      "loss": 0.2106,
      "step": 173900
    },
    {
      "epoch": 0.5725567620927937,
      "grad_norm": 0.0002992037043441087,
      "learning_rate": 0.0001282329713721619,
      "loss": 0.3858,
      "step": 174000
    },
    {
      "epoch": 0.5728858177031918,
      "grad_norm": 28.93830108642578,
      "learning_rate": 0.00012813425468904244,
      "loss": 0.5368,
      "step": 174100
    },
    {
      "epoch": 0.57321487331359,
      "grad_norm": 122.91854858398438,
      "learning_rate": 0.000128035538005923,
      "loss": 0.3939,
      "step": 174200
    },
    {
      "epoch": 0.5735439289239882,
      "grad_norm": 0.0004543204267974943,
      "learning_rate": 0.00012793682132280354,
      "loss": 0.2799,
      "step": 174300
    },
    {
      "epoch": 0.5738729845343863,
      "grad_norm": 0.2776713967323303,
      "learning_rate": 0.00012783810463968412,
      "loss": 0.4655,
      "step": 174400
    },
    {
      "epoch": 0.5742020401447845,
      "grad_norm": 0.00019285778398625553,
      "learning_rate": 0.00012773938795656464,
      "loss": 0.2796,
      "step": 174500
    },
    {
      "epoch": 0.5745310957551826,
      "grad_norm": 0.0002427645231364295,
      "learning_rate": 0.0001276406712734452,
      "loss": 0.4594,
      "step": 174600
    },
    {
      "epoch": 0.5748601513655808,
      "grad_norm": 0.004377498757094145,
      "learning_rate": 0.00012754195459032576,
      "loss": 0.3598,
      "step": 174700
    },
    {
      "epoch": 0.575189206975979,
      "grad_norm": 0.0018778161611407995,
      "learning_rate": 0.0001274432379072063,
      "loss": 0.3738,
      "step": 174800
    },
    {
      "epoch": 0.5755182625863771,
      "grad_norm": 0.07888903468847275,
      "learning_rate": 0.00012734452122408686,
      "loss": 0.2884,
      "step": 174900
    },
    {
      "epoch": 0.5758473181967753,
      "grad_norm": 0.0005647761863656342,
      "learning_rate": 0.0001272458045409674,
      "loss": 0.3021,
      "step": 175000
    },
    {
      "epoch": 0.5761763738071735,
      "grad_norm": 0.013299934566020966,
      "learning_rate": 0.00012714708785784796,
      "loss": 0.3702,
      "step": 175100
    },
    {
      "epoch": 0.5765054294175715,
      "grad_norm": 5.383449554443359,
      "learning_rate": 0.00012704837117472853,
      "loss": 0.1906,
      "step": 175200
    },
    {
      "epoch": 0.5768344850279697,
      "grad_norm": 0.003292744979262352,
      "learning_rate": 0.00012694965449160906,
      "loss": 0.3836,
      "step": 175300
    },
    {
      "epoch": 0.5771635406383678,
      "grad_norm": 0.0006364235305227339,
      "learning_rate": 0.00012685093780848963,
      "loss": 0.4226,
      "step": 175400
    },
    {
      "epoch": 0.577492596248766,
      "grad_norm": 0.032136425375938416,
      "learning_rate": 0.00012675222112537018,
      "loss": 0.2713,
      "step": 175500
    },
    {
      "epoch": 0.5778216518591642,
      "grad_norm": 0.0007218316313810647,
      "learning_rate": 0.00012665350444225073,
      "loss": 0.3083,
      "step": 175600
    },
    {
      "epoch": 0.5781507074695623,
      "grad_norm": 0.008747439831495285,
      "learning_rate": 0.00012655478775913128,
      "loss": 0.2109,
      "step": 175700
    },
    {
      "epoch": 0.5784797630799605,
      "grad_norm": 0.00022467329108621925,
      "learning_rate": 0.00012645607107601183,
      "loss": 0.3694,
      "step": 175800
    },
    {
      "epoch": 0.5788088186903587,
      "grad_norm": 0.002092986600473523,
      "learning_rate": 0.0001263573543928924,
      "loss": 0.3996,
      "step": 175900
    },
    {
      "epoch": 0.5791378743007568,
      "grad_norm": 0.0006268292781896889,
      "learning_rate": 0.00012625863770977295,
      "loss": 0.348,
      "step": 176000
    },
    {
      "epoch": 0.579466929911155,
      "grad_norm": 0.0009000686695799232,
      "learning_rate": 0.0001261599210266535,
      "loss": 0.4419,
      "step": 176100
    },
    {
      "epoch": 0.5797959855215531,
      "grad_norm": 0.0026464806869626045,
      "learning_rate": 0.00012606120434353405,
      "loss": 0.4978,
      "step": 176200
    },
    {
      "epoch": 0.5801250411319513,
      "grad_norm": 0.408829927444458,
      "learning_rate": 0.0001259624876604146,
      "loss": 0.2754,
      "step": 176300
    },
    {
      "epoch": 0.5804540967423495,
      "grad_norm": 0.022992534562945366,
      "learning_rate": 0.00012586377097729515,
      "loss": 0.3217,
      "step": 176400
    },
    {
      "epoch": 0.5807831523527476,
      "grad_norm": 0.0027183806523680687,
      "learning_rate": 0.0001257650542941757,
      "loss": 0.1341,
      "step": 176500
    },
    {
      "epoch": 0.5811122079631458,
      "grad_norm": 0.00021774675406049937,
      "learning_rate": 0.00012566633761105624,
      "loss": 0.2567,
      "step": 176600
    },
    {
      "epoch": 0.581441263573544,
      "grad_norm": 0.00021305262634996325,
      "learning_rate": 0.00012556762092793682,
      "loss": 0.4018,
      "step": 176700
    },
    {
      "epoch": 0.5817703191839421,
      "grad_norm": 0.013868103735148907,
      "learning_rate": 0.00012546890424481737,
      "loss": 0.2293,
      "step": 176800
    },
    {
      "epoch": 0.5820993747943403,
      "grad_norm": 0.029917556792497635,
      "learning_rate": 0.00012537018756169792,
      "loss": 0.4044,
      "step": 176900
    },
    {
      "epoch": 0.5824284304047384,
      "grad_norm": 0.0022327178157866,
      "learning_rate": 0.00012527147087857847,
      "loss": 0.3144,
      "step": 177000
    },
    {
      "epoch": 0.5827574860151365,
      "grad_norm": 0.001461258390918374,
      "learning_rate": 0.00012517275419545902,
      "loss": 0.5858,
      "step": 177100
    },
    {
      "epoch": 0.5830865416255347,
      "grad_norm": 0.006755993235856295,
      "learning_rate": 0.0001250740375123396,
      "loss": 0.5494,
      "step": 177200
    },
    {
      "epoch": 0.5834155972359328,
      "grad_norm": 0.014781365171074867,
      "learning_rate": 0.0001249753208292201,
      "loss": 0.575,
      "step": 177300
    },
    {
      "epoch": 0.583744652846331,
      "grad_norm": 0.006238642614334822,
      "learning_rate": 0.0001248766041461007,
      "loss": 0.2032,
      "step": 177400
    },
    {
      "epoch": 0.5840737084567292,
      "grad_norm": 0.009561213664710522,
      "learning_rate": 0.00012477788746298124,
      "loss": 0.4626,
      "step": 177500
    },
    {
      "epoch": 0.5844027640671273,
      "grad_norm": 8.08018684387207,
      "learning_rate": 0.00012467917077986179,
      "loss": 0.345,
      "step": 177600
    },
    {
      "epoch": 0.5847318196775255,
      "grad_norm": 0.01730055920779705,
      "learning_rate": 0.00012458045409674234,
      "loss": 0.1961,
      "step": 177700
    },
    {
      "epoch": 0.5850608752879236,
      "grad_norm": 0.001052508014254272,
      "learning_rate": 0.00012448173741362288,
      "loss": 0.42,
      "step": 177800
    },
    {
      "epoch": 0.5853899308983218,
      "grad_norm": 0.09584086388349533,
      "learning_rate": 0.00012438302073050346,
      "loss": 0.203,
      "step": 177900
    },
    {
      "epoch": 0.58571898650872,
      "grad_norm": 0.0017661628080531955,
      "learning_rate": 0.000124284304047384,
      "loss": 0.2039,
      "step": 178000
    },
    {
      "epoch": 0.5860480421191181,
      "grad_norm": 0.04869190603494644,
      "learning_rate": 0.00012418558736426453,
      "loss": 0.3713,
      "step": 178100
    },
    {
      "epoch": 0.5863770977295163,
      "grad_norm": 71.05596923828125,
      "learning_rate": 0.0001240868706811451,
      "loss": 0.4015,
      "step": 178200
    },
    {
      "epoch": 0.5867061533399145,
      "grad_norm": 19.23663902282715,
      "learning_rate": 0.00012398815399802565,
      "loss": 0.2646,
      "step": 178300
    },
    {
      "epoch": 0.5870352089503126,
      "grad_norm": 0.03035648539662361,
      "learning_rate": 0.0001238894373149062,
      "loss": 0.2549,
      "step": 178400
    },
    {
      "epoch": 0.5873642645607108,
      "grad_norm": 0.03370398283004761,
      "learning_rate": 0.00012379072063178675,
      "loss": 0.3366,
      "step": 178500
    },
    {
      "epoch": 0.5876933201711089,
      "grad_norm": 0.10962804406881332,
      "learning_rate": 0.0001236920039486673,
      "loss": 0.3577,
      "step": 178600
    },
    {
      "epoch": 0.5880223757815071,
      "grad_norm": 55.22108459472656,
      "learning_rate": 0.00012359328726554788,
      "loss": 0.407,
      "step": 178700
    },
    {
      "epoch": 0.5883514313919053,
      "grad_norm": 1.7858185768127441,
      "learning_rate": 0.00012349457058242843,
      "loss": 0.3684,
      "step": 178800
    },
    {
      "epoch": 0.5886804870023034,
      "grad_norm": 0.0006365796434693038,
      "learning_rate": 0.00012339585389930897,
      "loss": 0.2687,
      "step": 178900
    },
    {
      "epoch": 0.5890095426127016,
      "grad_norm": 16.076875686645508,
      "learning_rate": 0.00012329713721618952,
      "loss": 0.2338,
      "step": 179000
    },
    {
      "epoch": 0.5893385982230998,
      "grad_norm": 0.00037706157309003174,
      "learning_rate": 0.00012319842053307007,
      "loss": 0.2031,
      "step": 179100
    },
    {
      "epoch": 0.5896676538334978,
      "grad_norm": 0.002410197164863348,
      "learning_rate": 0.00012309970384995065,
      "loss": 0.3566,
      "step": 179200
    },
    {
      "epoch": 0.589996709443896,
      "grad_norm": 0.0033630961552262306,
      "learning_rate": 0.00012300098716683117,
      "loss": 0.3518,
      "step": 179300
    },
    {
      "epoch": 0.5903257650542941,
      "grad_norm": 0.0033174247946590185,
      "learning_rate": 0.00012290227048371175,
      "loss": 0.1945,
      "step": 179400
    },
    {
      "epoch": 0.5906548206646923,
      "grad_norm": 4.562010288238525,
      "learning_rate": 0.0001228035538005923,
      "loss": 0.2657,
      "step": 179500
    },
    {
      "epoch": 0.5909838762750905,
      "grad_norm": 0.0029675173573195934,
      "learning_rate": 0.00012270483711747284,
      "loss": 0.4358,
      "step": 179600
    },
    {
      "epoch": 0.5913129318854886,
      "grad_norm": 0.7603601813316345,
      "learning_rate": 0.0001226061204343534,
      "loss": 0.4717,
      "step": 179700
    },
    {
      "epoch": 0.5916419874958868,
      "grad_norm": 69.95272064208984,
      "learning_rate": 0.00012250740375123394,
      "loss": 0.2784,
      "step": 179800
    },
    {
      "epoch": 0.591971043106285,
      "grad_norm": 0.00016846814833115786,
      "learning_rate": 0.0001224086870681145,
      "loss": 0.3236,
      "step": 179900
    },
    {
      "epoch": 0.5923000987166831,
      "grad_norm": 0.013324473984539509,
      "learning_rate": 0.00012230997038499507,
      "loss": 0.1929,
      "step": 180000
    },
    {
      "epoch": 0.5926291543270813,
      "grad_norm": 17.35292625427246,
      "learning_rate": 0.0001222112537018756,
      "loss": 0.3905,
      "step": 180100
    },
    {
      "epoch": 0.5929582099374794,
      "grad_norm": 0.5998227596282959,
      "learning_rate": 0.00012211253701875616,
      "loss": 0.3053,
      "step": 180200
    },
    {
      "epoch": 0.5932872655478776,
      "grad_norm": 0.0012825530720874667,
      "learning_rate": 0.00012201382033563671,
      "loss": 0.3522,
      "step": 180300
    },
    {
      "epoch": 0.5936163211582758,
      "grad_norm": 0.00027794315246865153,
      "learning_rate": 0.00012191510365251727,
      "loss": 0.2464,
      "step": 180400
    },
    {
      "epoch": 0.5939453767686739,
      "grad_norm": 0.006528634112328291,
      "learning_rate": 0.00012181638696939781,
      "loss": 0.4288,
      "step": 180500
    },
    {
      "epoch": 0.5942744323790721,
      "grad_norm": 0.00015780377725604922,
      "learning_rate": 0.00012171767028627837,
      "loss": 0.256,
      "step": 180600
    },
    {
      "epoch": 0.5946034879894703,
      "grad_norm": 0.05464373156428337,
      "learning_rate": 0.00012161895360315892,
      "loss": 0.4739,
      "step": 180700
    },
    {
      "epoch": 0.5949325435998684,
      "grad_norm": 2.7526962757110596,
      "learning_rate": 0.00012152023692003948,
      "loss": 0.3181,
      "step": 180800
    },
    {
      "epoch": 0.5952615992102666,
      "grad_norm": 0.0020426521077752113,
      "learning_rate": 0.00012142152023692002,
      "loss": 0.473,
      "step": 180900
    },
    {
      "epoch": 0.5955906548206646,
      "grad_norm": 0.6672980189323425,
      "learning_rate": 0.00012132280355380058,
      "loss": 0.2563,
      "step": 181000
    },
    {
      "epoch": 0.5959197104310628,
      "grad_norm": 0.056748028844594955,
      "learning_rate": 0.00012122408687068113,
      "loss": 0.2667,
      "step": 181100
    },
    {
      "epoch": 0.596248766041461,
      "grad_norm": 0.002526951488107443,
      "learning_rate": 0.00012112537018756169,
      "loss": 0.2695,
      "step": 181200
    },
    {
      "epoch": 0.5965778216518591,
      "grad_norm": 0.0029600043781101704,
      "learning_rate": 0.00012102665350444223,
      "loss": 0.2478,
      "step": 181300
    },
    {
      "epoch": 0.5969068772622573,
      "grad_norm": 0.005402833689004183,
      "learning_rate": 0.00012092793682132279,
      "loss": 0.3034,
      "step": 181400
    },
    {
      "epoch": 0.5972359328726555,
      "grad_norm": 39.13494110107422,
      "learning_rate": 0.00012082922013820335,
      "loss": 0.3512,
      "step": 181500
    },
    {
      "epoch": 0.5975649884830536,
      "grad_norm": 0.0011797716142609715,
      "learning_rate": 0.0001207305034550839,
      "loss": 0.3609,
      "step": 181600
    },
    {
      "epoch": 0.5978940440934518,
      "grad_norm": 21.507352828979492,
      "learning_rate": 0.00012063178677196445,
      "loss": 0.2811,
      "step": 181700
    },
    {
      "epoch": 0.5982230997038499,
      "grad_norm": 0.007020742632448673,
      "learning_rate": 0.000120533070088845,
      "loss": 0.4903,
      "step": 181800
    },
    {
      "epoch": 0.5985521553142481,
      "grad_norm": 0.005945986602455378,
      "learning_rate": 0.00012043435340572556,
      "loss": 0.261,
      "step": 181900
    },
    {
      "epoch": 0.5988812109246463,
      "grad_norm": 0.023541675880551338,
      "learning_rate": 0.00012033563672260611,
      "loss": 0.44,
      "step": 182000
    },
    {
      "epoch": 0.5992102665350444,
      "grad_norm": 70.4626693725586,
      "learning_rate": 0.00012023692003948666,
      "loss": 0.2896,
      "step": 182100
    },
    {
      "epoch": 0.5995393221454426,
      "grad_norm": 0.0011651755776256323,
      "learning_rate": 0.00012013820335636721,
      "loss": 0.2443,
      "step": 182200
    },
    {
      "epoch": 0.5998683777558408,
      "grad_norm": 0.0004639240796677768,
      "learning_rate": 0.00012003948667324777,
      "loss": 0.2637,
      "step": 182300
    },
    {
      "epoch": 0.6001974333662389,
      "grad_norm": 0.0027916040271520615,
      "learning_rate": 0.00011994076999012833,
      "loss": 0.1784,
      "step": 182400
    },
    {
      "epoch": 0.6005264889766371,
      "grad_norm": 0.002819563029333949,
      "learning_rate": 0.00011984205330700887,
      "loss": 0.468,
      "step": 182500
    },
    {
      "epoch": 0.6008555445870352,
      "grad_norm": 0.010858557187020779,
      "learning_rate": 0.00011974333662388943,
      "loss": 0.3949,
      "step": 182600
    },
    {
      "epoch": 0.6011846001974334,
      "grad_norm": 0.026676416397094727,
      "learning_rate": 0.00011964461994076998,
      "loss": 0.3574,
      "step": 182700
    },
    {
      "epoch": 0.6015136558078316,
      "grad_norm": 67.64378356933594,
      "learning_rate": 0.00011954590325765054,
      "loss": 0.1916,
      "step": 182800
    },
    {
      "epoch": 0.6018427114182296,
      "grad_norm": 49.895111083984375,
      "learning_rate": 0.00011944718657453108,
      "loss": 0.4737,
      "step": 182900
    },
    {
      "epoch": 0.6021717670286278,
      "grad_norm": 0.006542944349348545,
      "learning_rate": 0.00011934846989141164,
      "loss": 0.6221,
      "step": 183000
    },
    {
      "epoch": 0.602500822639026,
      "grad_norm": 0.009548609144985676,
      "learning_rate": 0.00011924975320829219,
      "loss": 0.3375,
      "step": 183100
    },
    {
      "epoch": 0.6028298782494241,
      "grad_norm": 0.0002806323755066842,
      "learning_rate": 0.00011915103652517275,
      "loss": 0.3514,
      "step": 183200
    },
    {
      "epoch": 0.6031589338598223,
      "grad_norm": 0.004513143561780453,
      "learning_rate": 0.00011905231984205329,
      "loss": 0.3286,
      "step": 183300
    },
    {
      "epoch": 0.6034879894702204,
      "grad_norm": 0.22697444260120392,
      "learning_rate": 0.00011895360315893385,
      "loss": 0.3519,
      "step": 183400
    },
    {
      "epoch": 0.6038170450806186,
      "grad_norm": 6.153747081756592,
      "learning_rate": 0.00011885488647581441,
      "loss": 0.1895,
      "step": 183500
    },
    {
      "epoch": 0.6041461006910168,
      "grad_norm": 0.00033142787287943065,
      "learning_rate": 0.00011875616979269496,
      "loss": 0.2382,
      "step": 183600
    },
    {
      "epoch": 0.6044751563014149,
      "grad_norm": 0.004406727384775877,
      "learning_rate": 0.00011865745310957551,
      "loss": 0.211,
      "step": 183700
    },
    {
      "epoch": 0.6048042119118131,
      "grad_norm": 0.005996942985802889,
      "learning_rate": 0.00011855873642645606,
      "loss": 0.2786,
      "step": 183800
    },
    {
      "epoch": 0.6051332675222113,
      "grad_norm": 0.014985454268753529,
      "learning_rate": 0.00011846001974333662,
      "loss": 0.4576,
      "step": 183900
    },
    {
      "epoch": 0.6054623231326094,
      "grad_norm": 0.0010021779453381896,
      "learning_rate": 0.00011836130306021717,
      "loss": 0.267,
      "step": 184000
    },
    {
      "epoch": 0.6057913787430076,
      "grad_norm": 0.0016960878856480122,
      "learning_rate": 0.00011826258637709772,
      "loss": 0.1593,
      "step": 184100
    },
    {
      "epoch": 0.6061204343534057,
      "grad_norm": 0.05695047974586487,
      "learning_rate": 0.00011816386969397827,
      "loss": 0.3977,
      "step": 184200
    },
    {
      "epoch": 0.6064494899638039,
      "grad_norm": 0.0004581852408591658,
      "learning_rate": 0.00011806515301085883,
      "loss": 0.2239,
      "step": 184300
    },
    {
      "epoch": 0.6067785455742021,
      "grad_norm": 0.002992659341543913,
      "learning_rate": 0.00011796643632773939,
      "loss": 0.191,
      "step": 184400
    },
    {
      "epoch": 0.6071076011846002,
      "grad_norm": 8.809091567993164,
      "learning_rate": 0.00011786771964461993,
      "loss": 0.1596,
      "step": 184500
    },
    {
      "epoch": 0.6074366567949984,
      "grad_norm": 0.0009393765940330923,
      "learning_rate": 0.00011776900296150047,
      "loss": 0.1468,
      "step": 184600
    },
    {
      "epoch": 0.6077657124053966,
      "grad_norm": 11.357967376708984,
      "learning_rate": 0.00011767028627838104,
      "loss": 0.2756,
      "step": 184700
    },
    {
      "epoch": 0.6080947680157947,
      "grad_norm": 28.771820068359375,
      "learning_rate": 0.0001175715695952616,
      "loss": 0.1511,
      "step": 184800
    },
    {
      "epoch": 0.6084238236261929,
      "grad_norm": 2.401968240737915,
      "learning_rate": 0.00011747285291214213,
      "loss": 0.2413,
      "step": 184900
    },
    {
      "epoch": 0.6087528792365909,
      "grad_norm": 0.06667441874742508,
      "learning_rate": 0.0001173741362290227,
      "loss": 0.3096,
      "step": 185000
    },
    {
      "epoch": 0.6090819348469891,
      "grad_norm": 0.0002191195817431435,
      "learning_rate": 0.00011727541954590325,
      "loss": 0.3689,
      "step": 185100
    },
    {
      "epoch": 0.6094109904573873,
      "grad_norm": 0.002185216872021556,
      "learning_rate": 0.00011717670286278381,
      "loss": 0.2624,
      "step": 185200
    },
    {
      "epoch": 0.6097400460677854,
      "grad_norm": 40.736419677734375,
      "learning_rate": 0.00011707798617966434,
      "loss": 0.4294,
      "step": 185300
    },
    {
      "epoch": 0.6100691016781836,
      "grad_norm": 0.002465731929987669,
      "learning_rate": 0.0001169792694965449,
      "loss": 0.4542,
      "step": 185400
    },
    {
      "epoch": 0.6103981572885818,
      "grad_norm": 0.001176572754047811,
      "learning_rate": 0.00011688055281342545,
      "loss": 0.341,
      "step": 185500
    },
    {
      "epoch": 0.6107272128989799,
      "grad_norm": 16.357769012451172,
      "learning_rate": 0.00011678183613030602,
      "loss": 0.3471,
      "step": 185600
    },
    {
      "epoch": 0.6110562685093781,
      "grad_norm": 0.0002831303863786161,
      "learning_rate": 0.00011668311944718655,
      "loss": 0.3925,
      "step": 185700
    },
    {
      "epoch": 0.6113853241197762,
      "grad_norm": 0.0014955996302887797,
      "learning_rate": 0.00011658440276406711,
      "loss": 0.407,
      "step": 185800
    },
    {
      "epoch": 0.6117143797301744,
      "grad_norm": 19.583681106567383,
      "learning_rate": 0.00011648568608094768,
      "loss": 0.2845,
      "step": 185900
    },
    {
      "epoch": 0.6120434353405726,
      "grad_norm": 32.306312561035156,
      "learning_rate": 0.00011638696939782822,
      "loss": 0.3045,
      "step": 186000
    },
    {
      "epoch": 0.6123724909509707,
      "grad_norm": 0.17277130484580994,
      "learning_rate": 0.00011628825271470877,
      "loss": 0.1602,
      "step": 186100
    },
    {
      "epoch": 0.6127015465613689,
      "grad_norm": 0.0003944546333514154,
      "learning_rate": 0.00011618953603158932,
      "loss": 0.373,
      "step": 186200
    },
    {
      "epoch": 0.6130306021717671,
      "grad_norm": 5.765069961547852,
      "learning_rate": 0.00011609081934846988,
      "loss": 0.1586,
      "step": 186300
    },
    {
      "epoch": 0.6133596577821652,
      "grad_norm": 0.06640765070915222,
      "learning_rate": 0.00011599210266535043,
      "loss": 0.3316,
      "step": 186400
    },
    {
      "epoch": 0.6136887133925634,
      "grad_norm": 0.00025604601250961423,
      "learning_rate": 0.00011589338598223098,
      "loss": 0.4091,
      "step": 186500
    },
    {
      "epoch": 0.6140177690029615,
      "grad_norm": 0.005509636830538511,
      "learning_rate": 0.00011579466929911153,
      "loss": 0.2031,
      "step": 186600
    },
    {
      "epoch": 0.6143468246133597,
      "grad_norm": 0.00022614649788010865,
      "learning_rate": 0.0001156959526159921,
      "loss": 0.2404,
      "step": 186700
    },
    {
      "epoch": 0.6146758802237579,
      "grad_norm": 16.339712142944336,
      "learning_rate": 0.00011559723593287266,
      "loss": 0.4016,
      "step": 186800
    },
    {
      "epoch": 0.6150049358341559,
      "grad_norm": 25.930282592773438,
      "learning_rate": 0.00011549851924975319,
      "loss": 0.361,
      "step": 186900
    },
    {
      "epoch": 0.6153339914445541,
      "grad_norm": 0.0009289846639148891,
      "learning_rate": 0.00011539980256663375,
      "loss": 0.3293,
      "step": 187000
    },
    {
      "epoch": 0.6156630470549523,
      "grad_norm": 0.0034504756331443787,
      "learning_rate": 0.0001153010858835143,
      "loss": 0.3114,
      "step": 187100
    },
    {
      "epoch": 0.6159921026653504,
      "grad_norm": 0.001986969728022814,
      "learning_rate": 0.00011520236920039486,
      "loss": 0.2183,
      "step": 187200
    },
    {
      "epoch": 0.6163211582757486,
      "grad_norm": 0.5524510741233826,
      "learning_rate": 0.0001151036525172754,
      "loss": 0.3412,
      "step": 187300
    },
    {
      "epoch": 0.6166502138861467,
      "grad_norm": 0.010300193913280964,
      "learning_rate": 0.00011500493583415596,
      "loss": 0.2628,
      "step": 187400
    },
    {
      "epoch": 0.6169792694965449,
      "grad_norm": 0.008574213832616806,
      "learning_rate": 0.00011490621915103651,
      "loss": 0.292,
      "step": 187500
    },
    {
      "epoch": 0.6173083251069431,
      "grad_norm": 1.8929744328488596e-05,
      "learning_rate": 0.00011480750246791707,
      "loss": 0.2885,
      "step": 187600
    },
    {
      "epoch": 0.6176373807173412,
      "grad_norm": 0.00192604202311486,
      "learning_rate": 0.00011470878578479761,
      "loss": 0.2064,
      "step": 187700
    },
    {
      "epoch": 0.6179664363277394,
      "grad_norm": 4.988923319615424e-05,
      "learning_rate": 0.00011461006910167817,
      "loss": 0.3632,
      "step": 187800
    },
    {
      "epoch": 0.6182954919381376,
      "grad_norm": 70.24119567871094,
      "learning_rate": 0.00011451135241855873,
      "loss": 0.3508,
      "step": 187900
    },
    {
      "epoch": 0.6186245475485357,
      "grad_norm": 0.2503913938999176,
      "learning_rate": 0.00011441263573543928,
      "loss": 0.2594,
      "step": 188000
    },
    {
      "epoch": 0.6189536031589339,
      "grad_norm": 0.0017841202206909657,
      "learning_rate": 0.00011431391905231983,
      "loss": 0.268,
      "step": 188100
    },
    {
      "epoch": 0.619282658769332,
      "grad_norm": 0.006385165266692638,
      "learning_rate": 0.00011421520236920038,
      "loss": 0.2646,
      "step": 188200
    },
    {
      "epoch": 0.6196117143797302,
      "grad_norm": 52.69880676269531,
      "learning_rate": 0.00011411648568608094,
      "loss": 0.2213,
      "step": 188300
    },
    {
      "epoch": 0.6199407699901284,
      "grad_norm": 0.0016819043084979057,
      "learning_rate": 0.00011401776900296149,
      "loss": 0.3084,
      "step": 188400
    },
    {
      "epoch": 0.6202698256005265,
      "grad_norm": 117.69217681884766,
      "learning_rate": 0.00011391905231984204,
      "loss": 0.3068,
      "step": 188500
    },
    {
      "epoch": 0.6205988812109247,
      "grad_norm": 0.0009199027554132044,
      "learning_rate": 0.00011382033563672259,
      "loss": 0.2446,
      "step": 188600
    },
    {
      "epoch": 0.6209279368213229,
      "grad_norm": 0.0014360867207869887,
      "learning_rate": 0.00011372161895360315,
      "loss": 0.3451,
      "step": 188700
    },
    {
      "epoch": 0.621256992431721,
      "grad_norm": 0.000749958329834044,
      "learning_rate": 0.0001136229022704837,
      "loss": 0.3854,
      "step": 188800
    },
    {
      "epoch": 0.6215860480421191,
      "grad_norm": 0.00036360096419230103,
      "learning_rate": 0.00011352418558736425,
      "loss": 0.593,
      "step": 188900
    },
    {
      "epoch": 0.6219151036525172,
      "grad_norm": 0.18471820652484894,
      "learning_rate": 0.0001134254689042448,
      "loss": 0.2701,
      "step": 189000
    },
    {
      "epoch": 0.6222441592629154,
      "grad_norm": 0.0029710973612964153,
      "learning_rate": 0.00011332675222112536,
      "loss": 0.2849,
      "step": 189100
    },
    {
      "epoch": 0.6225732148733136,
      "grad_norm": 0.00041794878779910505,
      "learning_rate": 0.00011322803553800592,
      "loss": 0.2242,
      "step": 189200
    },
    {
      "epoch": 0.6229022704837117,
      "grad_norm": 0.0030325865373015404,
      "learning_rate": 0.00011312931885488646,
      "loss": 0.2211,
      "step": 189300
    },
    {
      "epoch": 0.6232313260941099,
      "grad_norm": 0.02850356511771679,
      "learning_rate": 0.00011303060217176702,
      "loss": 0.2602,
      "step": 189400
    },
    {
      "epoch": 0.6235603817045081,
      "grad_norm": 0.0031473359558731318,
      "learning_rate": 0.00011293188548864757,
      "loss": 0.5975,
      "step": 189500
    },
    {
      "epoch": 0.6238894373149062,
      "grad_norm": 3.4034082889556885,
      "learning_rate": 0.00011283316880552813,
      "loss": 0.3713,
      "step": 189600
    },
    {
      "epoch": 0.6242184929253044,
      "grad_norm": 0.0009297766955569386,
      "learning_rate": 0.00011273445212240867,
      "loss": 0.3837,
      "step": 189700
    },
    {
      "epoch": 0.6245475485357025,
      "grad_norm": 0.0009724052506498992,
      "learning_rate": 0.00011263573543928923,
      "loss": 0.2761,
      "step": 189800
    },
    {
      "epoch": 0.6248766041461007,
      "grad_norm": 0.0038222845178097486,
      "learning_rate": 0.00011253701875616978,
      "loss": 0.3245,
      "step": 189900
    },
    {
      "epoch": 0.6252056597564989,
      "grad_norm": 0.13908639550209045,
      "learning_rate": 0.00011243830207305034,
      "loss": 0.1996,
      "step": 190000
    },
    {
      "epoch": 0.625534715366897,
      "grad_norm": 3.9451380871469155e-05,
      "learning_rate": 0.00011233958538993088,
      "loss": 0.2029,
      "step": 190100
    },
    {
      "epoch": 0.6258637709772952,
      "grad_norm": 32.70063400268555,
      "learning_rate": 0.00011224086870681144,
      "loss": 0.6002,
      "step": 190200
    },
    {
      "epoch": 0.6261928265876934,
      "grad_norm": 0.0033198350574821234,
      "learning_rate": 0.000112142152023692,
      "loss": 0.3029,
      "step": 190300
    },
    {
      "epoch": 0.6265218821980915,
      "grad_norm": 0.005779789295047522,
      "learning_rate": 0.00011204343534057255,
      "loss": 0.2599,
      "step": 190400
    },
    {
      "epoch": 0.6268509378084897,
      "grad_norm": 33.35313415527344,
      "learning_rate": 0.0001119447186574531,
      "loss": 0.3461,
      "step": 190500
    },
    {
      "epoch": 0.6271799934188877,
      "grad_norm": 0.4149281084537506,
      "learning_rate": 0.00011184600197433365,
      "loss": 0.401,
      "step": 190600
    },
    {
      "epoch": 0.627509049029286,
      "grad_norm": 0.009577114135026932,
      "learning_rate": 0.00011174728529121421,
      "loss": 0.3299,
      "step": 190700
    },
    {
      "epoch": 0.6278381046396841,
      "grad_norm": 0.002088009612634778,
      "learning_rate": 0.00011164856860809476,
      "loss": 0.2574,
      "step": 190800
    },
    {
      "epoch": 0.6281671602500822,
      "grad_norm": 0.0017473010811954737,
      "learning_rate": 0.0001115498519249753,
      "loss": 0.2709,
      "step": 190900
    },
    {
      "epoch": 0.6284962158604804,
      "grad_norm": 0.00011925639410037547,
      "learning_rate": 0.00011145113524185586,
      "loss": 0.2864,
      "step": 191000
    },
    {
      "epoch": 0.6288252714708786,
      "grad_norm": 0.002496462082490325,
      "learning_rate": 0.00011135241855873642,
      "loss": 0.4698,
      "step": 191100
    },
    {
      "epoch": 0.6291543270812767,
      "grad_norm": 2.2885451316833496,
      "learning_rate": 0.00011125370187561698,
      "loss": 0.3849,
      "step": 191200
    },
    {
      "epoch": 0.6294833826916749,
      "grad_norm": 4.966085433959961,
      "learning_rate": 0.00011115498519249752,
      "loss": 0.3583,
      "step": 191300
    },
    {
      "epoch": 0.629812438302073,
      "grad_norm": 0.00013260194100439548,
      "learning_rate": 0.00011105626850937808,
      "loss": 0.2282,
      "step": 191400
    },
    {
      "epoch": 0.6301414939124712,
      "grad_norm": 0.004401687532663345,
      "learning_rate": 0.00011095755182625863,
      "loss": 0.2595,
      "step": 191500
    },
    {
      "epoch": 0.6304705495228694,
      "grad_norm": 0.02279534935951233,
      "learning_rate": 0.00011085883514313919,
      "loss": 0.2584,
      "step": 191600
    },
    {
      "epoch": 0.6307996051332675,
      "grad_norm": 0.014897031709551811,
      "learning_rate": 0.00011076011846001972,
      "loss": 0.2292,
      "step": 191700
    },
    {
      "epoch": 0.6311286607436657,
      "grad_norm": 0.00045330129796639085,
      "learning_rate": 0.00011066140177690029,
      "loss": 0.2619,
      "step": 191800
    },
    {
      "epoch": 0.6314577163540639,
      "grad_norm": 0.0009358310489915311,
      "learning_rate": 0.00011056268509378084,
      "loss": 0.2875,
      "step": 191900
    },
    {
      "epoch": 0.631786771964462,
      "grad_norm": 35.76456832885742,
      "learning_rate": 0.0001104639684106614,
      "loss": 0.2102,
      "step": 192000
    },
    {
      "epoch": 0.6321158275748602,
      "grad_norm": 99.27677154541016,
      "learning_rate": 0.00011036525172754193,
      "loss": 0.4218,
      "step": 192100
    },
    {
      "epoch": 0.6324448831852583,
      "grad_norm": 0.75941002368927,
      "learning_rate": 0.0001102665350444225,
      "loss": 0.3105,
      "step": 192200
    },
    {
      "epoch": 0.6327739387956565,
      "grad_norm": 0.0022865000646561384,
      "learning_rate": 0.00011016781836130306,
      "loss": 0.3648,
      "step": 192300
    },
    {
      "epoch": 0.6331029944060547,
      "grad_norm": 0.0019302103901281953,
      "learning_rate": 0.0001100691016781836,
      "loss": 0.3279,
      "step": 192400
    },
    {
      "epoch": 0.6334320500164528,
      "grad_norm": 0.07414939999580383,
      "learning_rate": 0.00010997038499506414,
      "loss": 0.2122,
      "step": 192500
    },
    {
      "epoch": 0.633761105626851,
      "grad_norm": 0.000466619327198714,
      "learning_rate": 0.0001098716683119447,
      "loss": 0.3593,
      "step": 192600
    },
    {
      "epoch": 0.6340901612372492,
      "grad_norm": 0.007673303596675396,
      "learning_rate": 0.00010977295162882527,
      "loss": 0.3239,
      "step": 192700
    },
    {
      "epoch": 0.6344192168476472,
      "grad_norm": 7.26751708984375,
      "learning_rate": 0.00010967423494570581,
      "loss": 0.2441,
      "step": 192800
    },
    {
      "epoch": 0.6347482724580454,
      "grad_norm": 0.0005368115962482989,
      "learning_rate": 0.00010957551826258636,
      "loss": 0.2531,
      "step": 192900
    },
    {
      "epoch": 0.6350773280684435,
      "grad_norm": 103.65637969970703,
      "learning_rate": 0.00010947680157946691,
      "loss": 0.3356,
      "step": 193000
    },
    {
      "epoch": 0.6354063836788417,
      "grad_norm": 0.0009206510148942471,
      "learning_rate": 0.00010937808489634747,
      "loss": 0.2923,
      "step": 193100
    },
    {
      "epoch": 0.6357354392892399,
      "grad_norm": 0.0003040713781956583,
      "learning_rate": 0.00010927936821322802,
      "loss": 0.0965,
      "step": 193200
    },
    {
      "epoch": 0.636064494899638,
      "grad_norm": 0.0005775668541900814,
      "learning_rate": 0.00010918065153010857,
      "loss": 0.2306,
      "step": 193300
    },
    {
      "epoch": 0.6363935505100362,
      "grad_norm": 0.12166329473257065,
      "learning_rate": 0.00010908193484698912,
      "loss": 0.3132,
      "step": 193400
    },
    {
      "epoch": 0.6367226061204343,
      "grad_norm": 0.0003882808960042894,
      "learning_rate": 0.00010898321816386968,
      "loss": 0.3896,
      "step": 193500
    },
    {
      "epoch": 0.6370516617308325,
      "grad_norm": 45.38405990600586,
      "learning_rate": 0.00010888450148075025,
      "loss": 0.3192,
      "step": 193600
    },
    {
      "epoch": 0.6373807173412307,
      "grad_norm": 0.0002796812041196972,
      "learning_rate": 0.0001087857847976308,
      "loss": 0.3294,
      "step": 193700
    },
    {
      "epoch": 0.6377097729516288,
      "grad_norm": 0.10490763932466507,
      "learning_rate": 0.00010868706811451134,
      "loss": 0.2102,
      "step": 193800
    },
    {
      "epoch": 0.638038828562027,
      "grad_norm": 0.13015855848789215,
      "learning_rate": 0.00010858835143139189,
      "loss": 0.3589,
      "step": 193900
    },
    {
      "epoch": 0.6383678841724252,
      "grad_norm": 25.09897232055664,
      "learning_rate": 0.00010848963474827245,
      "loss": 0.2864,
      "step": 194000
    },
    {
      "epoch": 0.6386969397828233,
      "grad_norm": 1.8474774360656738,
      "learning_rate": 0.000108390918065153,
      "loss": 0.4661,
      "step": 194100
    },
    {
      "epoch": 0.6390259953932215,
      "grad_norm": 5.592545986175537,
      "learning_rate": 0.00010829220138203355,
      "loss": 0.3621,
      "step": 194200
    },
    {
      "epoch": 0.6393550510036196,
      "grad_norm": 0.005837546661496162,
      "learning_rate": 0.0001081934846989141,
      "loss": 0.3887,
      "step": 194300
    },
    {
      "epoch": 0.6396841066140178,
      "grad_norm": 85.12220001220703,
      "learning_rate": 0.00010809476801579466,
      "loss": 0.3997,
      "step": 194400
    },
    {
      "epoch": 0.640013162224416,
      "grad_norm": 0.05664321035146713,
      "learning_rate": 0.00010799605133267523,
      "loss": 0.1924,
      "step": 194500
    },
    {
      "epoch": 0.640342217834814,
      "grad_norm": 0.00893182959407568,
      "learning_rate": 0.00010789733464955576,
      "loss": 0.3176,
      "step": 194600
    },
    {
      "epoch": 0.6406712734452122,
      "grad_norm": 0.0002441930409986526,
      "learning_rate": 0.00010779861796643632,
      "loss": 0.2437,
      "step": 194700
    },
    {
      "epoch": 0.6410003290556104,
      "grad_norm": 0.001957579515874386,
      "learning_rate": 0.00010769990128331687,
      "loss": 0.4224,
      "step": 194800
    },
    {
      "epoch": 0.6413293846660085,
      "grad_norm": 0.004330797586590052,
      "learning_rate": 0.00010760118460019743,
      "loss": 0.2053,
      "step": 194900
    },
    {
      "epoch": 0.6416584402764067,
      "grad_norm": 0.0034354734234511852,
      "learning_rate": 0.00010750246791707797,
      "loss": 0.3204,
      "step": 195000
    },
    {
      "epoch": 0.6419874958868048,
      "grad_norm": 0.0014612199738621712,
      "learning_rate": 0.00010740375123395853,
      "loss": 0.1771,
      "step": 195100
    },
    {
      "epoch": 0.642316551497203,
      "grad_norm": 0.05525677278637886,
      "learning_rate": 0.00010730503455083908,
      "loss": 0.2454,
      "step": 195200
    },
    {
      "epoch": 0.6426456071076012,
      "grad_norm": 0.05943870171904564,
      "learning_rate": 0.00010720631786771964,
      "loss": 0.2219,
      "step": 195300
    },
    {
      "epoch": 0.6429746627179993,
      "grad_norm": 0.0009107023943215609,
      "learning_rate": 0.00010710760118460018,
      "loss": 0.3374,
      "step": 195400
    },
    {
      "epoch": 0.6433037183283975,
      "grad_norm": 62.03236389160156,
      "learning_rate": 0.00010700888450148074,
      "loss": 0.1668,
      "step": 195500
    },
    {
      "epoch": 0.6436327739387957,
      "grad_norm": 0.020655779168009758,
      "learning_rate": 0.0001069101678183613,
      "loss": 0.2976,
      "step": 195600
    },
    {
      "epoch": 0.6439618295491938,
      "grad_norm": 0.0002851793833542615,
      "learning_rate": 0.00010681145113524185,
      "loss": 0.1856,
      "step": 195700
    },
    {
      "epoch": 0.644290885159592,
      "grad_norm": 24.247631072998047,
      "learning_rate": 0.0001067127344521224,
      "loss": 0.1629,
      "step": 195800
    },
    {
      "epoch": 0.6446199407699901,
      "grad_norm": 0.005415189545601606,
      "learning_rate": 0.00010661401776900295,
      "loss": 0.4606,
      "step": 195900
    },
    {
      "epoch": 0.6449489963803883,
      "grad_norm": 0.003137057414278388,
      "learning_rate": 0.00010651530108588351,
      "loss": 0.2804,
      "step": 196000
    },
    {
      "epoch": 0.6452780519907865,
      "grad_norm": 0.0002585228648968041,
      "learning_rate": 0.00010641658440276406,
      "loss": 0.4072,
      "step": 196100
    },
    {
      "epoch": 0.6456071076011846,
      "grad_norm": 101.53216552734375,
      "learning_rate": 0.00010631786771964461,
      "loss": 0.2809,
      "step": 196200
    },
    {
      "epoch": 0.6459361632115828,
      "grad_norm": 0.0009790133917704225,
      "learning_rate": 0.00010621915103652516,
      "loss": 0.345,
      "step": 196300
    },
    {
      "epoch": 0.646265218821981,
      "grad_norm": 0.5007800459861755,
      "learning_rate": 0.00010612043435340572,
      "loss": 0.3892,
      "step": 196400
    },
    {
      "epoch": 0.646594274432379,
      "grad_norm": 0.004304839298129082,
      "learning_rate": 0.00010602171767028628,
      "loss": 0.1634,
      "step": 196500
    },
    {
      "epoch": 0.6469233300427772,
      "grad_norm": 8.450292079942301e-05,
      "learning_rate": 0.00010592300098716682,
      "loss": 0.2194,
      "step": 196600
    },
    {
      "epoch": 0.6472523856531753,
      "grad_norm": 56.4372444152832,
      "learning_rate": 0.00010582428430404737,
      "loss": 0.2881,
      "step": 196700
    },
    {
      "epoch": 0.6475814412635735,
      "grad_norm": 0.0005099453264847398,
      "learning_rate": 0.00010572556762092793,
      "loss": 0.2225,
      "step": 196800
    },
    {
      "epoch": 0.6479104968739717,
      "grad_norm": 0.0011101099662482738,
      "learning_rate": 0.00010562685093780849,
      "loss": 0.3099,
      "step": 196900
    },
    {
      "epoch": 0.6482395524843698,
      "grad_norm": 0.32728931307792664,
      "learning_rate": 0.00010552813425468903,
      "loss": 0.1263,
      "step": 197000
    },
    {
      "epoch": 0.648568608094768,
      "grad_norm": 0.001936539774760604,
      "learning_rate": 0.00010542941757156959,
      "loss": 0.1445,
      "step": 197100
    },
    {
      "epoch": 0.6488976637051662,
      "grad_norm": 8.510985935572535e-05,
      "learning_rate": 0.00010533070088845014,
      "loss": 0.1708,
      "step": 197200
    },
    {
      "epoch": 0.6492267193155643,
      "grad_norm": 0.003552855923771858,
      "learning_rate": 0.0001052319842053307,
      "loss": 0.1918,
      "step": 197300
    },
    {
      "epoch": 0.6495557749259625,
      "grad_norm": 0.1001167818903923,
      "learning_rate": 0.00010513326752221124,
      "loss": 0.1882,
      "step": 197400
    },
    {
      "epoch": 0.6498848305363606,
      "grad_norm": 21.972396850585938,
      "learning_rate": 0.0001050345508390918,
      "loss": 0.329,
      "step": 197500
    },
    {
      "epoch": 0.6502138861467588,
      "grad_norm": 0.0012937422143295407,
      "learning_rate": 0.00010493583415597235,
      "loss": 0.1766,
      "step": 197600
    },
    {
      "epoch": 0.650542941757157,
      "grad_norm": 38.319950103759766,
      "learning_rate": 0.00010483711747285291,
      "loss": 0.2578,
      "step": 197700
    },
    {
      "epoch": 0.6508719973675551,
      "grad_norm": 0.00031840207520872355,
      "learning_rate": 0.00010473840078973345,
      "loss": 0.1902,
      "step": 197800
    },
    {
      "epoch": 0.6512010529779533,
      "grad_norm": 0.0010554047767072916,
      "learning_rate": 0.00010463968410661401,
      "loss": 0.2845,
      "step": 197900
    },
    {
      "epoch": 0.6515301085883515,
      "grad_norm": 0.004341775085777044,
      "learning_rate": 0.00010454096742349457,
      "loss": 0.4158,
      "step": 198000
    },
    {
      "epoch": 0.6518591641987496,
      "grad_norm": 0.00305355666205287,
      "learning_rate": 0.00010444225074037512,
      "loss": 0.2528,
      "step": 198100
    },
    {
      "epoch": 0.6521882198091478,
      "grad_norm": 0.0008750133565627038,
      "learning_rate": 0.00010434353405725567,
      "loss": 0.4434,
      "step": 198200
    },
    {
      "epoch": 0.6525172754195459,
      "grad_norm": 0.0018034698441624641,
      "learning_rate": 0.00010424481737413622,
      "loss": 0.3194,
      "step": 198300
    },
    {
      "epoch": 0.652846331029944,
      "grad_norm": 0.0018567227525636554,
      "learning_rate": 0.00010414610069101678,
      "loss": 0.4408,
      "step": 198400
    },
    {
      "epoch": 0.6531753866403422,
      "grad_norm": 0.00016482094360981137,
      "learning_rate": 0.00010404738400789733,
      "loss": 0.3505,
      "step": 198500
    },
    {
      "epoch": 0.6535044422507403,
      "grad_norm": 0.0005572430673055351,
      "learning_rate": 0.00010394866732477788,
      "loss": 0.3683,
      "step": 198600
    },
    {
      "epoch": 0.6538334978611385,
      "grad_norm": 3.4697766304016113,
      "learning_rate": 0.00010384995064165843,
      "loss": 0.3276,
      "step": 198700
    },
    {
      "epoch": 0.6541625534715367,
      "grad_norm": 0.0017435778863728046,
      "learning_rate": 0.00010375123395853899,
      "loss": 0.2146,
      "step": 198800
    },
    {
      "epoch": 0.6544916090819348,
      "grad_norm": 0.10762682557106018,
      "learning_rate": 0.00010365251727541955,
      "loss": 0.2873,
      "step": 198900
    },
    {
      "epoch": 0.654820664692333,
      "grad_norm": 0.005200863350182772,
      "learning_rate": 0.00010355380059230009,
      "loss": 0.3382,
      "step": 199000
    },
    {
      "epoch": 0.6551497203027311,
      "grad_norm": 1.0009030103683472,
      "learning_rate": 0.00010345508390918065,
      "loss": 0.2493,
      "step": 199100
    },
    {
      "epoch": 0.6554787759131293,
      "grad_norm": 0.06987583637237549,
      "learning_rate": 0.0001033563672260612,
      "loss": 0.2386,
      "step": 199200
    },
    {
      "epoch": 0.6558078315235275,
      "grad_norm": 0.015767902135849,
      "learning_rate": 0.00010325765054294176,
      "loss": 0.2469,
      "step": 199300
    },
    {
      "epoch": 0.6561368871339256,
      "grad_norm": 26.568246841430664,
      "learning_rate": 0.0001031589338598223,
      "loss": 0.3474,
      "step": 199400
    },
    {
      "epoch": 0.6564659427443238,
      "grad_norm": 0.0027172223199158907,
      "learning_rate": 0.00010306021717670286,
      "loss": 0.3655,
      "step": 199500
    },
    {
      "epoch": 0.656794998354722,
      "grad_norm": 0.07770133763551712,
      "learning_rate": 0.0001029615004935834,
      "loss": 0.1915,
      "step": 199600
    },
    {
      "epoch": 0.6571240539651201,
      "grad_norm": 3.5304994583129883,
      "learning_rate": 0.00010286278381046397,
      "loss": 0.2535,
      "step": 199700
    },
    {
      "epoch": 0.6574531095755183,
      "grad_norm": 0.0011906172148883343,
      "learning_rate": 0.0001027640671273445,
      "loss": 0.2763,
      "step": 199800
    },
    {
      "epoch": 0.6577821651859164,
      "grad_norm": 0.0005101605784147978,
      "learning_rate": 0.00010266535044422506,
      "loss": 0.3523,
      "step": 199900
    },
    {
      "epoch": 0.6581112207963146,
      "grad_norm": 1.2082010507583618,
      "learning_rate": 0.00010256663376110563,
      "loss": 0.3112,
      "step": 200000
    },
    {
      "epoch": 0.6584402764067128,
      "grad_norm": 44.1561393737793,
      "learning_rate": 0.00010246791707798618,
      "loss": 0.3817,
      "step": 200100
    },
    {
      "epoch": 0.6587693320171109,
      "grad_norm": 0.0005159970605745912,
      "learning_rate": 0.00010236920039486672,
      "loss": 0.3874,
      "step": 200200
    },
    {
      "epoch": 0.659098387627509,
      "grad_norm": 0.002512905513867736,
      "learning_rate": 0.00010227048371174727,
      "loss": 0.2785,
      "step": 200300
    },
    {
      "epoch": 0.6594274432379073,
      "grad_norm": 0.0005058712558820844,
      "learning_rate": 0.00010217176702862784,
      "loss": 0.3939,
      "step": 200400
    },
    {
      "epoch": 0.6597564988483053,
      "grad_norm": 0.009688256308436394,
      "learning_rate": 0.00010207305034550838,
      "loss": 0.2378,
      "step": 200500
    },
    {
      "epoch": 0.6600855544587035,
      "grad_norm": 0.007534524891525507,
      "learning_rate": 0.00010197433366238893,
      "loss": 0.2233,
      "step": 200600
    },
    {
      "epoch": 0.6604146100691016,
      "grad_norm": 0.005572123918682337,
      "learning_rate": 0.00010187561697926948,
      "loss": 0.4078,
      "step": 200700
    },
    {
      "epoch": 0.6607436656794998,
      "grad_norm": 53.37162399291992,
      "learning_rate": 0.00010177690029615004,
      "loss": 0.2984,
      "step": 200800
    },
    {
      "epoch": 0.661072721289898,
      "grad_norm": 2.321505598956719e-05,
      "learning_rate": 0.0001016781836130306,
      "loss": 0.3747,
      "step": 200900
    },
    {
      "epoch": 0.6614017769002961,
      "grad_norm": 15.428498268127441,
      "learning_rate": 0.00010157946692991114,
      "loss": 0.2389,
      "step": 201000
    },
    {
      "epoch": 0.6617308325106943,
      "grad_norm": 3.919145819963887e-05,
      "learning_rate": 0.00010148075024679169,
      "loss": 0.1185,
      "step": 201100
    },
    {
      "epoch": 0.6620598881210925,
      "grad_norm": 0.045123450458049774,
      "learning_rate": 0.00010138203356367225,
      "loss": 0.1996,
      "step": 201200
    },
    {
      "epoch": 0.6623889437314906,
      "grad_norm": 0.09781382232904434,
      "learning_rate": 0.00010128331688055282,
      "loss": 0.167,
      "step": 201300
    },
    {
      "epoch": 0.6627179993418888,
      "grad_norm": 0.002584125380963087,
      "learning_rate": 0.00010118460019743335,
      "loss": 0.3608,
      "step": 201400
    },
    {
      "epoch": 0.6630470549522869,
      "grad_norm": 0.00508067337796092,
      "learning_rate": 0.00010108588351431391,
      "loss": 0.2447,
      "step": 201500
    },
    {
      "epoch": 0.6633761105626851,
      "grad_norm": 0.0005044417339377105,
      "learning_rate": 0.00010098716683119446,
      "loss": 0.321,
      "step": 201600
    },
    {
      "epoch": 0.6637051661730833,
      "grad_norm": 2.925764799118042,
      "learning_rate": 0.00010088845014807502,
      "loss": 0.1942,
      "step": 201700
    },
    {
      "epoch": 0.6640342217834814,
      "grad_norm": 0.00482243113219738,
      "learning_rate": 0.00010078973346495556,
      "loss": 0.2562,
      "step": 201800
    },
    {
      "epoch": 0.6643632773938796,
      "grad_norm": 7.575178460683674e-05,
      "learning_rate": 0.00010069101678183612,
      "loss": 0.2717,
      "step": 201900
    },
    {
      "epoch": 0.6646923330042778,
      "grad_norm": 0.0003793069627135992,
      "learning_rate": 0.00010059230009871667,
      "loss": 0.2635,
      "step": 202000
    },
    {
      "epoch": 0.6650213886146759,
      "grad_norm": 104.5770034790039,
      "learning_rate": 0.00010049358341559723,
      "loss": 0.2024,
      "step": 202100
    },
    {
      "epoch": 0.6653504442250741,
      "grad_norm": 0.0012759160017594695,
      "learning_rate": 0.00010039486673247777,
      "loss": 0.1476,
      "step": 202200
    },
    {
      "epoch": 0.6656794998354721,
      "grad_norm": 0.000333676376612857,
      "learning_rate": 0.00010029615004935833,
      "loss": 0.199,
      "step": 202300
    },
    {
      "epoch": 0.6660085554458703,
      "grad_norm": 21.87459945678711,
      "learning_rate": 0.0001001974333662389,
      "loss": 0.3073,
      "step": 202400
    },
    {
      "epoch": 0.6663376110562685,
      "grad_norm": 0.0010816602734848857,
      "learning_rate": 0.00010009871668311944,
      "loss": 0.3303,
      "step": 202500
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.0010716415708884597,
      "learning_rate": 9.999999999999999e-05,
      "loss": 0.4657,
      "step": 202600
    },
    {
      "epoch": 0.6669957222770648,
      "grad_norm": 0.01024590153247118,
      "learning_rate": 9.990128331688054e-05,
      "loss": 0.2713,
      "step": 202700
    },
    {
      "epoch": 0.667324777887463,
      "grad_norm": 0.0003668831486720592,
      "learning_rate": 9.98025666337611e-05,
      "loss": 0.3414,
      "step": 202800
    },
    {
      "epoch": 0.6676538334978611,
      "grad_norm": 0.0005231006653048098,
      "learning_rate": 9.970384995064165e-05,
      "loss": 0.173,
      "step": 202900
    },
    {
      "epoch": 0.6679828891082593,
      "grad_norm": 0.0002741125354077667,
      "learning_rate": 9.96051332675222e-05,
      "loss": 0.2713,
      "step": 203000
    },
    {
      "epoch": 0.6683119447186574,
      "grad_norm": 43.3188591003418,
      "learning_rate": 9.950641658440275e-05,
      "loss": 0.3472,
      "step": 203100
    },
    {
      "epoch": 0.6686410003290556,
      "grad_norm": 0.0024223027285188437,
      "learning_rate": 9.940769990128331e-05,
      "loss": 0.3109,
      "step": 203200
    },
    {
      "epoch": 0.6689700559394538,
      "grad_norm": 0.0018109509255737066,
      "learning_rate": 9.930898321816387e-05,
      "loss": 0.1519,
      "step": 203300
    },
    {
      "epoch": 0.6692991115498519,
      "grad_norm": 0.0014146692119538784,
      "learning_rate": 9.921026653504441e-05,
      "loss": 0.2366,
      "step": 203400
    },
    {
      "epoch": 0.6696281671602501,
      "grad_norm": 0.0003837532131001353,
      "learning_rate": 9.911154985192497e-05,
      "loss": 0.2654,
      "step": 203500
    },
    {
      "epoch": 0.6699572227706483,
      "grad_norm": 0.1230769157409668,
      "learning_rate": 9.901283316880552e-05,
      "loss": 0.4269,
      "step": 203600
    },
    {
      "epoch": 0.6702862783810464,
      "grad_norm": 0.0007556570926681161,
      "learning_rate": 9.891411648568608e-05,
      "loss": 0.1819,
      "step": 203700
    },
    {
      "epoch": 0.6706153339914446,
      "grad_norm": 67.3195571899414,
      "learning_rate": 9.881539980256662e-05,
      "loss": 0.2388,
      "step": 203800
    },
    {
      "epoch": 0.6709443896018427,
      "grad_norm": 0.1932211071252823,
      "learning_rate": 9.871668311944718e-05,
      "loss": 0.2626,
      "step": 203900
    },
    {
      "epoch": 0.6712734452122409,
      "grad_norm": 0.0002742991200648248,
      "learning_rate": 9.861796643632773e-05,
      "loss": 0.3216,
      "step": 204000
    },
    {
      "epoch": 0.6716025008226391,
      "grad_norm": 0.00653251213952899,
      "learning_rate": 9.851924975320829e-05,
      "loss": 0.3009,
      "step": 204100
    },
    {
      "epoch": 0.6719315564330371,
      "grad_norm": 0.0008651544922031462,
      "learning_rate": 9.842053307008883e-05,
      "loss": 0.2731,
      "step": 204200
    },
    {
      "epoch": 0.6722606120434353,
      "grad_norm": 0.03222576901316643,
      "learning_rate": 9.832181638696939e-05,
      "loss": 0.2574,
      "step": 204300
    },
    {
      "epoch": 0.6725896676538335,
      "grad_norm": 0.0036018327809870243,
      "learning_rate": 9.822309970384995e-05,
      "loss": 0.2715,
      "step": 204400
    },
    {
      "epoch": 0.6729187232642316,
      "grad_norm": 1.0813199878612068e-05,
      "learning_rate": 9.81243830207305e-05,
      "loss": 0.562,
      "step": 204500
    },
    {
      "epoch": 0.6732477788746298,
      "grad_norm": 0.0002078549878206104,
      "learning_rate": 9.802566633761104e-05,
      "loss": 0.2715,
      "step": 204600
    },
    {
      "epoch": 0.6735768344850279,
      "grad_norm": 0.00811755657196045,
      "learning_rate": 9.79269496544916e-05,
      "loss": 0.5248,
      "step": 204700
    },
    {
      "epoch": 0.6739058900954261,
      "grad_norm": 0.012654842808842659,
      "learning_rate": 9.782823297137216e-05,
      "loss": 0.2169,
      "step": 204800
    },
    {
      "epoch": 0.6742349457058243,
      "grad_norm": 0.009467259049415588,
      "learning_rate": 9.772951628825271e-05,
      "loss": 0.3266,
      "step": 204900
    },
    {
      "epoch": 0.6745640013162224,
      "grad_norm": 0.03196246549487114,
      "learning_rate": 9.763079960513326e-05,
      "loss": 0.2991,
      "step": 205000
    },
    {
      "epoch": 0.6748930569266206,
      "grad_norm": 0.00021989087690599263,
      "learning_rate": 9.75320829220138e-05,
      "loss": 0.2487,
      "step": 205100
    },
    {
      "epoch": 0.6752221125370188,
      "grad_norm": 0.03378729149699211,
      "learning_rate": 9.743336623889437e-05,
      "loss": 0.2112,
      "step": 205200
    },
    {
      "epoch": 0.6755511681474169,
      "grad_norm": 14.558574676513672,
      "learning_rate": 9.733464955577492e-05,
      "loss": 0.2679,
      "step": 205300
    },
    {
      "epoch": 0.6758802237578151,
      "grad_norm": 0.002017930382862687,
      "learning_rate": 9.723593287265547e-05,
      "loss": 0.3578,
      "step": 205400
    },
    {
      "epoch": 0.6762092793682132,
      "grad_norm": 11.73685359954834,
      "learning_rate": 9.713721618953602e-05,
      "loss": 0.3868,
      "step": 205500
    },
    {
      "epoch": 0.6765383349786114,
      "grad_norm": 0.005324456840753555,
      "learning_rate": 9.703849950641658e-05,
      "loss": 0.2356,
      "step": 205600
    },
    {
      "epoch": 0.6768673905890096,
      "grad_norm": 0.0007555466727353632,
      "learning_rate": 9.693978282329714e-05,
      "loss": 0.3944,
      "step": 205700
    },
    {
      "epoch": 0.6771964461994077,
      "grad_norm": 0.934980034828186,
      "learning_rate": 9.684106614017768e-05,
      "loss": 0.1748,
      "step": 205800
    },
    {
      "epoch": 0.6775255018098059,
      "grad_norm": 9.385871089762077e-05,
      "learning_rate": 9.674234945705824e-05,
      "loss": 0.1705,
      "step": 205900
    },
    {
      "epoch": 0.6778545574202041,
      "grad_norm": 6.982516060816124e-05,
      "learning_rate": 9.664363277393879e-05,
      "loss": 0.3186,
      "step": 206000
    },
    {
      "epoch": 0.6781836130306022,
      "grad_norm": 30.322975158691406,
      "learning_rate": 9.654491609081935e-05,
      "loss": 0.1866,
      "step": 206100
    },
    {
      "epoch": 0.6785126686410003,
      "grad_norm": 0.39059752225875854,
      "learning_rate": 9.644619940769988e-05,
      "loss": 0.243,
      "step": 206200
    },
    {
      "epoch": 0.6788417242513984,
      "grad_norm": 0.00508154509589076,
      "learning_rate": 9.634748272458045e-05,
      "loss": 0.3959,
      "step": 206300
    },
    {
      "epoch": 0.6791707798617966,
      "grad_norm": 0.001246807281859219,
      "learning_rate": 9.6248766041461e-05,
      "loss": 0.3006,
      "step": 206400
    },
    {
      "epoch": 0.6794998354721948,
      "grad_norm": 0.0003579826152417809,
      "learning_rate": 9.615004935834156e-05,
      "loss": 0.2107,
      "step": 206500
    },
    {
      "epoch": 0.6798288910825929,
      "grad_norm": 0.0016386057250201702,
      "learning_rate": 9.605133267522209e-05,
      "loss": 0.3576,
      "step": 206600
    },
    {
      "epoch": 0.6801579466929911,
      "grad_norm": 0.00012685469118878245,
      "learning_rate": 9.595261599210266e-05,
      "loss": 0.1634,
      "step": 206700
    },
    {
      "epoch": 0.6804870023033893,
      "grad_norm": 0.0006545823998749256,
      "learning_rate": 9.585389930898322e-05,
      "loss": 0.1363,
      "step": 206800
    },
    {
      "epoch": 0.6808160579137874,
      "grad_norm": 0.005212890915572643,
      "learning_rate": 9.575518262586377e-05,
      "loss": 0.3654,
      "step": 206900
    },
    {
      "epoch": 0.6811451135241856,
      "grad_norm": 0.0002606527123134583,
      "learning_rate": 9.565646594274431e-05,
      "loss": 0.311,
      "step": 207000
    },
    {
      "epoch": 0.6814741691345837,
      "grad_norm": 0.015395394526422024,
      "learning_rate": 9.555774925962486e-05,
      "loss": 0.3443,
      "step": 207100
    },
    {
      "epoch": 0.6818032247449819,
      "grad_norm": 65.23784637451172,
      "learning_rate": 9.545903257650543e-05,
      "loss": 0.3609,
      "step": 207200
    },
    {
      "epoch": 0.6821322803553801,
      "grad_norm": 0.5748656988143921,
      "learning_rate": 9.536031589338597e-05,
      "loss": 0.3726,
      "step": 207300
    },
    {
      "epoch": 0.6824613359657782,
      "grad_norm": 0.0411185696721077,
      "learning_rate": 9.526159921026652e-05,
      "loss": 0.3566,
      "step": 207400
    },
    {
      "epoch": 0.6827903915761764,
      "grad_norm": 0.0005560571444220841,
      "learning_rate": 9.516288252714707e-05,
      "loss": 0.1259,
      "step": 207500
    },
    {
      "epoch": 0.6831194471865746,
      "grad_norm": 0.00921541079878807,
      "learning_rate": 9.506416584402763e-05,
      "loss": 0.3169,
      "step": 207600
    },
    {
      "epoch": 0.6834485027969727,
      "grad_norm": 0.0001301145675824955,
      "learning_rate": 9.49654491609082e-05,
      "loss": 0.2849,
      "step": 207700
    },
    {
      "epoch": 0.6837775584073709,
      "grad_norm": 18.308841705322266,
      "learning_rate": 9.486673247778873e-05,
      "loss": 0.2601,
      "step": 207800
    },
    {
      "epoch": 0.684106614017769,
      "grad_norm": 0.07671515643596649,
      "learning_rate": 9.47680157946693e-05,
      "loss": 0.2078,
      "step": 207900
    },
    {
      "epoch": 0.6844356696281672,
      "grad_norm": 0.0004999067750759423,
      "learning_rate": 9.466929911154984e-05,
      "loss": 0.1164,
      "step": 208000
    },
    {
      "epoch": 0.6847647252385654,
      "grad_norm": 4.802197508979589e-05,
      "learning_rate": 9.45705824284304e-05,
      "loss": 0.2591,
      "step": 208100
    },
    {
      "epoch": 0.6850937808489634,
      "grad_norm": 0.0017729677492752671,
      "learning_rate": 9.447186574531094e-05,
      "loss": 0.1907,
      "step": 208200
    },
    {
      "epoch": 0.6854228364593616,
      "grad_norm": 0.01776357740163803,
      "learning_rate": 9.43731490621915e-05,
      "loss": 0.2262,
      "step": 208300
    },
    {
      "epoch": 0.6857518920697598,
      "grad_norm": 8.70624789968133e-05,
      "learning_rate": 9.427443237907205e-05,
      "loss": 0.2192,
      "step": 208400
    },
    {
      "epoch": 0.6860809476801579,
      "grad_norm": 0.08759284019470215,
      "learning_rate": 9.417571569595261e-05,
      "loss": 0.2382,
      "step": 208500
    },
    {
      "epoch": 0.6864100032905561,
      "grad_norm": 0.00018007979087997228,
      "learning_rate": 9.407699901283315e-05,
      "loss": 0.2664,
      "step": 208600
    },
    {
      "epoch": 0.6867390589009542,
      "grad_norm": 88.00210571289062,
      "learning_rate": 9.397828232971371e-05,
      "loss": 0.3257,
      "step": 208700
    },
    {
      "epoch": 0.6870681145113524,
      "grad_norm": 0.00031310395570471883,
      "learning_rate": 9.387956564659426e-05,
      "loss": 0.2673,
      "step": 208800
    },
    {
      "epoch": 0.6873971701217506,
      "grad_norm": 0.0015467166667804122,
      "learning_rate": 9.378084896347482e-05,
      "loss": 0.0858,
      "step": 208900
    },
    {
      "epoch": 0.6877262257321487,
      "grad_norm": 36.86371994018555,
      "learning_rate": 9.368213228035536e-05,
      "loss": 0.1741,
      "step": 209000
    },
    {
      "epoch": 0.6880552813425469,
      "grad_norm": 7.68250036239624,
      "learning_rate": 9.358341559723592e-05,
      "loss": 0.2728,
      "step": 209100
    },
    {
      "epoch": 0.6883843369529451,
      "grad_norm": 0.005758048500865698,
      "learning_rate": 9.348469891411648e-05,
      "loss": 0.1171,
      "step": 209200
    },
    {
      "epoch": 0.6887133925633432,
      "grad_norm": 0.0024830447509884834,
      "learning_rate": 9.338598223099703e-05,
      "loss": 0.2493,
      "step": 209300
    },
    {
      "epoch": 0.6890424481737414,
      "grad_norm": 0.0008134743547998369,
      "learning_rate": 9.328726554787758e-05,
      "loss": 0.3201,
      "step": 209400
    },
    {
      "epoch": 0.6893715037841395,
      "grad_norm": 7.716853618621826,
      "learning_rate": 9.318854886475813e-05,
      "loss": 0.1645,
      "step": 209500
    },
    {
      "epoch": 0.6897005593945377,
      "grad_norm": 9.601229667663574,
      "learning_rate": 9.308983218163869e-05,
      "loss": 0.2548,
      "step": 209600
    },
    {
      "epoch": 0.6900296150049359,
      "grad_norm": 0.026853159070014954,
      "learning_rate": 9.299111549851924e-05,
      "loss": 0.277,
      "step": 209700
    },
    {
      "epoch": 0.690358670615334,
      "grad_norm": 0.001206022105179727,
      "learning_rate": 9.289239881539979e-05,
      "loss": 0.4534,
      "step": 209800
    },
    {
      "epoch": 0.6906877262257322,
      "grad_norm": 0.17192211747169495,
      "learning_rate": 9.279368213228034e-05,
      "loss": 0.2328,
      "step": 209900
    },
    {
      "epoch": 0.6910167818361304,
      "grad_norm": 0.003455352270975709,
      "learning_rate": 9.26949654491609e-05,
      "loss": 0.2876,
      "step": 210000
    },
    {
      "epoch": 0.6913458374465284,
      "grad_norm": 14.057530403137207,
      "learning_rate": 9.259624876604146e-05,
      "loss": 0.3031,
      "step": 210100
    },
    {
      "epoch": 0.6916748930569266,
      "grad_norm": 0.002011939650401473,
      "learning_rate": 9.2497532082922e-05,
      "loss": 0.4145,
      "step": 210200
    },
    {
      "epoch": 0.6920039486673247,
      "grad_norm": 8.900503598852083e-05,
      "learning_rate": 9.239881539980256e-05,
      "loss": 0.1538,
      "step": 210300
    },
    {
      "epoch": 0.6923330042777229,
      "grad_norm": 0.006418263074010611,
      "learning_rate": 9.230009871668311e-05,
      "loss": 0.1248,
      "step": 210400
    },
    {
      "epoch": 0.6926620598881211,
      "grad_norm": 0.014285258017480373,
      "learning_rate": 9.220138203356367e-05,
      "loss": 0.1744,
      "step": 210500
    },
    {
      "epoch": 0.6929911154985192,
      "grad_norm": 0.0005600407021120191,
      "learning_rate": 9.210266535044421e-05,
      "loss": 0.1723,
      "step": 210600
    },
    {
      "epoch": 0.6933201711089174,
      "grad_norm": 0.03974227234721184,
      "learning_rate": 9.200394866732477e-05,
      "loss": 0.2672,
      "step": 210700
    },
    {
      "epoch": 0.6936492267193156,
      "grad_norm": 0.009110220707952976,
      "learning_rate": 9.190523198420532e-05,
      "loss": 0.3194,
      "step": 210800
    },
    {
      "epoch": 0.6939782823297137,
      "grad_norm": 0.0028555900789797306,
      "learning_rate": 9.180651530108588e-05,
      "loss": 0.2792,
      "step": 210900
    },
    {
      "epoch": 0.6943073379401119,
      "grad_norm": 0.008595719933509827,
      "learning_rate": 9.170779861796642e-05,
      "loss": 0.3699,
      "step": 211000
    },
    {
      "epoch": 0.69463639355051,
      "grad_norm": 34.253665924072266,
      "learning_rate": 9.160908193484698e-05,
      "loss": 0.1943,
      "step": 211100
    },
    {
      "epoch": 0.6949654491609082,
      "grad_norm": 0.0008739710319787264,
      "learning_rate": 9.151036525172754e-05,
      "loss": 0.3427,
      "step": 211200
    },
    {
      "epoch": 0.6952945047713064,
      "grad_norm": 0.0002850096789188683,
      "learning_rate": 9.141164856860809e-05,
      "loss": 0.2076,
      "step": 211300
    },
    {
      "epoch": 0.6956235603817045,
      "grad_norm": 0.008093520067632198,
      "learning_rate": 9.131293188548864e-05,
      "loss": 0.1969,
      "step": 211400
    },
    {
      "epoch": 0.6959526159921027,
      "grad_norm": 46.56425857543945,
      "learning_rate": 9.121421520236919e-05,
      "loss": 0.2429,
      "step": 211500
    },
    {
      "epoch": 0.6962816716025009,
      "grad_norm": 0.015935640782117844,
      "learning_rate": 9.111549851924975e-05,
      "loss": 0.3425,
      "step": 211600
    },
    {
      "epoch": 0.696610727212899,
      "grad_norm": 0.0022172501776367426,
      "learning_rate": 9.10167818361303e-05,
      "loss": 0.2783,
      "step": 211700
    },
    {
      "epoch": 0.6969397828232972,
      "grad_norm": 0.0017602427396923304,
      "learning_rate": 9.091806515301085e-05,
      "loss": 0.3193,
      "step": 211800
    },
    {
      "epoch": 0.6972688384336952,
      "grad_norm": 0.000729635707102716,
      "learning_rate": 9.08193484698914e-05,
      "loss": 0.1649,
      "step": 211900
    },
    {
      "epoch": 0.6975978940440934,
      "grad_norm": 0.0010881293565034866,
      "learning_rate": 9.072063178677196e-05,
      "loss": 0.3095,
      "step": 212000
    },
    {
      "epoch": 0.6979269496544916,
      "grad_norm": 14.904186248779297,
      "learning_rate": 9.062191510365252e-05,
      "loss": 0.2488,
      "step": 212100
    },
    {
      "epoch": 0.6982560052648897,
      "grad_norm": 0.0012847509933635592,
      "learning_rate": 9.052319842053306e-05,
      "loss": 0.3273,
      "step": 212200
    },
    {
      "epoch": 0.6985850608752879,
      "grad_norm": 0.0008584000752307475,
      "learning_rate": 9.042448173741362e-05,
      "loss": 0.1717,
      "step": 212300
    },
    {
      "epoch": 0.6989141164856861,
      "grad_norm": 0.005434552673250437,
      "learning_rate": 9.032576505429417e-05,
      "loss": 0.1784,
      "step": 212400
    },
    {
      "epoch": 0.6992431720960842,
      "grad_norm": 0.0020128991454839706,
      "learning_rate": 9.022704837117473e-05,
      "loss": 0.2415,
      "step": 212500
    },
    {
      "epoch": 0.6995722277064824,
      "grad_norm": 0.00042453387868590653,
      "learning_rate": 9.012833168805527e-05,
      "loss": 0.2559,
      "step": 212600
    },
    {
      "epoch": 0.6999012833168805,
      "grad_norm": 0.00084636639803648,
      "learning_rate": 9.002961500493583e-05,
      "loss": 0.2103,
      "step": 212700
    },
    {
      "epoch": 0.7002303389272787,
      "grad_norm": 0.001844232901930809,
      "learning_rate": 8.993089832181638e-05,
      "loss": 0.2216,
      "step": 212800
    },
    {
      "epoch": 0.7005593945376769,
      "grad_norm": 0.0027761743403971195,
      "learning_rate": 8.983218163869694e-05,
      "loss": 0.3267,
      "step": 212900
    },
    {
      "epoch": 0.700888450148075,
      "grad_norm": 0.009505887515842915,
      "learning_rate": 8.973346495557747e-05,
      "loss": 0.2988,
      "step": 213000
    },
    {
      "epoch": 0.7012175057584732,
      "grad_norm": 0.0007045404054224491,
      "learning_rate": 8.963474827245804e-05,
      "loss": 0.4884,
      "step": 213100
    },
    {
      "epoch": 0.7015465613688714,
      "grad_norm": 0.001151154632680118,
      "learning_rate": 8.953603158933859e-05,
      "loss": 0.356,
      "step": 213200
    },
    {
      "epoch": 0.7018756169792695,
      "grad_norm": 0.03641727939248085,
      "learning_rate": 8.943731490621915e-05,
      "loss": 0.1852,
      "step": 213300
    },
    {
      "epoch": 0.7022046725896677,
      "grad_norm": 0.010370909236371517,
      "learning_rate": 8.933859822309968e-05,
      "loss": 0.524,
      "step": 213400
    },
    {
      "epoch": 0.7025337282000658,
      "grad_norm": 0.02864219807088375,
      "learning_rate": 8.923988153998025e-05,
      "loss": 0.1637,
      "step": 213500
    },
    {
      "epoch": 0.702862783810464,
      "grad_norm": 0.005250026937574148,
      "learning_rate": 8.914116485686081e-05,
      "loss": 0.3003,
      "step": 213600
    },
    {
      "epoch": 0.7031918394208622,
      "grad_norm": 0.058145925402641296,
      "learning_rate": 8.904244817374136e-05,
      "loss": 0.2011,
      "step": 213700
    },
    {
      "epoch": 0.7035208950312603,
      "grad_norm": 0.0002702636702451855,
      "learning_rate": 8.89437314906219e-05,
      "loss": 0.3554,
      "step": 213800
    },
    {
      "epoch": 0.7038499506416585,
      "grad_norm": 1.3897110223770142,
      "learning_rate": 8.884501480750245e-05,
      "loss": 0.2251,
      "step": 213900
    },
    {
      "epoch": 0.7041790062520566,
      "grad_norm": 0.002852507634088397,
      "learning_rate": 8.874629812438302e-05,
      "loss": 0.2388,
      "step": 214000
    },
    {
      "epoch": 0.7045080618624547,
      "grad_norm": 0.004821131005883217,
      "learning_rate": 8.864758144126357e-05,
      "loss": 0.2786,
      "step": 214100
    },
    {
      "epoch": 0.7048371174728529,
      "grad_norm": 25.803499221801758,
      "learning_rate": 8.854886475814411e-05,
      "loss": 0.3505,
      "step": 214200
    },
    {
      "epoch": 0.705166173083251,
      "grad_norm": 0.0008879242232069373,
      "learning_rate": 8.845014807502466e-05,
      "loss": 0.3819,
      "step": 214300
    },
    {
      "epoch": 0.7054952286936492,
      "grad_norm": 0.01487756334245205,
      "learning_rate": 8.835143139190522e-05,
      "loss": 0.4359,
      "step": 214400
    },
    {
      "epoch": 0.7058242843040474,
      "grad_norm": 0.0008167001069523394,
      "learning_rate": 8.825271470878579e-05,
      "loss": 0.2963,
      "step": 214500
    },
    {
      "epoch": 0.7061533399144455,
      "grad_norm": 0.008347847498953342,
      "learning_rate": 8.815399802566632e-05,
      "loss": 0.246,
      "step": 214600
    },
    {
      "epoch": 0.7064823955248437,
      "grad_norm": 0.011221018619835377,
      "learning_rate": 8.805528134254688e-05,
      "loss": 0.1715,
      "step": 214700
    },
    {
      "epoch": 0.7068114511352419,
      "grad_norm": 19.708881378173828,
      "learning_rate": 8.795656465942743e-05,
      "loss": 0.201,
      "step": 214800
    },
    {
      "epoch": 0.70714050674564,
      "grad_norm": 0.00037068623350933194,
      "learning_rate": 8.7857847976308e-05,
      "loss": 0.2665,
      "step": 214900
    },
    {
      "epoch": 0.7074695623560382,
      "grad_norm": 0.0007762794848531485,
      "learning_rate": 8.775913129318853e-05,
      "loss": 0.1928,
      "step": 215000
    },
    {
      "epoch": 0.7077986179664363,
      "grad_norm": 0.006520343013107777,
      "learning_rate": 8.76604146100691e-05,
      "loss": 0.2735,
      "step": 215100
    },
    {
      "epoch": 0.7081276735768345,
      "grad_norm": 0.009292508475482464,
      "learning_rate": 8.756169792694964e-05,
      "loss": 0.2962,
      "step": 215200
    },
    {
      "epoch": 0.7084567291872327,
      "grad_norm": 0.060549937188625336,
      "learning_rate": 8.74629812438302e-05,
      "loss": 0.2536,
      "step": 215300
    },
    {
      "epoch": 0.7087857847976308,
      "grad_norm": 8.687808990478516,
      "learning_rate": 8.736426456071074e-05,
      "loss": 0.2134,
      "step": 215400
    },
    {
      "epoch": 0.709114840408029,
      "grad_norm": 63.24037551879883,
      "learning_rate": 8.72655478775913e-05,
      "loss": 0.3523,
      "step": 215500
    },
    {
      "epoch": 0.7094438960184272,
      "grad_norm": 13.910781860351562,
      "learning_rate": 8.716683119447186e-05,
      "loss": 0.2245,
      "step": 215600
    },
    {
      "epoch": 0.7097729516288253,
      "grad_norm": 1.0586223602294922,
      "learning_rate": 8.706811451135241e-05,
      "loss": 0.1861,
      "step": 215700
    },
    {
      "epoch": 0.7101020072392235,
      "grad_norm": 0.0002921474224422127,
      "learning_rate": 8.696939782823296e-05,
      "loss": 0.3054,
      "step": 215800
    },
    {
      "epoch": 0.7104310628496215,
      "grad_norm": 0.0007315735565498471,
      "learning_rate": 8.687068114511351e-05,
      "loss": 0.3328,
      "step": 215900
    },
    {
      "epoch": 0.7107601184600197,
      "grad_norm": 0.00015452249499503523,
      "learning_rate": 8.677196446199407e-05,
      "loss": 0.278,
      "step": 216000
    },
    {
      "epoch": 0.7110891740704179,
      "grad_norm": 107.43665313720703,
      "learning_rate": 8.667324777887462e-05,
      "loss": 0.1796,
      "step": 216100
    },
    {
      "epoch": 0.711418229680816,
      "grad_norm": 0.018911106511950493,
      "learning_rate": 8.657453109575517e-05,
      "loss": 0.2602,
      "step": 216200
    },
    {
      "epoch": 0.7117472852912142,
      "grad_norm": 0.0004167183069512248,
      "learning_rate": 8.647581441263572e-05,
      "loss": 0.1601,
      "step": 216300
    },
    {
      "epoch": 0.7120763409016124,
      "grad_norm": 0.0025702498387545347,
      "learning_rate": 8.637709772951628e-05,
      "loss": 0.246,
      "step": 216400
    },
    {
      "epoch": 0.7124053965120105,
      "grad_norm": 0.00018389096658211201,
      "learning_rate": 8.627838104639684e-05,
      "loss": 0.1952,
      "step": 216500
    },
    {
      "epoch": 0.7127344521224087,
      "grad_norm": 0.01023333054035902,
      "learning_rate": 8.617966436327738e-05,
      "loss": 0.1309,
      "step": 216600
    },
    {
      "epoch": 0.7130635077328068,
      "grad_norm": 34.052642822265625,
      "learning_rate": 8.608094768015793e-05,
      "loss": 0.2276,
      "step": 216700
    },
    {
      "epoch": 0.713392563343205,
      "grad_norm": 0.013517223298549652,
      "learning_rate": 8.598223099703849e-05,
      "loss": 0.1563,
      "step": 216800
    },
    {
      "epoch": 0.7137216189536032,
      "grad_norm": 72.25952911376953,
      "learning_rate": 8.588351431391905e-05,
      "loss": 0.2628,
      "step": 216900
    },
    {
      "epoch": 0.7140506745640013,
      "grad_norm": 36.064395904541016,
      "learning_rate": 8.578479763079959e-05,
      "loss": 0.17,
      "step": 217000
    },
    {
      "epoch": 0.7143797301743995,
      "grad_norm": 108.77197265625,
      "learning_rate": 8.568608094768015e-05,
      "loss": 0.1339,
      "step": 217100
    },
    {
      "epoch": 0.7147087857847977,
      "grad_norm": 0.00031427378416992724,
      "learning_rate": 8.55873642645607e-05,
      "loss": 0.2247,
      "step": 217200
    },
    {
      "epoch": 0.7150378413951958,
      "grad_norm": 6.716396808624268,
      "learning_rate": 8.548864758144126e-05,
      "loss": 0.1745,
      "step": 217300
    },
    {
      "epoch": 0.715366897005594,
      "grad_norm": 0.01556477602571249,
      "learning_rate": 8.53899308983218e-05,
      "loss": 0.2762,
      "step": 217400
    },
    {
      "epoch": 0.7156959526159921,
      "grad_norm": 0.0059399777092039585,
      "learning_rate": 8.529121421520236e-05,
      "loss": 0.1415,
      "step": 217500
    },
    {
      "epoch": 0.7160250082263903,
      "grad_norm": 0.0028672998305410147,
      "learning_rate": 8.519249753208291e-05,
      "loss": 0.075,
      "step": 217600
    },
    {
      "epoch": 0.7163540638367885,
      "grad_norm": 4.8426618576049805,
      "learning_rate": 8.509378084896347e-05,
      "loss": 0.1408,
      "step": 217700
    },
    {
      "epoch": 0.7166831194471865,
      "grad_norm": 5.212688847677782e-05,
      "learning_rate": 8.4995064165844e-05,
      "loss": 0.2313,
      "step": 217800
    },
    {
      "epoch": 0.7170121750575847,
      "grad_norm": 84.35125732421875,
      "learning_rate": 8.489634748272457e-05,
      "loss": 0.42,
      "step": 217900
    },
    {
      "epoch": 0.7173412306679829,
      "grad_norm": 24.67974281311035,
      "learning_rate": 8.479763079960513e-05,
      "loss": 0.3344,
      "step": 218000
    },
    {
      "epoch": 0.717670286278381,
      "grad_norm": 0.0003064449119847268,
      "learning_rate": 8.469891411648568e-05,
      "loss": 0.2731,
      "step": 218100
    },
    {
      "epoch": 0.7179993418887792,
      "grad_norm": 0.016080480068922043,
      "learning_rate": 8.460019743336623e-05,
      "loss": 0.2052,
      "step": 218200
    },
    {
      "epoch": 0.7183283974991773,
      "grad_norm": 0.0016229348257184029,
      "learning_rate": 8.450148075024678e-05,
      "loss": 0.1832,
      "step": 218300
    },
    {
      "epoch": 0.7186574531095755,
      "grad_norm": 0.00044289560173638165,
      "learning_rate": 8.440276406712734e-05,
      "loss": 0.2536,
      "step": 218400
    },
    {
      "epoch": 0.7189865087199737,
      "grad_norm": 0.0068304589949548244,
      "learning_rate": 8.430404738400789e-05,
      "loss": 0.2167,
      "step": 218500
    },
    {
      "epoch": 0.7193155643303718,
      "grad_norm": 1.9883310794830322,
      "learning_rate": 8.420533070088844e-05,
      "loss": 0.2855,
      "step": 218600
    },
    {
      "epoch": 0.71964461994077,
      "grad_norm": 0.0003505087806843221,
      "learning_rate": 8.410661401776899e-05,
      "loss": 0.2072,
      "step": 218700
    },
    {
      "epoch": 0.7199736755511682,
      "grad_norm": 0.00015521164459642023,
      "learning_rate": 8.400789733464955e-05,
      "loss": 0.286,
      "step": 218800
    },
    {
      "epoch": 0.7203027311615663,
      "grad_norm": 0.00764639163389802,
      "learning_rate": 8.390918065153011e-05,
      "loss": 0.255,
      "step": 218900
    },
    {
      "epoch": 0.7206317867719645,
      "grad_norm": 0.0021306246053427458,
      "learning_rate": 8.381046396841065e-05,
      "loss": 0.439,
      "step": 219000
    },
    {
      "epoch": 0.7209608423823626,
      "grad_norm": 0.0001263885060325265,
      "learning_rate": 8.371174728529121e-05,
      "loss": 0.3593,
      "step": 219100
    },
    {
      "epoch": 0.7212898979927608,
      "grad_norm": 0.002548839198425412,
      "learning_rate": 8.361303060217176e-05,
      "loss": 0.2708,
      "step": 219200
    },
    {
      "epoch": 0.721618953603159,
      "grad_norm": 50.01400375366211,
      "learning_rate": 8.351431391905232e-05,
      "loss": 0.2289,
      "step": 219300
    },
    {
      "epoch": 0.7219480092135571,
      "grad_norm": 0.003836800577118993,
      "learning_rate": 8.341559723593286e-05,
      "loss": 0.2603,
      "step": 219400
    },
    {
      "epoch": 0.7222770648239553,
      "grad_norm": 12.534186363220215,
      "learning_rate": 8.331688055281342e-05,
      "loss": 0.1805,
      "step": 219500
    },
    {
      "epoch": 0.7226061204343535,
      "grad_norm": 17.39512825012207,
      "learning_rate": 8.321816386969397e-05,
      "loss": 0.189,
      "step": 219600
    },
    {
      "epoch": 0.7229351760447515,
      "grad_norm": 0.0009773541241884232,
      "learning_rate": 8.311944718657453e-05,
      "loss": 0.2781,
      "step": 219700
    },
    {
      "epoch": 0.7232642316551497,
      "grad_norm": 105.19061279296875,
      "learning_rate": 8.302073050345506e-05,
      "loss": 0.3451,
      "step": 219800
    },
    {
      "epoch": 0.7235932872655478,
      "grad_norm": 0.011676672846078873,
      "learning_rate": 8.292201382033563e-05,
      "loss": 0.2613,
      "step": 219900
    },
    {
      "epoch": 0.723922342875946,
      "grad_norm": 0.007489246316254139,
      "learning_rate": 8.282329713721619e-05,
      "loss": 0.1979,
      "step": 220000
    },
    {
      "epoch": 0.7242513984863442,
      "grad_norm": 6.694805051665753e-05,
      "learning_rate": 8.272458045409674e-05,
      "loss": 0.199,
      "step": 220100
    },
    {
      "epoch": 0.7245804540967423,
      "grad_norm": 0.00011324689694447443,
      "learning_rate": 8.262586377097729e-05,
      "loss": 0.2746,
      "step": 220200
    },
    {
      "epoch": 0.7249095097071405,
      "grad_norm": 0.0425875261425972,
      "learning_rate": 8.252714708785784e-05,
      "loss": 0.2991,
      "step": 220300
    },
    {
      "epoch": 0.7252385653175387,
      "grad_norm": 0.002612409647554159,
      "learning_rate": 8.24284304047384e-05,
      "loss": 0.4081,
      "step": 220400
    },
    {
      "epoch": 0.7255676209279368,
      "grad_norm": 0.005204176064580679,
      "learning_rate": 8.232971372161895e-05,
      "loss": 0.368,
      "step": 220500
    },
    {
      "epoch": 0.725896676538335,
      "grad_norm": 83.08019256591797,
      "learning_rate": 8.22309970384995e-05,
      "loss": 0.1307,
      "step": 220600
    },
    {
      "epoch": 0.7262257321487331,
      "grad_norm": 0.002583869965746999,
      "learning_rate": 8.213228035538004e-05,
      "loss": 0.0978,
      "step": 220700
    },
    {
      "epoch": 0.7265547877591313,
      "grad_norm": 0.00025637977523729205,
      "learning_rate": 8.20335636722606e-05,
      "loss": 0.1875,
      "step": 220800
    },
    {
      "epoch": 0.7268838433695295,
      "grad_norm": 0.0026011033914983273,
      "learning_rate": 8.193484698914117e-05,
      "loss": 0.4433,
      "step": 220900
    },
    {
      "epoch": 0.7272128989799276,
      "grad_norm": 0.0028864177875220776,
      "learning_rate": 8.18361303060217e-05,
      "loss": 0.3786,
      "step": 221000
    },
    {
      "epoch": 0.7275419545903258,
      "grad_norm": 0.0008966830791905522,
      "learning_rate": 8.173741362290225e-05,
      "loss": 0.3409,
      "step": 221100
    },
    {
      "epoch": 0.7278710102007239,
      "grad_norm": 0.00010085733083542436,
      "learning_rate": 8.163869693978282e-05,
      "loss": 0.2383,
      "step": 221200
    },
    {
      "epoch": 0.7282000658111221,
      "grad_norm": 0.011149552650749683,
      "learning_rate": 8.153998025666338e-05,
      "loss": 0.1972,
      "step": 221300
    },
    {
      "epoch": 0.7285291214215203,
      "grad_norm": 0.01759152300655842,
      "learning_rate": 8.144126357354393e-05,
      "loss": 0.2205,
      "step": 221400
    },
    {
      "epoch": 0.7288581770319184,
      "grad_norm": 0.0015787282027304173,
      "learning_rate": 8.134254689042447e-05,
      "loss": 0.2389,
      "step": 221500
    },
    {
      "epoch": 0.7291872326423166,
      "grad_norm": 0.000748610938899219,
      "learning_rate": 8.124383020730502e-05,
      "loss": 0.3701,
      "step": 221600
    },
    {
      "epoch": 0.7295162882527148,
      "grad_norm": 0.00711452541872859,
      "learning_rate": 8.114511352418559e-05,
      "loss": 0.3553,
      "step": 221700
    },
    {
      "epoch": 0.7298453438631128,
      "grad_norm": 3.550276517868042,
      "learning_rate": 8.104639684106613e-05,
      "loss": 0.3897,
      "step": 221800
    },
    {
      "epoch": 0.730174399473511,
      "grad_norm": 0.00015919795259833336,
      "learning_rate": 8.094768015794668e-05,
      "loss": 0.2937,
      "step": 221900
    },
    {
      "epoch": 0.7305034550839091,
      "grad_norm": 0.04337497800588608,
      "learning_rate": 8.084896347482723e-05,
      "loss": 0.301,
      "step": 222000
    },
    {
      "epoch": 0.7308325106943073,
      "grad_norm": 0.00021739082876592875,
      "learning_rate": 8.07502467917078e-05,
      "loss": 0.2031,
      "step": 222100
    },
    {
      "epoch": 0.7311615663047055,
      "grad_norm": 8.164276123046875,
      "learning_rate": 8.065153010858836e-05,
      "loss": 0.1832,
      "step": 222200
    },
    {
      "epoch": 0.7314906219151036,
      "grad_norm": 0.0031059281900525093,
      "learning_rate": 8.055281342546889e-05,
      "loss": 0.229,
      "step": 222300
    },
    {
      "epoch": 0.7318196775255018,
      "grad_norm": 0.0002936889068223536,
      "learning_rate": 8.045409674234945e-05,
      "loss": 0.3492,
      "step": 222400
    },
    {
      "epoch": 0.7321487331359,
      "grad_norm": 0.04521188139915466,
      "learning_rate": 8.035538005923e-05,
      "loss": 0.3043,
      "step": 222500
    },
    {
      "epoch": 0.7324777887462981,
      "grad_norm": 0.00016246389714069664,
      "learning_rate": 8.025666337611057e-05,
      "loss": 0.2131,
      "step": 222600
    },
    {
      "epoch": 0.7328068443566963,
      "grad_norm": 0.001059239380992949,
      "learning_rate": 8.01579466929911e-05,
      "loss": 0.3014,
      "step": 222700
    },
    {
      "epoch": 0.7331358999670944,
      "grad_norm": 0.0021439685951918364,
      "learning_rate": 8.005923000987166e-05,
      "loss": 0.2947,
      "step": 222800
    },
    {
      "epoch": 0.7334649555774926,
      "grad_norm": 0.003734468249604106,
      "learning_rate": 7.996051332675221e-05,
      "loss": 0.3344,
      "step": 222900
    },
    {
      "epoch": 0.7337940111878908,
      "grad_norm": 0.4084450900554657,
      "learning_rate": 7.986179664363277e-05,
      "loss": 0.4474,
      "step": 223000
    },
    {
      "epoch": 0.7341230667982889,
      "grad_norm": 0.0005258680321276188,
      "learning_rate": 7.976307996051331e-05,
      "loss": 0.1651,
      "step": 223100
    },
    {
      "epoch": 0.7344521224086871,
      "grad_norm": 5.486359119415283,
      "learning_rate": 7.966436327739387e-05,
      "loss": 0.1892,
      "step": 223200
    },
    {
      "epoch": 0.7347811780190853,
      "grad_norm": 0.0014952522469684482,
      "learning_rate": 7.956564659427443e-05,
      "loss": 0.3718,
      "step": 223300
    },
    {
      "epoch": 0.7351102336294834,
      "grad_norm": 45.72056198120117,
      "learning_rate": 7.946692991115498e-05,
      "loss": 0.1067,
      "step": 223400
    },
    {
      "epoch": 0.7354392892398816,
      "grad_norm": 0.0035649181809276342,
      "learning_rate": 7.936821322803553e-05,
      "loss": 0.1718,
      "step": 223500
    },
    {
      "epoch": 0.7357683448502796,
      "grad_norm": 0.0002073102368740365,
      "learning_rate": 7.926949654491608e-05,
      "loss": 0.1788,
      "step": 223600
    },
    {
      "epoch": 0.7360974004606778,
      "grad_norm": 0.024740278720855713,
      "learning_rate": 7.917077986179664e-05,
      "loss": 0.166,
      "step": 223700
    },
    {
      "epoch": 0.736426456071076,
      "grad_norm": 7.5311737060546875,
      "learning_rate": 7.907206317867719e-05,
      "loss": 0.3208,
      "step": 223800
    },
    {
      "epoch": 0.7367555116814741,
      "grad_norm": 17.366600036621094,
      "learning_rate": 7.897334649555774e-05,
      "loss": 0.352,
      "step": 223900
    },
    {
      "epoch": 0.7370845672918723,
      "grad_norm": 43.63774490356445,
      "learning_rate": 7.887462981243829e-05,
      "loss": 0.2115,
      "step": 224000
    },
    {
      "epoch": 0.7374136229022705,
      "grad_norm": 0.0041889892891049385,
      "learning_rate": 7.877591312931885e-05,
      "loss": 0.2148,
      "step": 224100
    },
    {
      "epoch": 0.7377426785126686,
      "grad_norm": 0.006772778462618589,
      "learning_rate": 7.867719644619941e-05,
      "loss": 0.128,
      "step": 224200
    },
    {
      "epoch": 0.7380717341230668,
      "grad_norm": 77.73867797851562,
      "learning_rate": 7.857847976307995e-05,
      "loss": 0.1663,
      "step": 224300
    },
    {
      "epoch": 0.7384007897334649,
      "grad_norm": 0.0009694989421404898,
      "learning_rate": 7.847976307996051e-05,
      "loss": 0.163,
      "step": 224400
    },
    {
      "epoch": 0.7387298453438631,
      "grad_norm": 0.005654821638017893,
      "learning_rate": 7.838104639684106e-05,
      "loss": 0.2675,
      "step": 224500
    },
    {
      "epoch": 0.7390589009542613,
      "grad_norm": 0.00327006122097373,
      "learning_rate": 7.828232971372162e-05,
      "loss": 0.3997,
      "step": 224600
    },
    {
      "epoch": 0.7393879565646594,
      "grad_norm": 9.409496307373047,
      "learning_rate": 7.818361303060216e-05,
      "loss": 0.1392,
      "step": 224700
    },
    {
      "epoch": 0.7397170121750576,
      "grad_norm": 0.001718692947179079,
      "learning_rate": 7.808489634748272e-05,
      "loss": 0.2433,
      "step": 224800
    },
    {
      "epoch": 0.7400460677854558,
      "grad_norm": 0.010883227922022343,
      "learning_rate": 7.798617966436327e-05,
      "loss": 0.1317,
      "step": 224900
    },
    {
      "epoch": 0.7403751233958539,
      "grad_norm": 0.00043228131835348904,
      "learning_rate": 7.788746298124383e-05,
      "loss": 0.2041,
      "step": 225000
    },
    {
      "epoch": 0.7407041790062521,
      "grad_norm": 60.64434051513672,
      "learning_rate": 7.778874629812437e-05,
      "loss": 0.2163,
      "step": 225100
    },
    {
      "epoch": 0.7410332346166502,
      "grad_norm": 0.00024195080914068967,
      "learning_rate": 7.769002961500493e-05,
      "loss": 0.1861,
      "step": 225200
    },
    {
      "epoch": 0.7413622902270484,
      "grad_norm": 0.44429251551628113,
      "learning_rate": 7.759131293188548e-05,
      "loss": 0.2037,
      "step": 225300
    },
    {
      "epoch": 0.7416913458374466,
      "grad_norm": 0.00014134829689282924,
      "learning_rate": 7.749259624876604e-05,
      "loss": 0.1303,
      "step": 225400
    },
    {
      "epoch": 0.7420204014478446,
      "grad_norm": 0.00046767917228862643,
      "learning_rate": 7.739387956564658e-05,
      "loss": 0.2034,
      "step": 225500
    },
    {
      "epoch": 0.7423494570582428,
      "grad_norm": 0.0003493329859338701,
      "learning_rate": 7.729516288252714e-05,
      "loss": 0.3792,
      "step": 225600
    },
    {
      "epoch": 0.742678512668641,
      "grad_norm": 0.0004119937657378614,
      "learning_rate": 7.71964461994077e-05,
      "loss": 0.1283,
      "step": 225700
    },
    {
      "epoch": 0.7430075682790391,
      "grad_norm": 0.0012859326088801026,
      "learning_rate": 7.709772951628825e-05,
      "loss": 0.2967,
      "step": 225800
    },
    {
      "epoch": 0.7433366238894373,
      "grad_norm": 0.013943304307758808,
      "learning_rate": 7.69990128331688e-05,
      "loss": 0.3852,
      "step": 225900
    },
    {
      "epoch": 0.7436656794998354,
      "grad_norm": 0.0002236377913504839,
      "learning_rate": 7.690029615004935e-05,
      "loss": 0.1751,
      "step": 226000
    },
    {
      "epoch": 0.7439947351102336,
      "grad_norm": 0.00019676468218676746,
      "learning_rate": 7.680157946692991e-05,
      "loss": 0.2474,
      "step": 226100
    },
    {
      "epoch": 0.7443237907206318,
      "grad_norm": 0.00031660153763368726,
      "learning_rate": 7.670286278381046e-05,
      "loss": 0.1782,
      "step": 226200
    },
    {
      "epoch": 0.7446528463310299,
      "grad_norm": 0.007247999310493469,
      "learning_rate": 7.660414610069101e-05,
      "loss": 0.1864,
      "step": 226300
    },
    {
      "epoch": 0.7449819019414281,
      "grad_norm": 0.0011787465773522854,
      "learning_rate": 7.650542941757156e-05,
      "loss": 0.3957,
      "step": 226400
    },
    {
      "epoch": 0.7453109575518263,
      "grad_norm": 0.32724839448928833,
      "learning_rate": 7.640671273445212e-05,
      "loss": 0.3384,
      "step": 226500
    },
    {
      "epoch": 0.7456400131622244,
      "grad_norm": 0.0002116919495165348,
      "learning_rate": 7.630799605133268e-05,
      "loss": 0.1627,
      "step": 226600
    },
    {
      "epoch": 0.7459690687726226,
      "grad_norm": 0.00037997361505404115,
      "learning_rate": 7.620927936821322e-05,
      "loss": 0.2242,
      "step": 226700
    },
    {
      "epoch": 0.7462981243830207,
      "grad_norm": 0.0001781238679541275,
      "learning_rate": 7.611056268509378e-05,
      "loss": 0.2829,
      "step": 226800
    },
    {
      "epoch": 0.7466271799934189,
      "grad_norm": 0.0015274406177923083,
      "learning_rate": 7.601184600197433e-05,
      "loss": 0.1856,
      "step": 226900
    },
    {
      "epoch": 0.7469562356038171,
      "grad_norm": 0.00026627257466316223,
      "learning_rate": 7.591312931885489e-05,
      "loss": 0.272,
      "step": 227000
    },
    {
      "epoch": 0.7472852912142152,
      "grad_norm": 0.015167011879384518,
      "learning_rate": 7.581441263573543e-05,
      "loss": 0.3657,
      "step": 227100
    },
    {
      "epoch": 0.7476143468246134,
      "grad_norm": 18.34540367126465,
      "learning_rate": 7.571569595261599e-05,
      "loss": 0.1959,
      "step": 227200
    },
    {
      "epoch": 0.7479434024350116,
      "grad_norm": 0.0001344395859632641,
      "learning_rate": 7.561697926949654e-05,
      "loss": 0.2247,
      "step": 227300
    },
    {
      "epoch": 0.7482724580454096,
      "grad_norm": 0.0007637760136276484,
      "learning_rate": 7.55182625863771e-05,
      "loss": 0.1543,
      "step": 227400
    },
    {
      "epoch": 0.7486015136558078,
      "grad_norm": 4.3581931095104665e-05,
      "learning_rate": 7.541954590325763e-05,
      "loss": 0.1354,
      "step": 227500
    },
    {
      "epoch": 0.7489305692662059,
      "grad_norm": 0.0006554159917868674,
      "learning_rate": 7.53208292201382e-05,
      "loss": 0.3214,
      "step": 227600
    },
    {
      "epoch": 0.7492596248766041,
      "grad_norm": 0.002036596182733774,
      "learning_rate": 7.522211253701876e-05,
      "loss": 0.1994,
      "step": 227700
    },
    {
      "epoch": 0.7495886804870023,
      "grad_norm": 0.0017944903811439872,
      "learning_rate": 7.512339585389931e-05,
      "loss": 0.2425,
      "step": 227800
    },
    {
      "epoch": 0.7499177360974004,
      "grad_norm": 0.00740276463329792,
      "learning_rate": 7.502467917077986e-05,
      "loss": 0.1621,
      "step": 227900
    },
    {
      "epoch": 0.7502467917077986,
      "grad_norm": 47.844234466552734,
      "learning_rate": 7.49259624876604e-05,
      "loss": 0.2254,
      "step": 228000
    },
    {
      "epoch": 0.7505758473181968,
      "grad_norm": 0.0038981640245765448,
      "learning_rate": 7.482724580454097e-05,
      "loss": 0.1431,
      "step": 228100
    },
    {
      "epoch": 0.7509049029285949,
      "grad_norm": 0.006814818363636732,
      "learning_rate": 7.472852912142152e-05,
      "loss": 0.1351,
      "step": 228200
    },
    {
      "epoch": 0.7512339585389931,
      "grad_norm": 0.0021248129196465015,
      "learning_rate": 7.462981243830207e-05,
      "loss": 0.2404,
      "step": 228300
    },
    {
      "epoch": 0.7515630141493912,
      "grad_norm": 0.005008377600461245,
      "learning_rate": 7.453109575518261e-05,
      "loss": 0.2591,
      "step": 228400
    },
    {
      "epoch": 0.7518920697597894,
      "grad_norm": 0.0001452985015930608,
      "learning_rate": 7.443237907206318e-05,
      "loss": 0.2022,
      "step": 228500
    },
    {
      "epoch": 0.7522211253701876,
      "grad_norm": 0.15740461647510529,
      "learning_rate": 7.433366238894372e-05,
      "loss": 0.2707,
      "step": 228600
    },
    {
      "epoch": 0.7525501809805857,
      "grad_norm": 0.0007745231268927455,
      "learning_rate": 7.423494570582429e-05,
      "loss": 0.2832,
      "step": 228700
    },
    {
      "epoch": 0.7528792365909839,
      "grad_norm": 0.003998161293566227,
      "learning_rate": 7.413622902270484e-05,
      "loss": 0.2456,
      "step": 228800
    },
    {
      "epoch": 0.7532082922013821,
      "grad_norm": 0.0018481692532077432,
      "learning_rate": 7.403751233958538e-05,
      "loss": 0.3277,
      "step": 228900
    },
    {
      "epoch": 0.7535373478117802,
      "grad_norm": 0.017195651307702065,
      "learning_rate": 7.393879565646593e-05,
      "loss": 0.3544,
      "step": 229000
    },
    {
      "epoch": 0.7538664034221784,
      "grad_norm": 0.00016463178326375782,
      "learning_rate": 7.38400789733465e-05,
      "loss": 0.2294,
      "step": 229100
    },
    {
      "epoch": 0.7541954590325765,
      "grad_norm": 0.005337189882993698,
      "learning_rate": 7.374136229022704e-05,
      "loss": 0.1342,
      "step": 229200
    },
    {
      "epoch": 0.7545245146429747,
      "grad_norm": 0.001320110633969307,
      "learning_rate": 7.36426456071076e-05,
      "loss": 0.233,
      "step": 229300
    },
    {
      "epoch": 0.7548535702533729,
      "grad_norm": 0.0003651929728221148,
      "learning_rate": 7.354392892398814e-05,
      "loss": 0.1886,
      "step": 229400
    },
    {
      "epoch": 0.7551826258637709,
      "grad_norm": 0.0018135681748390198,
      "learning_rate": 7.34452122408687e-05,
      "loss": 0.2596,
      "step": 229500
    },
    {
      "epoch": 0.7555116814741691,
      "grad_norm": 0.00024501740699633956,
      "learning_rate": 7.334649555774925e-05,
      "loss": 0.3744,
      "step": 229600
    },
    {
      "epoch": 0.7558407370845673,
      "grad_norm": 0.025019878521561623,
      "learning_rate": 7.32477788746298e-05,
      "loss": 0.3063,
      "step": 229700
    },
    {
      "epoch": 0.7561697926949654,
      "grad_norm": 0.0008828069549053907,
      "learning_rate": 7.314906219151035e-05,
      "loss": 0.227,
      "step": 229800
    },
    {
      "epoch": 0.7564988483053636,
      "grad_norm": 0.0003752861521206796,
      "learning_rate": 7.305034550839091e-05,
      "loss": 0.1759,
      "step": 229900
    },
    {
      "epoch": 0.7568279039157617,
      "grad_norm": 8.574397087097168,
      "learning_rate": 7.295162882527146e-05,
      "loss": 0.0819,
      "step": 230000
    },
    {
      "epoch": 0.7571569595261599,
      "grad_norm": 0.06178733706474304,
      "learning_rate": 7.285291214215202e-05,
      "loss": 0.3586,
      "step": 230100
    },
    {
      "epoch": 0.7574860151365581,
      "grad_norm": 0.0034073805436491966,
      "learning_rate": 7.275419545903257e-05,
      "loss": 0.179,
      "step": 230200
    },
    {
      "epoch": 0.7578150707469562,
      "grad_norm": 0.0015366721199825406,
      "learning_rate": 7.265547877591312e-05,
      "loss": 0.2339,
      "step": 230300
    },
    {
      "epoch": 0.7581441263573544,
      "grad_norm": 0.00025312977959401906,
      "learning_rate": 7.255676209279367e-05,
      "loss": 0.2007,
      "step": 230400
    },
    {
      "epoch": 0.7584731819677526,
      "grad_norm": 40.990203857421875,
      "learning_rate": 7.245804540967423e-05,
      "loss": 0.3427,
      "step": 230500
    },
    {
      "epoch": 0.7588022375781507,
      "grad_norm": 0.029780251905322075,
      "learning_rate": 7.235932872655478e-05,
      "loss": 0.2621,
      "step": 230600
    },
    {
      "epoch": 0.7591312931885489,
      "grad_norm": 0.00501598697155714,
      "learning_rate": 7.226061204343533e-05,
      "loss": 0.0643,
      "step": 230700
    },
    {
      "epoch": 0.759460348798947,
      "grad_norm": 0.00017089945322368294,
      "learning_rate": 7.216189536031588e-05,
      "loss": 0.1758,
      "step": 230800
    },
    {
      "epoch": 0.7597894044093452,
      "grad_norm": 0.00038087385473772883,
      "learning_rate": 7.206317867719644e-05,
      "loss": 0.2626,
      "step": 230900
    },
    {
      "epoch": 0.7601184600197434,
      "grad_norm": 0.009405401535332203,
      "learning_rate": 7.196446199407699e-05,
      "loss": 0.3037,
      "step": 231000
    },
    {
      "epoch": 0.7604475156301415,
      "grad_norm": 48.79792404174805,
      "learning_rate": 7.186574531095755e-05,
      "loss": 0.2882,
      "step": 231100
    },
    {
      "epoch": 0.7607765712405397,
      "grad_norm": 0.0009094203123822808,
      "learning_rate": 7.17670286278381e-05,
      "loss": 0.2013,
      "step": 231200
    },
    {
      "epoch": 0.7611056268509379,
      "grad_norm": 68.6833267211914,
      "learning_rate": 7.166831194471865e-05,
      "loss": 0.2755,
      "step": 231300
    },
    {
      "epoch": 0.7614346824613359,
      "grad_norm": 0.06704319268465042,
      "learning_rate": 7.15695952615992e-05,
      "loss": 0.3698,
      "step": 231400
    },
    {
      "epoch": 0.7617637380717341,
      "grad_norm": 0.09999500960111618,
      "learning_rate": 7.147087857847976e-05,
      "loss": 0.2279,
      "step": 231500
    },
    {
      "epoch": 0.7620927936821322,
      "grad_norm": 36.509700775146484,
      "learning_rate": 7.137216189536031e-05,
      "loss": 0.2762,
      "step": 231600
    },
    {
      "epoch": 0.7624218492925304,
      "grad_norm": 0.0004915744648315012,
      "learning_rate": 7.127344521224086e-05,
      "loss": 0.2062,
      "step": 231700
    },
    {
      "epoch": 0.7627509049029286,
      "grad_norm": 0.00497889518737793,
      "learning_rate": 7.117472852912141e-05,
      "loss": 0.4486,
      "step": 231800
    },
    {
      "epoch": 0.7630799605133267,
      "grad_norm": 0.00013219844549894333,
      "learning_rate": 7.107601184600197e-05,
      "loss": 0.192,
      "step": 231900
    },
    {
      "epoch": 0.7634090161237249,
      "grad_norm": 0.004713601432740688,
      "learning_rate": 7.097729516288252e-05,
      "loss": 0.2559,
      "step": 232000
    },
    {
      "epoch": 0.7637380717341231,
      "grad_norm": 0.0010663431603461504,
      "learning_rate": 7.087857847976308e-05,
      "loss": 0.2348,
      "step": 232100
    },
    {
      "epoch": 0.7640671273445212,
      "grad_norm": 0.00029709452064707875,
      "learning_rate": 7.077986179664363e-05,
      "loss": 0.2832,
      "step": 232200
    },
    {
      "epoch": 0.7643961829549194,
      "grad_norm": 26.83209991455078,
      "learning_rate": 7.068114511352418e-05,
      "loss": 0.2573,
      "step": 232300
    },
    {
      "epoch": 0.7647252385653175,
      "grad_norm": 0.001246838248334825,
      "learning_rate": 7.058242843040473e-05,
      "loss": 0.1766,
      "step": 232400
    },
    {
      "epoch": 0.7650542941757157,
      "grad_norm": 5.398948669433594,
      "learning_rate": 7.048371174728529e-05,
      "loss": 0.3308,
      "step": 232500
    },
    {
      "epoch": 0.7653833497861139,
      "grad_norm": 6.613227014895529e-05,
      "learning_rate": 7.038499506416584e-05,
      "loss": 0.3129,
      "step": 232600
    },
    {
      "epoch": 0.765712405396512,
      "grad_norm": 119.13850402832031,
      "learning_rate": 7.028627838104639e-05,
      "loss": 0.2567,
      "step": 232700
    },
    {
      "epoch": 0.7660414610069102,
      "grad_norm": 0.00023823435185477138,
      "learning_rate": 7.018756169792694e-05,
      "loss": 0.2738,
      "step": 232800
    },
    {
      "epoch": 0.7663705166173084,
      "grad_norm": 0.0015055022668093443,
      "learning_rate": 7.00888450148075e-05,
      "loss": 0.2549,
      "step": 232900
    },
    {
      "epoch": 0.7666995722277065,
      "grad_norm": 108.04753875732422,
      "learning_rate": 6.999012833168805e-05,
      "loss": 0.2685,
      "step": 233000
    },
    {
      "epoch": 0.7670286278381047,
      "grad_norm": 90.55714416503906,
      "learning_rate": 6.98914116485686e-05,
      "loss": 0.1781,
      "step": 233100
    },
    {
      "epoch": 0.7673576834485027,
      "grad_norm": 0.000501709058880806,
      "learning_rate": 6.979269496544915e-05,
      "loss": 0.2412,
      "step": 233200
    },
    {
      "epoch": 0.767686739058901,
      "grad_norm": 14.049138069152832,
      "learning_rate": 6.969397828232971e-05,
      "loss": 0.2807,
      "step": 233300
    },
    {
      "epoch": 0.7680157946692991,
      "grad_norm": 0.005047765094786882,
      "learning_rate": 6.959526159921026e-05,
      "loss": 0.2201,
      "step": 233400
    },
    {
      "epoch": 0.7683448502796972,
      "grad_norm": 0.001963905291631818,
      "learning_rate": 6.949654491609082e-05,
      "loss": 0.1697,
      "step": 233500
    },
    {
      "epoch": 0.7686739058900954,
      "grad_norm": 0.03618140518665314,
      "learning_rate": 6.939782823297137e-05,
      "loss": 0.133,
      "step": 233600
    },
    {
      "epoch": 0.7690029615004936,
      "grad_norm": 7.111116409301758,
      "learning_rate": 6.929911154985192e-05,
      "loss": 0.2294,
      "step": 233700
    },
    {
      "epoch": 0.7693320171108917,
      "grad_norm": 0.0059239864349365234,
      "learning_rate": 6.920039486673247e-05,
      "loss": 0.2756,
      "step": 233800
    },
    {
      "epoch": 0.7696610727212899,
      "grad_norm": 0.00010873073915718123,
      "learning_rate": 6.910167818361303e-05,
      "loss": 0.2093,
      "step": 233900
    },
    {
      "epoch": 0.769990128331688,
      "grad_norm": 0.0003072776598855853,
      "learning_rate": 6.900296150049358e-05,
      "loss": 0.1162,
      "step": 234000
    },
    {
      "epoch": 0.7703191839420862,
      "grad_norm": 0.014021617360413074,
      "learning_rate": 6.890424481737413e-05,
      "loss": 0.2819,
      "step": 234100
    },
    {
      "epoch": 0.7706482395524844,
      "grad_norm": 0.0007208988536149263,
      "learning_rate": 6.880552813425468e-05,
      "loss": 0.1886,
      "step": 234200
    },
    {
      "epoch": 0.7709772951628825,
      "grad_norm": 125.87893676757812,
      "learning_rate": 6.870681145113524e-05,
      "loss": 0.2478,
      "step": 234300
    },
    {
      "epoch": 0.7713063507732807,
      "grad_norm": 0.0006022455054335296,
      "learning_rate": 6.860809476801579e-05,
      "loss": 0.1753,
      "step": 234400
    },
    {
      "epoch": 0.7716354063836789,
      "grad_norm": 0.00012551774852909148,
      "learning_rate": 6.850937808489635e-05,
      "loss": 0.3707,
      "step": 234500
    },
    {
      "epoch": 0.771964461994077,
      "grad_norm": 0.059998080134391785,
      "learning_rate": 6.84106614017769e-05,
      "loss": 0.2648,
      "step": 234600
    },
    {
      "epoch": 0.7722935176044752,
      "grad_norm": 0.00029085701680742204,
      "learning_rate": 6.831194471865745e-05,
      "loss": 0.1469,
      "step": 234700
    },
    {
      "epoch": 0.7726225732148733,
      "grad_norm": 0.06956800073385239,
      "learning_rate": 6.8213228035538e-05,
      "loss": 0.2862,
      "step": 234800
    },
    {
      "epoch": 0.7729516288252715,
      "grad_norm": 0.019020473584532738,
      "learning_rate": 6.811451135241856e-05,
      "loss": 0.3496,
      "step": 234900
    },
    {
      "epoch": 0.7732806844356697,
      "grad_norm": 0.022139446809887886,
      "learning_rate": 6.80157946692991e-05,
      "loss": 0.2042,
      "step": 235000
    },
    {
      "epoch": 0.7736097400460678,
      "grad_norm": 0.8790938854217529,
      "learning_rate": 6.791707798617966e-05,
      "loss": 0.1637,
      "step": 235100
    },
    {
      "epoch": 0.773938795656466,
      "grad_norm": 0.004130175802856684,
      "learning_rate": 6.78183613030602e-05,
      "loss": 0.4569,
      "step": 235200
    },
    {
      "epoch": 0.7742678512668641,
      "grad_norm": 0.0032717022113502026,
      "learning_rate": 6.771964461994077e-05,
      "loss": 0.2357,
      "step": 235300
    },
    {
      "epoch": 0.7745969068772622,
      "grad_norm": 0.0003710465389303863,
      "learning_rate": 6.762092793682132e-05,
      "loss": 0.2458,
      "step": 235400
    },
    {
      "epoch": 0.7749259624876604,
      "grad_norm": 42.0477180480957,
      "learning_rate": 6.752221125370188e-05,
      "loss": 0.2855,
      "step": 235500
    },
    {
      "epoch": 0.7752550180980585,
      "grad_norm": 0.0008589195786044002,
      "learning_rate": 6.742349457058243e-05,
      "loss": 0.1808,
      "step": 235600
    },
    {
      "epoch": 0.7755840737084567,
      "grad_norm": 0.00028573538293130696,
      "learning_rate": 6.732477788746297e-05,
      "loss": 0.1769,
      "step": 235700
    },
    {
      "epoch": 0.7759131293188549,
      "grad_norm": 0.0013147108256816864,
      "learning_rate": 6.722606120434352e-05,
      "loss": 0.324,
      "step": 235800
    },
    {
      "epoch": 0.776242184929253,
      "grad_norm": 9.569415124133229e-05,
      "learning_rate": 6.712734452122409e-05,
      "loss": 0.2807,
      "step": 235900
    },
    {
      "epoch": 0.7765712405396512,
      "grad_norm": 0.0003283747937530279,
      "learning_rate": 6.702862783810463e-05,
      "loss": 0.3825,
      "step": 236000
    },
    {
      "epoch": 0.7769002961500494,
      "grad_norm": 0.001729038543999195,
      "learning_rate": 6.692991115498518e-05,
      "loss": 0.1706,
      "step": 236100
    },
    {
      "epoch": 0.7772293517604475,
      "grad_norm": 0.0011940111871808767,
      "learning_rate": 6.683119447186573e-05,
      "loss": 0.1325,
      "step": 236200
    },
    {
      "epoch": 0.7775584073708457,
      "grad_norm": 0.00163955707103014,
      "learning_rate": 6.67324777887463e-05,
      "loss": 0.3768,
      "step": 236300
    },
    {
      "epoch": 0.7778874629812438,
      "grad_norm": 0.0001864235382527113,
      "learning_rate": 6.663376110562684e-05,
      "loss": 0.2554,
      "step": 236400
    },
    {
      "epoch": 0.778216518591642,
      "grad_norm": 6.7581987380981445,
      "learning_rate": 6.65350444225074e-05,
      "loss": 0.3033,
      "step": 236500
    },
    {
      "epoch": 0.7785455742020402,
      "grad_norm": 0.0012455765390768647,
      "learning_rate": 6.643632773938795e-05,
      "loss": 0.0585,
      "step": 236600
    },
    {
      "epoch": 0.7788746298124383,
      "grad_norm": 0.026143662631511688,
      "learning_rate": 6.63376110562685e-05,
      "loss": 0.1731,
      "step": 236700
    },
    {
      "epoch": 0.7792036854228365,
      "grad_norm": 0.0005328368279151618,
      "learning_rate": 6.623889437314905e-05,
      "loss": 0.3511,
      "step": 236800
    },
    {
      "epoch": 0.7795327410332347,
      "grad_norm": 0.02599584124982357,
      "learning_rate": 6.614017769002961e-05,
      "loss": 0.2066,
      "step": 236900
    },
    {
      "epoch": 0.7798617966436328,
      "grad_norm": 0.009206579066812992,
      "learning_rate": 6.604146100691016e-05,
      "loss": 0.106,
      "step": 237000
    },
    {
      "epoch": 0.780190852254031,
      "grad_norm": 0.000281380518572405,
      "learning_rate": 6.594274432379071e-05,
      "loss": 0.1225,
      "step": 237100
    },
    {
      "epoch": 0.780519907864429,
      "grad_norm": 18.531755447387695,
      "learning_rate": 6.584402764067126e-05,
      "loss": 0.151,
      "step": 237200
    },
    {
      "epoch": 0.7808489634748272,
      "grad_norm": 0.0008771148859523237,
      "learning_rate": 6.574531095755182e-05,
      "loss": 0.2512,
      "step": 237300
    },
    {
      "epoch": 0.7811780190852254,
      "grad_norm": 0.00018058008572552353,
      "learning_rate": 6.564659427443237e-05,
      "loss": 0.4668,
      "step": 237400
    },
    {
      "epoch": 0.7815070746956235,
      "grad_norm": 0.0003903491306118667,
      "learning_rate": 6.554787759131292e-05,
      "loss": 0.326,
      "step": 237500
    },
    {
      "epoch": 0.7818361303060217,
      "grad_norm": 0.04624118655920029,
      "learning_rate": 6.544916090819347e-05,
      "loss": 0.3576,
      "step": 237600
    },
    {
      "epoch": 0.7821651859164199,
      "grad_norm": 0.0015050102956593037,
      "learning_rate": 6.535044422507403e-05,
      "loss": 0.2691,
      "step": 237700
    },
    {
      "epoch": 0.782494241526818,
      "grad_norm": 0.0008393216994591057,
      "learning_rate": 6.525172754195458e-05,
      "loss": 0.1316,
      "step": 237800
    },
    {
      "epoch": 0.7828232971372162,
      "grad_norm": 0.00044148406595923007,
      "learning_rate": 6.515301085883514e-05,
      "loss": 0.3173,
      "step": 237900
    },
    {
      "epoch": 0.7831523527476143,
      "grad_norm": 0.6158831715583801,
      "learning_rate": 6.505429417571569e-05,
      "loss": 0.3538,
      "step": 238000
    },
    {
      "epoch": 0.7834814083580125,
      "grad_norm": 0.0007315888651646674,
      "learning_rate": 6.495557749259624e-05,
      "loss": 0.3486,
      "step": 238100
    },
    {
      "epoch": 0.7838104639684107,
      "grad_norm": 0.015192150138318539,
      "learning_rate": 6.485686080947679e-05,
      "loss": 0.2698,
      "step": 238200
    },
    {
      "epoch": 0.7841395195788088,
      "grad_norm": 9.387797035742551e-05,
      "learning_rate": 6.475814412635735e-05,
      "loss": 0.372,
      "step": 238300
    },
    {
      "epoch": 0.784468575189207,
      "grad_norm": 0.002149036154150963,
      "learning_rate": 6.46594274432379e-05,
      "loss": 0.2001,
      "step": 238400
    },
    {
      "epoch": 0.7847976307996052,
      "grad_norm": 0.0009423067676834762,
      "learning_rate": 6.456071076011845e-05,
      "loss": 0.2911,
      "step": 238500
    },
    {
      "epoch": 0.7851266864100033,
      "grad_norm": 10.485790252685547,
      "learning_rate": 6.4461994076999e-05,
      "loss": 0.242,
      "step": 238600
    },
    {
      "epoch": 0.7854557420204015,
      "grad_norm": 0.015949677675962448,
      "learning_rate": 6.436327739387956e-05,
      "loss": 0.2397,
      "step": 238700
    },
    {
      "epoch": 0.7857847976307996,
      "grad_norm": 0.00021664843370672315,
      "learning_rate": 6.426456071076011e-05,
      "loss": 0.3365,
      "step": 238800
    },
    {
      "epoch": 0.7861138532411978,
      "grad_norm": 0.03153200075030327,
      "learning_rate": 6.416584402764067e-05,
      "loss": 0.235,
      "step": 238900
    },
    {
      "epoch": 0.786442908851596,
      "grad_norm": 0.0003950430254917592,
      "learning_rate": 6.406712734452122e-05,
      "loss": 0.2393,
      "step": 239000
    },
    {
      "epoch": 0.786771964461994,
      "grad_norm": 0.0003413564118091017,
      "learning_rate": 6.396841066140177e-05,
      "loss": 0.3181,
      "step": 239100
    },
    {
      "epoch": 0.7871010200723922,
      "grad_norm": 18.871212005615234,
      "learning_rate": 6.386969397828232e-05,
      "loss": 0.2615,
      "step": 239200
    },
    {
      "epoch": 0.7874300756827904,
      "grad_norm": 0.0017693957779556513,
      "learning_rate": 6.377097729516288e-05,
      "loss": 0.1772,
      "step": 239300
    },
    {
      "epoch": 0.7877591312931885,
      "grad_norm": 0.00031623762333765626,
      "learning_rate": 6.367226061204343e-05,
      "loss": 0.1278,
      "step": 239400
    },
    {
      "epoch": 0.7880881869035867,
      "grad_norm": 1.0461199283599854,
      "learning_rate": 6.357354392892398e-05,
      "loss": 0.0862,
      "step": 239500
    },
    {
      "epoch": 0.7884172425139848,
      "grad_norm": 0.0803232342004776,
      "learning_rate": 6.347482724580453e-05,
      "loss": 0.177,
      "step": 239600
    },
    {
      "epoch": 0.788746298124383,
      "grad_norm": 0.0003028765204362571,
      "learning_rate": 6.337611056268509e-05,
      "loss": 0.0684,
      "step": 239700
    },
    {
      "epoch": 0.7890753537347812,
      "grad_norm": 7.483647641493008e-05,
      "learning_rate": 6.327739387956564e-05,
      "loss": 0.2988,
      "step": 239800
    },
    {
      "epoch": 0.7894044093451793,
      "grad_norm": 7.76978486101143e-05,
      "learning_rate": 6.31786771964462e-05,
      "loss": 0.2287,
      "step": 239900
    },
    {
      "epoch": 0.7897334649555775,
      "grad_norm": 0.0006069146911613643,
      "learning_rate": 6.307996051332675e-05,
      "loss": 0.2676,
      "step": 240000
    },
    {
      "epoch": 0.7900625205659757,
      "grad_norm": 0.00023699586745351553,
      "learning_rate": 6.29812438302073e-05,
      "loss": 0.3909,
      "step": 240100
    },
    {
      "epoch": 0.7903915761763738,
      "grad_norm": 2.9844419259461574e-05,
      "learning_rate": 6.288252714708785e-05,
      "loss": 0.0892,
      "step": 240200
    },
    {
      "epoch": 0.790720631786772,
      "grad_norm": 0.01640825904905796,
      "learning_rate": 6.278381046396841e-05,
      "loss": 0.3248,
      "step": 240300
    },
    {
      "epoch": 0.7910496873971701,
      "grad_norm": 75.85504913330078,
      "learning_rate": 6.268509378084896e-05,
      "loss": 0.1408,
      "step": 240400
    },
    {
      "epoch": 0.7913787430075683,
      "grad_norm": 5.8141889894613996e-05,
      "learning_rate": 6.258637709772951e-05,
      "loss": 0.3725,
      "step": 240500
    },
    {
      "epoch": 0.7917077986179665,
      "grad_norm": 7.168183219619095e-05,
      "learning_rate": 6.248766041461006e-05,
      "loss": 0.1983,
      "step": 240600
    },
    {
      "epoch": 0.7920368542283646,
      "grad_norm": 0.0008154110983014107,
      "learning_rate": 6.238894373149062e-05,
      "loss": 0.3862,
      "step": 240700
    },
    {
      "epoch": 0.7923659098387628,
      "grad_norm": 18.16164207458496,
      "learning_rate": 6.229022704837117e-05,
      "loss": 0.1197,
      "step": 240800
    },
    {
      "epoch": 0.792694965449161,
      "grad_norm": 0.0009310776367783546,
      "learning_rate": 6.219151036525173e-05,
      "loss": 0.2693,
      "step": 240900
    },
    {
      "epoch": 0.793024021059559,
      "grad_norm": 5.953551292419434,
      "learning_rate": 6.209279368213227e-05,
      "loss": 0.2135,
      "step": 241000
    },
    {
      "epoch": 0.7933530766699572,
      "grad_norm": 8.626195267424919e-06,
      "learning_rate": 6.199407699901283e-05,
      "loss": 0.2063,
      "step": 241100
    },
    {
      "epoch": 0.7936821322803553,
      "grad_norm": 0.020685134455561638,
      "learning_rate": 6.189536031589338e-05,
      "loss": 0.1745,
      "step": 241200
    },
    {
      "epoch": 0.7940111878907535,
      "grad_norm": 0.011804824694991112,
      "learning_rate": 6.179664363277394e-05,
      "loss": 0.2303,
      "step": 241300
    },
    {
      "epoch": 0.7943402435011517,
      "grad_norm": 0.004198800772428513,
      "learning_rate": 6.169792694965449e-05,
      "loss": 0.1093,
      "step": 241400
    },
    {
      "epoch": 0.7946692991115498,
      "grad_norm": 0.0011559316189959645,
      "learning_rate": 6.159921026653504e-05,
      "loss": 0.1419,
      "step": 241500
    },
    {
      "epoch": 0.794998354721948,
      "grad_norm": 0.060986440628767014,
      "learning_rate": 6.150049358341559e-05,
      "loss": 0.2037,
      "step": 241600
    },
    {
      "epoch": 0.7953274103323462,
      "grad_norm": 0.0017942574340850115,
      "learning_rate": 6.140177690029615e-05,
      "loss": 0.1944,
      "step": 241700
    },
    {
      "epoch": 0.7956564659427443,
      "grad_norm": 0.001544200349599123,
      "learning_rate": 6.13030602171767e-05,
      "loss": 0.2963,
      "step": 241800
    },
    {
      "epoch": 0.7959855215531425,
      "grad_norm": 0.042324550449848175,
      "learning_rate": 6.120434353405725e-05,
      "loss": 0.1741,
      "step": 241900
    },
    {
      "epoch": 0.7963145771635406,
      "grad_norm": 0.0017201404552906752,
      "learning_rate": 6.11056268509378e-05,
      "loss": 0.351,
      "step": 242000
    },
    {
      "epoch": 0.7966436327739388,
      "grad_norm": 0.00030170107493177056,
      "learning_rate": 6.1006910167818356e-05,
      "loss": 0.2375,
      "step": 242100
    },
    {
      "epoch": 0.796972688384337,
      "grad_norm": 0.003259483492001891,
      "learning_rate": 6.0908193484698905e-05,
      "loss": 0.2422,
      "step": 242200
    },
    {
      "epoch": 0.7973017439947351,
      "grad_norm": 23.114151000976562,
      "learning_rate": 6.080947680157946e-05,
      "loss": 0.2284,
      "step": 242300
    },
    {
      "epoch": 0.7976307996051333,
      "grad_norm": 2.377492904663086,
      "learning_rate": 6.071076011846001e-05,
      "loss": 0.2506,
      "step": 242400
    },
    {
      "epoch": 0.7979598552155315,
      "grad_norm": 0.048213910311460495,
      "learning_rate": 6.0612043435340565e-05,
      "loss": 0.2553,
      "step": 242500
    },
    {
      "epoch": 0.7982889108259296,
      "grad_norm": 2.393896102148574e-05,
      "learning_rate": 6.0513326752221114e-05,
      "loss": 0.1802,
      "step": 242600
    },
    {
      "epoch": 0.7986179664363278,
      "grad_norm": 0.00031652217148803174,
      "learning_rate": 6.0414610069101676e-05,
      "loss": 0.2052,
      "step": 242700
    },
    {
      "epoch": 0.7989470220467259,
      "grad_norm": 183.1364288330078,
      "learning_rate": 6.0315893385982225e-05,
      "loss": 0.1667,
      "step": 242800
    },
    {
      "epoch": 0.799276077657124,
      "grad_norm": 0.000398671836592257,
      "learning_rate": 6.021717670286278e-05,
      "loss": 0.2263,
      "step": 242900
    },
    {
      "epoch": 0.7996051332675223,
      "grad_norm": 3.769043178181164e-05,
      "learning_rate": 6.011846001974333e-05,
      "loss": 0.2138,
      "step": 243000
    },
    {
      "epoch": 0.7999341888779203,
      "grad_norm": 0.00301264482550323,
      "learning_rate": 6.0019743336623885e-05,
      "loss": 0.2835,
      "step": 243100
    },
    {
      "epoch": 0.8002632444883185,
      "grad_norm": 0.0004842543858103454,
      "learning_rate": 5.9921026653504434e-05,
      "loss": 0.4579,
      "step": 243200
    },
    {
      "epoch": 0.8005923000987167,
      "grad_norm": 0.00011081960110459477,
      "learning_rate": 5.982230997038499e-05,
      "loss": 0.2265,
      "step": 243300
    },
    {
      "epoch": 0.8009213557091148,
      "grad_norm": 0.002482193987816572,
      "learning_rate": 5.972359328726554e-05,
      "loss": 0.3098,
      "step": 243400
    },
    {
      "epoch": 0.801250411319513,
      "grad_norm": 0.000233651720918715,
      "learning_rate": 5.9624876604146094e-05,
      "loss": 0.187,
      "step": 243500
    },
    {
      "epoch": 0.8015794669299111,
      "grad_norm": 0.0021722200326621532,
      "learning_rate": 5.952615992102664e-05,
      "loss": 0.1961,
      "step": 243600
    },
    {
      "epoch": 0.8019085225403093,
      "grad_norm": 0.0016448748065158725,
      "learning_rate": 5.9427443237907205e-05,
      "loss": 0.2515,
      "step": 243700
    },
    {
      "epoch": 0.8022375781507075,
      "grad_norm": 0.03950586915016174,
      "learning_rate": 5.9328726554787754e-05,
      "loss": 0.1763,
      "step": 243800
    },
    {
      "epoch": 0.8025666337611056,
      "grad_norm": 11.234374046325684,
      "learning_rate": 5.923000987166831e-05,
      "loss": 0.2361,
      "step": 243900
    },
    {
      "epoch": 0.8028956893715038,
      "grad_norm": 0.0015272409655153751,
      "learning_rate": 5.913129318854886e-05,
      "loss": 0.1446,
      "step": 244000
    },
    {
      "epoch": 0.803224744981902,
      "grad_norm": 5.8157827879767865e-05,
      "learning_rate": 5.9032576505429414e-05,
      "loss": 0.3438,
      "step": 244100
    },
    {
      "epoch": 0.8035538005923001,
      "grad_norm": 0.0009579105535522103,
      "learning_rate": 5.893385982230996e-05,
      "loss": 0.1198,
      "step": 244200
    },
    {
      "epoch": 0.8038828562026983,
      "grad_norm": 0.0005821578088216484,
      "learning_rate": 5.883514313919052e-05,
      "loss": 0.2668,
      "step": 244300
    },
    {
      "epoch": 0.8042119118130964,
      "grad_norm": 0.00031678503728471696,
      "learning_rate": 5.873642645607107e-05,
      "loss": 0.1498,
      "step": 244400
    },
    {
      "epoch": 0.8045409674234946,
      "grad_norm": 0.004167679697275162,
      "learning_rate": 5.863770977295162e-05,
      "loss": 0.3628,
      "step": 244500
    },
    {
      "epoch": 0.8048700230338928,
      "grad_norm": 0.004446950275450945,
      "learning_rate": 5.853899308983217e-05,
      "loss": 0.2131,
      "step": 244600
    },
    {
      "epoch": 0.8051990786442909,
      "grad_norm": 0.0006987499655224383,
      "learning_rate": 5.844027640671273e-05,
      "loss": 0.1927,
      "step": 244700
    },
    {
      "epoch": 0.805528134254689,
      "grad_norm": 0.000402606267016381,
      "learning_rate": 5.8341559723593276e-05,
      "loss": 0.1637,
      "step": 244800
    },
    {
      "epoch": 0.8058571898650873,
      "grad_norm": 0.0001473834563512355,
      "learning_rate": 5.824284304047384e-05,
      "loss": 0.1854,
      "step": 244900
    },
    {
      "epoch": 0.8061862454754853,
      "grad_norm": 0.00021665572421625257,
      "learning_rate": 5.814412635735439e-05,
      "loss": 0.4856,
      "step": 245000
    },
    {
      "epoch": 0.8065153010858835,
      "grad_norm": 0.0017913212068378925,
      "learning_rate": 5.804540967423494e-05,
      "loss": 0.1987,
      "step": 245100
    },
    {
      "epoch": 0.8068443566962816,
      "grad_norm": 0.005743727087974548,
      "learning_rate": 5.794669299111549e-05,
      "loss": 0.1966,
      "step": 245200
    },
    {
      "epoch": 0.8071734123066798,
      "grad_norm": 15.3953218460083,
      "learning_rate": 5.784797630799605e-05,
      "loss": 0.3154,
      "step": 245300
    },
    {
      "epoch": 0.807502467917078,
      "grad_norm": 0.0003367768076714128,
      "learning_rate": 5.7749259624876596e-05,
      "loss": 0.3673,
      "step": 245400
    },
    {
      "epoch": 0.8078315235274761,
      "grad_norm": 7.91327329352498e-05,
      "learning_rate": 5.765054294175715e-05,
      "loss": 0.3221,
      "step": 245500
    },
    {
      "epoch": 0.8081605791378743,
      "grad_norm": 0.0030302677769213915,
      "learning_rate": 5.75518262586377e-05,
      "loss": 0.3105,
      "step": 245600
    },
    {
      "epoch": 0.8084896347482725,
      "grad_norm": 0.003735360223799944,
      "learning_rate": 5.7453109575518256e-05,
      "loss": 0.1693,
      "step": 245700
    },
    {
      "epoch": 0.8088186903586706,
      "grad_norm": 0.00014922505943104625,
      "learning_rate": 5.7354392892398804e-05,
      "loss": 0.2016,
      "step": 245800
    },
    {
      "epoch": 0.8091477459690688,
      "grad_norm": 0.00011961658310610801,
      "learning_rate": 5.725567620927937e-05,
      "loss": 0.0822,
      "step": 245900
    },
    {
      "epoch": 0.8094768015794669,
      "grad_norm": 0.5609272718429565,
      "learning_rate": 5.7156959526159916e-05,
      "loss": 0.1756,
      "step": 246000
    },
    {
      "epoch": 0.8098058571898651,
      "grad_norm": 13.96211051940918,
      "learning_rate": 5.705824284304047e-05,
      "loss": 0.2008,
      "step": 246100
    },
    {
      "epoch": 0.8101349128002633,
      "grad_norm": 0.002225229050964117,
      "learning_rate": 5.695952615992102e-05,
      "loss": 0.1992,
      "step": 246200
    },
    {
      "epoch": 0.8104639684106614,
      "grad_norm": 0.00029751777765341103,
      "learning_rate": 5.6860809476801576e-05,
      "loss": 0.2627,
      "step": 246300
    },
    {
      "epoch": 0.8107930240210596,
      "grad_norm": 0.0001747569622239098,
      "learning_rate": 5.6762092793682124e-05,
      "loss": 0.2165,
      "step": 246400
    },
    {
      "epoch": 0.8111220796314578,
      "grad_norm": 0.0005881506367586553,
      "learning_rate": 5.666337611056268e-05,
      "loss": 0.2141,
      "step": 246500
    },
    {
      "epoch": 0.8114511352418559,
      "grad_norm": 0.0003883245517499745,
      "learning_rate": 5.656465942744323e-05,
      "loss": 0.1293,
      "step": 246600
    },
    {
      "epoch": 0.8117801908522541,
      "grad_norm": 0.28722479939460754,
      "learning_rate": 5.6465942744323784e-05,
      "loss": 0.1036,
      "step": 246700
    },
    {
      "epoch": 0.8121092464626521,
      "grad_norm": 0.0026973977219313383,
      "learning_rate": 5.636722606120433e-05,
      "loss": 0.1964,
      "step": 246800
    },
    {
      "epoch": 0.8124383020730503,
      "grad_norm": 59.02855682373047,
      "learning_rate": 5.626850937808489e-05,
      "loss": 0.1811,
      "step": 246900
    },
    {
      "epoch": 0.8127673576834485,
      "grad_norm": 0.0005836420459672809,
      "learning_rate": 5.616979269496544e-05,
      "loss": 0.2085,
      "step": 247000
    },
    {
      "epoch": 0.8130964132938466,
      "grad_norm": 0.0005473478231579065,
      "learning_rate": 5.6071076011846e-05,
      "loss": 0.1717,
      "step": 247100
    },
    {
      "epoch": 0.8134254689042448,
      "grad_norm": 0.0047140466049313545,
      "learning_rate": 5.597235932872655e-05,
      "loss": 0.117,
      "step": 247200
    },
    {
      "epoch": 0.813754524514643,
      "grad_norm": 6.592325371457264e-05,
      "learning_rate": 5.5873642645607104e-05,
      "loss": 0.2142,
      "step": 247300
    },
    {
      "epoch": 0.8140835801250411,
      "grad_norm": 0.0013391232350841165,
      "learning_rate": 5.577492596248765e-05,
      "loss": 0.2224,
      "step": 247400
    },
    {
      "epoch": 0.8144126357354393,
      "grad_norm": 0.0003226911649107933,
      "learning_rate": 5.567620927936821e-05,
      "loss": 0.2838,
      "step": 247500
    },
    {
      "epoch": 0.8147416913458374,
      "grad_norm": 85.3026123046875,
      "learning_rate": 5.557749259624876e-05,
      "loss": 0.1764,
      "step": 247600
    },
    {
      "epoch": 0.8150707469562356,
      "grad_norm": 0.00015736494970042259,
      "learning_rate": 5.547877591312931e-05,
      "loss": 0.2475,
      "step": 247700
    },
    {
      "epoch": 0.8153998025666338,
      "grad_norm": 0.0007104815449565649,
      "learning_rate": 5.538005923000986e-05,
      "loss": 0.2118,
      "step": 247800
    },
    {
      "epoch": 0.8157288581770319,
      "grad_norm": 0.009748345240950584,
      "learning_rate": 5.528134254689042e-05,
      "loss": 0.1237,
      "step": 247900
    },
    {
      "epoch": 0.8160579137874301,
      "grad_norm": 0.00043975358130410314,
      "learning_rate": 5.5182625863770966e-05,
      "loss": 0.2016,
      "step": 248000
    },
    {
      "epoch": 0.8163869693978283,
      "grad_norm": 23.29308319091797,
      "learning_rate": 5.508390918065153e-05,
      "loss": 0.0691,
      "step": 248100
    },
    {
      "epoch": 0.8167160250082264,
      "grad_norm": 0.0005360438954085112,
      "learning_rate": 5.498519249753207e-05,
      "loss": 0.2369,
      "step": 248200
    },
    {
      "epoch": 0.8170450806186246,
      "grad_norm": 0.00022216203797142953,
      "learning_rate": 5.488647581441263e-05,
      "loss": 0.1267,
      "step": 248300
    },
    {
      "epoch": 0.8173741362290227,
      "grad_norm": 0.002179102273657918,
      "learning_rate": 5.478775913129318e-05,
      "loss": 0.2279,
      "step": 248400
    },
    {
      "epoch": 0.8177031918394209,
      "grad_norm": 0.015353565104305744,
      "learning_rate": 5.468904244817374e-05,
      "loss": 0.1617,
      "step": 248500
    },
    {
      "epoch": 0.8180322474498191,
      "grad_norm": 0.0020084292627871037,
      "learning_rate": 5.4590325765054286e-05,
      "loss": 0.1917,
      "step": 248600
    },
    {
      "epoch": 0.8183613030602171,
      "grad_norm": 66.38685607910156,
      "learning_rate": 5.449160908193484e-05,
      "loss": 0.2452,
      "step": 248700
    },
    {
      "epoch": 0.8186903586706153,
      "grad_norm": 62.36863708496094,
      "learning_rate": 5.43928923988154e-05,
      "loss": 0.28,
      "step": 248800
    },
    {
      "epoch": 0.8190194142810134,
      "grad_norm": 0.0008170059300027788,
      "learning_rate": 5.4294175715695946e-05,
      "loss": 0.2012,
      "step": 248900
    },
    {
      "epoch": 0.8193484698914116,
      "grad_norm": 0.0025406095664948225,
      "learning_rate": 5.41954590325765e-05,
      "loss": 0.3339,
      "step": 249000
    },
    {
      "epoch": 0.8196775255018098,
      "grad_norm": 3.616030693054199,
      "learning_rate": 5.409674234945705e-05,
      "loss": 0.1656,
      "step": 249100
    },
    {
      "epoch": 0.8200065811122079,
      "grad_norm": 0.00040382801671512425,
      "learning_rate": 5.399802566633761e-05,
      "loss": 0.1722,
      "step": 249200
    },
    {
      "epoch": 0.8203356367226061,
      "grad_norm": 19.660158157348633,
      "learning_rate": 5.389930898321816e-05,
      "loss": 0.1537,
      "step": 249300
    },
    {
      "epoch": 0.8206646923330043,
      "grad_norm": 238.13429260253906,
      "learning_rate": 5.380059230009872e-05,
      "loss": 0.2552,
      "step": 249400
    },
    {
      "epoch": 0.8209937479434024,
      "grad_norm": 0.02360372431576252,
      "learning_rate": 5.3701875616979266e-05,
      "loss": 0.3907,
      "step": 249500
    },
    {
      "epoch": 0.8213228035538006,
      "grad_norm": 0.00025919286417774856,
      "learning_rate": 5.360315893385982e-05,
      "loss": 0.2192,
      "step": 249600
    },
    {
      "epoch": 0.8216518591641987,
      "grad_norm": 16.578960418701172,
      "learning_rate": 5.350444225074037e-05,
      "loss": 0.3499,
      "step": 249700
    },
    {
      "epoch": 0.8219809147745969,
      "grad_norm": 0.0003134564030915499,
      "learning_rate": 5.3405725567620926e-05,
      "loss": 0.1992,
      "step": 249800
    },
    {
      "epoch": 0.8223099703849951,
      "grad_norm": 0.00537909334525466,
      "learning_rate": 5.3307008884501475e-05,
      "loss": 0.2736,
      "step": 249900
    },
    {
      "epoch": 0.8226390259953932,
      "grad_norm": 0.03380517661571503,
      "learning_rate": 5.320829220138203e-05,
      "loss": 0.2833,
      "step": 250000
    },
    {
      "epoch": 0.8229680816057914,
      "grad_norm": 0.000898047408554703,
      "learning_rate": 5.310957551826258e-05,
      "loss": 0.1601,
      "step": 250100
    },
    {
      "epoch": 0.8232971372161896,
      "grad_norm": 0.00025184021797031164,
      "learning_rate": 5.301085883514314e-05,
      "loss": 0.1824,
      "step": 250200
    },
    {
      "epoch": 0.8236261928265877,
      "grad_norm": 6.787897291360423e-05,
      "learning_rate": 5.2912142152023684e-05,
      "loss": 0.2919,
      "step": 250300
    },
    {
      "epoch": 0.8239552484369859,
      "grad_norm": 0.000199983871425502,
      "learning_rate": 5.2813425468904246e-05,
      "loss": 0.163,
      "step": 250400
    },
    {
      "epoch": 0.824284304047384,
      "grad_norm": 0.0005878218216821551,
      "learning_rate": 5.2714708785784795e-05,
      "loss": 0.1317,
      "step": 250500
    },
    {
      "epoch": 0.8246133596577822,
      "grad_norm": 0.0002742755750659853,
      "learning_rate": 5.261599210266535e-05,
      "loss": 0.2322,
      "step": 250600
    },
    {
      "epoch": 0.8249424152681804,
      "grad_norm": 0.0004940098733641207,
      "learning_rate": 5.25172754195459e-05,
      "loss": 0.2258,
      "step": 250700
    },
    {
      "epoch": 0.8252714708785784,
      "grad_norm": 0.0008622900932095945,
      "learning_rate": 5.2418558736426455e-05,
      "loss": 0.1516,
      "step": 250800
    },
    {
      "epoch": 0.8256005264889766,
      "grad_norm": 0.00042610548553057015,
      "learning_rate": 5.2319842053307004e-05,
      "loss": 0.2294,
      "step": 250900
    },
    {
      "epoch": 0.8259295820993748,
      "grad_norm": 0.0006327597075141966,
      "learning_rate": 5.222112537018756e-05,
      "loss": 0.2098,
      "step": 251000
    },
    {
      "epoch": 0.8262586377097729,
      "grad_norm": 0.0014554858207702637,
      "learning_rate": 5.212240868706811e-05,
      "loss": 0.2394,
      "step": 251100
    },
    {
      "epoch": 0.8265876933201711,
      "grad_norm": 4.319492816925049,
      "learning_rate": 5.2023692003948664e-05,
      "loss": 0.2007,
      "step": 251200
    },
    {
      "epoch": 0.8269167489305692,
      "grad_norm": 0.00028380469302646816,
      "learning_rate": 5.192497532082921e-05,
      "loss": 0.1424,
      "step": 251300
    },
    {
      "epoch": 0.8272458045409674,
      "grad_norm": 0.00048392126336693764,
      "learning_rate": 5.1826258637709775e-05,
      "loss": 0.151,
      "step": 251400
    },
    {
      "epoch": 0.8275748601513656,
      "grad_norm": 0.019561979919672012,
      "learning_rate": 5.1727541954590324e-05,
      "loss": 0.2231,
      "step": 251500
    },
    {
      "epoch": 0.8279039157617637,
      "grad_norm": 20.84638214111328,
      "learning_rate": 5.162882527147088e-05,
      "loss": 0.2014,
      "step": 251600
    },
    {
      "epoch": 0.8282329713721619,
      "grad_norm": 22.982858657836914,
      "learning_rate": 5.153010858835143e-05,
      "loss": 0.2366,
      "step": 251700
    },
    {
      "epoch": 0.8285620269825601,
      "grad_norm": 13.473040580749512,
      "learning_rate": 5.1431391905231984e-05,
      "loss": 0.3551,
      "step": 251800
    },
    {
      "epoch": 0.8288910825929582,
      "grad_norm": 0.0004682674480136484,
      "learning_rate": 5.133267522211253e-05,
      "loss": 0.1915,
      "step": 251900
    },
    {
      "epoch": 0.8292201382033564,
      "grad_norm": 14.875654220581055,
      "learning_rate": 5.123395853899309e-05,
      "loss": 0.2168,
      "step": 252000
    },
    {
      "epoch": 0.8295491938137545,
      "grad_norm": 0.00044698198325932026,
      "learning_rate": 5.113524185587364e-05,
      "loss": 0.2447,
      "step": 252100
    },
    {
      "epoch": 0.8298782494241527,
      "grad_norm": 77.11666107177734,
      "learning_rate": 5.103652517275419e-05,
      "loss": 0.0625,
      "step": 252200
    },
    {
      "epoch": 0.8302073050345509,
      "grad_norm": 0.034535810351371765,
      "learning_rate": 5.093780848963474e-05,
      "loss": 0.2517,
      "step": 252300
    },
    {
      "epoch": 0.830536360644949,
      "grad_norm": 0.03480768948793411,
      "learning_rate": 5.08390918065153e-05,
      "loss": 0.228,
      "step": 252400
    },
    {
      "epoch": 0.8308654162553472,
      "grad_norm": 0.005587759427726269,
      "learning_rate": 5.0740375123395846e-05,
      "loss": 0.2441,
      "step": 252500
    },
    {
      "epoch": 0.8311944718657454,
      "grad_norm": 0.008706031367182732,
      "learning_rate": 5.064165844027641e-05,
      "loss": 0.226,
      "step": 252600
    },
    {
      "epoch": 0.8315235274761434,
      "grad_norm": 0.27906712889671326,
      "learning_rate": 5.054294175715696e-05,
      "loss": 0.3045,
      "step": 252700
    },
    {
      "epoch": 0.8318525830865416,
      "grad_norm": 0.001044476288370788,
      "learning_rate": 5.044422507403751e-05,
      "loss": 0.2669,
      "step": 252800
    },
    {
      "epoch": 0.8321816386969397,
      "grad_norm": 0.0013684553559869528,
      "learning_rate": 5.034550839091806e-05,
      "loss": 0.1364,
      "step": 252900
    },
    {
      "epoch": 0.8325106943073379,
      "grad_norm": 0.005572995636612177,
      "learning_rate": 5.024679170779862e-05,
      "loss": 0.1692,
      "step": 253000
    },
    {
      "epoch": 0.8328397499177361,
      "grad_norm": 0.002820872003212571,
      "learning_rate": 5.0148075024679166e-05,
      "loss": 0.4314,
      "step": 253100
    },
    {
      "epoch": 0.8331688055281342,
      "grad_norm": 11.28756332397461,
      "learning_rate": 5.004935834155972e-05,
      "loss": 0.1764,
      "step": 253200
    },
    {
      "epoch": 0.8334978611385324,
      "grad_norm": 0.21947389841079712,
      "learning_rate": 4.995064165844027e-05,
      "loss": 0.1919,
      "step": 253300
    },
    {
      "epoch": 0.8338269167489306,
      "grad_norm": 0.05150073394179344,
      "learning_rate": 4.9851924975320826e-05,
      "loss": 0.1933,
      "step": 253400
    },
    {
      "epoch": 0.8341559723593287,
      "grad_norm": 0.0002481511328369379,
      "learning_rate": 4.9753208292201374e-05,
      "loss": 0.2322,
      "step": 253500
    },
    {
      "epoch": 0.8344850279697269,
      "grad_norm": 0.022311298176646233,
      "learning_rate": 4.965449160908194e-05,
      "loss": 0.2594,
      "step": 253600
    },
    {
      "epoch": 0.834814083580125,
      "grad_norm": 0.008081126026809216,
      "learning_rate": 4.9555774925962486e-05,
      "loss": 0.2015,
      "step": 253700
    },
    {
      "epoch": 0.8351431391905232,
      "grad_norm": 0.00029808300314471126,
      "learning_rate": 4.945705824284304e-05,
      "loss": 0.3649,
      "step": 253800
    },
    {
      "epoch": 0.8354721948009214,
      "grad_norm": 56.48125076293945,
      "learning_rate": 4.935834155972359e-05,
      "loss": 0.1176,
      "step": 253900
    },
    {
      "epoch": 0.8358012504113195,
      "grad_norm": 0.0044042025692760944,
      "learning_rate": 4.9259624876604146e-05,
      "loss": 0.2364,
      "step": 254000
    },
    {
      "epoch": 0.8361303060217177,
      "grad_norm": 0.012164580635726452,
      "learning_rate": 4.9160908193484694e-05,
      "loss": 0.1322,
      "step": 254100
    },
    {
      "epoch": 0.8364593616321159,
      "grad_norm": 0.0035719256848096848,
      "learning_rate": 4.906219151036525e-05,
      "loss": 0.1481,
      "step": 254200
    },
    {
      "epoch": 0.836788417242514,
      "grad_norm": 0.29976505041122437,
      "learning_rate": 4.89634748272458e-05,
      "loss": 0.1825,
      "step": 254300
    },
    {
      "epoch": 0.8371174728529122,
      "grad_norm": 0.053728312253952026,
      "learning_rate": 4.8864758144126354e-05,
      "loss": 0.3058,
      "step": 254400
    },
    {
      "epoch": 0.8374465284633102,
      "grad_norm": 3.380359703442082e-05,
      "learning_rate": 4.87660414610069e-05,
      "loss": 0.205,
      "step": 254500
    },
    {
      "epoch": 0.8377755840737084,
      "grad_norm": 0.0018889118218794465,
      "learning_rate": 4.866732477788746e-05,
      "loss": 0.2598,
      "step": 254600
    },
    {
      "epoch": 0.8381046396841066,
      "grad_norm": 0.002591915661469102,
      "learning_rate": 4.856860809476801e-05,
      "loss": 0.1804,
      "step": 254700
    },
    {
      "epoch": 0.8384336952945047,
      "grad_norm": 0.002442584838718176,
      "learning_rate": 4.846989141164857e-05,
      "loss": 0.2842,
      "step": 254800
    },
    {
      "epoch": 0.8387627509049029,
      "grad_norm": 0.0001878342591226101,
      "learning_rate": 4.837117472852912e-05,
      "loss": 0.2324,
      "step": 254900
    },
    {
      "epoch": 0.8390918065153011,
      "grad_norm": 0.0007104082033038139,
      "learning_rate": 4.8272458045409674e-05,
      "loss": 0.1256,
      "step": 255000
    },
    {
      "epoch": 0.8394208621256992,
      "grad_norm": 124.34927368164062,
      "learning_rate": 4.817374136229022e-05,
      "loss": 0.231,
      "step": 255100
    },
    {
      "epoch": 0.8397499177360974,
      "grad_norm": 0.00015483814058825374,
      "learning_rate": 4.807502467917078e-05,
      "loss": 0.3022,
      "step": 255200
    },
    {
      "epoch": 0.8400789733464955,
      "grad_norm": 0.0006665632245130837,
      "learning_rate": 4.797630799605133e-05,
      "loss": 0.15,
      "step": 255300
    },
    {
      "epoch": 0.8404080289568937,
      "grad_norm": 0.0012412177165970206,
      "learning_rate": 4.787759131293188e-05,
      "loss": 0.1555,
      "step": 255400
    },
    {
      "epoch": 0.8407370845672919,
      "grad_norm": 0.004949160385876894,
      "learning_rate": 4.777887462981243e-05,
      "loss": 0.1693,
      "step": 255500
    },
    {
      "epoch": 0.84106614017769,
      "grad_norm": 0.004310895688831806,
      "learning_rate": 4.768015794669299e-05,
      "loss": 0.1969,
      "step": 255600
    },
    {
      "epoch": 0.8413951957880882,
      "grad_norm": 0.0005511904018931091,
      "learning_rate": 4.7581441263573536e-05,
      "loss": 0.2228,
      "step": 255700
    },
    {
      "epoch": 0.8417242513984864,
      "grad_norm": 0.4284544885158539,
      "learning_rate": 4.74827245804541e-05,
      "loss": 0.2212,
      "step": 255800
    },
    {
      "epoch": 0.8420533070088845,
      "grad_norm": 0.0004555938648991287,
      "learning_rate": 4.738400789733465e-05,
      "loss": 0.313,
      "step": 255900
    },
    {
      "epoch": 0.8423823626192827,
      "grad_norm": 0.03486068174242973,
      "learning_rate": 4.72852912142152e-05,
      "loss": 0.2194,
      "step": 256000
    },
    {
      "epoch": 0.8427114182296808,
      "grad_norm": 2.7545247077941895,
      "learning_rate": 4.718657453109575e-05,
      "loss": 0.1952,
      "step": 256100
    },
    {
      "epoch": 0.843040473840079,
      "grad_norm": 0.0009266987326554954,
      "learning_rate": 4.708785784797631e-05,
      "loss": 0.4088,
      "step": 256200
    },
    {
      "epoch": 0.8433695294504772,
      "grad_norm": 0.029286231845617294,
      "learning_rate": 4.6989141164856856e-05,
      "loss": 0.1936,
      "step": 256300
    },
    {
      "epoch": 0.8436985850608753,
      "grad_norm": 2.0501644030446187e-05,
      "learning_rate": 4.689042448173741e-05,
      "loss": 0.0916,
      "step": 256400
    },
    {
      "epoch": 0.8440276406712734,
      "grad_norm": 81.62059783935547,
      "learning_rate": 4.679170779861796e-05,
      "loss": 0.3004,
      "step": 256500
    },
    {
      "epoch": 0.8443566962816716,
      "grad_norm": 28.55693244934082,
      "learning_rate": 4.6692991115498516e-05,
      "loss": 0.4014,
      "step": 256600
    },
    {
      "epoch": 0.8446857518920697,
      "grad_norm": 2.2772605419158936,
      "learning_rate": 4.6594274432379065e-05,
      "loss": 0.1775,
      "step": 256700
    },
    {
      "epoch": 0.8450148075024679,
      "grad_norm": 0.0023161601275205612,
      "learning_rate": 4.649555774925962e-05,
      "loss": 0.1827,
      "step": 256800
    },
    {
      "epoch": 0.845343863112866,
      "grad_norm": 0.00048035240615718067,
      "learning_rate": 4.639684106614017e-05,
      "loss": 0.1602,
      "step": 256900
    },
    {
      "epoch": 0.8456729187232642,
      "grad_norm": 0.0020950576290488243,
      "learning_rate": 4.629812438302073e-05,
      "loss": 0.3714,
      "step": 257000
    },
    {
      "epoch": 0.8460019743336624,
      "grad_norm": 0.00038091023452579975,
      "learning_rate": 4.619940769990128e-05,
      "loss": 0.174,
      "step": 257100
    },
    {
      "epoch": 0.8463310299440605,
      "grad_norm": 0.0002611590316519141,
      "learning_rate": 4.6100691016781836e-05,
      "loss": 0.055,
      "step": 257200
    },
    {
      "epoch": 0.8466600855544587,
      "grad_norm": 0.0012399855768308043,
      "learning_rate": 4.6001974333662385e-05,
      "loss": 0.1224,
      "step": 257300
    },
    {
      "epoch": 0.8469891411648569,
      "grad_norm": 0.022472631186246872,
      "learning_rate": 4.590325765054294e-05,
      "loss": 0.1989,
      "step": 257400
    },
    {
      "epoch": 0.847318196775255,
      "grad_norm": 0.001058757770806551,
      "learning_rate": 4.580454096742349e-05,
      "loss": 0.1422,
      "step": 257500
    },
    {
      "epoch": 0.8476472523856532,
      "grad_norm": 4.736507892608643,
      "learning_rate": 4.5705824284304045e-05,
      "loss": 0.1336,
      "step": 257600
    },
    {
      "epoch": 0.8479763079960513,
      "grad_norm": 0.00020864706311840564,
      "learning_rate": 4.5607107601184594e-05,
      "loss": 0.2115,
      "step": 257700
    },
    {
      "epoch": 0.8483053636064495,
      "grad_norm": 0.13625580072402954,
      "learning_rate": 4.550839091806515e-05,
      "loss": 0.2133,
      "step": 257800
    },
    {
      "epoch": 0.8486344192168477,
      "grad_norm": 0.0005161024746485054,
      "learning_rate": 4.54096742349457e-05,
      "loss": 0.1478,
      "step": 257900
    },
    {
      "epoch": 0.8489634748272458,
      "grad_norm": 0.011592451483011246,
      "learning_rate": 4.531095755182626e-05,
      "loss": 0.1436,
      "step": 258000
    },
    {
      "epoch": 0.849292530437644,
      "grad_norm": 0.007788580376654863,
      "learning_rate": 4.521224086870681e-05,
      "loss": 0.1961,
      "step": 258100
    },
    {
      "epoch": 0.8496215860480422,
      "grad_norm": 3.766449935937999e-06,
      "learning_rate": 4.5113524185587365e-05,
      "loss": 0.1663,
      "step": 258200
    },
    {
      "epoch": 0.8499506416584403,
      "grad_norm": 0.00027135349228046834,
      "learning_rate": 4.5014807502467914e-05,
      "loss": 0.2262,
      "step": 258300
    },
    {
      "epoch": 0.8502796972688385,
      "grad_norm": 18.62965965270996,
      "learning_rate": 4.491609081934847e-05,
      "loss": 0.0728,
      "step": 258400
    },
    {
      "epoch": 0.8506087528792365,
      "grad_norm": 0.0007202890119515359,
      "learning_rate": 4.481737413622902e-05,
      "loss": 0.2403,
      "step": 258500
    },
    {
      "epoch": 0.8509378084896347,
      "grad_norm": 0.0010897362371906638,
      "learning_rate": 4.4718657453109574e-05,
      "loss": 0.4324,
      "step": 258600
    },
    {
      "epoch": 0.8512668641000329,
      "grad_norm": 3.818752884399146e-05,
      "learning_rate": 4.461994076999012e-05,
      "loss": 0.282,
      "step": 258700
    },
    {
      "epoch": 0.851595919710431,
      "grad_norm": 0.0001816262083593756,
      "learning_rate": 4.452122408687068e-05,
      "loss": 0.0775,
      "step": 258800
    },
    {
      "epoch": 0.8519249753208292,
      "grad_norm": 100.44439697265625,
      "learning_rate": 4.442250740375123e-05,
      "loss": 0.1894,
      "step": 258900
    },
    {
      "epoch": 0.8522540309312274,
      "grad_norm": 0.0004211626946926117,
      "learning_rate": 4.432379072063178e-05,
      "loss": 0.266,
      "step": 259000
    },
    {
      "epoch": 0.8525830865416255,
      "grad_norm": 0.0014977635582908988,
      "learning_rate": 4.422507403751233e-05,
      "loss": 0.1221,
      "step": 259100
    },
    {
      "epoch": 0.8529121421520237,
      "grad_norm": 0.0004855708102695644,
      "learning_rate": 4.4126357354392894e-05,
      "loss": 0.2079,
      "step": 259200
    },
    {
      "epoch": 0.8532411977624218,
      "grad_norm": 0.00024465739261358976,
      "learning_rate": 4.402764067127344e-05,
      "loss": 0.1418,
      "step": 259300
    },
    {
      "epoch": 0.85357025337282,
      "grad_norm": 0.0015595315489917994,
      "learning_rate": 4.3928923988154e-05,
      "loss": 0.2152,
      "step": 259400
    },
    {
      "epoch": 0.8538993089832182,
      "grad_norm": 43.131248474121094,
      "learning_rate": 4.383020730503455e-05,
      "loss": 0.3848,
      "step": 259500
    },
    {
      "epoch": 0.8542283645936163,
      "grad_norm": 0.0007604787824675441,
      "learning_rate": 4.37314906219151e-05,
      "loss": 0.0634,
      "step": 259600
    },
    {
      "epoch": 0.8545574202040145,
      "grad_norm": 0.002949791494756937,
      "learning_rate": 4.363277393879565e-05,
      "loss": 0.2531,
      "step": 259700
    },
    {
      "epoch": 0.8548864758144127,
      "grad_norm": 0.0017570180352777243,
      "learning_rate": 4.353405725567621e-05,
      "loss": 0.1498,
      "step": 259800
    },
    {
      "epoch": 0.8552155314248108,
      "grad_norm": 0.0076000927947461605,
      "learning_rate": 4.3435340572556756e-05,
      "loss": 0.168,
      "step": 259900
    },
    {
      "epoch": 0.855544587035209,
      "grad_norm": 0.012418489903211594,
      "learning_rate": 4.333662388943731e-05,
      "loss": 0.1325,
      "step": 260000
    },
    {
      "epoch": 0.8558736426456071,
      "grad_norm": 0.03232072666287422,
      "learning_rate": 4.323790720631786e-05,
      "loss": 0.1948,
      "step": 260100
    },
    {
      "epoch": 0.8562026982560053,
      "grad_norm": 0.0004668685724027455,
      "learning_rate": 4.313919052319842e-05,
      "loss": 0.2533,
      "step": 260200
    },
    {
      "epoch": 0.8565317538664035,
      "grad_norm": 0.0018483663443475962,
      "learning_rate": 4.3040473840078964e-05,
      "loss": 0.1584,
      "step": 260300
    },
    {
      "epoch": 0.8568608094768015,
      "grad_norm": 0.0006288345321081579,
      "learning_rate": 4.294175715695953e-05,
      "loss": 0.2565,
      "step": 260400
    },
    {
      "epoch": 0.8571898650871997,
      "grad_norm": 0.0004986167186871171,
      "learning_rate": 4.2843040473840076e-05,
      "loss": 0.2039,
      "step": 260500
    },
    {
      "epoch": 0.8575189206975979,
      "grad_norm": 0.0006285379640758038,
      "learning_rate": 4.274432379072063e-05,
      "loss": 0.1544,
      "step": 260600
    },
    {
      "epoch": 0.857847976307996,
      "grad_norm": 0.0003049962397199124,
      "learning_rate": 4.264560710760118e-05,
      "loss": 0.4477,
      "step": 260700
    },
    {
      "epoch": 0.8581770319183942,
      "grad_norm": 0.0007360426243394613,
      "learning_rate": 4.2546890424481736e-05,
      "loss": 0.2319,
      "step": 260800
    },
    {
      "epoch": 0.8585060875287923,
      "grad_norm": 0.0012054906692355871,
      "learning_rate": 4.2448173741362284e-05,
      "loss": 0.2324,
      "step": 260900
    },
    {
      "epoch": 0.8588351431391905,
      "grad_norm": 0.0075922152027487755,
      "learning_rate": 4.234945705824284e-05,
      "loss": 0.2033,
      "step": 261000
    },
    {
      "epoch": 0.8591641987495887,
      "grad_norm": 0.0005113843362778425,
      "learning_rate": 4.225074037512339e-05,
      "loss": 0.1555,
      "step": 261100
    },
    {
      "epoch": 0.8594932543599868,
      "grad_norm": 0.0015645240200683475,
      "learning_rate": 4.2152023692003944e-05,
      "loss": 0.2104,
      "step": 261200
    },
    {
      "epoch": 0.859822309970385,
      "grad_norm": 0.005098710767924786,
      "learning_rate": 4.205330700888449e-05,
      "loss": 0.2175,
      "step": 261300
    },
    {
      "epoch": 0.8601513655807832,
      "grad_norm": 0.0021541204769164324,
      "learning_rate": 4.1954590325765056e-05,
      "loss": 0.1638,
      "step": 261400
    },
    {
      "epoch": 0.8604804211911813,
      "grad_norm": 0.000640526763163507,
      "learning_rate": 4.1855873642645604e-05,
      "loss": 0.2254,
      "step": 261500
    },
    {
      "epoch": 0.8608094768015795,
      "grad_norm": 0.00030352050089277327,
      "learning_rate": 4.175715695952616e-05,
      "loss": 0.23,
      "step": 261600
    },
    {
      "epoch": 0.8611385324119776,
      "grad_norm": 47.00874710083008,
      "learning_rate": 4.165844027640671e-05,
      "loss": 0.2282,
      "step": 261700
    },
    {
      "epoch": 0.8614675880223758,
      "grad_norm": 0.0002168108767364174,
      "learning_rate": 4.1559723593287264e-05,
      "loss": 0.2256,
      "step": 261800
    },
    {
      "epoch": 0.861796643632774,
      "grad_norm": 0.0029811507556587458,
      "learning_rate": 4.146100691016781e-05,
      "loss": 0.1553,
      "step": 261900
    },
    {
      "epoch": 0.8621256992431721,
      "grad_norm": 0.0012446336913853884,
      "learning_rate": 4.136229022704837e-05,
      "loss": 0.1955,
      "step": 262000
    },
    {
      "epoch": 0.8624547548535703,
      "grad_norm": 0.0746912956237793,
      "learning_rate": 4.126357354392892e-05,
      "loss": 0.1301,
      "step": 262100
    },
    {
      "epoch": 0.8627838104639685,
      "grad_norm": 6.942242907825857e-05,
      "learning_rate": 4.116485686080947e-05,
      "loss": 0.1507,
      "step": 262200
    },
    {
      "epoch": 0.8631128660743665,
      "grad_norm": 0.001238796510733664,
      "learning_rate": 4.106614017769002e-05,
      "loss": 0.1878,
      "step": 262300
    },
    {
      "epoch": 0.8634419216847647,
      "grad_norm": 0.0003049080551136285,
      "learning_rate": 4.0967423494570584e-05,
      "loss": 0.2353,
      "step": 262400
    },
    {
      "epoch": 0.8637709772951628,
      "grad_norm": 0.04399166628718376,
      "learning_rate": 4.0868706811451126e-05,
      "loss": 0.2321,
      "step": 262500
    },
    {
      "epoch": 0.864100032905561,
      "grad_norm": 8.758631706237793,
      "learning_rate": 4.076999012833169e-05,
      "loss": 0.135,
      "step": 262600
    },
    {
      "epoch": 0.8644290885159592,
      "grad_norm": 0.002653702860698104,
      "learning_rate": 4.067127344521224e-05,
      "loss": 0.2067,
      "step": 262700
    },
    {
      "epoch": 0.8647581441263573,
      "grad_norm": 0.004842324648052454,
      "learning_rate": 4.057255676209279e-05,
      "loss": 0.1814,
      "step": 262800
    },
    {
      "epoch": 0.8650871997367555,
      "grad_norm": 2.105827808380127,
      "learning_rate": 4.047384007897334e-05,
      "loss": 0.1936,
      "step": 262900
    },
    {
      "epoch": 0.8654162553471537,
      "grad_norm": 3.7704459828091785e-05,
      "learning_rate": 4.03751233958539e-05,
      "loss": 0.1751,
      "step": 263000
    },
    {
      "epoch": 0.8657453109575518,
      "grad_norm": 0.0005513628711923957,
      "learning_rate": 4.0276406712734446e-05,
      "loss": 0.0925,
      "step": 263100
    },
    {
      "epoch": 0.86607436656795,
      "grad_norm": 0.00026898429496213794,
      "learning_rate": 4.0177690029615e-05,
      "loss": 0.2351,
      "step": 263200
    },
    {
      "epoch": 0.8664034221783481,
      "grad_norm": 6.374844815582037e-05,
      "learning_rate": 4.007897334649555e-05,
      "loss": 0.1992,
      "step": 263300
    },
    {
      "epoch": 0.8667324777887463,
      "grad_norm": 0.001342720352113247,
      "learning_rate": 3.9980256663376106e-05,
      "loss": 0.2522,
      "step": 263400
    },
    {
      "epoch": 0.8670615333991445,
      "grad_norm": 6.451237277360633e-05,
      "learning_rate": 3.9881539980256655e-05,
      "loss": 0.1836,
      "step": 263500
    },
    {
      "epoch": 0.8673905890095426,
      "grad_norm": 9.688141822814941,
      "learning_rate": 3.978282329713722e-05,
      "loss": 0.1007,
      "step": 263600
    },
    {
      "epoch": 0.8677196446199408,
      "grad_norm": 0.0016569813014939427,
      "learning_rate": 3.9684106614017766e-05,
      "loss": 0.1497,
      "step": 263700
    },
    {
      "epoch": 0.868048700230339,
      "grad_norm": 0.0009820745326578617,
      "learning_rate": 3.958538993089832e-05,
      "loss": 0.241,
      "step": 263800
    },
    {
      "epoch": 0.8683777558407371,
      "grad_norm": 0.006855177693068981,
      "learning_rate": 3.948667324777887e-05,
      "loss": 0.275,
      "step": 263900
    },
    {
      "epoch": 0.8687068114511353,
      "grad_norm": 0.0005980082205496728,
      "learning_rate": 3.9387956564659426e-05,
      "loss": 0.3208,
      "step": 264000
    },
    {
      "epoch": 0.8690358670615334,
      "grad_norm": 0.00010411570110591128,
      "learning_rate": 3.9289239881539975e-05,
      "loss": 0.2034,
      "step": 264100
    },
    {
      "epoch": 0.8693649226719316,
      "grad_norm": 0.0003031970118172467,
      "learning_rate": 3.919052319842053e-05,
      "loss": 0.2166,
      "step": 264200
    },
    {
      "epoch": 0.8696939782823297,
      "grad_norm": 0.0002494110958650708,
      "learning_rate": 3.909180651530108e-05,
      "loss": 0.1867,
      "step": 264300
    },
    {
      "epoch": 0.8700230338927278,
      "grad_norm": 0.003938498441129923,
      "learning_rate": 3.8993089832181635e-05,
      "loss": 0.0897,
      "step": 264400
    },
    {
      "epoch": 0.870352089503126,
      "grad_norm": 0.008203506469726562,
      "learning_rate": 3.8894373149062184e-05,
      "loss": 0.1396,
      "step": 264500
    },
    {
      "epoch": 0.8706811451135242,
      "grad_norm": 5.0662605644902214e-05,
      "learning_rate": 3.879565646594274e-05,
      "loss": 0.247,
      "step": 264600
    },
    {
      "epoch": 0.8710102007239223,
      "grad_norm": 0.0009330192697234452,
      "learning_rate": 3.869693978282329e-05,
      "loss": 0.117,
      "step": 264700
    },
    {
      "epoch": 0.8713392563343205,
      "grad_norm": 0.0004120331141166389,
      "learning_rate": 3.859822309970385e-05,
      "loss": 0.0987,
      "step": 264800
    },
    {
      "epoch": 0.8716683119447186,
      "grad_norm": 0.00016999155923258513,
      "learning_rate": 3.84995064165844e-05,
      "loss": 0.2366,
      "step": 264900
    },
    {
      "epoch": 0.8719973675551168,
      "grad_norm": 53.26500701904297,
      "learning_rate": 3.8400789733464955e-05,
      "loss": 0.2078,
      "step": 265000
    },
    {
      "epoch": 0.872326423165515,
      "grad_norm": 0.00037674003397114575,
      "learning_rate": 3.8302073050345504e-05,
      "loss": 0.2799,
      "step": 265100
    },
    {
      "epoch": 0.8726554787759131,
      "grad_norm": 0.0005238375742919743,
      "learning_rate": 3.820335636722606e-05,
      "loss": 0.3443,
      "step": 265200
    },
    {
      "epoch": 0.8729845343863113,
      "grad_norm": 0.0002509176847524941,
      "learning_rate": 3.810463968410661e-05,
      "loss": 0.1911,
      "step": 265300
    },
    {
      "epoch": 0.8733135899967095,
      "grad_norm": 60.02603530883789,
      "learning_rate": 3.8005923000987164e-05,
      "loss": 0.3339,
      "step": 265400
    },
    {
      "epoch": 0.8736426456071076,
      "grad_norm": 0.0009469258366152644,
      "learning_rate": 3.790720631786771e-05,
      "loss": 0.1778,
      "step": 265500
    },
    {
      "epoch": 0.8739717012175058,
      "grad_norm": 0.00012623310612980276,
      "learning_rate": 3.780848963474827e-05,
      "loss": 0.2758,
      "step": 265600
    },
    {
      "epoch": 0.8743007568279039,
      "grad_norm": 0.0008997570257633924,
      "learning_rate": 3.770977295162882e-05,
      "loss": 0.1446,
      "step": 265700
    },
    {
      "epoch": 0.8746298124383021,
      "grad_norm": 0.0016758125275373459,
      "learning_rate": 3.761105626850938e-05,
      "loss": 0.2113,
      "step": 265800
    },
    {
      "epoch": 0.8749588680487003,
      "grad_norm": 0.0002777260378934443,
      "learning_rate": 3.751233958538993e-05,
      "loss": 0.1305,
      "step": 265900
    },
    {
      "epoch": 0.8752879236590984,
      "grad_norm": 0.01467001624405384,
      "learning_rate": 3.7413622902270484e-05,
      "loss": 0.278,
      "step": 266000
    },
    {
      "epoch": 0.8756169792694966,
      "grad_norm": 0.023794950917363167,
      "learning_rate": 3.731490621915103e-05,
      "loss": 0.205,
      "step": 266100
    },
    {
      "epoch": 0.8759460348798948,
      "grad_norm": 0.000629431800916791,
      "learning_rate": 3.721618953603159e-05,
      "loss": 0.2341,
      "step": 266200
    },
    {
      "epoch": 0.8762750904902928,
      "grad_norm": 7.639933755854145e-05,
      "learning_rate": 3.7117472852912144e-05,
      "loss": 0.1434,
      "step": 266300
    },
    {
      "epoch": 0.876604146100691,
      "grad_norm": 2.9111504554748535,
      "learning_rate": 3.701875616979269e-05,
      "loss": 0.1548,
      "step": 266400
    },
    {
      "epoch": 0.8769332017110891,
      "grad_norm": 0.006925142370164394,
      "learning_rate": 3.692003948667325e-05,
      "loss": 0.3197,
      "step": 266500
    },
    {
      "epoch": 0.8772622573214873,
      "grad_norm": 0.0017912385519593954,
      "learning_rate": 3.68213228035538e-05,
      "loss": 0.2383,
      "step": 266600
    },
    {
      "epoch": 0.8775913129318855,
      "grad_norm": 0.00031434642733074725,
      "learning_rate": 3.672260612043435e-05,
      "loss": 0.1165,
      "step": 266700
    },
    {
      "epoch": 0.8779203685422836,
      "grad_norm": 0.00014606714830733836,
      "learning_rate": 3.66238894373149e-05,
      "loss": 0.1092,
      "step": 266800
    },
    {
      "epoch": 0.8782494241526818,
      "grad_norm": 0.06680706143379211,
      "learning_rate": 3.652517275419546e-05,
      "loss": 0.3238,
      "step": 266900
    },
    {
      "epoch": 0.87857847976308,
      "grad_norm": 0.00045410447637550533,
      "learning_rate": 3.642645607107601e-05,
      "loss": 0.2653,
      "step": 267000
    },
    {
      "epoch": 0.8789075353734781,
      "grad_norm": 0.13347378373146057,
      "learning_rate": 3.632773938795656e-05,
      "loss": 0.1528,
      "step": 267100
    },
    {
      "epoch": 0.8792365909838763,
      "grad_norm": 0.0004083015664946288,
      "learning_rate": 3.622902270483712e-05,
      "loss": 0.1442,
      "step": 267200
    },
    {
      "epoch": 0.8795656465942744,
      "grad_norm": 0.000838041421957314,
      "learning_rate": 3.6130306021717666e-05,
      "loss": 0.1441,
      "step": 267300
    },
    {
      "epoch": 0.8798947022046726,
      "grad_norm": 207.52821350097656,
      "learning_rate": 3.603158933859822e-05,
      "loss": 0.2091,
      "step": 267400
    },
    {
      "epoch": 0.8802237578150708,
      "grad_norm": 2.8029026985168457,
      "learning_rate": 3.593287265547878e-05,
      "loss": 0.0664,
      "step": 267500
    },
    {
      "epoch": 0.8805528134254689,
      "grad_norm": 0.02122098207473755,
      "learning_rate": 3.5834155972359326e-05,
      "loss": 0.1621,
      "step": 267600
    },
    {
      "epoch": 0.8808818690358671,
      "grad_norm": 7.757194543955848e-05,
      "learning_rate": 3.573543928923988e-05,
      "loss": 0.1655,
      "step": 267700
    },
    {
      "epoch": 0.8812109246462653,
      "grad_norm": 5.612377643585205,
      "learning_rate": 3.563672260612043e-05,
      "loss": 0.1869,
      "step": 267800
    },
    {
      "epoch": 0.8815399802566634,
      "grad_norm": 0.00010189951717620715,
      "learning_rate": 3.5538005923000986e-05,
      "loss": 0.0831,
      "step": 267900
    },
    {
      "epoch": 0.8818690358670616,
      "grad_norm": 0.00917822029441595,
      "learning_rate": 3.543928923988154e-05,
      "loss": 0.1964,
      "step": 268000
    },
    {
      "epoch": 0.8821980914774596,
      "grad_norm": 0.19311648607254028,
      "learning_rate": 3.534057255676209e-05,
      "loss": 0.3351,
      "step": 268100
    },
    {
      "epoch": 0.8825271470878578,
      "grad_norm": 2.67901923507452e-05,
      "learning_rate": 3.5241855873642646e-05,
      "loss": 0.2506,
      "step": 268200
    },
    {
      "epoch": 0.882856202698256,
      "grad_norm": 2.9190597534179688,
      "learning_rate": 3.5143139190523194e-05,
      "loss": 0.1225,
      "step": 268300
    },
    {
      "epoch": 0.8831852583086541,
      "grad_norm": 0.00013379515439737588,
      "learning_rate": 3.504442250740375e-05,
      "loss": 0.2024,
      "step": 268400
    },
    {
      "epoch": 0.8835143139190523,
      "grad_norm": 0.0014820494689047337,
      "learning_rate": 3.49457058242843e-05,
      "loss": 0.1346,
      "step": 268500
    },
    {
      "epoch": 0.8838433695294505,
      "grad_norm": 0.012855839915573597,
      "learning_rate": 3.4846989141164854e-05,
      "loss": 0.2451,
      "step": 268600
    },
    {
      "epoch": 0.8841724251398486,
      "grad_norm": 0.11060415953397751,
      "learning_rate": 3.474827245804541e-05,
      "loss": 0.1243,
      "step": 268700
    },
    {
      "epoch": 0.8845014807502468,
      "grad_norm": 0.0008688612724654377,
      "learning_rate": 3.464955577492596e-05,
      "loss": 0.2731,
      "step": 268800
    },
    {
      "epoch": 0.8848305363606449,
      "grad_norm": 0.0003452796954661608,
      "learning_rate": 3.4550839091806514e-05,
      "loss": 0.1695,
      "step": 268900
    },
    {
      "epoch": 0.8851595919710431,
      "grad_norm": 0.01247700210660696,
      "learning_rate": 3.445212240868706e-05,
      "loss": 0.2013,
      "step": 269000
    },
    {
      "epoch": 0.8854886475814413,
      "grad_norm": 0.00025906754308380187,
      "learning_rate": 3.435340572556762e-05,
      "loss": 0.0576,
      "step": 269100
    },
    {
      "epoch": 0.8858177031918394,
      "grad_norm": 16.603363037109375,
      "learning_rate": 3.4254689042448174e-05,
      "loss": 0.1729,
      "step": 269200
    },
    {
      "epoch": 0.8861467588022376,
      "grad_norm": 0.0005868406151421368,
      "learning_rate": 3.415597235932872e-05,
      "loss": 0.1921,
      "step": 269300
    },
    {
      "epoch": 0.8864758144126358,
      "grad_norm": 0.00011074829671997577,
      "learning_rate": 3.405725567620928e-05,
      "loss": 0.2951,
      "step": 269400
    },
    {
      "epoch": 0.8868048700230339,
      "grad_norm": 0.010608368553221226,
      "learning_rate": 3.395853899308983e-05,
      "loss": 0.2578,
      "step": 269500
    },
    {
      "epoch": 0.8871339256334321,
      "grad_norm": 51.188255310058594,
      "learning_rate": 3.385982230997038e-05,
      "loss": 0.132,
      "step": 269600
    },
    {
      "epoch": 0.8874629812438302,
      "grad_norm": 0.00016192893963307142,
      "learning_rate": 3.376110562685094e-05,
      "loss": 0.3013,
      "step": 269700
    },
    {
      "epoch": 0.8877920368542284,
      "grad_norm": 0.0017734180437400937,
      "learning_rate": 3.366238894373149e-05,
      "loss": 0.3023,
      "step": 269800
    },
    {
      "epoch": 0.8881210924646266,
      "grad_norm": 0.007325923535972834,
      "learning_rate": 3.356367226061204e-05,
      "loss": 0.2059,
      "step": 269900
    },
    {
      "epoch": 0.8884501480750246,
      "grad_norm": 0.00010989951260853559,
      "learning_rate": 3.346495557749259e-05,
      "loss": 0.0826,
      "step": 270000
    },
    {
      "epoch": 0.8887792036854228,
      "grad_norm": 0.000502050737850368,
      "learning_rate": 3.336623889437315e-05,
      "loss": 0.2166,
      "step": 270100
    },
    {
      "epoch": 0.889108259295821,
      "grad_norm": 0.00021973678667563945,
      "learning_rate": 3.32675222112537e-05,
      "loss": 0.1249,
      "step": 270200
    },
    {
      "epoch": 0.8894373149062191,
      "grad_norm": 0.00047595740761607885,
      "learning_rate": 3.316880552813425e-05,
      "loss": 0.2466,
      "step": 270300
    },
    {
      "epoch": 0.8897663705166173,
      "grad_norm": 0.015384010970592499,
      "learning_rate": 3.307008884501481e-05,
      "loss": 0.211,
      "step": 270400
    },
    {
      "epoch": 0.8900954261270154,
      "grad_norm": 0.0005062967538833618,
      "learning_rate": 3.2971372161895356e-05,
      "loss": 0.2561,
      "step": 270500
    },
    {
      "epoch": 0.8904244817374136,
      "grad_norm": 0.004656163044273853,
      "learning_rate": 3.287265547877591e-05,
      "loss": 0.1691,
      "step": 270600
    },
    {
      "epoch": 0.8907535373478118,
      "grad_norm": 0.0005834305193275213,
      "learning_rate": 3.277393879565646e-05,
      "loss": 0.1931,
      "step": 270700
    },
    {
      "epoch": 0.8910825929582099,
      "grad_norm": 0.0006814266671426594,
      "learning_rate": 3.2675222112537016e-05,
      "loss": 0.2468,
      "step": 270800
    },
    {
      "epoch": 0.8914116485686081,
      "grad_norm": 0.0007664614240638912,
      "learning_rate": 3.257650542941757e-05,
      "loss": 0.1873,
      "step": 270900
    },
    {
      "epoch": 0.8917407041790063,
      "grad_norm": 0.0019467378733679652,
      "learning_rate": 3.247778874629812e-05,
      "loss": 0.1955,
      "step": 271000
    },
    {
      "epoch": 0.8920697597894044,
      "grad_norm": 0.003677495289593935,
      "learning_rate": 3.2379072063178676e-05,
      "loss": 0.3209,
      "step": 271100
    },
    {
      "epoch": 0.8923988153998026,
      "grad_norm": 0.0005454121856018901,
      "learning_rate": 3.2280355380059225e-05,
      "loss": 0.0725,
      "step": 271200
    },
    {
      "epoch": 0.8927278710102007,
      "grad_norm": 0.00010556483175605536,
      "learning_rate": 3.218163869693978e-05,
      "loss": 0.1218,
      "step": 271300
    },
    {
      "epoch": 0.8930569266205989,
      "grad_norm": 0.10111114382743835,
      "learning_rate": 3.2082922013820336e-05,
      "loss": 0.1651,
      "step": 271400
    },
    {
      "epoch": 0.8933859822309971,
      "grad_norm": 19.95752716064453,
      "learning_rate": 3.1984205330700885e-05,
      "loss": 0.2298,
      "step": 271500
    },
    {
      "epoch": 0.8937150378413952,
      "grad_norm": 0.8332714438438416,
      "learning_rate": 3.188548864758144e-05,
      "loss": 0.202,
      "step": 271600
    },
    {
      "epoch": 0.8940440934517934,
      "grad_norm": 0.009726912714540958,
      "learning_rate": 3.178677196446199e-05,
      "loss": 0.1364,
      "step": 271700
    },
    {
      "epoch": 0.8943731490621916,
      "grad_norm": 0.0007191846380010247,
      "learning_rate": 3.1688055281342545e-05,
      "loss": 0.3619,
      "step": 271800
    },
    {
      "epoch": 0.8947022046725897,
      "grad_norm": 0.002929533598944545,
      "learning_rate": 3.15893385982231e-05,
      "loss": 0.1936,
      "step": 271900
    },
    {
      "epoch": 0.8950312602829879,
      "grad_norm": 0.01125949714332819,
      "learning_rate": 3.149062191510365e-05,
      "loss": 0.0911,
      "step": 272000
    },
    {
      "epoch": 0.8953603158933859,
      "grad_norm": 0.000105202583654318,
      "learning_rate": 3.1391905231984205e-05,
      "loss": 0.3226,
      "step": 272100
    },
    {
      "epoch": 0.8956893715037841,
      "grad_norm": 0.002623044652864337,
      "learning_rate": 3.1293188548864754e-05,
      "loss": 0.1491,
      "step": 272200
    },
    {
      "epoch": 0.8960184271141823,
      "grad_norm": 0.30339813232421875,
      "learning_rate": 3.119447186574531e-05,
      "loss": 0.2432,
      "step": 272300
    },
    {
      "epoch": 0.8963474827245804,
      "grad_norm": 0.00604929169639945,
      "learning_rate": 3.1095755182625865e-05,
      "loss": 0.2154,
      "step": 272400
    },
    {
      "epoch": 0.8966765383349786,
      "grad_norm": 0.024851588532328606,
      "learning_rate": 3.0997038499506414e-05,
      "loss": 0.1999,
      "step": 272500
    },
    {
      "epoch": 0.8970055939453768,
      "grad_norm": 12.771160125732422,
      "learning_rate": 3.089832181638697e-05,
      "loss": 0.1387,
      "step": 272600
    },
    {
      "epoch": 0.8973346495557749,
      "grad_norm": 0.0005233376869000494,
      "learning_rate": 3.079960513326752e-05,
      "loss": 0.0775,
      "step": 272700
    },
    {
      "epoch": 0.8976637051661731,
      "grad_norm": 0.00042524473974481225,
      "learning_rate": 3.0700888450148074e-05,
      "loss": 0.2653,
      "step": 272800
    },
    {
      "epoch": 0.8979927607765712,
      "grad_norm": 0.0001845088554546237,
      "learning_rate": 3.060217176702862e-05,
      "loss": 0.1063,
      "step": 272900
    },
    {
      "epoch": 0.8983218163869694,
      "grad_norm": 136.85296630859375,
      "learning_rate": 3.0503455083909178e-05,
      "loss": 0.1784,
      "step": 273000
    },
    {
      "epoch": 0.8986508719973676,
      "grad_norm": 0.05752963572740555,
      "learning_rate": 3.040473840078973e-05,
      "loss": 0.2361,
      "step": 273100
    },
    {
      "epoch": 0.8989799276077657,
      "grad_norm": 0.0027498933486640453,
      "learning_rate": 3.0306021717670283e-05,
      "loss": 0.1644,
      "step": 273200
    },
    {
      "epoch": 0.8993089832181639,
      "grad_norm": 0.0006456241826526821,
      "learning_rate": 3.0207305034550838e-05,
      "loss": 0.1165,
      "step": 273300
    },
    {
      "epoch": 0.8996380388285621,
      "grad_norm": 54.02753448486328,
      "learning_rate": 3.010858835143139e-05,
      "loss": 0.4066,
      "step": 273400
    },
    {
      "epoch": 0.8999670944389602,
      "grad_norm": 0.0039136046543717384,
      "learning_rate": 3.0009871668311942e-05,
      "loss": 0.214,
      "step": 273500
    },
    {
      "epoch": 0.9002961500493584,
      "grad_norm": 0.0008554154774174094,
      "learning_rate": 2.9911154985192495e-05,
      "loss": 0.0503,
      "step": 273600
    },
    {
      "epoch": 0.9006252056597565,
      "grad_norm": 0.20424415171146393,
      "learning_rate": 2.9812438302073047e-05,
      "loss": 0.1255,
      "step": 273700
    },
    {
      "epoch": 0.9009542612701547,
      "grad_norm": 0.000554403814021498,
      "learning_rate": 2.9713721618953602e-05,
      "loss": 0.068,
      "step": 273800
    },
    {
      "epoch": 0.9012833168805529,
      "grad_norm": 0.0009005325264297426,
      "learning_rate": 2.9615004935834155e-05,
      "loss": 0.268,
      "step": 273900
    },
    {
      "epoch": 0.9016123724909509,
      "grad_norm": 0.0008827596320770681,
      "learning_rate": 2.9516288252714707e-05,
      "loss": 0.3067,
      "step": 274000
    },
    {
      "epoch": 0.9019414281013491,
      "grad_norm": 0.0032544557470828295,
      "learning_rate": 2.941757156959526e-05,
      "loss": 0.1265,
      "step": 274100
    },
    {
      "epoch": 0.9022704837117473,
      "grad_norm": 0.05368110537528992,
      "learning_rate": 2.931885488647581e-05,
      "loss": 0.1711,
      "step": 274200
    },
    {
      "epoch": 0.9025995393221454,
      "grad_norm": 0.002409324049949646,
      "learning_rate": 2.9220138203356363e-05,
      "loss": 0.1851,
      "step": 274300
    },
    {
      "epoch": 0.9029285949325436,
      "grad_norm": 0.00242399494163692,
      "learning_rate": 2.912142152023692e-05,
      "loss": 0.2174,
      "step": 274400
    },
    {
      "epoch": 0.9032576505429417,
      "grad_norm": 0.00015686231199651957,
      "learning_rate": 2.902270483711747e-05,
      "loss": 0.3453,
      "step": 274500
    },
    {
      "epoch": 0.9035867061533399,
      "grad_norm": 0.0007058516493998468,
      "learning_rate": 2.8923988153998023e-05,
      "loss": 0.2809,
      "step": 274600
    },
    {
      "epoch": 0.9039157617637381,
      "grad_norm": 0.0008780384669080377,
      "learning_rate": 2.8825271470878576e-05,
      "loss": 0.2104,
      "step": 274700
    },
    {
      "epoch": 0.9042448173741362,
      "grad_norm": 0.0009118653833866119,
      "learning_rate": 2.8726554787759128e-05,
      "loss": 0.2834,
      "step": 274800
    },
    {
      "epoch": 0.9045738729845344,
      "grad_norm": 0.00028123194351792336,
      "learning_rate": 2.8627838104639683e-05,
      "loss": 0.1872,
      "step": 274900
    },
    {
      "epoch": 0.9049029285949326,
      "grad_norm": 0.02413293719291687,
      "learning_rate": 2.8529121421520236e-05,
      "loss": 0.1667,
      "step": 275000
    },
    {
      "epoch": 0.9052319842053307,
      "grad_norm": 1.1863367557525635,
      "learning_rate": 2.8430404738400788e-05,
      "loss": 0.2939,
      "step": 275100
    },
    {
      "epoch": 0.9055610398157289,
      "grad_norm": 0.1479593962430954,
      "learning_rate": 2.833168805528134e-05,
      "loss": 0.1664,
      "step": 275200
    },
    {
      "epoch": 0.905890095426127,
      "grad_norm": 15.811450958251953,
      "learning_rate": 2.8232971372161892e-05,
      "loss": 0.3149,
      "step": 275300
    },
    {
      "epoch": 0.9062191510365252,
      "grad_norm": 0.001215584110468626,
      "learning_rate": 2.8134254689042444e-05,
      "loss": 0.1786,
      "step": 275400
    },
    {
      "epoch": 0.9065482066469234,
      "grad_norm": 4.630462353816256e-05,
      "learning_rate": 2.8035538005923e-05,
      "loss": 0.3093,
      "step": 275500
    },
    {
      "epoch": 0.9068772622573215,
      "grad_norm": 0.5985162854194641,
      "learning_rate": 2.7936821322803552e-05,
      "loss": 0.3413,
      "step": 275600
    },
    {
      "epoch": 0.9072063178677197,
      "grad_norm": 0.004070799797773361,
      "learning_rate": 2.7838104639684104e-05,
      "loss": 0.04,
      "step": 275700
    },
    {
      "epoch": 0.9075353734781179,
      "grad_norm": 0.10164891183376312,
      "learning_rate": 2.7739387956564657e-05,
      "loss": 0.1109,
      "step": 275800
    },
    {
      "epoch": 0.907864429088516,
      "grad_norm": 0.0012635329039767385,
      "learning_rate": 2.764067127344521e-05,
      "loss": 0.1039,
      "step": 275900
    },
    {
      "epoch": 0.9081934846989141,
      "grad_norm": 0.002445991849526763,
      "learning_rate": 2.7541954590325764e-05,
      "loss": 0.1291,
      "step": 276000
    },
    {
      "epoch": 0.9085225403093122,
      "grad_norm": 16.470008850097656,
      "learning_rate": 2.7443237907206317e-05,
      "loss": 0.2208,
      "step": 276100
    },
    {
      "epoch": 0.9088515959197104,
      "grad_norm": 0.0019420678727328777,
      "learning_rate": 2.734452122408687e-05,
      "loss": 0.192,
      "step": 276200
    },
    {
      "epoch": 0.9091806515301086,
      "grad_norm": 0.0001690325589152053,
      "learning_rate": 2.724580454096742e-05,
      "loss": 0.275,
      "step": 276300
    },
    {
      "epoch": 0.9095097071405067,
      "grad_norm": 0.0002180530718760565,
      "learning_rate": 2.7147087857847973e-05,
      "loss": 0.1421,
      "step": 276400
    },
    {
      "epoch": 0.9098387627509049,
      "grad_norm": 0.005286374595016241,
      "learning_rate": 2.7048371174728525e-05,
      "loss": 0.2508,
      "step": 276500
    },
    {
      "epoch": 0.910167818361303,
      "grad_norm": 10.334622383117676,
      "learning_rate": 2.694965449160908e-05,
      "loss": 0.1664,
      "step": 276600
    },
    {
      "epoch": 0.9104968739717012,
      "grad_norm": 0.0006203202647157013,
      "learning_rate": 2.6850937808489633e-05,
      "loss": 0.0917,
      "step": 276700
    },
    {
      "epoch": 0.9108259295820994,
      "grad_norm": 0.00043843817547895014,
      "learning_rate": 2.6752221125370185e-05,
      "loss": 0.2017,
      "step": 276800
    },
    {
      "epoch": 0.9111549851924975,
      "grad_norm": 0.00091795704793185,
      "learning_rate": 2.6653504442250738e-05,
      "loss": 0.2807,
      "step": 276900
    },
    {
      "epoch": 0.9114840408028957,
      "grad_norm": 0.035589445382356644,
      "learning_rate": 2.655478775913129e-05,
      "loss": 0.1395,
      "step": 277000
    },
    {
      "epoch": 0.9118130964132939,
      "grad_norm": 0.00044133991468697786,
      "learning_rate": 2.6456071076011842e-05,
      "loss": 0.278,
      "step": 277100
    },
    {
      "epoch": 0.912142152023692,
      "grad_norm": 0.001657757326029241,
      "learning_rate": 2.6357354392892397e-05,
      "loss": 0.0961,
      "step": 277200
    },
    {
      "epoch": 0.9124712076340902,
      "grad_norm": 0.0034252640325576067,
      "learning_rate": 2.625863770977295e-05,
      "loss": 0.1542,
      "step": 277300
    },
    {
      "epoch": 0.9128002632444883,
      "grad_norm": 6.734838098054752e-05,
      "learning_rate": 2.6159921026653502e-05,
      "loss": 0.2045,
      "step": 277400
    },
    {
      "epoch": 0.9131293188548865,
      "grad_norm": 0.0003814492665696889,
      "learning_rate": 2.6061204343534054e-05,
      "loss": 0.3094,
      "step": 277500
    },
    {
      "epoch": 0.9134583744652847,
      "grad_norm": 0.00019575720943976194,
      "learning_rate": 2.5962487660414606e-05,
      "loss": 0.23,
      "step": 277600
    },
    {
      "epoch": 0.9137874300756827,
      "grad_norm": 58.37448501586914,
      "learning_rate": 2.5863770977295162e-05,
      "loss": 0.1089,
      "step": 277700
    },
    {
      "epoch": 0.914116485686081,
      "grad_norm": 2.216803841292858e-05,
      "learning_rate": 2.5765054294175714e-05,
      "loss": 0.1849,
      "step": 277800
    },
    {
      "epoch": 0.9144455412964791,
      "grad_norm": 0.0013700202107429504,
      "learning_rate": 2.5666337611056266e-05,
      "loss": 0.1748,
      "step": 277900
    },
    {
      "epoch": 0.9147745969068772,
      "grad_norm": 0.004559679888188839,
      "learning_rate": 2.556762092793682e-05,
      "loss": 0.1501,
      "step": 278000
    },
    {
      "epoch": 0.9151036525172754,
      "grad_norm": 51.36652755737305,
      "learning_rate": 2.546890424481737e-05,
      "loss": 0.2073,
      "step": 278100
    },
    {
      "epoch": 0.9154327081276735,
      "grad_norm": 0.018953518941998482,
      "learning_rate": 2.5370187561697923e-05,
      "loss": 0.0999,
      "step": 278200
    },
    {
      "epoch": 0.9157617637380717,
      "grad_norm": 0.0039056662935763597,
      "learning_rate": 2.527147087857848e-05,
      "loss": 0.3745,
      "step": 278300
    },
    {
      "epoch": 0.9160908193484699,
      "grad_norm": 0.01927627995610237,
      "learning_rate": 2.517275419545903e-05,
      "loss": 0.077,
      "step": 278400
    },
    {
      "epoch": 0.916419874958868,
      "grad_norm": 0.00012631133722607046,
      "learning_rate": 2.5074037512339583e-05,
      "loss": 0.1769,
      "step": 278500
    },
    {
      "epoch": 0.9167489305692662,
      "grad_norm": 0.0020013507455587387,
      "learning_rate": 2.4975320829220135e-05,
      "loss": 0.2077,
      "step": 278600
    },
    {
      "epoch": 0.9170779861796644,
      "grad_norm": 0.07651907950639725,
      "learning_rate": 2.4876604146100687e-05,
      "loss": 0.1861,
      "step": 278700
    },
    {
      "epoch": 0.9174070417900625,
      "grad_norm": 39.66830062866211,
      "learning_rate": 2.4777887462981243e-05,
      "loss": 0.209,
      "step": 278800
    },
    {
      "epoch": 0.9177360974004607,
      "grad_norm": 0.0006404445739462972,
      "learning_rate": 2.4679170779861795e-05,
      "loss": 0.1249,
      "step": 278900
    },
    {
      "epoch": 0.9180651530108588,
      "grad_norm": 0.0922323688864708,
      "learning_rate": 2.4580454096742347e-05,
      "loss": 0.0695,
      "step": 279000
    },
    {
      "epoch": 0.918394208621257,
      "grad_norm": 0.00012428183981683105,
      "learning_rate": 2.44817374136229e-05,
      "loss": 0.1802,
      "step": 279100
    },
    {
      "epoch": 0.9187232642316552,
      "grad_norm": 0.0009537927689962089,
      "learning_rate": 2.438302073050345e-05,
      "loss": 0.0773,
      "step": 279200
    },
    {
      "epoch": 0.9190523198420533,
      "grad_norm": 0.0011571997310966253,
      "learning_rate": 2.4284304047384004e-05,
      "loss": 0.0918,
      "step": 279300
    },
    {
      "epoch": 0.9193813754524515,
      "grad_norm": 0.15331986546516418,
      "learning_rate": 2.418558736426456e-05,
      "loss": 0.062,
      "step": 279400
    },
    {
      "epoch": 0.9197104310628497,
      "grad_norm": 0.00038556091021746397,
      "learning_rate": 2.408687068114511e-05,
      "loss": 0.1883,
      "step": 279500
    },
    {
      "epoch": 0.9200394866732478,
      "grad_norm": 0.0010640649124979973,
      "learning_rate": 2.3988153998025664e-05,
      "loss": 0.103,
      "step": 279600
    },
    {
      "epoch": 0.920368542283646,
      "grad_norm": 0.5459033846855164,
      "learning_rate": 2.3889437314906216e-05,
      "loss": 0.1527,
      "step": 279700
    },
    {
      "epoch": 0.920697597894044,
      "grad_norm": 0.008521500043570995,
      "learning_rate": 2.3790720631786768e-05,
      "loss": 0.0933,
      "step": 279800
    },
    {
      "epoch": 0.9210266535044422,
      "grad_norm": 135.52731323242188,
      "learning_rate": 2.3692003948667324e-05,
      "loss": 0.2095,
      "step": 279900
    },
    {
      "epoch": 0.9213557091148404,
      "grad_norm": 0.008099997416138649,
      "learning_rate": 2.3593287265547876e-05,
      "loss": 0.1655,
      "step": 280000
    },
    {
      "epoch": 0.9216847647252385,
      "grad_norm": 89.81695556640625,
      "learning_rate": 2.3494570582428428e-05,
      "loss": 0.3247,
      "step": 280100
    },
    {
      "epoch": 0.9220138203356367,
      "grad_norm": 0.012168731540441513,
      "learning_rate": 2.339585389930898e-05,
      "loss": 0.2535,
      "step": 280200
    },
    {
      "epoch": 0.9223428759460349,
      "grad_norm": 0.009636777453124523,
      "learning_rate": 2.3297137216189533e-05,
      "loss": 0.2671,
      "step": 280300
    },
    {
      "epoch": 0.922671931556433,
      "grad_norm": 0.0013416826259344816,
      "learning_rate": 2.3198420533070085e-05,
      "loss": 0.1511,
      "step": 280400
    },
    {
      "epoch": 0.9230009871668312,
      "grad_norm": 0.0012078015133738518,
      "learning_rate": 2.309970384995064e-05,
      "loss": 0.2354,
      "step": 280500
    },
    {
      "epoch": 0.9233300427772293,
      "grad_norm": 0.0003758828970603645,
      "learning_rate": 2.3000987166831192e-05,
      "loss": 0.2324,
      "step": 280600
    },
    {
      "epoch": 0.9236590983876275,
      "grad_norm": 43.33512878417969,
      "learning_rate": 2.2902270483711745e-05,
      "loss": 0.1534,
      "step": 280700
    },
    {
      "epoch": 0.9239881539980257,
      "grad_norm": 0.00013665568258147687,
      "learning_rate": 2.2803553800592297e-05,
      "loss": 0.2395,
      "step": 280800
    },
    {
      "epoch": 0.9243172096084238,
      "grad_norm": 6.495071284007281e-05,
      "learning_rate": 2.270483711747285e-05,
      "loss": 0.3942,
      "step": 280900
    },
    {
      "epoch": 0.924646265218822,
      "grad_norm": 0.0001409468677593395,
      "learning_rate": 2.2606120434353405e-05,
      "loss": 0.1363,
      "step": 281000
    },
    {
      "epoch": 0.9249753208292202,
      "grad_norm": 0.001206684042699635,
      "learning_rate": 2.2507403751233957e-05,
      "loss": 0.1744,
      "step": 281100
    },
    {
      "epoch": 0.9253043764396183,
      "grad_norm": 6.700512312818319e-05,
      "learning_rate": 2.240868706811451e-05,
      "loss": 0.1596,
      "step": 281200
    },
    {
      "epoch": 0.9256334320500165,
      "grad_norm": 7.59874310460873e-05,
      "learning_rate": 2.230997038499506e-05,
      "loss": 0.1179,
      "step": 281300
    },
    {
      "epoch": 0.9259624876604146,
      "grad_norm": 0.0001233894727192819,
      "learning_rate": 2.2211253701875613e-05,
      "loss": 0.1756,
      "step": 281400
    },
    {
      "epoch": 0.9262915432708128,
      "grad_norm": 92.61642456054688,
      "learning_rate": 2.2112537018756166e-05,
      "loss": 0.2562,
      "step": 281500
    },
    {
      "epoch": 0.926620598881211,
      "grad_norm": 0.0038832940626889467,
      "learning_rate": 2.201382033563672e-05,
      "loss": 0.2304,
      "step": 281600
    },
    {
      "epoch": 0.926949654491609,
      "grad_norm": 0.01622100919485092,
      "learning_rate": 2.1915103652517273e-05,
      "loss": 0.1049,
      "step": 281700
    },
    {
      "epoch": 0.9272787101020072,
      "grad_norm": 0.13930562138557434,
      "learning_rate": 2.1816386969397826e-05,
      "loss": 0.2199,
      "step": 281800
    },
    {
      "epoch": 0.9276077657124054,
      "grad_norm": 80.59288024902344,
      "learning_rate": 2.1717670286278378e-05,
      "loss": 0.252,
      "step": 281900
    },
    {
      "epoch": 0.9279368213228035,
      "grad_norm": 0.0016456478042528033,
      "learning_rate": 2.161895360315893e-05,
      "loss": 0.1063,
      "step": 282000
    },
    {
      "epoch": 0.9282658769332017,
      "grad_norm": 0.0003768634924199432,
      "learning_rate": 2.1520236920039482e-05,
      "loss": 0.1835,
      "step": 282100
    },
    {
      "epoch": 0.9285949325435998,
      "grad_norm": 18.06125259399414,
      "learning_rate": 2.1421520236920038e-05,
      "loss": 0.0926,
      "step": 282200
    },
    {
      "epoch": 0.928923988153998,
      "grad_norm": 0.000494874722789973,
      "learning_rate": 2.132280355380059e-05,
      "loss": 0.1979,
      "step": 282300
    },
    {
      "epoch": 0.9292530437643962,
      "grad_norm": 0.0004006863455288112,
      "learning_rate": 2.1224086870681142e-05,
      "loss": 0.1956,
      "step": 282400
    },
    {
      "epoch": 0.9295820993747943,
      "grad_norm": 0.0004355568380560726,
      "learning_rate": 2.1125370187561694e-05,
      "loss": 0.1973,
      "step": 282500
    },
    {
      "epoch": 0.9299111549851925,
      "grad_norm": 0.0014416155172511935,
      "learning_rate": 2.1026653504442247e-05,
      "loss": 0.1261,
      "step": 282600
    },
    {
      "epoch": 0.9302402105955907,
      "grad_norm": 0.18899010121822357,
      "learning_rate": 2.0927936821322802e-05,
      "loss": 0.203,
      "step": 282700
    },
    {
      "epoch": 0.9305692662059888,
      "grad_norm": 0.016778748482465744,
      "learning_rate": 2.0829220138203354e-05,
      "loss": 0.3757,
      "step": 282800
    },
    {
      "epoch": 0.930898321816387,
      "grad_norm": 0.0042072501964867115,
      "learning_rate": 2.0730503455083907e-05,
      "loss": 0.1195,
      "step": 282900
    },
    {
      "epoch": 0.9312273774267851,
      "grad_norm": 5.8930560044245794e-05,
      "learning_rate": 2.063178677196446e-05,
      "loss": 0.1767,
      "step": 283000
    },
    {
      "epoch": 0.9315564330371833,
      "grad_norm": 0.1556445211172104,
      "learning_rate": 2.053307008884501e-05,
      "loss": 0.2834,
      "step": 283100
    },
    {
      "epoch": 0.9318854886475815,
      "grad_norm": 0.003596676979213953,
      "learning_rate": 2.0434353405725563e-05,
      "loss": 0.0652,
      "step": 283200
    },
    {
      "epoch": 0.9322145442579796,
      "grad_norm": 0.004670598544180393,
      "learning_rate": 2.033563672260612e-05,
      "loss": 0.3167,
      "step": 283300
    },
    {
      "epoch": 0.9325435998683778,
      "grad_norm": 20.330219268798828,
      "learning_rate": 2.023692003948667e-05,
      "loss": 0.3739,
      "step": 283400
    },
    {
      "epoch": 0.932872655478776,
      "grad_norm": 0.008979802951216698,
      "learning_rate": 2.0138203356367223e-05,
      "loss": 0.2025,
      "step": 283500
    },
    {
      "epoch": 0.933201711089174,
      "grad_norm": 7.660858682356775e-05,
      "learning_rate": 2.0039486673247775e-05,
      "loss": 0.3144,
      "step": 283600
    },
    {
      "epoch": 0.9335307666995722,
      "grad_norm": 0.6368169188499451,
      "learning_rate": 1.9940769990128328e-05,
      "loss": 0.1796,
      "step": 283700
    },
    {
      "epoch": 0.9338598223099703,
      "grad_norm": 0.002016057027503848,
      "learning_rate": 1.9842053307008883e-05,
      "loss": 0.1958,
      "step": 283800
    },
    {
      "epoch": 0.9341888779203685,
      "grad_norm": 0.0025516117457300425,
      "learning_rate": 1.9743336623889435e-05,
      "loss": 0.1403,
      "step": 283900
    },
    {
      "epoch": 0.9345179335307667,
      "grad_norm": 0.0007996013155207038,
      "learning_rate": 1.9644619940769988e-05,
      "loss": 0.2674,
      "step": 284000
    },
    {
      "epoch": 0.9348469891411648,
      "grad_norm": 0.0005205990164540708,
      "learning_rate": 1.954590325765054e-05,
      "loss": 0.208,
      "step": 284100
    },
    {
      "epoch": 0.935176044751563,
      "grad_norm": 0.12781907618045807,
      "learning_rate": 1.9447186574531092e-05,
      "loss": 0.2477,
      "step": 284200
    },
    {
      "epoch": 0.9355051003619612,
      "grad_norm": 0.027897745370864868,
      "learning_rate": 1.9348469891411644e-05,
      "loss": 0.1423,
      "step": 284300
    },
    {
      "epoch": 0.9358341559723593,
      "grad_norm": 0.0001951596641447395,
      "learning_rate": 1.92497532082922e-05,
      "loss": 0.2089,
      "step": 284400
    },
    {
      "epoch": 0.9361632115827575,
      "grad_norm": 6.63640967104584e-05,
      "learning_rate": 1.9151036525172752e-05,
      "loss": 0.1705,
      "step": 284500
    },
    {
      "epoch": 0.9364922671931556,
      "grad_norm": 0.0023539194371551275,
      "learning_rate": 1.9052319842053304e-05,
      "loss": 0.2335,
      "step": 284600
    },
    {
      "epoch": 0.9368213228035538,
      "grad_norm": 0.0007150353631004691,
      "learning_rate": 1.8953603158933856e-05,
      "loss": 0.1883,
      "step": 284700
    },
    {
      "epoch": 0.937150378413952,
      "grad_norm": 0.0029148305766284466,
      "learning_rate": 1.885488647581441e-05,
      "loss": 0.0752,
      "step": 284800
    },
    {
      "epoch": 0.9374794340243501,
      "grad_norm": 0.0002892373886425048,
      "learning_rate": 1.8756169792694964e-05,
      "loss": 0.0414,
      "step": 284900
    },
    {
      "epoch": 0.9378084896347483,
      "grad_norm": 0.0004354491538833827,
      "learning_rate": 1.8657453109575516e-05,
      "loss": 0.197,
      "step": 285000
    },
    {
      "epoch": 0.9381375452451465,
      "grad_norm": 0.0006998105091042817,
      "learning_rate": 1.8558736426456072e-05,
      "loss": 0.0105,
      "step": 285100
    },
    {
      "epoch": 0.9384666008555446,
      "grad_norm": 0.0022434708662331104,
      "learning_rate": 1.8460019743336624e-05,
      "loss": 0.176,
      "step": 285200
    },
    {
      "epoch": 0.9387956564659428,
      "grad_norm": 0.0003323216224089265,
      "learning_rate": 1.8361303060217176e-05,
      "loss": 0.199,
      "step": 285300
    },
    {
      "epoch": 0.9391247120763409,
      "grad_norm": 18.407482147216797,
      "learning_rate": 1.826258637709773e-05,
      "loss": 0.0896,
      "step": 285400
    },
    {
      "epoch": 0.939453767686739,
      "grad_norm": 0.014407183043658733,
      "learning_rate": 1.816386969397828e-05,
      "loss": 0.2813,
      "step": 285500
    },
    {
      "epoch": 0.9397828232971372,
      "grad_norm": 0.0019449276151135564,
      "learning_rate": 1.8065153010858833e-05,
      "loss": 0.0598,
      "step": 285600
    },
    {
      "epoch": 0.9401118789075353,
      "grad_norm": 0.002452010987326503,
      "learning_rate": 1.796643632773939e-05,
      "loss": 0.168,
      "step": 285700
    },
    {
      "epoch": 0.9404409345179335,
      "grad_norm": 0.0005518824909813702,
      "learning_rate": 1.786771964461994e-05,
      "loss": 0.2091,
      "step": 285800
    },
    {
      "epoch": 0.9407699901283317,
      "grad_norm": 0.012958742678165436,
      "learning_rate": 1.7769002961500493e-05,
      "loss": 0.2435,
      "step": 285900
    },
    {
      "epoch": 0.9410990457387298,
      "grad_norm": 0.002182822208851576,
      "learning_rate": 1.7670286278381045e-05,
      "loss": 0.2731,
      "step": 286000
    },
    {
      "epoch": 0.941428101349128,
      "grad_norm": 0.045617975294589996,
      "learning_rate": 1.7571569595261597e-05,
      "loss": 0.1227,
      "step": 286100
    },
    {
      "epoch": 0.9417571569595261,
      "grad_norm": 0.00016844624769873917,
      "learning_rate": 1.747285291214215e-05,
      "loss": 0.3082,
      "step": 286200
    },
    {
      "epoch": 0.9420862125699243,
      "grad_norm": 0.0011903848499059677,
      "learning_rate": 1.7374136229022705e-05,
      "loss": 0.1249,
      "step": 286300
    },
    {
      "epoch": 0.9424152681803225,
      "grad_norm": 35.28777313232422,
      "learning_rate": 1.7275419545903257e-05,
      "loss": 0.1862,
      "step": 286400
    },
    {
      "epoch": 0.9427443237907206,
      "grad_norm": 0.00018368572636973113,
      "learning_rate": 1.717670286278381e-05,
      "loss": 0.2275,
      "step": 286500
    },
    {
      "epoch": 0.9430733794011188,
      "grad_norm": 0.0005883036064915359,
      "learning_rate": 1.707798617966436e-05,
      "loss": 0.2264,
      "step": 286600
    },
    {
      "epoch": 0.943402435011517,
      "grad_norm": 0.1268429011106491,
      "learning_rate": 1.6979269496544914e-05,
      "loss": 0.1272,
      "step": 286700
    },
    {
      "epoch": 0.9437314906219151,
      "grad_norm": 2.0875542759313248e-05,
      "learning_rate": 1.688055281342547e-05,
      "loss": 0.2683,
      "step": 286800
    },
    {
      "epoch": 0.9440605462323133,
      "grad_norm": 0.003405311843380332,
      "learning_rate": 1.678183613030602e-05,
      "loss": 0.1639,
      "step": 286900
    },
    {
      "epoch": 0.9443896018427114,
      "grad_norm": 5.977612090646289e-05,
      "learning_rate": 1.6683119447186574e-05,
      "loss": 0.16,
      "step": 287000
    },
    {
      "epoch": 0.9447186574531096,
      "grad_norm": 0.00018880567222367972,
      "learning_rate": 1.6584402764067126e-05,
      "loss": 0.3238,
      "step": 287100
    },
    {
      "epoch": 0.9450477130635078,
      "grad_norm": 0.004174497909843922,
      "learning_rate": 1.6485686080947678e-05,
      "loss": 0.1294,
      "step": 287200
    },
    {
      "epoch": 0.9453767686739059,
      "grad_norm": 0.0008735088049434125,
      "learning_rate": 1.638696939782823e-05,
      "loss": 0.1837,
      "step": 287300
    },
    {
      "epoch": 0.945705824284304,
      "grad_norm": 0.0007229014299809933,
      "learning_rate": 1.6288252714708786e-05,
      "loss": 0.2141,
      "step": 287400
    },
    {
      "epoch": 0.9460348798947023,
      "grad_norm": 0.00024204423243645579,
      "learning_rate": 1.6189536031589338e-05,
      "loss": 0.1913,
      "step": 287500
    },
    {
      "epoch": 0.9463639355051003,
      "grad_norm": 0.22128824889659882,
      "learning_rate": 1.609081934846989e-05,
      "loss": 0.0829,
      "step": 287600
    },
    {
      "epoch": 0.9466929911154985,
      "grad_norm": 0.0011741453781723976,
      "learning_rate": 1.5992102665350443e-05,
      "loss": 0.1815,
      "step": 287700
    },
    {
      "epoch": 0.9470220467258966,
      "grad_norm": 39.36235809326172,
      "learning_rate": 1.5893385982230995e-05,
      "loss": 0.112,
      "step": 287800
    },
    {
      "epoch": 0.9473511023362948,
      "grad_norm": 0.00025980142527259886,
      "learning_rate": 1.579466929911155e-05,
      "loss": 0.1646,
      "step": 287900
    },
    {
      "epoch": 0.947680157946693,
      "grad_norm": 0.001285797217860818,
      "learning_rate": 1.5695952615992102e-05,
      "loss": 0.1254,
      "step": 288000
    },
    {
      "epoch": 0.9480092135570911,
      "grad_norm": 0.0034250968601554632,
      "learning_rate": 1.5597235932872655e-05,
      "loss": 0.145,
      "step": 288100
    },
    {
      "epoch": 0.9483382691674893,
      "grad_norm": 8.984291343949735e-05,
      "learning_rate": 1.5498519249753207e-05,
      "loss": 0.1318,
      "step": 288200
    },
    {
      "epoch": 0.9486673247778875,
      "grad_norm": 0.0072205387987196445,
      "learning_rate": 1.539980256663376e-05,
      "loss": 0.1232,
      "step": 288300
    },
    {
      "epoch": 0.9489963803882856,
      "grad_norm": 0.004688972141593695,
      "learning_rate": 1.530108588351431e-05,
      "loss": 0.1866,
      "step": 288400
    },
    {
      "epoch": 0.9493254359986838,
      "grad_norm": 0.00018627612735144794,
      "learning_rate": 1.5202369200394865e-05,
      "loss": 0.1248,
      "step": 288500
    },
    {
      "epoch": 0.9496544916090819,
      "grad_norm": 2.2022868506610394e-05,
      "learning_rate": 1.5103652517275419e-05,
      "loss": 0.2203,
      "step": 288600
    },
    {
      "epoch": 0.9499835472194801,
      "grad_norm": 0.0010305204195901752,
      "learning_rate": 1.5004935834155971e-05,
      "loss": 0.2241,
      "step": 288700
    },
    {
      "epoch": 0.9503126028298783,
      "grad_norm": 0.012088838033378124,
      "learning_rate": 1.4906219151036523e-05,
      "loss": 0.0764,
      "step": 288800
    },
    {
      "epoch": 0.9506416584402764,
      "grad_norm": 0.009103255346417427,
      "learning_rate": 1.4807502467917077e-05,
      "loss": 0.2174,
      "step": 288900
    },
    {
      "epoch": 0.9509707140506746,
      "grad_norm": 0.0009939961601048708,
      "learning_rate": 1.470878578479763e-05,
      "loss": 0.1008,
      "step": 289000
    },
    {
      "epoch": 0.9512997696610728,
      "grad_norm": 2.902940511703491,
      "learning_rate": 1.4610069101678182e-05,
      "loss": 0.1971,
      "step": 289100
    },
    {
      "epoch": 0.9516288252714709,
      "grad_norm": 0.010256057605147362,
      "learning_rate": 1.4511352418558736e-05,
      "loss": 0.1326,
      "step": 289200
    },
    {
      "epoch": 0.9519578808818691,
      "grad_norm": 41.83106231689453,
      "learning_rate": 1.4412635735439288e-05,
      "loss": 0.2775,
      "step": 289300
    },
    {
      "epoch": 0.9522869364922671,
      "grad_norm": 71.99544525146484,
      "learning_rate": 1.4313919052319842e-05,
      "loss": 0.2263,
      "step": 289400
    },
    {
      "epoch": 0.9526159921026653,
      "grad_norm": 0.0003753236378543079,
      "learning_rate": 1.4215202369200394e-05,
      "loss": 0.1425,
      "step": 289500
    },
    {
      "epoch": 0.9529450477130635,
      "grad_norm": 0.001270286156795919,
      "learning_rate": 1.4116485686080946e-05,
      "loss": 0.1683,
      "step": 289600
    },
    {
      "epoch": 0.9532741033234616,
      "grad_norm": 0.00271865027025342,
      "learning_rate": 1.40177690029615e-05,
      "loss": 0.2962,
      "step": 289700
    },
    {
      "epoch": 0.9536031589338598,
      "grad_norm": 0.0006105283391661942,
      "learning_rate": 1.3919052319842052e-05,
      "loss": 0.1889,
      "step": 289800
    },
    {
      "epoch": 0.953932214544258,
      "grad_norm": 0.00035234453389421105,
      "learning_rate": 1.3820335636722604e-05,
      "loss": 0.1498,
      "step": 289900
    },
    {
      "epoch": 0.9542612701546561,
      "grad_norm": 0.00031222469988279045,
      "learning_rate": 1.3721618953603158e-05,
      "loss": 0.1123,
      "step": 290000
    },
    {
      "epoch": 0.9545903257650543,
      "grad_norm": 0.019235046580433846,
      "learning_rate": 1.362290227048371e-05,
      "loss": 0.1676,
      "step": 290100
    },
    {
      "epoch": 0.9549193813754524,
      "grad_norm": 0.0002710040134843439,
      "learning_rate": 1.3524185587364263e-05,
      "loss": 0.1855,
      "step": 290200
    },
    {
      "epoch": 0.9552484369858506,
      "grad_norm": 23.853395462036133,
      "learning_rate": 1.3425468904244817e-05,
      "loss": 0.1481,
      "step": 290300
    },
    {
      "epoch": 0.9555774925962488,
      "grad_norm": 0.0006330323521979153,
      "learning_rate": 1.3326752221125369e-05,
      "loss": 0.2214,
      "step": 290400
    },
    {
      "epoch": 0.9559065482066469,
      "grad_norm": 0.0001561115641379729,
      "learning_rate": 1.3228035538005921e-05,
      "loss": 0.2776,
      "step": 290500
    },
    {
      "epoch": 0.9562356038170451,
      "grad_norm": 0.0027084401808679104,
      "learning_rate": 1.3129318854886475e-05,
      "loss": 0.1742,
      "step": 290600
    },
    {
      "epoch": 0.9565646594274433,
      "grad_norm": 4.712726149591617e-05,
      "learning_rate": 1.3030602171767027e-05,
      "loss": 0.1593,
      "step": 290700
    },
    {
      "epoch": 0.9568937150378414,
      "grad_norm": 0.03392520546913147,
      "learning_rate": 1.2931885488647581e-05,
      "loss": 0.1818,
      "step": 290800
    },
    {
      "epoch": 0.9572227706482396,
      "grad_norm": 0.00021633259893860668,
      "learning_rate": 1.2833168805528133e-05,
      "loss": 0.3305,
      "step": 290900
    },
    {
      "epoch": 0.9575518262586377,
      "grad_norm": 0.0006323588895611465,
      "learning_rate": 1.2734452122408685e-05,
      "loss": 0.2384,
      "step": 291000
    },
    {
      "epoch": 0.9578808818690359,
      "grad_norm": 0.0023809815756976604,
      "learning_rate": 1.263573543928924e-05,
      "loss": 0.2628,
      "step": 291100
    },
    {
      "epoch": 0.9582099374794341,
      "grad_norm": 0.1647910624742508,
      "learning_rate": 1.2537018756169791e-05,
      "loss": 0.1317,
      "step": 291200
    },
    {
      "epoch": 0.9585389930898321,
      "grad_norm": 0.10164131969213486,
      "learning_rate": 1.2438302073050344e-05,
      "loss": 0.0975,
      "step": 291300
    },
    {
      "epoch": 0.9588680487002303,
      "grad_norm": 0.0007455648155882955,
      "learning_rate": 1.2339585389930897e-05,
      "loss": 0.1135,
      "step": 291400
    },
    {
      "epoch": 0.9591971043106285,
      "grad_norm": 0.0010571740567684174,
      "learning_rate": 1.224086870681145e-05,
      "loss": 0.2803,
      "step": 291500
    },
    {
      "epoch": 0.9595261599210266,
      "grad_norm": 0.0007171511533670127,
      "learning_rate": 1.2142152023692002e-05,
      "loss": 0.1625,
      "step": 291600
    },
    {
      "epoch": 0.9598552155314248,
      "grad_norm": 2.143697020073887e-05,
      "learning_rate": 1.2043435340572556e-05,
      "loss": 0.2059,
      "step": 291700
    },
    {
      "epoch": 0.9601842711418229,
      "grad_norm": 0.0014360753120854497,
      "learning_rate": 1.1944718657453108e-05,
      "loss": 0.2656,
      "step": 291800
    },
    {
      "epoch": 0.9605133267522211,
      "grad_norm": 0.00030891632195562124,
      "learning_rate": 1.1846001974333662e-05,
      "loss": 0.1933,
      "step": 291900
    },
    {
      "epoch": 0.9608423823626193,
      "grad_norm": 0.013543419539928436,
      "learning_rate": 1.1747285291214214e-05,
      "loss": 0.1016,
      "step": 292000
    },
    {
      "epoch": 0.9611714379730174,
      "grad_norm": 0.002046245848760009,
      "learning_rate": 1.1648568608094766e-05,
      "loss": 0.2917,
      "step": 292100
    },
    {
      "epoch": 0.9615004935834156,
      "grad_norm": 0.0001667813048698008,
      "learning_rate": 1.154985192497532e-05,
      "loss": 0.1382,
      "step": 292200
    },
    {
      "epoch": 0.9618295491938138,
      "grad_norm": 0.007024130783975124,
      "learning_rate": 1.1451135241855872e-05,
      "loss": 0.2246,
      "step": 292300
    },
    {
      "epoch": 0.9621586048042119,
      "grad_norm": 0.008548076264560223,
      "learning_rate": 1.1352418558736425e-05,
      "loss": 0.2051,
      "step": 292400
    },
    {
      "epoch": 0.9624876604146101,
      "grad_norm": 0.00019502225040923804,
      "learning_rate": 1.1253701875616978e-05,
      "loss": 0.1024,
      "step": 292500
    },
    {
      "epoch": 0.9628167160250082,
      "grad_norm": 0.001048530451953411,
      "learning_rate": 1.115498519249753e-05,
      "loss": 0.2333,
      "step": 292600
    },
    {
      "epoch": 0.9631457716354064,
      "grad_norm": 0.0003522176994010806,
      "learning_rate": 1.1056268509378083e-05,
      "loss": 0.1811,
      "step": 292700
    },
    {
      "epoch": 0.9634748272458046,
      "grad_norm": 0.0003661530208773911,
      "learning_rate": 1.0957551826258637e-05,
      "loss": 0.2016,
      "step": 292800
    },
    {
      "epoch": 0.9638038828562027,
      "grad_norm": 1.9517097473144531,
      "learning_rate": 1.0858835143139189e-05,
      "loss": 0.186,
      "step": 292900
    },
    {
      "epoch": 0.9641329384666009,
      "grad_norm": 0.00023643179156351835,
      "learning_rate": 1.0760118460019741e-05,
      "loss": 0.1701,
      "step": 293000
    },
    {
      "epoch": 0.9644619940769991,
      "grad_norm": 11.674858093261719,
      "learning_rate": 1.0661401776900295e-05,
      "loss": 0.0614,
      "step": 293100
    },
    {
      "epoch": 0.9647910496873972,
      "grad_norm": 0.0022262833081185818,
      "learning_rate": 1.0562685093780847e-05,
      "loss": 0.3867,
      "step": 293200
    },
    {
      "epoch": 0.9651201052977953,
      "grad_norm": 0.0003778516547754407,
      "learning_rate": 1.0463968410661401e-05,
      "loss": 0.217,
      "step": 293300
    },
    {
      "epoch": 0.9654491609081934,
      "grad_norm": 0.0006618905463255942,
      "learning_rate": 1.0365251727541953e-05,
      "loss": 0.231,
      "step": 293400
    },
    {
      "epoch": 0.9657782165185916,
      "grad_norm": 0.00023093476193025708,
      "learning_rate": 1.0266535044422505e-05,
      "loss": 0.1344,
      "step": 293500
    },
    {
      "epoch": 0.9661072721289898,
      "grad_norm": 0.0003818117256741971,
      "learning_rate": 1.016781836130306e-05,
      "loss": 0.1341,
      "step": 293600
    },
    {
      "epoch": 0.9664363277393879,
      "grad_norm": 8.373175660381094e-05,
      "learning_rate": 1.0069101678183612e-05,
      "loss": 0.3257,
      "step": 293700
    },
    {
      "epoch": 0.9667653833497861,
      "grad_norm": 0.003149809082970023,
      "learning_rate": 9.970384995064164e-06,
      "loss": 0.2537,
      "step": 293800
    },
    {
      "epoch": 0.9670944389601843,
      "grad_norm": 0.0012974260607734323,
      "learning_rate": 9.871668311944718e-06,
      "loss": 0.2914,
      "step": 293900
    },
    {
      "epoch": 0.9674234945705824,
      "grad_norm": 0.0032319175079464912,
      "learning_rate": 9.77295162882527e-06,
      "loss": 0.2121,
      "step": 294000
    },
    {
      "epoch": 0.9677525501809806,
      "grad_norm": 47.483402252197266,
      "learning_rate": 9.674234945705822e-06,
      "loss": 0.223,
      "step": 294100
    },
    {
      "epoch": 0.9680816057913787,
      "grad_norm": 0.016065111383795738,
      "learning_rate": 9.575518262586376e-06,
      "loss": 0.2424,
      "step": 294200
    },
    {
      "epoch": 0.9684106614017769,
      "grad_norm": 0.0003571454726625234,
      "learning_rate": 9.476801579466928e-06,
      "loss": 0.1646,
      "step": 294300
    },
    {
      "epoch": 0.9687397170121751,
      "grad_norm": 0.00020612470689229667,
      "learning_rate": 9.378084896347482e-06,
      "loss": 0.1481,
      "step": 294400
    },
    {
      "epoch": 0.9690687726225732,
      "grad_norm": 0.0007636119262315333,
      "learning_rate": 9.279368213228036e-06,
      "loss": 0.1293,
      "step": 294500
    },
    {
      "epoch": 0.9693978282329714,
      "grad_norm": 18.32060432434082,
      "learning_rate": 9.180651530108588e-06,
      "loss": 0.1726,
      "step": 294600
    },
    {
      "epoch": 0.9697268838433696,
      "grad_norm": 0.0007547899149358273,
      "learning_rate": 9.08193484698914e-06,
      "loss": 0.1402,
      "step": 294700
    },
    {
      "epoch": 0.9700559394537677,
      "grad_norm": 0.00028598023345693946,
      "learning_rate": 8.983218163869694e-06,
      "loss": 0.116,
      "step": 294800
    },
    {
      "epoch": 0.9703849950641659,
      "grad_norm": 94.08488464355469,
      "learning_rate": 8.884501480750246e-06,
      "loss": 0.1574,
      "step": 294900
    },
    {
      "epoch": 0.970714050674564,
      "grad_norm": 0.0002129026543116197,
      "learning_rate": 8.785784797630799e-06,
      "loss": 0.1731,
      "step": 295000
    },
    {
      "epoch": 0.9710431062849622,
      "grad_norm": 0.0006344802095554769,
      "learning_rate": 8.687068114511352e-06,
      "loss": 0.1249,
      "step": 295100
    },
    {
      "epoch": 0.9713721618953604,
      "grad_norm": 0.02601093053817749,
      "learning_rate": 8.588351431391905e-06,
      "loss": 0.1945,
      "step": 295200
    },
    {
      "epoch": 0.9717012175057584,
      "grad_norm": 0.028759587556123734,
      "learning_rate": 8.489634748272457e-06,
      "loss": 0.2405,
      "step": 295300
    },
    {
      "epoch": 0.9720302731161566,
      "grad_norm": 77.7935791015625,
      "learning_rate": 8.39091806515301e-06,
      "loss": 0.0987,
      "step": 295400
    },
    {
      "epoch": 0.9723593287265548,
      "grad_norm": 0.006906210910528898,
      "learning_rate": 8.292201382033563e-06,
      "loss": 0.3165,
      "step": 295500
    },
    {
      "epoch": 0.9726883843369529,
      "grad_norm": 0.0006948179216124117,
      "learning_rate": 8.193484698914115e-06,
      "loss": 0.1445,
      "step": 295600
    },
    {
      "epoch": 0.9730174399473511,
      "grad_norm": 0.00017329360707663,
      "learning_rate": 8.094768015794669e-06,
      "loss": 0.0914,
      "step": 295700
    },
    {
      "epoch": 0.9733464955577492,
      "grad_norm": 40.51301956176758,
      "learning_rate": 7.996051332675221e-06,
      "loss": 0.1651,
      "step": 295800
    },
    {
      "epoch": 0.9736755511681474,
      "grad_norm": 31.320280075073242,
      "learning_rate": 7.897334649555775e-06,
      "loss": 0.1442,
      "step": 295900
    },
    {
      "epoch": 0.9740046067785456,
      "grad_norm": 8.636164420749992e-05,
      "learning_rate": 7.798617966436327e-06,
      "loss": 0.0836,
      "step": 296000
    },
    {
      "epoch": 0.9743336623889437,
      "grad_norm": 0.0010565954726189375,
      "learning_rate": 7.69990128331688e-06,
      "loss": 0.289,
      "step": 296100
    },
    {
      "epoch": 0.9746627179993419,
      "grad_norm": 0.0005075821536593139,
      "learning_rate": 7.601184600197433e-06,
      "loss": 0.15,
      "step": 296200
    },
    {
      "epoch": 0.9749917736097401,
      "grad_norm": 0.00013651176413986832,
      "learning_rate": 7.502467917077986e-06,
      "loss": 0.1249,
      "step": 296300
    },
    {
      "epoch": 0.9753208292201382,
      "grad_norm": 0.00012187913671368733,
      "learning_rate": 7.403751233958539e-06,
      "loss": 0.1409,
      "step": 296400
    },
    {
      "epoch": 0.9756498848305364,
      "grad_norm": 6.900104926899076e-05,
      "learning_rate": 7.305034550839091e-06,
      "loss": 0.1583,
      "step": 296500
    },
    {
      "epoch": 0.9759789404409345,
      "grad_norm": 0.0003771044430322945,
      "learning_rate": 7.206317867719644e-06,
      "loss": 0.141,
      "step": 296600
    },
    {
      "epoch": 0.9763079960513327,
      "grad_norm": 0.08113213628530502,
      "learning_rate": 7.107601184600197e-06,
      "loss": 0.1876,
      "step": 296700
    },
    {
      "epoch": 0.9766370516617309,
      "grad_norm": 0.22319452464580536,
      "learning_rate": 7.00888450148075e-06,
      "loss": 0.1273,
      "step": 296800
    },
    {
      "epoch": 0.976966107272129,
      "grad_norm": 0.0003266855201218277,
      "learning_rate": 6.910167818361302e-06,
      "loss": 0.1777,
      "step": 296900
    },
    {
      "epoch": 0.9772951628825272,
      "grad_norm": 0.0013545615365728736,
      "learning_rate": 6.811451135241855e-06,
      "loss": 0.1051,
      "step": 297000
    },
    {
      "epoch": 0.9776242184929254,
      "grad_norm": 0.00021004331938456744,
      "learning_rate": 6.712734452122408e-06,
      "loss": 0.1239,
      "step": 297100
    },
    {
      "epoch": 0.9779532741033234,
      "grad_norm": 62.46742248535156,
      "learning_rate": 6.6140177690029605e-06,
      "loss": 0.2536,
      "step": 297200
    },
    {
      "epoch": 0.9782823297137216,
      "grad_norm": 0.0030618051532655954,
      "learning_rate": 6.5153010858835135e-06,
      "loss": 0.1855,
      "step": 297300
    },
    {
      "epoch": 0.9786113853241197,
      "grad_norm": 0.00033108823117800057,
      "learning_rate": 6.4165844027640666e-06,
      "loss": 0.1316,
      "step": 297400
    },
    {
      "epoch": 0.9789404409345179,
      "grad_norm": 3.8400448829634115e-05,
      "learning_rate": 6.31786771964462e-06,
      "loss": 0.1781,
      "step": 297500
    },
    {
      "epoch": 0.9792694965449161,
      "grad_norm": 0.00021320082305464894,
      "learning_rate": 6.219151036525172e-06,
      "loss": 0.0551,
      "step": 297600
    },
    {
      "epoch": 0.9795985521553142,
      "grad_norm": 0.013795053586363792,
      "learning_rate": 6.120434353405725e-06,
      "loss": 0.098,
      "step": 297700
    },
    {
      "epoch": 0.9799276077657124,
      "grad_norm": 0.14564236998558044,
      "learning_rate": 6.021717670286278e-06,
      "loss": 0.2854,
      "step": 297800
    },
    {
      "epoch": 0.9802566633761106,
      "grad_norm": 0.002790438709780574,
      "learning_rate": 5.923000987166831e-06,
      "loss": 0.1431,
      "step": 297900
    },
    {
      "epoch": 0.9805857189865087,
      "grad_norm": 0.0026519866660237312,
      "learning_rate": 5.824284304047383e-06,
      "loss": 0.2281,
      "step": 298000
    },
    {
      "epoch": 0.9809147745969069,
      "grad_norm": 0.0004053238080814481,
      "learning_rate": 5.725567620927936e-06,
      "loss": 0.1399,
      "step": 298100
    },
    {
      "epoch": 0.981243830207305,
      "grad_norm": 0.001485417946241796,
      "learning_rate": 5.626850937808489e-06,
      "loss": 0.2008,
      "step": 298200
    },
    {
      "epoch": 0.9815728858177032,
      "grad_norm": 0.00043734756764024496,
      "learning_rate": 5.528134254689041e-06,
      "loss": 0.1773,
      "step": 298300
    },
    {
      "epoch": 0.9819019414281014,
      "grad_norm": 0.005618420895189047,
      "learning_rate": 5.4294175715695945e-06,
      "loss": 0.1425,
      "step": 298400
    },
    {
      "epoch": 0.9822309970384995,
      "grad_norm": 0.00020055884670000523,
      "learning_rate": 5.3307008884501475e-06,
      "loss": 0.2252,
      "step": 298500
    },
    {
      "epoch": 0.9825600526488977,
      "grad_norm": 0.013669582083821297,
      "learning_rate": 5.2319842053307005e-06,
      "loss": 0.0976,
      "step": 298600
    },
    {
      "epoch": 0.9828891082592959,
      "grad_norm": 31.342992782592773,
      "learning_rate": 5.133267522211253e-06,
      "loss": 0.2,
      "step": 298700
    },
    {
      "epoch": 0.983218163869694,
      "grad_norm": 0.00328629813157022,
      "learning_rate": 5.034550839091806e-06,
      "loss": 0.1391,
      "step": 298800
    },
    {
      "epoch": 0.9835472194800922,
      "grad_norm": 0.0005745290545746684,
      "learning_rate": 4.935834155972359e-06,
      "loss": 0.2011,
      "step": 298900
    },
    {
      "epoch": 0.9838762750904902,
      "grad_norm": 0.00035574991488829255,
      "learning_rate": 4.837117472852911e-06,
      "loss": 0.0747,
      "step": 299000
    },
    {
      "epoch": 0.9842053307008884,
      "grad_norm": 0.0006120084435679018,
      "learning_rate": 4.738400789733464e-06,
      "loss": 0.1102,
      "step": 299100
    },
    {
      "epoch": 0.9845343863112866,
      "grad_norm": 0.0010649517644196749,
      "learning_rate": 4.639684106614018e-06,
      "loss": 0.2272,
      "step": 299200
    },
    {
      "epoch": 0.9848634419216847,
      "grad_norm": 0.00018341575923841447,
      "learning_rate": 4.54096742349457e-06,
      "loss": 0.2488,
      "step": 299300
    },
    {
      "epoch": 0.9851924975320829,
      "grad_norm": 0.0003959618916269392,
      "learning_rate": 4.442250740375123e-06,
      "loss": 0.053,
      "step": 299400
    },
    {
      "epoch": 0.9855215531424811,
      "grad_norm": 0.00046693015610799193,
      "learning_rate": 4.343534057255676e-06,
      "loss": 0.0427,
      "step": 299500
    },
    {
      "epoch": 0.9858506087528792,
      "grad_norm": 0.00018261256627738476,
      "learning_rate": 4.2448173741362284e-06,
      "loss": 0.1101,
      "step": 299600
    },
    {
      "epoch": 0.9861796643632774,
      "grad_norm": 1.5492171049118042,
      "learning_rate": 4.1461006910167815e-06,
      "loss": 0.0316,
      "step": 299700
    },
    {
      "epoch": 0.9865087199736755,
      "grad_norm": 36.538028717041016,
      "learning_rate": 4.0473840078973345e-06,
      "loss": 0.2028,
      "step": 299800
    },
    {
      "epoch": 0.9868377755840737,
      "grad_norm": 0.07349921017885208,
      "learning_rate": 3.9486673247778876e-06,
      "loss": 0.1871,
      "step": 299900
    },
    {
      "epoch": 0.9871668311944719,
      "grad_norm": 0.000164554818184115,
      "learning_rate": 3.84995064165844e-06,
      "loss": 0.1631,
      "step": 300000
    },
    {
      "epoch": 0.98749588680487,
      "grad_norm": 0.0008032923215068877,
      "learning_rate": 3.751233958538993e-06,
      "loss": 0.2133,
      "step": 300100
    },
    {
      "epoch": 0.9878249424152682,
      "grad_norm": 0.007392433006316423,
      "learning_rate": 3.6525172754195454e-06,
      "loss": 0.1479,
      "step": 300200
    },
    {
      "epoch": 0.9881539980256664,
      "grad_norm": 0.004494446329772472,
      "learning_rate": 3.5538005923000985e-06,
      "loss": 0.1701,
      "step": 300300
    },
    {
      "epoch": 0.9884830536360645,
      "grad_norm": 0.5133750438690186,
      "learning_rate": 3.455083909180651e-06,
      "loss": 0.1953,
      "step": 300400
    },
    {
      "epoch": 0.9888121092464627,
      "grad_norm": 0.012296872213482857,
      "learning_rate": 3.356367226061204e-06,
      "loss": 0.1378,
      "step": 300500
    },
    {
      "epoch": 0.9891411648568608,
      "grad_norm": 0.0006347881280817091,
      "learning_rate": 3.2576505429417568e-06,
      "loss": 0.11,
      "step": 300600
    },
    {
      "epoch": 0.989470220467259,
      "grad_norm": 0.00038377096643671393,
      "learning_rate": 3.15893385982231e-06,
      "loss": 0.1203,
      "step": 300700
    },
    {
      "epoch": 0.9897992760776572,
      "grad_norm": 0.00013937753101345152,
      "learning_rate": 3.0602171767028624e-06,
      "loss": 0.185,
      "step": 300800
    },
    {
      "epoch": 0.9901283316880553,
      "grad_norm": 0.02412283606827259,
      "learning_rate": 2.9615004935834155e-06,
      "loss": 0.0899,
      "step": 300900
    },
    {
      "epoch": 0.9904573872984535,
      "grad_norm": 40.6806526184082,
      "learning_rate": 2.862783810463968e-06,
      "loss": 0.0418,
      "step": 301000
    },
    {
      "epoch": 0.9907864429088517,
      "grad_norm": 49.40616989135742,
      "learning_rate": 2.7640671273445207e-06,
      "loss": 0.1576,
      "step": 301100
    },
    {
      "epoch": 0.9911154985192497,
      "grad_norm": 3.032484531402588,
      "learning_rate": 2.6653504442250738e-06,
      "loss": 0.1865,
      "step": 301200
    },
    {
      "epoch": 0.9914445541296479,
      "grad_norm": 0.005930593237280846,
      "learning_rate": 2.5666337611056264e-06,
      "loss": 0.1394,
      "step": 301300
    },
    {
      "epoch": 0.991773609740046,
      "grad_norm": 0.06458639353513718,
      "learning_rate": 2.4679170779861794e-06,
      "loss": 0.1228,
      "step": 301400
    },
    {
      "epoch": 0.9921026653504442,
      "grad_norm": 0.0005223462358117104,
      "learning_rate": 2.369200394866732e-06,
      "loss": 0.3131,
      "step": 301500
    },
    {
      "epoch": 0.9924317209608424,
      "grad_norm": 0.0008752835565246642,
      "learning_rate": 2.270483711747285e-06,
      "loss": 0.2072,
      "step": 301600
    },
    {
      "epoch": 0.9927607765712405,
      "grad_norm": 0.003741741646081209,
      "learning_rate": 2.171767028627838e-06,
      "loss": 0.2343,
      "step": 301700
    },
    {
      "epoch": 0.9930898321816387,
      "grad_norm": 21.85167121887207,
      "learning_rate": 2.0730503455083907e-06,
      "loss": 0.2644,
      "step": 301800
    },
    {
      "epoch": 0.9934188877920369,
      "grad_norm": 0.0002419843222014606,
      "learning_rate": 1.9743336623889438e-06,
      "loss": 0.2199,
      "step": 301900
    },
    {
      "epoch": 0.993747943402435,
      "grad_norm": 0.0005627566133625805,
      "learning_rate": 1.8756169792694964e-06,
      "loss": 0.2722,
      "step": 302000
    },
    {
      "epoch": 0.9940769990128332,
      "grad_norm": 0.0004777779686264694,
      "learning_rate": 1.7769002961500492e-06,
      "loss": 0.1885,
      "step": 302100
    },
    {
      "epoch": 0.9944060546232313,
      "grad_norm": 0.0001981778914341703,
      "learning_rate": 1.678183613030602e-06,
      "loss": 0.2854,
      "step": 302200
    },
    {
      "epoch": 0.9947351102336295,
      "grad_norm": 0.0005049618775956333,
      "learning_rate": 1.579466929911155e-06,
      "loss": 0.3275,
      "step": 302300
    },
    {
      "epoch": 0.9950641658440277,
      "grad_norm": 18.62970542907715,
      "learning_rate": 1.4807502467917077e-06,
      "loss": 0.2361,
      "step": 302400
    },
    {
      "epoch": 0.9953932214544258,
      "grad_norm": 0.00030534336110576987,
      "learning_rate": 1.3820335636722604e-06,
      "loss": 0.1635,
      "step": 302500
    },
    {
      "epoch": 0.995722277064824,
      "grad_norm": 9.883265011012554e-05,
      "learning_rate": 1.2833168805528132e-06,
      "loss": 0.1597,
      "step": 302600
    },
    {
      "epoch": 0.9960513326752222,
      "grad_norm": 0.0008725462830625474,
      "learning_rate": 1.184600197433366e-06,
      "loss": 0.1847,
      "step": 302700
    },
    {
      "epoch": 0.9963803882856203,
      "grad_norm": 0.0005368500133045018,
      "learning_rate": 1.085883514313919e-06,
      "loss": 0.2534,
      "step": 302800
    },
    {
      "epoch": 0.9967094438960185,
      "grad_norm": 0.024645579978823662,
      "learning_rate": 9.871668311944719e-07,
      "loss": 0.2806,
      "step": 302900
    },
    {
      "epoch": 0.9970384995064165,
      "grad_norm": 94.21979522705078,
      "learning_rate": 8.884501480750246e-07,
      "loss": 0.1172,
      "step": 303000
    },
    {
      "epoch": 0.9973675551168147,
      "grad_norm": 0.0015230596764013171,
      "learning_rate": 7.897334649555775e-07,
      "loss": 0.1633,
      "step": 303100
    },
    {
      "epoch": 0.9976966107272129,
      "grad_norm": 0.0004429254913702607,
      "learning_rate": 6.910167818361302e-07,
      "loss": 0.1201,
      "step": 303200
    },
    {
      "epoch": 0.998025666337611,
      "grad_norm": 9.475024126004428e-05,
      "learning_rate": 5.92300098716683e-07,
      "loss": 0.1749,
      "step": 303300
    },
    {
      "epoch": 0.9983547219480092,
      "grad_norm": 0.00020653221872635186,
      "learning_rate": 4.935834155972359e-07,
      "loss": 0.1879,
      "step": 303400
    },
    {
      "epoch": 0.9986837775584074,
      "grad_norm": 0.0001345576165476814,
      "learning_rate": 3.948667324777887e-07,
      "loss": 0.2114,
      "step": 303500
    },
    {
      "epoch": 0.9990128331688055,
      "grad_norm": 0.052171047776937485,
      "learning_rate": 2.961500493583415e-07,
      "loss": 0.1247,
      "step": 303600
    },
    {
      "epoch": 0.9993418887792037,
      "grad_norm": 0.00026974015054292977,
      "learning_rate": 1.9743336623889436e-07,
      "loss": 0.1725,
      "step": 303700
    },
    {
      "epoch": 0.9996709443896018,
      "grad_norm": 0.0002325934183318168,
      "learning_rate": 9.871668311944718e-08,
      "loss": 0.16,
      "step": 303800
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.00034181022783741355,
      "learning_rate": 0.0,
      "loss": 0.2126,
      "step": 303900
    }
  ],
  "logging_steps": 100,
  "max_steps": 303900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2261713939167992e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
